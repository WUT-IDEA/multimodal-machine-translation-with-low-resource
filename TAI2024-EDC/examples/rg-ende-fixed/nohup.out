2022-09-08 00:06:03 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': 'simple', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'results/en-fr/mmt/bl_log1', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/data1/home/turghun/project/acmmt/examples/rg-ende-fixed', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1536, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1536, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 150000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [4], 'lr': [0.0005], 'stop_min_lr': 1e-09, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'results/en-fr/mmt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 12, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='rgfix_model', activation_dropout=0.0, activation_fn='relu', adam_betas='0.9,0.98', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='rgfix_model', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='rgfix_criterion', cross_self_attention=False, curriculum=0, data='/data1/home/turghun/project/acmmt/data_bin/en-fr', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, img_grid_prefix='/data1/home/turghun/project/images/features/resnet101-dlmmt', img_region_prefix='/data1/home/turghun/project/images/features/faster-dlmmt', keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=12, label_smoothing=0.2, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', mask_loss_weight=0.03, max_epoch=0, max_tokens=1536, max_tokens_valid=1536, max_update=150000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='rgfix_adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=True, restore_file='checkpoint_last.pt', save_dir='results/en-fr/mmt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang='en', stop_min_lr=1e-09, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang='fr', task='rgfix_translation_task', tensorboard_logdir='results/en-fr/mmt/bl_log1', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[4], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/data1/home/turghun/project/acmmt/examples/rg-ende-fixed', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'rgfix_translation_task', 'data': '/data1/home/turghun/project/acmmt/data_bin/en-fr', 'source_lang': 'en', 'target_lang': 'fr', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False, 'img_grid_prefix': '/data1/home/turghun/project/images/features/resnet101-dlmmt', 'img_region_prefix': '/data1/home/turghun/project/images/features/faster-dlmmt'}, 'criterion': {'_name': 'rgfix_criterion', 'mask_loss_weight': 0.03, 'label_smoothing': 0.2, 'sentence_avg': False}, 'optimizer': {'_name': 'rgfix_adam', 'adam_betas': '0.9,0.98', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-09-08 00:06:03 | INFO | rg-ende-fixed.tasks.tasks | [en] dictionary: 8512 types
2022-09-08 00:06:03 | INFO | rg-ende-fixed.tasks.tasks | [fr] dictionary: 8752 types
2022-09-08 00:06:04 | INFO | fairseq_cli.train | RGMMTModel(
  (encoder): RGFIXEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(8512, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (region_img_project): Imgprojector(
      (linear): Linear(in_features=2048, out_features=512, bias=True)
    )
    (grid_img_project): Imgprojector(
      (linear): Linear(in_features=2048, out_features=512, bias=True)
    )
    (visual_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=512, out_features=512, bias=True)
      (v_proj): Linear(in_features=512, out_features=512, bias=True)
      (q_proj): Linear(in_features=512, out_features=512, bias=True)
      (out_proj): Linear(in_features=512, out_features=512, bias=True)
    )
    (merge_attn): AttackMultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=512, out_features=512, bias=True)
      (v_proj): Linear(in_features=512, out_features=512, bias=True)
      (q_proj): Linear(in_features=512, out_features=512, bias=True)
      (out_proj): Linear(in_features=512, out_features=512, bias=True)
    )
    (gate): Linear(in_features=1024, out_features=512, bias=True)
    (attacker): Attack(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=512, out_features=512, bias=True)
      (v_proj): Linear(in_features=512, out_features=512, bias=True)
      (q_proj): Linear(in_features=512, out_features=512, bias=True)
      (out_proj): Linear(in_features=512, out_features=512, bias=True)
    )
  )
  (decoder): RGFIXDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(8752, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): RGFIXDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): AttackMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): RGFIXDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): AttackMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): RGFIXDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): AttackMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): RGFIXDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): AttackMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): RGFIXDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): AttackMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): RGFIXDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): AttackMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=8752, bias=False)
    (attacker): ModuleList(
      (0): Attack(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=512, out_features=512, bias=True)
        (v_proj): Linear(in_features=512, out_features=512, bias=True)
        (q_proj): Linear(in_features=512, out_features=512, bias=True)
        (out_proj): Linear(in_features=512, out_features=512, bias=True)
      )
      (1): Attack(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=512, out_features=512, bias=True)
        (v_proj): Linear(in_features=512, out_features=512, bias=True)
        (q_proj): Linear(in_features=512, out_features=512, bias=True)
        (out_proj): Linear(in_features=512, out_features=512, bias=True)
      )
      (2): Attack(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=512, out_features=512, bias=True)
        (v_proj): Linear(in_features=512, out_features=512, bias=True)
        (q_proj): Linear(in_features=512, out_features=512, bias=True)
        (out_proj): Linear(in_features=512, out_features=512, bias=True)
      )
      (3): Attack(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=512, out_features=512, bias=True)
        (v_proj): Linear(in_features=512, out_features=512, bias=True)
        (q_proj): Linear(in_features=512, out_features=512, bias=True)
        (out_proj): Linear(in_features=512, out_features=512, bias=True)
      )
      (4): Attack(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=512, out_features=512, bias=True)
        (v_proj): Linear(in_features=512, out_features=512, bias=True)
        (q_proj): Linear(in_features=512, out_features=512, bias=True)
        (out_proj): Linear(in_features=512, out_features=512, bias=True)
      )
      (5): Attack(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=512, out_features=512, bias=True)
        (v_proj): Linear(in_features=512, out_features=512, bias=True)
        (q_proj): Linear(in_features=512, out_features=512, bias=True)
        (out_proj): Linear(in_features=512, out_features=512, bias=True)
      )
    )
  )
)
2022-09-08 00:06:04 | INFO | fairseq_cli.train | task: RGMMTTask
2022-09-08 00:06:04 | INFO | fairseq_cli.train | model: RGMMTModel
2022-09-08 00:06:04 | INFO | fairseq_cli.train | criterion: MyLabelSmoothedCrossEntropyCriterion
2022-09-08 00:06:04 | INFO | fairseq_cli.train | num. shared model params: 65,056,256 (num. trained: 65,056,256)
2022-09-08 00:06:04 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-09-08 00:06:04 | INFO | fairseq.data.data_utils | loaded 1,014 examples from: /data1/home/turghun/project/acmmt/data_bin/en-fr/valid.en-fr.en
2022-09-08 00:06:04 | INFO | fairseq.data.data_utils | loaded 1,014 examples from: /data1/home/turghun/project/acmmt/data_bin/en-fr/valid.en-fr.fr
2022-09-08 00:06:04 | INFO | rg-ende-fixed.tasks.tasks | /data1/home/turghun/project/acmmt/data_bin/en-fr valid en-fr 1014 examples
2022-09-08 00:06:04 | INFO | rg-ende-fixed.tasks.tasks | load 1014 grid image examples for valid
2022-09-08 00:06:04 | INFO | rg-ende-fixed.tasks.tasks | load 1014 region image examples for valid
2022-09-08 00:06:07 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-09-08 00:06:07 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-09-08 00:06:07 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 11.910 GB ; name = TITAN Xp                                
2022-09-08 00:06:07 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-09-08 00:06:07 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-09-08 00:06:07 | INFO | fairseq_cli.train | max tokens per device = 1536 and max sentences per device = None
2022-09-08 00:06:07 | INFO | fairseq.trainer | Preparing to load checkpoint results/en-fr/mmt/checkpoint_last.pt
2022-09-08 00:06:08 | INFO | fairseq.trainer | Loaded checkpoint results/en-fr/mmt/checkpoint_last.pt (epoch 2 @ 0 updates)
2022-09-08 00:06:08 | INFO | fairseq.trainer | loading train data for epoch 2
2022-09-08 00:06:08 | INFO | fairseq.data.data_utils | loaded 29,000 examples from: /data1/home/turghun/project/acmmt/data_bin/en-fr/train.en-fr.en
2022-09-08 00:06:08 | INFO | fairseq.data.data_utils | loaded 29,000 examples from: /data1/home/turghun/project/acmmt/data_bin/en-fr/train.en-fr.fr
2022-09-08 00:06:18 | INFO | rg-ende-fixed.tasks.tasks | /data1/home/turghun/project/acmmt/data_bin/en-fr train en-fr 29000 examples
2022-09-08 00:06:18 | INFO | rg-ende-fixed.tasks.tasks | load 29000 grid image examples for train
2022-09-08 00:06:18 | INFO | rg-ende-fixed.tasks.tasks | load 29000 region image examples for train
2022-09-08 00:06:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 00:06:18 | INFO | fairseq.trainer | begin training epoch 2
2022-09-08 00:06:18 | INFO | fairseq_cli.train | Start iterating over samples
/data1/home/turghun/project/acmmt/examples/rg-ende-fixed/tasks/data_loader.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  region_img_features = torch.tensor(torch.stack(region_img_features_tmp, dim=0))
2022-09-08 00:08:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
/data1/home/turghun/project/acmmt/fairseq/utils.py:361: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2022-09-08 00:09:09 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.498 | nll_loss 9.579 | mask_loss 10.4899 | p_2 0.01995 | mask_ave 1.039 | ppl 764.92 | bleu 0.01 | wps 238.1 | wpb 933.5 | bsz 59.6 | num_updates 81
2022-09-08 00:09:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 81 updates
2022-09-08 00:09:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint2.pt
2022-09-08 00:09:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint2.pt
2022-09-08 00:09:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint2.pt (epoch 2 @ 81 updates, score 0.01) (writing took 42.21791024878621 seconds)
2022-09-08 00:09:51 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-09-08 00:09:51 | INFO | train | epoch 002 | loss 10.874 | nll_loss 10.09 | mask_loss 10.8624 | p_2 0.02004 | mask_ave 0.657 | ppl 1090.02 | wps 2101 | ups 0.38 | wpb 5523.2 | bsz 358 | num_updates 81 | lr 1.0223e-05 | gnorm 1.464 | train_wall 103 | gb_free 9.3 | wall 224
2022-09-08 00:09:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 00:09:52 | INFO | fairseq.trainer | begin training epoch 3
2022-09-08 00:09:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 00:10:15 | INFO | train_inner | epoch 003:     19 / 81 loss=10.819, nll_loss=10.022, mask_loss=10.8075, p_2=0.01905, mask_ave=0.681, ppl=1039.81, wps=2323.6, ups=0.42, wpb=5501, bsz=355.8, num_updates=100, lr=1.25975e-05, gnorm=1.447, train_wall=126, gb_free=9.1, wall=248
2022-09-08 00:12:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 00:12:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 00:12:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 00:12:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 00:12:13 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.523 | nll_loss 8.288 | mask_loss 9.52916 | p_2 0.01712 | mask_ave 1.121 | ppl 312.46 | bleu 0.17 | wps 1804.3 | wpb 933.5 | bsz 59.6 | num_updates 162 | best_bleu 0.17
2022-09-08 00:12:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 162 updates
2022-09-08 00:12:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint3.pt
2022-09-08 00:12:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint3.pt
2022-09-08 00:12:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint3.pt (epoch 3 @ 162 updates, score 0.17) (writing took 45.863545417785645 seconds)
2022-09-08 00:12:59 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-09-08 00:12:59 | INFO | train | epoch 003 | loss 10.191 | nll_loss 9.227 | mask_loss 10.1737 | p_2 0.01406 | mask_ave 0.787 | ppl 599.05 | wps 2390.7 | ups 0.43 | wpb 5523.2 | bsz 358 | num_updates 162 | lr 2.0346e-05 | gnorm 2.2 | train_wall 130 | gb_free 9.2 | wall 411
2022-09-08 00:12:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 00:12:59 | INFO | fairseq.trainer | begin training epoch 4
2022-09-08 00:12:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 00:13:45 | INFO | train_inner | epoch 004:     38 / 81 loss=9.845, nll_loss=8.774, mask_loss=9.83149, p_2=0.01318, mask_ave=0.805, ppl=437.72, wps=2651.7, ups=0.48, wpb=5565.9, bsz=361.2, num_updates=200, lr=2.5095e-05, gnorm=2.198, train_wall=153, gb_free=9.1, wall=458
2022-09-08 00:14:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 00:14:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 00:14:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 00:14:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 00:14:50 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.814 | nll_loss 7.269 | mask_loss 8.82605 | p_2 0.01717 | mask_ave 1.153 | ppl 154.28 | bleu 3.21 | wps 1390.5 | wpb 933.5 | bsz 59.6 | num_updates 243 | best_bleu 3.21
2022-09-08 00:14:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 243 updates
2022-09-08 00:14:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint4.pt
2022-09-08 00:14:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint4.pt
2022-09-08 00:15:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint4.pt (epoch 4 @ 243 updates, score 3.21) (writing took 27.775679007172585 seconds)
2022-09-08 00:15:17 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-09-08 00:15:17 | INFO | train | epoch 004 | loss 9.284 | nll_loss 8.024 | mask_loss 9.27987 | p_2 0.01175 | mask_ave 0.849 | ppl 260.32 | wps 3223.8 | ups 0.58 | wpb 5523.2 | bsz 358 | num_updates 243 | lr 3.04689e-05 | gnorm 1.757 | train_wall 97 | gb_free 9 | wall 550
2022-09-08 00:15:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 00:15:17 | INFO | fairseq.trainer | begin training epoch 5
2022-09-08 00:15:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 00:16:28 | INFO | train_inner | epoch 005:     57 / 81 loss=8.895, nll_loss=7.488, mask_loss=8.89523, p_2=0.01179, mask_ave=0.859, ppl=179.56, wps=3394.4, ups=0.61, wpb=5527.2, bsz=356.6, num_updates=300, lr=3.75925e-05, gnorm=1.979, train_wall=121, gb_free=9, wall=621
2022-09-08 00:16:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 00:16:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 00:16:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 00:16:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 00:17:24 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.327 | nll_loss 6.554 | mask_loss 8.33864 | p_2 0.01939 | mask_ave 1.107 | ppl 93.95 | bleu 4.67 | wps 581.5 | wpb 933.5 | bsz 59.6 | num_updates 324 | best_bleu 4.67
2022-09-08 00:17:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 324 updates
2022-09-08 00:17:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint5.pt
2022-09-08 00:17:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint5.pt
2022-09-08 00:18:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint5.pt (epoch 5 @ 324 updates, score 4.67) (writing took 42.73954368010163 seconds)
2022-09-08 00:18:07 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-09-08 00:18:07 | INFO | train | epoch 005 | loss 8.647 | nll_loss 7.145 | mask_loss 8.64991 | p_2 0.01229 | mask_ave 0.85 | ppl 141.49 | wps 2635 | ups 0.48 | wpb 5523.2 | bsz 358 | num_updates 324 | lr 4.05919e-05 | gnorm 2.19 | train_wall 98 | gb_free 9.2 | wall 720
2022-09-08 00:18:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 00:18:07 | INFO | fairseq.trainer | begin training epoch 6
2022-09-08 00:18:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 00:19:44 | INFO | train_inner | epoch 006:     76 / 81 loss=8.271, nll_loss=6.627, mask_loss=8.27351, p_2=0.01336, mask_ave=0.828, ppl=98.87, wps=2817.3, ups=0.51, wpb=5525.1, bsz=357.1, num_updates=400, lr=5.009e-05, gnorm=2.157, train_wall=125, gb_free=9, wall=817
2022-09-08 00:19:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 00:19:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 00:19:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 00:19:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 00:20:08 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.926 | nll_loss 5.996 | mask_loss 7.93604 | p_2 0.02116 | mask_ave 1.066 | ppl 63.83 | bleu 7.5 | wps 903.2 | wpb 933.5 | bsz 59.6 | num_updates 405 | best_bleu 7.5
2022-09-08 00:20:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 405 updates
2022-09-08 00:20:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint6.pt
2022-09-08 00:20:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint6.pt
2022-09-08 00:21:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint6.pt (epoch 6 @ 405 updates, score 7.5) (writing took 64.64327911287546 seconds)
2022-09-08 00:21:12 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-09-08 00:21:12 | INFO | train | epoch 006 | loss 8.192 | nll_loss 6.521 | mask_loss 8.19464 | p_2 0.0136 | mask_ave 0.825 | ppl 91.84 | wps 2415.8 | ups 0.44 | wpb 5523.2 | bsz 358 | num_updates 405 | lr 5.07149e-05 | gnorm 2.146 | train_wall 101 | gb_free 9.1 | wall 905
2022-09-08 00:21:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 00:21:12 | INFO | fairseq.trainer | begin training epoch 7
2022-09-08 00:21:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 00:22:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 00:22:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 00:22:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 00:22:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 00:23:21 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.535 | nll_loss 5.498 | mask_loss 7.54005 | p_2 0.02141 | mask_ave 1.056 | ppl 45.21 | bleu 11.8 | wps 610.7 | wpb 933.5 | bsz 59.6 | num_updates 486 | best_bleu 11.8
2022-09-08 00:23:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 486 updates
2022-09-08 00:23:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint7.pt
2022-09-08 00:23:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint7.pt
2022-09-08 00:24:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint7.pt (epoch 7 @ 486 updates, score 11.8) (writing took 43.99203297495842 seconds)
2022-09-08 00:24:05 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-09-08 00:24:05 | INFO | train | epoch 007 | loss 7.82 | nll_loss 6.024 | mask_loss 7.82231 | p_2 0.01382 | mask_ave 0.819 | ppl 65.1 | wps 2596.1 | ups 0.47 | wpb 5523.2 | bsz 358 | num_updates 486 | lr 6.08379e-05 | gnorm 2.045 | train_wall 101 | gb_free 9.1 | wall 1077
2022-09-08 00:24:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 00:24:05 | INFO | fairseq.trainer | begin training epoch 8
2022-09-08 00:24:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 00:24:23 | INFO | train_inner | epoch 008:     14 / 81 loss=7.777, nll_loss=5.968, mask_loss=7.7799, p_2=0.01398, mask_ave=0.817, ppl=62.61, wps=1979.2, ups=0.36, wpb=5517.4, bsz=363.5, num_updates=500, lr=6.25875e-05, gnorm=2.048, train_wall=123, gb_free=9.1, wall=1096
2022-09-08 00:25:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 00:25:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 00:25:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 00:25:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 00:25:57 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.289 | nll_loss 5.146 | mask_loss 7.3022 | p_2 0.02404 | mask_ave 0.995 | ppl 35.42 | bleu 12.61 | wps 1478.2 | wpb 933.5 | bsz 59.6 | num_updates 567 | best_bleu 12.61
2022-09-08 00:25:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 567 updates
2022-09-08 00:25:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint8.pt
2022-09-08 00:25:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint8.pt
2022-09-08 00:26:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint8.pt (epoch 8 @ 567 updates, score 12.61) (writing took 45.49566109851003 seconds)
2022-09-08 00:26:43 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-09-08 00:26:43 | INFO | train | epoch 008 | loss 7.495 | nll_loss 5.596 | mask_loss 7.49974 | p_2 0.01416 | mask_ave 0.808 | ppl 48.38 | wps 2824.4 | ups 0.51 | wpb 5523.2 | bsz 358 | num_updates 567 | lr 7.09608e-05 | gnorm 1.893 | train_wall 100 | gb_free 9.2 | wall 1236
2022-09-08 00:26:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 00:26:43 | INFO | fairseq.trainer | begin training epoch 9
2022-09-08 00:26:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 00:27:25 | INFO | train_inner | epoch 009:     33 / 81 loss=7.433, nll_loss=5.514, mask_loss=7.43691, p_2=0.01433, mask_ave=0.803, ppl=45.7, wps=3031.6, ups=0.55, wpb=5502.5, bsz=352.7, num_updates=600, lr=7.5085e-05, gnorm=1.884, train_wall=123, gb_free=9, wall=1277
2022-09-08 00:28:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 00:28:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 00:28:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 00:28:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 00:28:35 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.092 | nll_loss 4.89 | mask_loss 7.1127 | p_2 0.0257 | mask_ave 0.948 | ppl 29.65 | bleu 14.35 | wps 1496.1 | wpb 933.5 | bsz 59.6 | num_updates 648 | best_bleu 14.35
2022-09-08 00:28:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 648 updates
2022-09-08 00:28:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint9.pt
2022-09-08 00:28:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint9.pt
2022-09-08 00:29:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint9.pt (epoch 9 @ 648 updates, score 14.35) (writing took 43.16206273436546 seconds)
2022-09-08 00:29:18 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-09-08 00:29:18 | INFO | train | epoch 009 | loss 7.239 | nll_loss 5.258 | mask_loss 7.24573 | p_2 0.01481 | mask_ave 0.788 | ppl 38.27 | wps 2884.3 | ups 0.52 | wpb 5523.2 | bsz 358 | num_updates 648 | lr 8.10838e-05 | gnorm 1.836 | train_wall 99 | gb_free 9.2 | wall 1391
2022-09-08 00:29:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 00:29:18 | INFO | fairseq.trainer | begin training epoch 10
2022-09-08 00:29:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 00:30:23 | INFO | train_inner | epoch 010:     52 / 81 loss=7.118, nll_loss=5.098, mask_loss=7.12848, p_2=0.01561, mask_ave=0.768, ppl=34.24, wps=3100.8, ups=0.56, wpb=5541.7, bsz=360.3, num_updates=700, lr=8.75825e-05, gnorm=1.773, train_wall=123, gb_free=9.1, wall=1456
2022-09-08 00:30:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 00:31:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 00:31:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 00:31:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 00:31:16 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.888 | nll_loss 4.601 | mask_loss 6.90291 | p_2 0.02706 | mask_ave 0.92 | ppl 24.27 | bleu 17.97 | wps 945.9 | wpb 933.5 | bsz 59.6 | num_updates 729 | best_bleu 17.97
2022-09-08 00:31:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 729 updates
2022-09-08 00:31:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint10.pt
2022-09-08 00:31:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint10.pt
2022-09-08 00:32:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint10.pt (epoch 10 @ 729 updates, score 17.97) (writing took 45.57364485785365 seconds)
2022-09-08 00:32:01 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-09-08 00:32:02 | INFO | train | epoch 010 | loss 7.026 | nll_loss 4.975 | mask_loss 7.03788 | p_2 0.01646 | mask_ave 0.752 | ppl 31.46 | wps 2738.6 | ups 0.5 | wpb 5523.2 | bsz 358 | num_updates 729 | lr 9.12068e-05 | gnorm 1.794 | train_wall 99 | gb_free 9 | wall 1554
2022-09-08 00:32:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 00:32:02 | INFO | fairseq.trainer | begin training epoch 11
2022-09-08 00:32:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 00:33:33 | INFO | train_inner | epoch 011:     71 / 81 loss=6.871, nll_loss=4.769, mask_loss=6.88975, p_2=0.01797, mask_ave=0.728, ppl=27.27, wps=2904.8, ups=0.53, wpb=5520, bsz=358.6, num_updates=800, lr=0.00010008, gnorm=1.762, train_wall=125, gb_free=9.1, wall=1646
2022-09-08 00:33:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 00:33:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 00:33:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 00:33:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 00:34:04 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.704 | nll_loss 4.4 | mask_loss 6.7153 | p_2 0.03019 | mask_ave 0.867 | ppl 21.12 | bleu 19.9 | wps 872.7 | wpb 933.5 | bsz 59.6 | num_updates 810 | best_bleu 19.9
2022-09-08 00:34:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 810 updates
2022-09-08 00:34:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint11.pt
2022-09-08 00:34:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint11.pt
2022-09-08 00:34:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint11.pt (epoch 11 @ 810 updates, score 19.9) (writing took 44.5505517013371 seconds)
2022-09-08 00:34:48 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-09-08 00:34:48 | INFO | train | epoch 011 | loss 6.827 | nll_loss 4.71 | mask_loss 6.8486 | p_2 0.0185 | mask_ave 0.716 | ppl 26.17 | wps 2681.7 | ups 0.49 | wpb 5523.2 | bsz 358 | num_updates 810 | lr 0.00010133 | gnorm 1.687 | train_wall 102 | gb_free 9.1 | wall 1721
2022-09-08 00:34:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 00:34:49 | INFO | fairseq.trainer | begin training epoch 12
2022-09-08 00:34:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 00:36:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 00:36:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 00:36:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 00:36:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 00:36:52 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.557 | nll_loss 4.16 | mask_loss 6.61935 | p_2 0.0353 | mask_ave 0.793 | ppl 17.88 | bleu 19.5 | wps 800.7 | wpb 933.5 | bsz 59.6 | num_updates 891 | best_bleu 19.9
2022-09-08 00:36:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 891 updates
2022-09-08 00:36:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint12.pt
2022-09-08 00:36:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint12.pt
2022-09-08 00:37:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint12.pt (epoch 12 @ 891 updates, score 19.5) (writing took 23.10475020483136 seconds)
2022-09-08 00:37:15 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-09-08 00:37:15 | INFO | train | epoch 012 | loss 6.659 | nll_loss 4.487 | mask_loss 6.69974 | p_2 0.02157 | mask_ave 0.667 | ppl 22.42 | wps 3046.1 | ups 0.55 | wpb 5523.2 | bsz 358 | num_updates 891 | lr 0.000111453 | gnorm 1.667 | train_wall 102 | gb_free 9.2 | wall 1868
2022-09-08 00:37:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 00:37:15 | INFO | fairseq.trainer | begin training epoch 13
2022-09-08 00:37:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 00:37:28 | INFO | train_inner | epoch 013:      9 / 81 loss=6.649, nll_loss=4.473, mask_loss=6.69142, p_2=0.02144, mask_ave=0.667, ppl=22.21, wps=2358.4, ups=0.43, wpb=5524.6, bsz=359.8, num_updates=900, lr=0.000112578, gnorm=1.638, train_wall=125, gb_free=9, wall=1880
2022-09-08 00:38:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 00:38:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 00:38:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 00:38:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 00:39:11 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.412 | nll_loss 3.928 | mask_loss 6.62119 | p_2 0.04068 | mask_ave 0.696 | ppl 15.22 | bleu 22.21 | wps 1266.1 | wpb 933.5 | bsz 59.6 | num_updates 972 | best_bleu 22.21
2022-09-08 00:39:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 972 updates
2022-09-08 00:39:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint13.pt
2022-09-08 00:39:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint13.pt
2022-09-08 00:39:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint13.pt (epoch 13 @ 972 updates, score 22.21) (writing took 43.63110988214612 seconds)
2022-09-08 00:39:54 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-09-08 00:39:54 | INFO | train | epoch 013 | loss 6.499 | nll_loss 4.271 | mask_loss 6.61132 | p_2 0.0252 | mask_ave 0.611 | ppl 19.31 | wps 2808.9 | ups 0.51 | wpb 5523.2 | bsz 358 | num_updates 972 | lr 0.000121576 | gnorm 1.615 | train_wall 101 | gb_free 9.1 | wall 2027
2022-09-08 00:39:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 00:39:55 | INFO | fairseq.trainer | begin training epoch 14
2022-09-08 00:39:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 00:40:30 | INFO | train_inner | epoch 014:     28 / 81 loss=6.459, nll_loss=4.217, mask_loss=6.59937, p_2=0.02621, mask_ave=0.597, ppl=18.6, wps=3027.3, ups=0.55, wpb=5516, bsz=357.1, num_updates=1000, lr=0.000125075, gnorm=1.601, train_wall=123, gb_free=9, wall=2063
2022-09-08 00:41:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 00:41:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 00:41:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 00:41:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 00:41:56 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.285 | nll_loss 3.738 | mask_loss 6.60572 | p_2 0.0454 | mask_ave 0.628 | ppl 13.34 | bleu 26.29 | wps 846.8 | wpb 933.5 | bsz 59.6 | num_updates 1053 | best_bleu 26.29
2022-09-08 00:41:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 1053 updates
2022-09-08 00:41:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint14.pt
2022-09-08 00:41:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint14.pt
2022-09-08 00:42:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint14.pt (epoch 14 @ 1053 updates, score 26.29) (writing took 24.524406347423792 seconds)
2022-09-08 00:42:20 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-09-08 00:42:20 | INFO | train | epoch 014 | loss 6.336 | nll_loss 4.054 | mask_loss 6.5782 | p_2 0.02947 | mask_ave 0.546 | ppl 16.61 | wps 3065.9 | ups 0.56 | wpb 5523.2 | bsz 358 | num_updates 1053 | lr 0.000131699 | gnorm 1.625 | train_wall 101 | gb_free 9.1 | wall 2173
2022-09-08 00:42:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 00:42:21 | INFO | fairseq.trainer | begin training epoch 15
2022-09-08 00:42:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 00:43:20 | INFO | train_inner | epoch 015:     47 / 81 loss=6.252, nll_loss=3.942, mask_loss=6.5615, p_2=0.03136, mask_ave=0.521, ppl=15.37, wps=3240.4, ups=0.59, wpb=5516.7, bsz=359.3, num_updates=1100, lr=0.000137573, gnorm=1.615, train_wall=125, gb_free=9.1, wall=2233
2022-09-08 00:44:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 00:44:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 00:44:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 00:44:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 00:44:17 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.101 | nll_loss 3.463 | mask_loss 6.55878 | p_2 0.04653 | mask_ave 0.626 | ppl 11.03 | bleu 30 | wps 1036.1 | wpb 933.5 | bsz 59.6 | num_updates 1134 | best_bleu 30
2022-09-08 00:44:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 1134 updates
2022-09-08 00:44:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint15.pt
2022-09-08 00:44:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint15.pt
2022-09-08 00:45:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint15.pt (epoch 15 @ 1134 updates, score 30.0) (writing took 44.61004776507616 seconds)
2022-09-08 00:45:01 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-09-08 00:45:02 | INFO | train | epoch 015 | loss 6.154 | nll_loss 3.811 | mask_loss 6.54739 | p_2 0.0328 | mask_ave 0.496 | ppl 14.03 | wps 2777.5 | ups 0.5 | wpb 5523.2 | bsz 358 | num_updates 1134 | lr 0.000141822 | gnorm 1.545 | train_wall 99 | gb_free 9.1 | wall 2334
2022-09-08 00:45:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 00:45:02 | INFO | fairseq.trainer | begin training epoch 16
2022-09-08 00:45:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 00:46:25 | INFO | train_inner | epoch 016:     66 / 81 loss=6.047, nll_loss=3.668, mask_loss=6.54871, p_2=0.03359, mask_ave=0.482, ppl=12.71, wps=2994.6, ups=0.54, wpb=5543.5, bsz=354.5, num_updates=1200, lr=0.00015007, gnorm=1.522, train_wall=123, gb_free=9, wall=2418
2022-09-08 00:46:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 00:46:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 00:46:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 00:46:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 00:47:01 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 5.964 | nll_loss 3.299 | mask_loss 6.51544 | p_2 0.04676 | mask_ave 0.625 | ppl 9.84 | bleu 32.58 | wps 920.9 | wpb 933.5 | bsz 59.6 | num_updates 1215 | best_bleu 32.58
2022-09-08 00:47:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 1215 updates
2022-09-08 00:47:01 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint16.pt
2022-09-08 00:47:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint16.pt
2022-09-08 00:47:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint16.pt (epoch 16 @ 1215 updates, score 32.58) (writing took 23.47015520185232 seconds)
2022-09-08 00:47:25 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-09-08 00:47:25 | INFO | train | epoch 016 | loss 5.989 | nll_loss 3.59 | mask_loss 6.53445 | p_2 0.03402 | mask_ave 0.484 | ppl 12.04 | wps 3118.1 | ups 0.56 | wpb 5523.2 | bsz 358 | num_updates 1215 | lr 0.000151945 | gnorm 1.498 | train_wall 101 | gb_free 9.2 | wall 2478
2022-09-08 00:47:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 00:47:25 | INFO | fairseq.trainer | begin training epoch 17
2022-09-08 00:47:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 00:49:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 00:49:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 00:49:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 00:49:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 00:49:22 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 5.822 | nll_loss 3.064 | mask_loss 6.55228 | p_2 0.04672 | mask_ave 0.639 | ppl 8.37 | bleu 35.63 | wps 974.4 | wpb 933.5 | bsz 59.6 | num_updates 1296 | best_bleu 35.63
2022-09-08 00:49:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1296 updates
2022-09-08 00:49:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint17.pt
2022-09-08 00:49:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint17.pt
2022-09-08 00:50:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint17.pt (epoch 17 @ 1296 updates, score 35.63) (writing took 43.07884585112333 seconds)
2022-09-08 00:50:06 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-09-08 00:50:06 | INFO | train | epoch 017 | loss 5.818 | nll_loss 3.363 | mask_loss 6.54052 | p_2 0.03428 | mask_ave 0.487 | ppl 10.29 | wps 2782.2 | ups 0.5 | wpb 5523.2 | bsz 358 | num_updates 1296 | lr 0.000162068 | gnorm 1.415 | train_wall 99 | gb_free 9.2 | wall 2639
2022-09-08 00:50:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 00:50:06 | INFO | fairseq.trainer | begin training epoch 18
2022-09-08 00:50:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 00:50:11 | INFO | train_inner | epoch 018:      4 / 81 loss=5.836, nll_loss=3.387, mask_loss=6.54493, p_2=0.03435, mask_ave=0.488, ppl=10.46, wps=2430.9, ups=0.44, wpb=5501.3, bsz=357.8, num_updates=1300, lr=0.000162568, gnorm=1.436, train_wall=122, gb_free=9.1, wall=2644
2022-09-08 00:51:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 00:51:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 00:51:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 00:51:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 00:51:56 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 5.698 | nll_loss 2.891 | mask_loss 6.61733 | p_2 0.04734 | mask_ave 0.632 | ppl 7.42 | bleu 38.05 | wps 1380.2 | wpb 933.5 | bsz 59.6 | num_updates 1377 | best_bleu 38.05
2022-09-08 00:51:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1377 updates
2022-09-08 00:51:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint18.pt
2022-09-08 00:51:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint18.pt
2022-09-08 00:52:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint18.pt (epoch 18 @ 1377 updates, score 38.05) (writing took 25.559374436736107 seconds)
2022-09-08 00:52:22 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-09-08 00:52:22 | INFO | train | epoch 018 | loss 5.672 | nll_loss 3.169 | mask_loss 6.55403 | p_2 0.03429 | mask_ave 0.496 | ppl 8.99 | wps 3294.6 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 1377 | lr 0.000172191 | gnorm 1.421 | train_wall 96 | gb_free 9.2 | wall 2774
2022-09-08 00:52:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 00:52:22 | INFO | fairseq.trainer | begin training epoch 19
2022-09-08 00:52:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 00:52:51 | INFO | train_inner | epoch 019:     23 / 81 loss=5.641, nll_loss=3.128, mask_loss=6.54658, p_2=0.03434, mask_ave=0.497, ppl=8.74, wps=3463.7, ups=0.63, wpb=5524.8, bsz=358.6, num_updates=1400, lr=0.000175065, gnorm=1.378, train_wall=120, gb_free=9.1, wall=2804
2022-09-08 00:54:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 00:54:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 00:54:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 00:54:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 00:54:19 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 5.609 | nll_loss 2.795 | mask_loss 6.58468 | p_2 0.04661 | mask_ave 0.649 | ppl 6.94 | bleu 40.4 | wps 1037.3 | wpb 933.5 | bsz 59.6 | num_updates 1458 | best_bleu 40.4
2022-09-08 00:54:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1458 updates
2022-09-08 00:54:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint19.pt
2022-09-08 00:54:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint19.pt
2022-09-08 00:54:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint19.pt (epoch 19 @ 1458 updates, score 40.4) (writing took 29.011382404714823 seconds)
2022-09-08 00:54:48 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-09-08 00:54:48 | INFO | train | epoch 019 | loss 5.511 | nll_loss 2.957 | mask_loss 6.56271 | p_2 0.03422 | mask_ave 0.502 | ppl 7.77 | wps 3051.8 | ups 0.55 | wpb 5523.2 | bsz 358 | num_updates 1458 | lr 0.000182314 | gnorm 1.315 | train_wall 100 | gb_free 9.2 | wall 2921
2022-09-08 00:54:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 00:54:48 | INFO | fairseq.trainer | begin training epoch 20
2022-09-08 00:54:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 00:55:40 | INFO | train_inner | epoch 020:     42 / 81 loss=5.469, nll_loss=2.901, mask_loss=6.57685, p_2=0.03405, mask_ave=0.505, ppl=7.47, wps=3271.2, ups=0.59, wpb=5543.1, bsz=359.3, num_updates=1500, lr=0.000187563, gnorm=1.364, train_wall=123, gb_free=9.1, wall=2973
2022-09-08 00:56:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 00:56:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 00:56:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 00:56:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 00:56:45 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 5.537 | nll_loss 2.613 | mask_loss 6.69416 | p_2 0.04693 | mask_ave 0.644 | ppl 6.12 | bleu 42.7 | wps 965.6 | wpb 933.5 | bsz 59.6 | num_updates 1539 | best_bleu 42.7
2022-09-08 00:56:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 1539 updates
2022-09-08 00:56:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint20.pt
2022-09-08 00:56:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint20.pt
2022-09-08 00:57:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint20.pt (epoch 20 @ 1539 updates, score 42.7) (writing took 44.400689609348774 seconds)
2022-09-08 00:57:30 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-09-08 00:57:30 | INFO | train | epoch 020 | loss 5.404 | nll_loss 2.817 | mask_loss 6.59651 | p_2 0.03409 | mask_ave 0.508 | ppl 7.05 | wps 2767 | ups 0.5 | wpb 5523.2 | bsz 358 | num_updates 1539 | lr 0.000192437 | gnorm 1.329 | train_wall 99 | gb_free 9.2 | wall 3083
2022-09-08 00:57:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 00:57:30 | INFO | fairseq.trainer | begin training epoch 21
2022-09-08 00:57:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 00:58:46 | INFO | train_inner | epoch 021:     61 / 81 loss=5.323, nll_loss=2.71, mask_loss=6.60946, p_2=0.03408, mask_ave=0.509, ppl=6.55, wps=2968.6, ups=0.54, wpb=5522, bsz=357.2, num_updates=1600, lr=0.00020006, gnorm=1.267, train_wall=123, gb_free=9.1, wall=3159
2022-09-08 00:59:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 00:59:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 00:59:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 00:59:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 00:59:28 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 5.469 | nll_loss 2.559 | mask_loss 6.74003 | p_2 0.04654 | mask_ave 0.651 | ppl 5.89 | bleu 44.11 | wps 972.1 | wpb 933.5 | bsz 59.6 | num_updates 1620 | best_bleu 44.11
2022-09-08 00:59:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 1620 updates
2022-09-08 00:59:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint21.pt
2022-09-08 00:59:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint21.pt
2022-09-08 01:00:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint21.pt (epoch 21 @ 1620 updates, score 44.11) (writing took 50.41378432884812 seconds)
2022-09-08 01:00:18 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-09-08 01:00:18 | INFO | train | epoch 021 | loss 5.278 | nll_loss 2.651 | mask_loss 6.60799 | p_2 0.03408 | mask_ave 0.51 | ppl 6.28 | wps 2656.1 | ups 0.48 | wpb 5523.2 | bsz 358 | num_updates 1620 | lr 0.00020256 | gnorm 1.259 | train_wall 100 | gb_free 9.1 | wall 3251
2022-09-08 01:00:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 01:00:18 | INFO | fairseq.trainer | begin training epoch 22
2022-09-08 01:00:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 01:01:56 | INFO | train_inner | epoch 022:     80 / 81 loss=5.171, nll_loss=2.512, mask_loss=6.61768, p_2=0.03393, mask_ave=0.516, ppl=5.7, wps=2910.5, ups=0.53, wpb=5530, bsz=359.1, num_updates=1700, lr=0.000212558, gnorm=1.145, train_wall=121, gb_free=9.1, wall=3349
2022-09-08 01:01:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 01:01:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 01:01:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 01:01:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 01:02:14 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 5.349 | nll_loss 2.41 | mask_loss 6.77374 | p_2 0.04663 | mask_ave 0.659 | ppl 5.31 | bleu 43.72 | wps 984.5 | wpb 933.5 | bsz 59.6 | num_updates 1701 | best_bleu 44.11
2022-09-08 01:02:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 1701 updates
2022-09-08 01:02:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint22.pt
2022-09-08 01:02:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint22.pt
2022-09-08 01:02:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint22.pt (epoch 22 @ 1701 updates, score 43.72) (writing took 18.1719140522182 seconds)
2022-09-08 01:02:32 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-09-08 01:02:33 | INFO | train | epoch 022 | loss 5.16 | nll_loss 2.496 | mask_loss 6.61844 | p_2 0.03386 | mask_ave 0.516 | ppl 5.64 | wps 3334.5 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 1701 | lr 0.000212682 | gnorm 1.153 | train_wall 98 | gb_free 9.2 | wall 3385
2022-09-08 01:02:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 01:02:33 | INFO | fairseq.trainer | begin training epoch 23
2022-09-08 01:02:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 01:04:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 01:04:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 01:04:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 01:04:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 01:04:26 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 5.274 | nll_loss 2.293 | mask_loss 6.7636 | p_2 0.04673 | mask_ave 0.653 | ppl 4.9 | bleu 46.33 | wps 1403.4 | wpb 933.5 | bsz 59.6 | num_updates 1782 | best_bleu 46.33
2022-09-08 01:04:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 1782 updates
2022-09-08 01:04:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint23.pt
2022-09-08 01:04:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint23.pt
2022-09-08 01:05:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint23.pt (epoch 23 @ 1782 updates, score 46.33) (writing took 44.62189054489136 seconds)
2022-09-08 01:05:11 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-09-08 01:05:11 | INFO | train | epoch 023 | loss 5.054 | nll_loss 2.36 | mask_loss 6.63298 | p_2 0.03383 | mask_ave 0.519 | ppl 5.13 | wps 2822.4 | ups 0.51 | wpb 5523.2 | bsz 358 | num_updates 1782 | lr 0.000222805 | gnorm 1.041 | train_wall 100 | gb_free 9 | wall 3544
2022-09-08 01:05:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 01:05:11 | INFO | fairseq.trainer | begin training epoch 24
2022-09-08 01:05:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 01:05:34 | INFO | train_inner | epoch 024:     18 / 81 loss=5.035, nll_loss=2.334, mask_loss=6.62103, p_2=0.03385, mask_ave=0.519, ppl=5.04, wps=2536.7, ups=0.46, wpb=5513.1, bsz=357.6, num_updates=1800, lr=0.000225055, gnorm=1.024, train_wall=123, gb_free=9, wall=3567
2022-09-08 01:06:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 01:06:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 01:06:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 01:06:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 01:07:04 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 5.233 | nll_loss 2.232 | mask_loss 6.7814 | p_2 0.04572 | mask_ave 0.681 | ppl 4.7 | bleu 47.94 | wps 1523.6 | wpb 933.5 | bsz 59.6 | num_updates 1863 | best_bleu 47.94
2022-09-08 01:07:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 1863 updates
2022-09-08 01:07:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint24.pt
2022-09-08 01:07:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint24.pt
2022-09-08 01:08:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint24.pt (epoch 24 @ 1863 updates, score 47.94) (writing took 71.52573140338063 seconds)
2022-09-08 01:08:16 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-09-08 01:08:16 | INFO | train | epoch 024 | loss 4.968 | nll_loss 2.246 | mask_loss 6.6134 | p_2 0.03368 | mask_ave 0.524 | ppl 4.74 | wps 2420.9 | ups 0.44 | wpb 5523.2 | bsz 358 | num_updates 1863 | lr 0.000232928 | gnorm 1.074 | train_wall 100 | gb_free 9.1 | wall 3729
2022-09-08 01:08:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 01:08:16 | INFO | fairseq.trainer | begin training epoch 25
2022-09-08 01:08:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 01:09:02 | INFO | train_inner | epoch 025:     37 / 81 loss=4.932, nll_loss=2.198, mask_loss=6.61123, p_2=0.03344, mask_ave=0.525, ppl=4.59, wps=2662, ups=0.48, wpb=5549.6, bsz=356.6, num_updates=1900, lr=0.000237553, gnorm=1.036, train_wall=124, gb_free=9.1, wall=3775
2022-09-08 01:09:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 01:09:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 01:09:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 01:09:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 01:10:10 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 5.173 | nll_loss 2.216 | mask_loss 6.77497 | p_2 0.0459 | mask_ave 0.675 | ppl 4.65 | bleu 49.05 | wps 1268.7 | wpb 933.5 | bsz 59.6 | num_updates 1944 | best_bleu 49.05
2022-09-08 01:10:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 1944 updates
2022-09-08 01:10:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint25.pt
2022-09-08 01:10:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint25.pt
2022-09-08 01:11:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint25.pt (epoch 25 @ 1944 updates, score 49.05) (writing took 86.2831111997366 seconds)
2022-09-08 01:11:37 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-09-08 01:11:37 | INFO | train | epoch 025 | loss 4.871 | nll_loss 2.119 | mask_loss 6.60068 | p_2 0.03357 | mask_ave 0.527 | ppl 4.35 | wps 2228.9 | ups 0.4 | wpb 5523.2 | bsz 358 | num_updates 1944 | lr 0.000243051 | gnorm 0.961 | train_wall 100 | gb_free 9.4 | wall 3929
2022-09-08 01:11:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 01:11:37 | INFO | fairseq.trainer | begin training epoch 26
2022-09-08 01:11:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 01:12:48 | INFO | train_inner | epoch 026:     56 / 81 loss=4.856, nll_loss=2.099, mask_loss=6.617, p_2=0.0335, mask_ave=0.53, ppl=4.28, wps=2446, ups=0.44, wpb=5524.3, bsz=361.3, num_updates=2000, lr=0.00025005, gnorm=1.053, train_wall=124, gb_free=9, wall=4001
2022-09-08 01:13:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 01:13:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 01:13:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 01:13:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 01:13:30 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 5.172 | nll_loss 2.111 | mask_loss 6.86684 | p_2 0.04543 | mask_ave 0.69 | ppl 4.32 | bleu 48.56 | wps 1536.8 | wpb 933.5 | bsz 59.6 | num_updates 2025 | best_bleu 49.05
2022-09-08 01:13:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 2025 updates
2022-09-08 01:13:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint26.pt
2022-09-08 01:13:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint26.pt
2022-09-08 01:14:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint26.pt (epoch 26 @ 2025 updates, score 48.56) (writing took 45.71972422301769 seconds)
2022-09-08 01:14:16 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-09-08 01:14:16 | INFO | train | epoch 026 | loss 4.818 | nll_loss 2.049 | mask_loss 6.62175 | p_2 0.03338 | mask_ave 0.533 | ppl 4.14 | wps 2808.5 | ups 0.51 | wpb 5523.2 | bsz 358 | num_updates 2025 | lr 0.000253174 | gnorm 1.029 | train_wall 101 | gb_free 9.4 | wall 4089
2022-09-08 01:14:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 01:14:16 | INFO | fairseq.trainer | begin training epoch 27
2022-09-08 01:14:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 01:15:52 | INFO | train_inner | epoch 027:     75 / 81 loss=4.741, nll_loss=1.949, mask_loss=6.62331, p_2=0.03355, mask_ave=0.537, ppl=3.86, wps=2982.3, ups=0.54, wpb=5495.8, bsz=356.3, num_updates=2100, lr=0.000262548, gnorm=0.944, train_wall=126, gb_free=9.1, wall=4185
2022-09-08 01:15:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 01:16:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 01:16:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 01:16:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 01:16:10 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 5.122 | nll_loss 2.114 | mask_loss 6.92566 | p_2 0.04526 | mask_ave 0.691 | ppl 4.33 | bleu 49.64 | wps 1499.5 | wpb 933.5 | bsz 59.6 | num_updates 2106 | best_bleu 49.64
2022-09-08 01:16:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 2106 updates
2022-09-08 01:16:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint27.pt
2022-09-08 01:16:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint27.pt
2022-09-08 01:17:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint27.pt (epoch 27 @ 2106 updates, score 49.64) (writing took 82.6059635207057 seconds)
2022-09-08 01:17:33 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-09-08 01:17:33 | INFO | train | epoch 027 | loss 4.734 | nll_loss 1.94 | mask_loss 6.62474 | p_2 0.03336 | mask_ave 0.535 | ppl 3.84 | wps 2266.8 | ups 0.41 | wpb 5523.2 | bsz 358 | num_updates 2106 | lr 0.000263297 | gnorm 0.963 | train_wall 101 | gb_free 9.1 | wall 4286
2022-09-08 01:17:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 01:17:33 | INFO | fairseq.trainer | begin training epoch 28
2022-09-08 01:17:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 01:19:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 01:19:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 01:19:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 01:19:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 01:19:23 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 5.104 | nll_loss 2.094 | mask_loss 6.82891 | p_2 0.04552 | mask_ave 0.685 | ppl 4.27 | bleu 49.95 | wps 1515.7 | wpb 933.5 | bsz 59.6 | num_updates 2187 | best_bleu 49.95
2022-09-08 01:19:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 2187 updates
2022-09-08 01:19:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint28.pt
2022-09-08 01:19:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint28.pt
2022-09-08 01:20:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint28.pt (epoch 28 @ 2187 updates, score 49.95) (writing took 87.1317035369575 seconds)
2022-09-08 01:20:51 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-09-08 01:20:51 | INFO | train | epoch 028 | loss 4.654 | nll_loss 1.835 | mask_loss 6.62263 | p_2 0.03347 | mask_ave 0.532 | ppl 3.57 | wps 2265.6 | ups 0.41 | wpb 5523.2 | bsz 358 | num_updates 2187 | lr 0.00027342 | gnorm 0.876 | train_wall 97 | gb_free 9.1 | wall 4484
2022-09-08 01:20:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 01:20:51 | INFO | fairseq.trainer | begin training epoch 29
2022-09-08 01:20:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 01:21:07 | INFO | train_inner | epoch 029:     13 / 81 loss=4.648, nll_loss=1.829, mask_loss=6.61312, p_2=0.03345, mask_ave=0.531, ppl=3.55, wps=1751.2, ups=0.32, wpb=5516.8, bsz=359.5, num_updates=2200, lr=0.000275045, gnorm=0.903, train_wall=119, gb_free=9.1, wall=4500
2022-09-08 01:22:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 01:22:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 01:22:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 01:22:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 01:22:45 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 5.038 | nll_loss 2.027 | mask_loss 6.91578 | p_2 0.04571 | mask_ave 0.681 | ppl 4.08 | bleu 51.19 | wps 1409.8 | wpb 933.5 | bsz 59.6 | num_updates 2268 | best_bleu 51.19
2022-09-08 01:22:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 2268 updates
2022-09-08 01:22:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint29.pt
2022-09-08 01:22:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint29.pt
2022-09-08 01:23:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint29.pt (epoch 29 @ 2268 updates, score 51.19) (writing took 44.17457711696625 seconds)
2022-09-08 01:23:29 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-09-08 01:23:29 | INFO | train | epoch 029 | loss 4.586 | nll_loss 1.747 | mask_loss 6.62514 | p_2 0.03351 | mask_ave 0.531 | ppl 3.36 | wps 2825.2 | ups 0.51 | wpb 5523.2 | bsz 358 | num_updates 2268 | lr 0.000283543 | gnorm 0.856 | train_wall 101 | gb_free 9.2 | wall 4642
2022-09-08 01:23:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 01:23:29 | INFO | fairseq.trainer | begin training epoch 30
2022-09-08 01:23:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 01:24:08 | INFO | train_inner | epoch 030:     32 / 81 loss=4.571, nll_loss=1.727, mask_loss=6.63411, p_2=0.0334, mask_ave=0.53, ppl=3.31, wps=3061.7, ups=0.55, wpb=5541, bsz=356.1, num_updates=2300, lr=0.000287543, gnorm=0.853, train_wall=123, gb_free=9.1, wall=4681
2022-09-08 01:25:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 01:25:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 01:25:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 01:25:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 01:25:21 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 5.007 | nll_loss 1.974 | mask_loss 6.96669 | p_2 0.04556 | mask_ave 0.685 | ppl 3.93 | bleu 52.94 | wps 1417.1 | wpb 933.5 | bsz 59.6 | num_updates 2349 | best_bleu 52.94
2022-09-08 01:25:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 2349 updates
2022-09-08 01:25:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint30.pt
2022-09-08 01:25:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint30.pt
2022-09-08 01:26:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint30.pt (epoch 30 @ 2349 updates, score 52.94) (writing took 46.912190824747086 seconds)
2022-09-08 01:26:08 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-09-08 01:26:08 | INFO | train | epoch 030 | loss 4.529 | nll_loss 1.672 | mask_loss 6.61766 | p_2 0.0336 | mask_ave 0.529 | ppl 3.19 | wps 2807.6 | ups 0.51 | wpb 5523.2 | bsz 358 | num_updates 2349 | lr 0.000293666 | gnorm 0.872 | train_wall 99 | gb_free 9.3 | wall 4801
2022-09-08 01:26:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 01:26:09 | INFO | fairseq.trainer | begin training epoch 31
2022-09-08 01:26:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 01:27:13 | INFO | train_inner | epoch 031:     51 / 81 loss=4.495, nll_loss=1.627, mask_loss=6.60329, p_2=0.03375, mask_ave=0.531, ppl=3.09, wps=2986.3, ups=0.54, wpb=5510.9, bsz=357.7, num_updates=2400, lr=0.00030004, gnorm=0.848, train_wall=124, gb_free=9, wall=4866
2022-09-08 01:27:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 01:27:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 01:27:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 01:27:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 01:28:02 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 4.998 | nll_loss 1.976 | mask_loss 6.99595 | p_2 0.04577 | mask_ave 0.682 | ppl 3.93 | bleu 52.76 | wps 1363.9 | wpb 933.5 | bsz 59.6 | num_updates 2430 | best_bleu 52.94
2022-09-08 01:28:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 2430 updates
2022-09-08 01:28:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint31.pt
2022-09-08 01:28:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint31.pt
2022-09-08 01:28:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint31.pt (epoch 31 @ 2430 updates, score 52.76) (writing took 21.373345036059618 seconds)
2022-09-08 01:28:24 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-09-08 01:28:24 | INFO | train | epoch 031 | loss 4.46 | nll_loss 1.582 | mask_loss 6.60555 | p_2 0.03356 | mask_ave 0.53 | ppl 2.99 | wps 3302.2 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 2430 | lr 0.000303789 | gnorm 0.82 | train_wall 100 | gb_free 9 | wall 4937
2022-09-08 01:28:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 01:28:24 | INFO | fairseq.trainer | begin training epoch 32
2022-09-08 01:28:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 01:29:51 | INFO | train_inner | epoch 032:     70 / 81 loss=4.418, nll_loss=1.528, mask_loss=6.62336, p_2=0.03352, mask_ave=0.526, ppl=2.88, wps=3509.9, ups=0.63, wpb=5537.7, bsz=357.8, num_updates=2500, lr=0.000312538, gnorm=0.8, train_wall=122, gb_free=9, wall=5024
2022-09-08 01:30:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 01:30:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 01:30:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 01:30:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 01:30:15 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 5.008 | nll_loss 1.98 | mask_loss 7.02599 | p_2 0.04545 | mask_ave 0.688 | ppl 3.95 | bleu 52.14 | wps 1606.7 | wpb 933.5 | bsz 59.6 | num_updates 2511 | best_bleu 52.94
2022-09-08 01:30:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 2511 updates
2022-09-08 01:30:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint32.pt
2022-09-08 01:30:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint32.pt
2022-09-08 01:30:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint32.pt (epoch 32 @ 2511 updates, score 52.14) (writing took 19.728216759860516 seconds)
2022-09-08 01:30:35 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-09-08 01:30:35 | INFO | train | epoch 032 | loss 4.401 | nll_loss 1.506 | mask_loss 6.61543 | p_2 0.03374 | mask_ave 0.526 | ppl 2.84 | wps 3420.3 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 2511 | lr 0.000313912 | gnorm 0.797 | train_wall 99 | gb_free 9.1 | wall 5068
2022-09-08 01:30:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 01:30:35 | INFO | fairseq.trainer | begin training epoch 33
2022-09-08 01:30:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 01:32:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 01:32:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 01:32:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 01:32:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 01:32:29 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 4.955 | nll_loss 1.928 | mask_loss 7.12305 | p_2 0.04586 | mask_ave 0.678 | ppl 3.81 | bleu 54.42 | wps 1298.2 | wpb 933.5 | bsz 59.6 | num_updates 2592 | best_bleu 54.42
2022-09-08 01:32:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 2592 updates
2022-09-08 01:32:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint33.pt
2022-09-08 01:32:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint33.pt
2022-09-08 01:33:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint33.pt (epoch 33 @ 2592 updates, score 54.42) (writing took 53.72896005958319 seconds)
2022-09-08 01:33:23 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-09-08 01:33:23 | INFO | train | epoch 033 | loss 4.349 | nll_loss 1.437 | mask_loss 6.61828 | p_2 0.03382 | mask_ave 0.524 | ppl 2.71 | wps 2665.4 | ups 0.48 | wpb 5523.2 | bsz 358 | num_updates 2592 | lr 0.000324035 | gnorm 0.803 | train_wall 99 | gb_free 9.3 | wall 5235
2022-09-08 01:33:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 01:33:23 | INFO | fairseq.trainer | begin training epoch 34
2022-09-08 01:33:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 01:33:34 | INFO | train_inner | epoch 034:      8 / 81 loss=4.346, nll_loss=1.434, mask_loss=6.6165, p_2=0.0339, mask_ave=0.526, ppl=2.7, wps=2463.2, ups=0.45, wpb=5494.4, bsz=357.9, num_updates=2600, lr=0.000325035, gnorm=0.793, train_wall=123, gb_free=9.1, wall=5247
2022-09-08 01:35:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 01:35:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 01:35:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 01:35:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 01:35:15 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 4.981 | nll_loss 1.928 | mask_loss 7.16034 | p_2 0.04558 | mask_ave 0.686 | ppl 3.81 | bleu 53.32 | wps 1522.4 | wpb 933.5 | bsz 59.6 | num_updates 2673 | best_bleu 54.42
2022-09-08 01:35:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 2673 updates
2022-09-08 01:35:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint34.pt
2022-09-08 01:35:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint34.pt
2022-09-08 01:35:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint34.pt (epoch 34 @ 2673 updates, score 53.32) (writing took 41.00952244177461 seconds)
2022-09-08 01:35:56 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-09-08 01:35:56 | INFO | train | epoch 034 | loss 4.301 | nll_loss 1.375 | mask_loss 6.65227 | p_2 0.03384 | mask_ave 0.524 | ppl 2.59 | wps 2906.5 | ups 0.53 | wpb 5523.2 | bsz 358 | num_updates 2673 | lr 0.000334158 | gnorm 0.805 | train_wall 100 | gb_free 9.2 | wall 5389
2022-09-08 01:35:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 01:35:57 | INFO | fairseq.trainer | begin training epoch 35
2022-09-08 01:35:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 01:36:31 | INFO | train_inner | epoch 035:     27 / 81 loss=4.289, nll_loss=1.358, mask_loss=6.64408, p_2=0.03413, mask_ave=0.524, ppl=2.56, wps=3120.4, ups=0.57, wpb=5514.1, bsz=357.6, num_updates=2700, lr=0.000337533, gnorm=0.8, train_wall=123, gb_free=9.2, wall=5423
2022-09-08 01:37:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 01:37:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 01:37:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 01:37:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 01:37:49 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 4.94 | nll_loss 1.914 | mask_loss 7.09541 | p_2 0.04554 | mask_ave 0.688 | ppl 3.77 | bleu 53.91 | wps 1457.6 | wpb 933.5 | bsz 59.6 | num_updates 2754 | best_bleu 54.42
2022-09-08 01:37:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 2754 updates
2022-09-08 01:37:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint35.pt
2022-09-08 01:37:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint35.pt
2022-09-08 01:38:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint35.pt (epoch 35 @ 2754 updates, score 53.91) (writing took 21.463020727038383 seconds)
2022-09-08 01:38:10 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-09-08 01:38:10 | INFO | train | epoch 035 | loss 4.247 | nll_loss 1.304 | mask_loss 6.67469 | p_2 0.03405 | mask_ave 0.519 | ppl 2.47 | wps 3341.2 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 2754 | lr 0.000344281 | gnorm 0.773 | train_wall 99 | gb_free 9.1 | wall 5523
2022-09-08 01:38:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 01:38:11 | INFO | fairseq.trainer | begin training epoch 36
2022-09-08 01:38:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 01:39:08 | INFO | train_inner | epoch 036:     46 / 81 loss=4.231, nll_loss=1.283, mask_loss=6.70122, p_2=0.03374, mask_ave=0.515, ppl=2.43, wps=3517.3, ups=0.63, wpb=5549.3, bsz=358, num_updates=2800, lr=0.00035003, gnorm=0.791, train_wall=123, gb_free=9, wall=5581
2022-09-08 01:39:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 01:39:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 01:39:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 01:39:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 01:40:03 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 4.923 | nll_loss 1.915 | mask_loss 7.27147 | p_2 0.04603 | mask_ave 0.676 | ppl 3.77 | bleu 52.76 | wps 1511.2 | wpb 933.5 | bsz 59.6 | num_updates 2835 | best_bleu 54.42
2022-09-08 01:40:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 2835 updates
2022-09-08 01:40:03 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint36.pt
2022-09-08 01:40:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint36.pt
2022-09-08 01:40:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint36.pt (epoch 36 @ 2835 updates, score 52.76) (writing took 19.314383447170258 seconds)
2022-09-08 01:40:23 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-09-08 01:40:23 | INFO | train | epoch 036 | loss 4.216 | nll_loss 1.261 | mask_loss 6.68612 | p_2 0.03409 | mask_ave 0.518 | ppl 2.4 | wps 3378.5 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 2835 | lr 0.000354404 | gnorm 0.806 | train_wall 100 | gb_free 9 | wall 5656
2022-09-08 01:40:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 01:40:23 | INFO | fairseq.trainer | begin training epoch 37
2022-09-08 01:40:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 01:41:45 | INFO | train_inner | epoch 037:     65 / 81 loss=4.173, nll_loss=1.207, mask_loss=6.67286, p_2=0.03445, mask_ave=0.514, ppl=2.31, wps=3524.9, ups=0.64, wpb=5533.5, bsz=360.7, num_updates=2900, lr=0.000362528, gnorm=0.751, train_wall=125, gb_free=9.1, wall=5738
2022-09-08 01:42:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 01:42:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 01:42:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 01:42:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 01:42:16 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 4.95 | nll_loss 1.911 | mask_loss 7.17804 | p_2 0.04631 | mask_ave 0.669 | ppl 3.76 | bleu 54.46 | wps 1442.8 | wpb 933.5 | bsz 59.6 | num_updates 2916 | best_bleu 54.46
2022-09-08 01:42:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 2916 updates
2022-09-08 01:42:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint37.pt
2022-09-08 01:42:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint37.pt
2022-09-08 01:43:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint37.pt (epoch 37 @ 2916 updates, score 54.46) (writing took 66.17949256673455 seconds)
2022-09-08 01:43:23 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-09-08 01:43:23 | INFO | train | epoch 037 | loss 4.151 | nll_loss 1.179 | mask_loss 6.70235 | p_2 0.03436 | mask_ave 0.511 | ppl 2.26 | wps 2485.7 | ups 0.45 | wpb 5523.2 | bsz 358 | num_updates 2916 | lr 0.000364527 | gnorm 0.731 | train_wall 101 | gb_free 9.2 | wall 5836
2022-09-08 01:43:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 01:43:23 | INFO | fairseq.trainer | begin training epoch 38
2022-09-08 01:43:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 01:45:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 01:45:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 01:45:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 01:45:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 01:45:17 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 4.946 | nll_loss 1.916 | mask_loss 7.13429 | p_2 0.04572 | mask_ave 0.685 | ppl 3.77 | bleu 53.63 | wps 1464.9 | wpb 933.5 | bsz 59.6 | num_updates 2997 | best_bleu 54.46
2022-09-08 01:45:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 2997 updates
2022-09-08 01:45:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint38.pt
2022-09-08 01:45:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint38.pt
2022-09-08 01:45:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint38.pt (epoch 38 @ 2997 updates, score 53.63) (writing took 18.853977967053652 seconds)
2022-09-08 01:45:36 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-09-08 01:45:36 | INFO | train | epoch 038 | loss 4.124 | nll_loss 1.142 | mask_loss 6.69374 | p_2 0.03434 | mask_ave 0.511 | ppl 2.21 | wps 3361.9 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 2997 | lr 0.00037465 | gnorm 0.756 | train_wall 101 | gb_free 9 | wall 5969
2022-09-08 01:45:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 01:45:36 | INFO | fairseq.trainer | begin training epoch 39
2022-09-08 01:45:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 01:45:41 | INFO | train_inner | epoch 039:      3 / 81 loss=4.129, nll_loss=1.149, mask_loss=6.71413, p_2=0.03421, mask_ave=0.511, ppl=2.22, wps=2337.4, ups=0.43, wpb=5499.2, bsz=355.1, num_updates=3000, lr=0.000375025, gnorm=0.753, train_wall=124, gb_free=9.1, wall=5973
2022-09-08 01:47:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 01:47:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 01:47:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 01:47:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 01:47:31 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 4.948 | nll_loss 1.928 | mask_loss 7.35021 | p_2 0.04553 | mask_ave 0.689 | ppl 3.8 | bleu 53.79 | wps 1490.2 | wpb 933.5 | bsz 59.6 | num_updates 3078 | best_bleu 54.46
2022-09-08 01:47:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 3078 updates
2022-09-08 01:47:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint39.pt
2022-09-08 01:47:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint39.pt
2022-09-08 01:47:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint39.pt (epoch 39 @ 3078 updates, score 53.79) (writing took 14.518719423562288 seconds)
2022-09-08 01:47:47 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-09-08 01:47:47 | INFO | train | epoch 039 | loss 4.085 | nll_loss 1.092 | mask_loss 6.717 | p_2 0.03448 | mask_ave 0.508 | ppl 2.13 | wps 3422.9 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 3078 | lr 0.000384773 | gnorm 0.75 | train_wall 102 | gb_free 9.2 | wall 6099
2022-09-08 01:47:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 01:47:47 | INFO | fairseq.trainer | begin training epoch 40
2022-09-08 01:47:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 01:48:15 | INFO | train_inner | epoch 040:     22 / 81 loss=4.079, nll_loss=1.083, mask_loss=6.72704, p_2=0.03431, mask_ave=0.507, ppl=2.12, wps=3595.7, ups=0.65, wpb=5542.2, bsz=357.4, num_updates=3100, lr=0.000387523, gnorm=0.755, train_wall=126, gb_free=9.2, wall=6128
2022-09-08 01:49:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 01:49:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 01:49:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 01:49:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 01:49:40 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 4.94 | nll_loss 1.938 | mask_loss 7.45855 | p_2 0.04638 | mask_ave 0.667 | ppl 3.83 | bleu 53.76 | wps 1444.2 | wpb 933.5 | bsz 59.6 | num_updates 3159 | best_bleu 54.46
2022-09-08 01:49:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 3159 updates
2022-09-08 01:49:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint40.pt
2022-09-08 01:49:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint40.pt
2022-09-08 01:50:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint40.pt (epoch 40 @ 3159 updates, score 53.76) (writing took 20.009436901658773 seconds)
2022-09-08 01:50:00 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-09-08 01:50:00 | INFO | train | epoch 040 | loss 4.047 | nll_loss 1.043 | mask_loss 6.75524 | p_2 0.03445 | mask_ave 0.509 | ppl 2.06 | wps 3341.8 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 3159 | lr 0.000394896 | gnorm 0.733 | train_wall 100 | gb_free 9.2 | wall 6233
2022-09-08 01:50:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 01:50:01 | INFO | fairseq.trainer | begin training epoch 41
2022-09-08 01:50:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 01:50:52 | INFO | train_inner | epoch 041:     41 / 81 loss=4.027, nll_loss=1.017, mask_loss=6.75356, p_2=0.03482, mask_ave=0.511, ppl=2.02, wps=3501.6, ups=0.64, wpb=5509.2, bsz=359.7, num_updates=3200, lr=0.00040002, gnorm=0.714, train_wall=124, gb_free=9.1, wall=6285
2022-09-08 01:51:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 01:51:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 01:51:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 01:51:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 01:51:55 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 4.943 | nll_loss 1.937 | mask_loss 7.62401 | p_2 0.04625 | mask_ave 0.67 | ppl 3.83 | bleu 53.3 | wps 1551 | wpb 933.5 | bsz 59.6 | num_updates 3240 | best_bleu 54.46
2022-09-08 01:51:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 3240 updates
2022-09-08 01:51:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint41.pt
2022-09-08 01:51:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint41.pt
2022-09-08 01:52:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint41.pt (epoch 41 @ 3240 updates, score 53.3) (writing took 17.129483319818974 seconds)
2022-09-08 01:52:12 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-09-08 01:52:12 | INFO | train | epoch 041 | loss 4.01 | nll_loss 0.995 | mask_loss 6.79099 | p_2 0.03447 | mask_ave 0.508 | ppl 1.99 | wps 3397.2 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 3240 | lr 0.000405019 | gnorm 0.705 | train_wall 102 | gb_free 9.1 | wall 6365
2022-09-08 01:52:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 01:52:12 | INFO | fairseq.trainer | begin training epoch 42
2022-09-08 01:52:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 01:53:28 | INFO | train_inner | epoch 042:     60 / 81 loss=3.989, nll_loss=0.968, mask_loss=6.82078, p_2=0.03452, mask_ave=0.506, ppl=1.96, wps=3540.9, ups=0.64, wpb=5510.5, bsz=357.8, num_updates=3300, lr=0.000412518, gnorm=0.704, train_wall=126, gb_free=9, wall=6441
2022-09-08 01:53:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 01:53:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 01:53:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 01:53:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 01:54:05 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 4.904 | nll_loss 1.908 | mask_loss 7.43686 | p_2 0.04709 | mask_ave 0.648 | ppl 3.75 | bleu 54.88 | wps 1462.8 | wpb 933.5 | bsz 59.6 | num_updates 3321 | best_bleu 54.88
2022-09-08 01:54:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 3321 updates
2022-09-08 01:54:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint42.pt
2022-09-08 01:54:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint42.pt
2022-09-08 01:54:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint42.pt (epoch 42 @ 3321 updates, score 54.88) (writing took 49.18313627317548 seconds)
2022-09-08 01:54:54 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-09-08 01:54:54 | INFO | train | epoch 042 | loss 3.979 | nll_loss 0.956 | mask_loss 6.82102 | p_2 0.03461 | mask_ave 0.505 | ppl 1.94 | wps 2756.4 | ups 0.5 | wpb 5523.2 | bsz 358 | num_updates 3321 | lr 0.000415142 | gnorm 0.708 | train_wall 100 | gb_free 9.2 | wall 6527
2022-09-08 01:54:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 01:54:55 | INFO | fairseq.trainer | begin training epoch 43
2022-09-08 01:54:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 01:56:33 | INFO | train_inner | epoch 043:     79 / 81 loss=3.96, nll_loss=0.932, mask_loss=6.84353, p_2=0.03458, mask_ave=0.502, ppl=1.91, wps=2989.1, ups=0.54, wpb=5550.8, bsz=359.3, num_updates=3400, lr=0.000425015, gnorm=0.691, train_wall=123, gb_free=9.1, wall=6626
2022-09-08 01:56:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 01:56:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 01:56:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 01:56:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 01:56:47 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 4.924 | nll_loss 1.928 | mask_loss 7.58976 | p_2 0.04612 | mask_ave 0.675 | ppl 3.81 | bleu 53.6 | wps 1440.6 | wpb 933.5 | bsz 59.6 | num_updates 3402 | best_bleu 54.88
2022-09-08 01:56:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 3402 updates
2022-09-08 01:56:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint43.pt
2022-09-08 01:56:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint43.pt
2022-09-08 01:57:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint43.pt (epoch 43 @ 3402 updates, score 53.6) (writing took 60.28278834745288 seconds)
2022-09-08 01:57:48 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-09-08 01:57:48 | INFO | train | epoch 043 | loss 3.946 | nll_loss 0.915 | mask_loss 6.84196 | p_2 0.03472 | mask_ave 0.502 | ppl 1.88 | wps 2581.6 | ups 0.47 | wpb 5523.2 | bsz 358 | num_updates 3402 | lr 0.000425265 | gnorm 0.687 | train_wall 99 | gb_free 9.2 | wall 6701
2022-09-08 01:57:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 01:57:48 | INFO | fairseq.trainer | begin training epoch 44
2022-09-08 01:57:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 01:59:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 01:59:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 01:59:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 01:59:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 01:59:42 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 4.976 | nll_loss 1.972 | mask_loss 7.58334 | p_2 0.04595 | mask_ave 0.679 | ppl 3.92 | bleu 53.72 | wps 1504.4 | wpb 933.5 | bsz 59.6 | num_updates 3483 | best_bleu 54.88
2022-09-08 01:59:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 3483 updates
2022-09-08 01:59:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint44.pt
2022-09-08 01:59:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint44.pt
2022-09-08 02:00:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint44.pt (epoch 44 @ 3483 updates, score 53.72) (writing took 18.648205868899822 seconds)
2022-09-08 02:00:01 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-09-08 02:00:01 | INFO | train | epoch 044 | loss 3.925 | nll_loss 0.887 | mask_loss 6.83299 | p_2 0.03466 | mask_ave 0.504 | ppl 1.85 | wps 3349 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 3483 | lr 0.000435388 | gnorm 0.698 | train_wall 102 | gb_free 9.1 | wall 6834
2022-09-08 02:00:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 02:00:02 | INFO | fairseq.trainer | begin training epoch 45
2022-09-08 02:00:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 02:00:22 | INFO | train_inner | epoch 045:     17 / 81 loss=3.92, nll_loss=0.88, mask_loss=6.80753, p_2=0.03484, mask_ave=0.505, ppl=1.84, wps=2398.4, ups=0.44, wpb=5490.3, bsz=358.3, num_updates=3500, lr=0.000437513, gnorm=0.708, train_wall=124, gb_free=9, wall=6855
2022-09-08 02:01:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 02:01:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 02:01:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 02:01:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 02:01:55 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 4.913 | nll_loss 1.917 | mask_loss 7.65743 | p_2 0.04594 | mask_ave 0.68 | ppl 3.78 | bleu 54.96 | wps 1538.8 | wpb 933.5 | bsz 59.6 | num_updates 3564 | best_bleu 54.96
2022-09-08 02:01:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 3564 updates
2022-09-08 02:01:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint45.pt
2022-09-08 02:01:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint45.pt
2022-09-08 02:03:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint45.pt (epoch 45 @ 3564 updates, score 54.96) (writing took 71.324541952461 seconds)
2022-09-08 02:03:07 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-09-08 02:03:07 | INFO | train | epoch 045 | loss 3.903 | nll_loss 0.86 | mask_loss 6.84138 | p_2 0.03468 | mask_ave 0.503 | ppl 1.82 | wps 2416.2 | ups 0.44 | wpb 5523.2 | bsz 358 | num_updates 3564 | lr 0.000445511 | gnorm 0.689 | train_wall 101 | gb_free 9 | wall 7019
2022-09-08 02:03:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 02:03:07 | INFO | fairseq.trainer | begin training epoch 46
2022-09-08 02:03:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 02:03:52 | INFO | train_inner | epoch 046:     36 / 81 loss=3.887, nll_loss=0.841, mask_loss=6.86114, p_2=0.03463, mask_ave=0.502, ppl=1.79, wps=2641.5, ups=0.48, wpb=5540.3, bsz=359.7, num_updates=3600, lr=0.00045001, gnorm=0.664, train_wall=125, gb_free=9, wall=7065
2022-09-08 02:04:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 02:04:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 02:04:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 02:04:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 02:04:59 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 4.93 | nll_loss 1.937 | mask_loss 7.63767 | p_2 0.04638 | mask_ave 0.668 | ppl 3.83 | bleu 54.74 | wps 1376.7 | wpb 933.5 | bsz 59.6 | num_updates 3645 | best_bleu 54.96
2022-09-08 02:04:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 3645 updates
2022-09-08 02:04:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint46.pt
2022-09-08 02:05:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint46.pt
2022-09-08 02:05:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint46.pt (epoch 46 @ 3645 updates, score 54.74) (writing took 18.386879976838827 seconds)
2022-09-08 02:05:18 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-09-08 02:05:18 | INFO | train | epoch 046 | loss 3.873 | nll_loss 0.824 | mask_loss 6.87035 | p_2 0.03471 | mask_ave 0.502 | ppl 1.77 | wps 3408 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 3645 | lr 0.000455634 | gnorm 0.661 | train_wall 99 | gb_free 9.1 | wall 7151
2022-09-08 02:05:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 02:05:18 | INFO | fairseq.trainer | begin training epoch 47
2022-09-08 02:05:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 02:06:28 | INFO | train_inner | epoch 047:     55 / 81 loss=3.864, nll_loss=0.812, mask_loss=6.89083, p_2=0.03444, mask_ave=0.5, ppl=1.76, wps=3538.2, ups=0.64, wpb=5523, bsz=350.8, num_updates=3700, lr=0.000462508, gnorm=0.658, train_wall=124, gb_free=9.2, wall=7221
2022-09-08 02:07:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 02:07:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 02:07:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 02:07:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 02:07:11 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 4.933 | nll_loss 1.973 | mask_loss 7.77836 | p_2 0.04622 | mask_ave 0.672 | ppl 3.93 | bleu 54.19 | wps 1551.3 | wpb 933.5 | bsz 59.6 | num_updates 3726 | best_bleu 54.96
2022-09-08 02:07:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 3726 updates
2022-09-08 02:07:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint47.pt
2022-09-08 02:07:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint47.pt
2022-09-08 02:07:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint47.pt (epoch 47 @ 3726 updates, score 54.19) (writing took 2.8877013958990574 seconds)
2022-09-08 02:07:14 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-09-08 02:07:14 | INFO | train | epoch 047 | loss 3.85 | nll_loss 0.795 | mask_loss 6.85826 | p_2 0.03473 | mask_ave 0.502 | ppl 1.74 | wps 3838.2 | ups 0.69 | wpb 5523.2 | bsz 358 | num_updates 3726 | lr 0.000465757 | gnorm 0.649 | train_wall 101 | gb_free 9 | wall 7267
2022-09-08 02:07:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 02:07:15 | INFO | fairseq.trainer | begin training epoch 48
2022-09-08 02:07:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 02:08:47 | INFO | train_inner | epoch 048:     74 / 81 loss=3.84, nll_loss=0.784, mask_loss=6.87641, p_2=0.03502, mask_ave=0.502, ppl=1.72, wps=3984, ups=0.72, wpb=5535.8, bsz=365.4, num_updates=3800, lr=0.000475005, gnorm=0.648, train_wall=123, gb_free=9.1, wall=7360
2022-09-08 02:08:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 02:08:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 02:08:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 02:08:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 02:09:06 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 4.918 | nll_loss 1.959 | mask_loss 7.82884 | p_2 0.04684 | mask_ave 0.656 | ppl 3.89 | bleu 54.41 | wps 1492.7 | wpb 933.5 | bsz 59.6 | num_updates 3807 | best_bleu 54.96
2022-09-08 02:09:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 3807 updates
2022-09-08 02:09:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint48.pt
2022-09-08 02:09:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint48.pt
2022-09-08 02:09:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint48.pt (epoch 48 @ 3807 updates, score 54.41) (writing took 18.564219027757645 seconds)
2022-09-08 02:09:25 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-09-08 02:09:25 | INFO | train | epoch 048 | loss 3.835 | nll_loss 0.778 | mask_loss 6.91614 | p_2 0.03479 | mask_ave 0.5 | ppl 1.71 | wps 3426.8 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 3807 | lr 0.00047588 | gnorm 0.647 | train_wall 99 | gb_free 9.2 | wall 7398
2022-09-08 02:09:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 02:09:25 | INFO | fairseq.trainer | begin training epoch 49
2022-09-08 02:09:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 02:11:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 02:11:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 02:11:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 02:11:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 02:11:17 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 4.921 | nll_loss 1.961 | mask_loss 7.73693 | p_2 0.04678 | mask_ave 0.658 | ppl 3.89 | bleu 55.25 | wps 1473.2 | wpb 933.5 | bsz 59.6 | num_updates 3888 | best_bleu 55.25
2022-09-08 02:11:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 3888 updates
2022-09-08 02:11:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint49.pt
2022-09-08 02:11:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint49.pt
2022-09-08 02:12:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint49.pt (epoch 49 @ 3888 updates, score 55.25) (writing took 44.49511178955436 seconds)
2022-09-08 02:12:02 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-09-08 02:12:02 | INFO | train | epoch 049 | loss 3.825 | nll_loss 0.767 | mask_loss 6.93392 | p_2 0.03481 | mask_ave 0.5 | ppl 1.7 | wps 2848.6 | ups 0.52 | wpb 5523.2 | bsz 358 | num_updates 3888 | lr 0.000486003 | gnorm 0.66 | train_wall 99 | gb_free 9.2 | wall 7555
2022-09-08 02:12:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 02:12:02 | INFO | fairseq.trainer | begin training epoch 50
2022-09-08 02:12:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 02:12:17 | INFO | train_inner | epoch 050:     12 / 81 loss=3.82, nll_loss=0.761, mask_loss=6.95825, p_2=0.03462, mask_ave=0.497, ppl=1.7, wps=2624.9, ups=0.48, wpb=5519, bsz=354.6, num_updates=3900, lr=0.000487503, gnorm=0.651, train_wall=121, gb_free=9.1, wall=7570
2022-09-08 02:13:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 02:13:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 02:13:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 02:13:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 02:13:55 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 4.937 | nll_loss 1.972 | mask_loss 7.652 | p_2 0.04638 | mask_ave 0.669 | ppl 3.92 | bleu 55.54 | wps 1376 | wpb 933.5 | bsz 59.6 | num_updates 3969 | best_bleu 55.54
2022-09-08 02:13:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 3969 updates
2022-09-08 02:13:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint50.pt
2022-09-08 02:13:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint50.pt
2022-09-08 02:15:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint50.pt (epoch 50 @ 3969 updates, score 55.54) (writing took 65.2218651138246 seconds)
2022-09-08 02:15:00 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-09-08 02:15:00 | INFO | train | epoch 050 | loss 3.799 | nll_loss 0.736 | mask_loss 6.91135 | p_2 0.0348 | mask_ave 0.5 | ppl 1.67 | wps 2507 | ups 0.45 | wpb 5523.2 | bsz 358 | num_updates 3969 | lr 0.000496126 | gnorm 0.618 | train_wall 99 | gb_free 9.2 | wall 7733
2022-09-08 02:15:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 02:15:01 | INFO | fairseq.trainer | begin training epoch 51
2022-09-08 02:15:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 02:15:38 | INFO | train_inner | epoch 051:     31 / 81 loss=3.797, nll_loss=0.733, mask_loss=6.90351, p_2=0.03509, mask_ave=0.505, ppl=1.66, wps=2733.3, ups=0.5, wpb=5481.2, bsz=355.8, num_updates=4000, lr=0.0005, gnorm=0.622, train_wall=121, gb_free=9.2, wall=7771
2022-09-08 02:16:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 02:16:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 02:16:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 02:16:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 02:16:51 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 4.928 | nll_loss 1.988 | mask_loss 7.63743 | p_2 0.04649 | mask_ave 0.666 | ppl 3.97 | bleu 54.29 | wps 1550.4 | wpb 933.5 | bsz 59.6 | num_updates 4050 | best_bleu 55.54
2022-09-08 02:16:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 4050 updates
2022-09-08 02:16:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint51.pt
2022-09-08 02:16:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint51.pt
2022-09-08 02:17:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint51.pt (epoch 51 @ 4050 updates, score 54.29) (writing took 43.40667434409261 seconds)
2022-09-08 02:17:35 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-09-08 02:17:35 | INFO | train | epoch 051 | loss 3.788 | nll_loss 0.723 | mask_loss 6.92787 | p_2 0.03481 | mask_ave 0.5 | ppl 1.65 | wps 2902.7 | ups 0.53 | wpb 5523.2 | bsz 358 | num_updates 4050 | lr 0.000496904 | gnorm 0.612 | train_wall 98 | gb_free 9.2 | wall 7887
2022-09-08 02:17:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 02:17:35 | INFO | fairseq.trainer | begin training epoch 52
2022-09-08 02:17:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 02:18:38 | INFO | train_inner | epoch 052:     50 / 81 loss=3.777, nll_loss=0.712, mask_loss=6.94191, p_2=0.03464, mask_ave=0.501, ppl=1.64, wps=3068.3, ups=0.55, wpb=5532.5, bsz=357.2, num_updates=4100, lr=0.000493865, gnorm=0.605, train_wall=124, gb_free=9.1, wall=7951
2022-09-08 02:19:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 02:19:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 02:19:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 02:19:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 02:19:29 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 4.937 | nll_loss 1.993 | mask_loss 7.79334 | p_2 0.0462 | mask_ave 0.674 | ppl 3.98 | bleu 54.95 | wps 1355.1 | wpb 933.5 | bsz 59.6 | num_updates 4131 | best_bleu 55.54
2022-09-08 02:19:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 4131 updates
2022-09-08 02:19:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint52.pt
2022-09-08 02:19:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint52.pt
2022-09-08 02:20:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint52.pt (epoch 52 @ 4131 updates, score 54.95) (writing took 45.18462423607707 seconds)
2022-09-08 02:20:15 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-09-08 02:20:15 | INFO | train | epoch 052 | loss 3.767 | nll_loss 0.7 | mask_loss 6.97833 | p_2 0.03474 | mask_ave 0.502 | ppl 1.62 | wps 2795.9 | ups 0.51 | wpb 5523.2 | bsz 358 | num_updates 4131 | lr 0.000492008 | gnorm 0.603 | train_wall 101 | gb_free 9.1 | wall 8047
2022-09-08 02:20:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 02:20:15 | INFO | fairseq.trainer | begin training epoch 53
2022-09-08 02:20:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 02:21:43 | INFO | train_inner | epoch 053:     69 / 81 loss=3.761, nll_loss=0.695, mask_loss=7.03681, p_2=0.03471, mask_ave=0.499, ppl=1.62, wps=2999.7, ups=0.54, wpb=5545.2, bsz=358.6, num_updates=4200, lr=0.00048795, gnorm=0.622, train_wall=125, gb_free=9, wall=8136
2022-09-08 02:21:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 02:21:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 02:21:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 02:21:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 02:22:09 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 4.913 | nll_loss 2.016 | mask_loss 7.86374 | p_2 0.04618 | mask_ave 0.675 | ppl 4.04 | bleu 54.94 | wps 1435.1 | wpb 933.5 | bsz 59.6 | num_updates 4212 | best_bleu 55.54
2022-09-08 02:22:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 4212 updates
2022-09-08 02:22:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint53.pt
2022-09-08 02:22:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint53.pt
2022-09-08 02:22:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint53.pt (epoch 53 @ 4212 updates, score 54.94) (writing took 3.0098540149629116 seconds)
2022-09-08 02:22:13 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-09-08 02:22:13 | INFO | train | epoch 053 | loss 3.754 | nll_loss 0.686 | mask_loss 7.03485 | p_2 0.03483 | mask_ave 0.5 | ppl 1.61 | wps 3789.2 | ups 0.69 | wpb 5523.2 | bsz 358 | num_updates 4212 | lr 0.000487254 | gnorm 0.62 | train_wall 102 | gb_free 9.2 | wall 8166
2022-09-08 02:22:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 02:22:13 | INFO | fairseq.trainer | begin training epoch 54
2022-09-08 02:22:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 02:23:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 02:23:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 02:23:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 02:23:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 02:24:05 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 4.922 | nll_loss 2.011 | mask_loss 7.89151 | p_2 0.04707 | mask_ave 0.65 | ppl 4.03 | bleu 55.53 | wps 1432.2 | wpb 933.5 | bsz 59.6 | num_updates 4293 | best_bleu 55.54
2022-09-08 02:24:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 4293 updates
2022-09-08 02:24:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint54.pt
2022-09-08 02:24:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint54.pt
2022-09-08 02:24:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint54.pt (epoch 54 @ 4293 updates, score 55.53) (writing took 20.629274118691683 seconds)
2022-09-08 02:24:26 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-09-08 02:24:26 | INFO | train | epoch 054 | loss 3.731 | nll_loss 0.66 | mask_loss 7.04125 | p_2 0.03497 | mask_ave 0.496 | ppl 1.58 | wps 3353.2 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 4293 | lr 0.000482636 | gnorm 0.584 | train_wall 99 | gb_free 9.5 | wall 8299
2022-09-08 02:24:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 02:24:26 | INFO | fairseq.trainer | begin training epoch 55
2022-09-08 02:24:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 02:24:36 | INFO | train_inner | epoch 055:      7 / 81 loss=3.731, nll_loss=0.66, mask_loss=7.02637, p_2=0.03504, mask_ave=0.496, ppl=1.58, wps=3192.1, ups=0.58, wpb=5517.9, bsz=360.8, num_updates=4300, lr=0.000482243, gnorm=0.582, train_wall=122, gb_free=9.1, wall=8309
2022-09-08 02:26:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 02:26:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 02:26:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 02:26:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 02:26:17 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 4.896 | nll_loss 1.981 | mask_loss 7.85603 | p_2 0.0467 | mask_ave 0.66 | ppl 3.95 | bleu 55.5 | wps 1517.5 | wpb 933.5 | bsz 59.6 | num_updates 4374 | best_bleu 55.54
2022-09-08 02:26:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 4374 updates
2022-09-08 02:26:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint55.pt
2022-09-08 02:26:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint55.pt
2022-09-08 02:26:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint55.pt (epoch 55 @ 4374 updates, score 55.5) (writing took 23.576417807489634 seconds)
2022-09-08 02:26:41 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-09-08 02:26:41 | INFO | train | epoch 055 | loss 3.715 | nll_loss 0.642 | mask_loss 6.98678 | p_2 0.03494 | mask_ave 0.497 | ppl 1.56 | wps 3313.7 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 4374 | lr 0.000478146 | gnorm 0.566 | train_wall 98 | gb_free 9 | wall 8434
2022-09-08 02:26:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 02:26:41 | INFO | fairseq.trainer | begin training epoch 56
2022-09-08 02:26:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 02:27:15 | INFO | train_inner | epoch 056:     26 / 81 loss=3.709, nll_loss=0.635, mask_loss=6.99137, p_2=0.03477, mask_ave=0.497, ppl=1.55, wps=3482.4, ups=0.63, wpb=5540.2, bsz=356.7, num_updates=4400, lr=0.000476731, gnorm=0.559, train_wall=123, gb_free=9.2, wall=8468
2022-09-08 02:28:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 02:28:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 02:28:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 02:28:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 02:28:34 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 4.894 | nll_loss 2.003 | mask_loss 7.84948 | p_2 0.04673 | mask_ave 0.66 | ppl 4.01 | bleu 54.81 | wps 1553.5 | wpb 933.5 | bsz 59.6 | num_updates 4455 | best_bleu 55.54
2022-09-08 02:28:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 4455 updates
2022-09-08 02:28:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint56.pt
2022-09-08 02:28:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint56.pt
2022-09-08 02:29:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint56.pt (epoch 56 @ 4455 updates, score 54.81) (writing took 28.08325705677271 seconds)
2022-09-08 02:29:02 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-09-08 02:29:02 | INFO | train | epoch 056 | loss 3.698 | nll_loss 0.625 | mask_loss 7.03284 | p_2 0.03492 | mask_ave 0.498 | ppl 1.54 | wps 3165.6 | ups 0.57 | wpb 5523.2 | bsz 358 | num_updates 4455 | lr 0.000473779 | gnorm 0.556 | train_wall 101 | gb_free 9.1 | wall 8575
2022-09-08 02:29:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 02:29:03 | INFO | fairseq.trainer | begin training epoch 57
2022-09-08 02:29:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 02:30:00 | INFO | train_inner | epoch 057:     45 / 81 loss=3.694, nll_loss=0.621, mask_loss=7.08551, p_2=0.03484, mask_ave=0.496, ppl=1.54, wps=3350, ups=0.61, wpb=5510.5, bsz=355.3, num_updates=4500, lr=0.000471405, gnorm=0.552, train_wall=124, gb_free=9.1, wall=8632
2022-09-08 02:30:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 02:30:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 02:30:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 02:30:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 02:30:56 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 4.912 | nll_loss 2.041 | mask_loss 7.96952 | p_2 0.04637 | mask_ave 0.669 | ppl 4.12 | bleu 55.57 | wps 1476.5 | wpb 933.5 | bsz 59.6 | num_updates 4536 | best_bleu 55.57
2022-09-08 02:30:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 4536 updates
2022-09-08 02:30:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint57.pt
2022-09-08 02:30:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint57.pt
2022-09-08 02:31:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint57.pt (epoch 57 @ 4536 updates, score 55.57) (writing took 44.32976108044386 seconds)
2022-09-08 02:31:40 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-09-08 02:31:40 | INFO | train | epoch 057 | loss 3.687 | nll_loss 0.612 | mask_loss 7.05426 | p_2 0.03496 | mask_ave 0.496 | ppl 1.53 | wps 2836.5 | ups 0.51 | wpb 5523.2 | bsz 358 | num_updates 4536 | lr 0.00046953 | gnorm 0.543 | train_wall 100 | gb_free 9.1 | wall 8733
2022-09-08 02:31:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 02:31:40 | INFO | fairseq.trainer | begin training epoch 58
2022-09-08 02:31:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 02:33:01 | INFO | train_inner | epoch 058:     64 / 81 loss=3.676, nll_loss=0.601, mask_loss=7.05171, p_2=0.0351, mask_ave=0.497, ppl=1.52, wps=3060, ups=0.55, wpb=5542.9, bsz=363.5, num_updates=4600, lr=0.000466252, gnorm=0.54, train_wall=123, gb_free=9, wall=8814
2022-09-08 02:33:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 02:33:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 02:33:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 02:33:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 02:33:32 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 4.921 | nll_loss 2.043 | mask_loss 8.16995 | p_2 0.04686 | mask_ave 0.657 | ppl 4.12 | bleu 55.45 | wps 1491.8 | wpb 933.5 | bsz 59.6 | num_updates 4617 | best_bleu 55.57
2022-09-08 02:33:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 4617 updates
2022-09-08 02:33:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint58.pt
2022-09-08 02:33:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint58.pt
2022-09-08 02:34:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint58.pt (epoch 58 @ 4617 updates, score 55.45) (writing took 41.56518046185374 seconds)
2022-09-08 02:34:14 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-09-08 02:34:14 | INFO | train | epoch 058 | loss 3.673 | nll_loss 0.599 | mask_loss 7.09448 | p_2 0.03494 | mask_ave 0.497 | ppl 1.51 | wps 2903.3 | ups 0.53 | wpb 5523.2 | bsz 358 | num_updates 4617 | lr 0.000465393 | gnorm 0.546 | train_wall 99 | gb_free 9.3 | wall 8887
2022-09-08 02:34:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 02:34:14 | INFO | fairseq.trainer | begin training epoch 59
2022-09-08 02:34:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 02:35:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 02:36:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 02:36:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 02:36:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 02:36:09 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 4.91 | nll_loss 2.052 | mask_loss 7.94904 | p_2 0.04711 | mask_ave 0.65 | ppl 4.15 | bleu 55.72 | wps 1559 | wpb 933.5 | bsz 59.6 | num_updates 4698 | best_bleu 55.72
2022-09-08 02:36:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 4698 updates
2022-09-08 02:36:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint59.pt
2022-09-08 02:36:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint59.pt
2022-09-08 02:37:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint59.pt (epoch 59 @ 4698 updates, score 55.72) (writing took 75.01130845025182 seconds)
2022-09-08 02:37:25 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-09-08 02:37:25 | INFO | train | epoch 059 | loss 3.666 | nll_loss 0.591 | mask_loss 7.09488 | p_2 0.03497 | mask_ave 0.496 | ppl 1.51 | wps 2349.6 | ups 0.43 | wpb 5523.2 | bsz 358 | num_updates 4698 | lr 0.000461364 | gnorm 0.544 | train_wall 103 | gb_free 9.1 | wall 9078
2022-09-08 02:37:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 02:37:25 | INFO | fairseq.trainer | begin training epoch 60
2022-09-08 02:37:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 02:37:28 | INFO | train_inner | epoch 060:      2 / 81 loss=3.672, nll_loss=0.598, mask_loss=7.08995, p_2=0.03509, mask_ave=0.498, ppl=1.51, wps=2053.7, ups=0.37, wpb=5487.2, bsz=353.8, num_updates=4700, lr=0.000461266, gnorm=0.554, train_wall=125, gb_free=9.1, wall=9081
2022-09-08 02:39:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 02:39:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 02:39:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 02:39:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 02:39:16 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 4.906 | nll_loss 2.064 | mask_loss 7.89496 | p_2 0.04695 | mask_ave 0.654 | ppl 4.18 | bleu 56.2 | wps 1469.7 | wpb 933.5 | bsz 59.6 | num_updates 4779 | best_bleu 56.2
2022-09-08 02:39:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 4779 updates
2022-09-08 02:39:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint60.pt
2022-09-08 02:39:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint60.pt
2022-09-08 02:39:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint60.pt (epoch 60 @ 4779 updates, score 56.2) (writing took 15.356463104486465 seconds)
2022-09-08 02:39:31 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-09-08 02:39:31 | INFO | train | epoch 060 | loss 3.648 | nll_loss 0.572 | mask_loss 7.14131 | p_2 0.03501 | mask_ave 0.496 | ppl 1.49 | wps 3530.7 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 4779 | lr 0.000457437 | gnorm 0.512 | train_wall 98 | gb_free 9.1 | wall 9204
2022-09-08 02:39:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 02:39:32 | INFO | fairseq.trainer | begin training epoch 61
2022-09-08 02:39:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 02:39:58 | INFO | train_inner | epoch 061:     21 / 81 loss=3.645, nll_loss=0.568, mask_loss=7.13379, p_2=0.03478, mask_ave=0.494, ppl=1.48, wps=3715, ups=0.67, wpb=5556.7, bsz=358.6, num_updates=4800, lr=0.000456435, gnorm=0.506, train_wall=121, gb_free=9, wall=9230
2022-09-08 02:41:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 02:41:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 02:41:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 02:41:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 02:41:22 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 4.9 | nll_loss 2.059 | mask_loss 8.03121 | p_2 0.04669 | mask_ave 0.661 | ppl 4.17 | bleu 54.74 | wps 1543.6 | wpb 933.5 | bsz 59.6 | num_updates 4860 | best_bleu 56.2
2022-09-08 02:41:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 4860 updates
2022-09-08 02:41:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint61.pt
2022-09-08 02:41:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint61.pt
2022-09-08 02:41:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint61.pt (epoch 61 @ 4860 updates, score 54.74) (writing took 17.561172284185886 seconds)
2022-09-08 02:41:40 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-09-08 02:41:40 | INFO | train | epoch 061 | loss 3.635 | nll_loss 0.558 | mask_loss 7.13574 | p_2 0.03492 | mask_ave 0.498 | ppl 1.47 | wps 3479.9 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 4860 | lr 0.000453609 | gnorm 0.503 | train_wall 98 | gb_free 9 | wall 9333
2022-09-08 02:41:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 02:41:40 | INFO | fairseq.trainer | begin training epoch 62
2022-09-08 02:41:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 02:42:30 | INFO | train_inner | epoch 062:     40 / 81 loss=3.631, nll_loss=0.555, mask_loss=7.16427, p_2=0.03506, mask_ave=0.499, ppl=1.47, wps=3607.7, ups=0.66, wpb=5507.3, bsz=357.2, num_updates=4900, lr=0.000451754, gnorm=0.506, train_wall=122, gb_free=9.1, wall=9383
2022-09-08 02:43:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 02:43:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 02:43:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 02:43:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 02:43:32 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 4.928 | nll_loss 2.084 | mask_loss 8.01697 | p_2 0.04665 | mask_ave 0.663 | ppl 4.24 | bleu 54.89 | wps 1571.5 | wpb 933.5 | bsz 59.6 | num_updates 4941 | best_bleu 56.2
2022-09-08 02:43:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 4941 updates
2022-09-08 02:43:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint62.pt
2022-09-08 02:43:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint62.pt
2022-09-08 02:43:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint62.pt (epoch 62 @ 4941 updates, score 54.89) (writing took 26.349778793752193 seconds)
2022-09-08 02:43:58 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-09-08 02:43:58 | INFO | train | epoch 062 | loss 3.63 | nll_loss 0.554 | mask_loss 7.16487 | p_2 0.035 | mask_ave 0.496 | ppl 1.47 | wps 3234.3 | ups 0.59 | wpb 5523.2 | bsz 358 | num_updates 4941 | lr 0.000449876 | gnorm 0.509 | train_wall 100 | gb_free 9 | wall 9471
2022-09-08 02:43:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 02:43:58 | INFO | fairseq.trainer | begin training epoch 63
2022-09-08 02:43:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 02:45:13 | INFO | train_inner | epoch 063:     59 / 81 loss=3.626, nll_loss=0.551, mask_loss=7.12185, p_2=0.035, mask_ave=0.496, ppl=1.47, wps=3410.4, ups=0.62, wpb=5541.6, bsz=360.7, num_updates=5000, lr=0.000447214, gnorm=0.507, train_wall=123, gb_free=9, wall=9546
2022-09-08 02:45:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 02:45:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 02:45:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 02:45:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 02:45:51 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 4.925 | nll_loss 2.09 | mask_loss 7.94531 | p_2 0.0466 | mask_ave 0.662 | ppl 4.26 | bleu 55.48 | wps 1525.2 | wpb 933.5 | bsz 59.6 | num_updates 5022 | best_bleu 56.2
2022-09-08 02:45:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 5022 updates
2022-09-08 02:45:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint63.pt
2022-09-08 02:45:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint63.pt
2022-09-08 02:46:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint63.pt (epoch 63 @ 5022 updates, score 55.48) (writing took 27.449304949492216 seconds)
2022-09-08 02:46:18 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-09-08 02:46:18 | INFO | train | epoch 063 | loss 3.623 | nll_loss 0.548 | mask_loss 7.09795 | p_2 0.03495 | mask_ave 0.497 | ppl 1.46 | wps 3193.7 | ups 0.58 | wpb 5523.2 | bsz 358 | num_updates 5022 | lr 0.000446233 | gnorm 0.506 | train_wall 100 | gb_free 9.1 | wall 9611
2022-09-08 02:46:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 02:46:19 | INFO | fairseq.trainer | begin training epoch 64
2022-09-08 02:46:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 02:47:55 | INFO | train_inner | epoch 064:     78 / 81 loss=3.614, nll_loss=0.539, mask_loss=7.08647, p_2=0.03506, mask_ave=0.496, ppl=1.45, wps=3413, ups=0.62, wpb=5530.4, bsz=360.2, num_updates=5100, lr=0.000442807, gnorm=0.492, train_wall=122, gb_free=9.1, wall=9708
2022-09-08 02:47:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 02:47:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 02:47:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 02:47:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 02:48:09 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 4.913 | nll_loss 2.078 | mask_loss 8.17036 | p_2 0.0467 | mask_ave 0.662 | ppl 4.22 | bleu 55.18 | wps 1538.7 | wpb 933.5 | bsz 59.6 | num_updates 5103 | best_bleu 56.2
2022-09-08 02:48:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 5103 updates
2022-09-08 02:48:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint64.pt
2022-09-08 02:48:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint64.pt
2022-09-08 02:48:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint64.pt (epoch 64 @ 5103 updates, score 55.18) (writing took 25.852941557765007 seconds)
2022-09-08 02:48:35 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-09-08 02:48:35 | INFO | train | epoch 064 | loss 3.611 | nll_loss 0.535 | mask_loss 7.10734 | p_2 0.03501 | mask_ave 0.496 | ppl 1.45 | wps 3285 | ups 0.59 | wpb 5523.2 | bsz 358 | num_updates 5103 | lr 0.000442677 | gnorm 0.489 | train_wall 98 | gb_free 9.2 | wall 9747
2022-09-08 02:48:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 02:48:35 | INFO | fairseq.trainer | begin training epoch 65
2022-09-08 02:48:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 02:50:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 02:50:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 02:50:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 02:50:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 02:50:27 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 4.918 | nll_loss 2.089 | mask_loss 7.92193 | p_2 0.0467 | mask_ave 0.661 | ppl 4.25 | bleu 56.35 | wps 1514.8 | wpb 933.5 | bsz 59.6 | num_updates 5184 | best_bleu 56.35
2022-09-08 02:50:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 5184 updates
2022-09-08 02:50:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint65.pt
2022-09-08 02:50:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint65.pt
2022-09-08 02:51:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint65.pt (epoch 65 @ 5184 updates, score 56.35) (writing took 81.92901421710849 seconds)
2022-09-08 02:51:50 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-09-08 02:51:50 | INFO | train | epoch 065 | loss 3.603 | nll_loss 0.528 | mask_loss 7.17526 | p_2 0.03502 | mask_ave 0.496 | ppl 1.44 | wps 2294.1 | ups 0.42 | wpb 5523.2 | bsz 358 | num_updates 5184 | lr 0.000439205 | gnorm 0.491 | train_wall 100 | gb_free 9.1 | wall 9942
2022-09-08 02:51:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 02:51:50 | INFO | fairseq.trainer | begin training epoch 66
2022-09-08 02:51:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 02:52:09 | INFO | train_inner | epoch 066:     16 / 81 loss=3.604, nll_loss=0.528, mask_loss=7.18935, p_2=0.03494, mask_ave=0.497, ppl=1.44, wps=2153.6, ups=0.39, wpb=5478.5, bsz=352.4, num_updates=5200, lr=0.000438529, gnorm=0.494, train_wall=121, gb_free=9.1, wall=9962
2022-09-08 02:53:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 02:53:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 02:53:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 02:53:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 02:53:41 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 4.916 | nll_loss 2.083 | mask_loss 8.08295 | p_2 0.04688 | mask_ave 0.655 | ppl 4.24 | bleu 56.2 | wps 1456.4 | wpb 933.5 | bsz 59.6 | num_updates 5265 | best_bleu 56.35
2022-09-08 02:53:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 5265 updates
2022-09-08 02:53:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint66.pt
2022-09-08 02:53:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint66.pt
2022-09-08 02:53:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint66.pt (epoch 66 @ 5265 updates, score 56.2) (writing took 15.874765563756227 seconds)
2022-09-08 02:53:57 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-09-08 02:53:57 | INFO | train | epoch 066 | loss 3.595 | nll_loss 0.52 | mask_loss 7.18098 | p_2 0.03505 | mask_ave 0.495 | ppl 1.43 | wps 3511.6 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 5265 | lr 0.000435814 | gnorm 0.481 | train_wall 98 | gb_free 9.2 | wall 10070
2022-09-08 02:53:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 02:53:57 | INFO | fairseq.trainer | begin training epoch 67
2022-09-08 02:53:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 02:54:42 | INFO | train_inner | epoch 067:     35 / 81 loss=3.591, nll_loss=0.514, mask_loss=7.22722, p_2=0.03492, mask_ave=0.494, ppl=1.43, wps=3639.5, ups=0.66, wpb=5551.7, bsz=356.6, num_updates=5300, lr=0.000434372, gnorm=0.471, train_wall=123, gb_free=9, wall=10115
2022-09-08 02:55:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 02:55:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 02:55:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 02:55:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 02:55:50 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 4.903 | nll_loss 2.086 | mask_loss 8.10583 | p_2 0.04753 | mask_ave 0.639 | ppl 4.25 | bleu 56.58 | wps 1535.4 | wpb 933.5 | bsz 59.6 | num_updates 5346 | best_bleu 56.58
2022-09-08 02:55:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 5346 updates
2022-09-08 02:55:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint67.pt
2022-09-08 02:55:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint67.pt
2022-09-08 02:56:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint67.pt (epoch 67 @ 5346 updates, score 56.58) (writing took 45.598041485995054 seconds)
2022-09-08 02:56:36 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-09-08 02:56:36 | INFO | train | epoch 067 | loss 3.586 | nll_loss 0.51 | mask_loss 7.22638 | p_2 0.035 | mask_ave 0.496 | ppl 1.42 | wps 2817.3 | ups 0.51 | wpb 5523.2 | bsz 358 | num_updates 5346 | lr 0.000432499 | gnorm 0.465 | train_wall 100 | gb_free 9.2 | wall 10229
2022-09-08 02:56:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 02:56:36 | INFO | fairseq.trainer | begin training epoch 68
2022-09-08 02:56:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 02:57:43 | INFO | train_inner | epoch 068:     54 / 81 loss=3.583, nll_loss=0.509, mask_loss=7.12699, p_2=0.03561, mask_ave=0.496, ppl=1.42, wps=3031.8, ups=0.55, wpb=5506.3, bsz=367.8, num_updates=5400, lr=0.000430331, gnorm=0.471, train_wall=123, gb_free=9.1, wall=10296
2022-09-08 02:58:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 02:58:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 02:58:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 02:58:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 02:58:27 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 4.908 | nll_loss 2.091 | mask_loss 8.08638 | p_2 0.04715 | mask_ave 0.649 | ppl 4.26 | bleu 55.83 | wps 1434.3 | wpb 933.5 | bsz 59.6 | num_updates 5427 | best_bleu 56.58
2022-09-08 02:58:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 5427 updates
2022-09-08 02:58:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint68.pt
2022-09-08 02:58:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint68.pt
2022-09-08 02:58:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint68.pt (epoch 68 @ 5427 updates, score 55.83) (writing took 21.053328096866608 seconds)
2022-09-08 02:58:49 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-09-08 02:58:49 | INFO | train | epoch 068 | loss 3.585 | nll_loss 0.512 | mask_loss 7.16746 | p_2 0.03511 | mask_ave 0.493 | ppl 1.43 | wps 3364.4 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 5427 | lr 0.00042926 | gnorm 0.476 | train_wall 98 | gb_free 9.2 | wall 10362
2022-09-08 02:58:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 02:58:49 | INFO | fairseq.trainer | begin training epoch 69
2022-09-08 02:58:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 03:00:20 | INFO | train_inner | epoch 069:     73 / 81 loss=3.579, nll_loss=0.505, mask_loss=7.19167, p_2=0.03472, mask_ave=0.492, ppl=1.42, wps=3534.1, ups=0.64, wpb=5551.3, bsz=355.5, num_updates=5500, lr=0.000426401, gnorm=0.465, train_wall=122, gb_free=9.2, wall=10453
2022-09-08 03:00:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 03:00:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 03:00:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 03:00:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 03:00:41 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 4.91 | nll_loss 2.083 | mask_loss 7.97628 | p_2 0.04763 | mask_ave 0.635 | ppl 4.24 | bleu 56.56 | wps 1475.1 | wpb 933.5 | bsz 59.6 | num_updates 5508 | best_bleu 56.58
2022-09-08 03:00:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 5508 updates
2022-09-08 03:00:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint69.pt
2022-09-08 03:00:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint69.pt
2022-09-08 03:01:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint69.pt (epoch 69 @ 5508 updates, score 56.56) (writing took 20.61758331209421 seconds)
2022-09-08 03:01:02 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-09-08 03:01:02 | INFO | train | epoch 069 | loss 3.575 | nll_loss 0.501 | mask_loss 7.15975 | p_2 0.03506 | mask_ave 0.494 | ppl 1.42 | wps 3353.1 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 5508 | lr 0.000426092 | gnorm 0.465 | train_wall 99 | gb_free 9.1 | wall 10495
2022-09-08 03:01:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 03:01:02 | INFO | fairseq.trainer | begin training epoch 70
2022-09-08 03:01:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 03:02:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 03:02:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 03:02:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 03:02:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 03:02:54 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 4.917 | nll_loss 2.12 | mask_loss 8.09937 | p_2 0.0473 | mask_ave 0.644 | ppl 4.35 | bleu 55.96 | wps 1450.9 | wpb 933.5 | bsz 59.6 | num_updates 5589 | best_bleu 56.58
2022-09-08 03:02:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 5589 updates
2022-09-08 03:02:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint70.pt
2022-09-08 03:02:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint70.pt
2022-09-08 03:03:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint70.pt (epoch 70 @ 5589 updates, score 55.96) (writing took 19.82962155714631 seconds)
2022-09-08 03:03:14 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-09-08 03:03:14 | INFO | train | epoch 070 | loss 3.57 | nll_loss 0.495 | mask_loss 7.16231 | p_2 0.03523 | mask_ave 0.489 | ppl 1.41 | wps 3403.3 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 5589 | lr 0.000422993 | gnorm 0.458 | train_wall 98 | gb_free 9.1 | wall 10627
2022-09-08 03:03:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 03:03:14 | INFO | fairseq.trainer | begin training epoch 71
2022-09-08 03:03:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 03:03:27 | INFO | train_inner | epoch 071:     11 / 81 loss=3.57, nll_loss=0.496, mask_loss=7.16412, p_2=0.03521, mask_ave=0.49, ppl=1.41, wps=2949.9, ups=0.54, wpb=5505.7, bsz=357, num_updates=5600, lr=0.000422577, gnorm=0.461, train_wall=120, gb_free=9, wall=10640
2022-09-08 03:04:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 03:04:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 03:04:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 03:04:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 03:05:05 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 4.926 | nll_loss 2.131 | mask_loss 8.11191 | p_2 0.04693 | mask_ave 0.656 | ppl 4.38 | bleu 56.12 | wps 1454.6 | wpb 933.5 | bsz 59.6 | num_updates 5670 | best_bleu 56.58
2022-09-08 03:05:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 5670 updates
2022-09-08 03:05:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint71.pt
2022-09-08 03:05:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint71.pt
2022-09-08 03:05:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint71.pt (epoch 71 @ 5670 updates, score 56.12) (writing took 19.138349380344152 seconds)
2022-09-08 03:05:24 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-09-08 03:05:24 | INFO | train | epoch 071 | loss 3.563 | nll_loss 0.488 | mask_loss 7.2671 | p_2 0.03512 | mask_ave 0.493 | ppl 1.4 | wps 3432.4 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 5670 | lr 0.000419961 | gnorm 0.447 | train_wall 98 | gb_free 9 | wall 10757
2022-09-08 03:05:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 03:05:24 | INFO | fairseq.trainer | begin training epoch 72
2022-09-08 03:05:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 03:06:01 | INFO | train_inner | epoch 072:     30 / 81 loss=3.561, nll_loss=0.486, mask_loss=7.2627, p_2=0.03526, mask_ave=0.495, ppl=1.4, wps=3588.6, ups=0.65, wpb=5517.7, bsz=355, num_updates=5700, lr=0.000418854, gnorm=0.443, train_wall=121, gb_free=9.1, wall=10794
2022-09-08 03:07:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 03:07:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 03:07:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 03:07:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 03:07:16 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 4.925 | nll_loss 2.124 | mask_loss 8.11682 | p_2 0.04696 | mask_ave 0.656 | ppl 4.36 | bleu 55.4 | wps 1438.8 | wpb 933.5 | bsz 59.6 | num_updates 5751 | best_bleu 56.58
2022-09-08 03:07:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 5751 updates
2022-09-08 03:07:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint72.pt
2022-09-08 03:07:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint72.pt
2022-09-08 03:07:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint72.pt (epoch 72 @ 5751 updates, score 55.4) (writing took 18.716693874448538 seconds)
2022-09-08 03:07:35 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-09-08 03:07:35 | INFO | train | epoch 072 | loss 3.558 | nll_loss 0.484 | mask_loss 7.25469 | p_2 0.0351 | mask_ave 0.494 | ppl 1.4 | wps 3427.1 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 5751 | lr 0.000416993 | gnorm 0.446 | train_wall 98 | gb_free 9.1 | wall 10887
2022-09-08 03:07:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 03:07:35 | INFO | fairseq.trainer | begin training epoch 73
2022-09-08 03:07:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 03:08:34 | INFO | train_inner | epoch 073:     49 / 81 loss=3.555, nll_loss=0.481, mask_loss=7.24502, p_2=0.03486, mask_ave=0.492, ppl=1.4, wps=3605.4, ups=0.65, wpb=5539.5, bsz=360.3, num_updates=5800, lr=0.000415227, gnorm=0.445, train_wall=121, gb_free=9.1, wall=10947
2022-09-08 03:09:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 03:09:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 03:09:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 03:09:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 03:09:24 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 4.906 | nll_loss 2.112 | mask_loss 8.05362 | p_2 0.04715 | mask_ave 0.649 | ppl 4.32 | bleu 56.13 | wps 1484.6 | wpb 933.5 | bsz 59.6 | num_updates 5832 | best_bleu 56.58
2022-09-08 03:09:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 5832 updates
2022-09-08 03:09:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint73.pt
2022-09-08 03:09:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint73.pt
2022-09-08 03:09:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint73.pt (epoch 73 @ 5832 updates, score 56.13) (writing took 21.640773478895426 seconds)
2022-09-08 03:09:46 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-09-08 03:09:46 | INFO | train | epoch 073 | loss 3.553 | nll_loss 0.48 | mask_loss 7.2262 | p_2 0.03502 | mask_ave 0.495 | ppl 1.4 | wps 3400.8 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 5832 | lr 0.000414087 | gnorm 0.447 | train_wall 97 | gb_free 9.2 | wall 11019
2022-09-08 03:09:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 03:09:46 | INFO | fairseq.trainer | begin training epoch 74
2022-09-08 03:09:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 03:11:12 | INFO | train_inner | epoch 074:     68 / 81 loss=3.551, nll_loss=0.479, mask_loss=7.23586, p_2=0.03513, mask_ave=0.493, ppl=1.39, wps=3513.7, ups=0.64, wpb=5525.6, bsz=358.5, num_updates=5900, lr=0.000411693, gnorm=0.448, train_wall=122, gb_free=9.2, wall=11105
2022-09-08 03:11:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 03:11:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 03:11:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 03:11:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 03:11:40 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 4.954 | nll_loss 2.157 | mask_loss 8.19758 | p_2 0.04726 | mask_ave 0.646 | ppl 4.46 | bleu 56.26 | wps 1429.3 | wpb 933.5 | bsz 59.6 | num_updates 5913 | best_bleu 56.58
2022-09-08 03:11:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 5913 updates
2022-09-08 03:11:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint74.pt
2022-09-08 03:11:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint74.pt
2022-09-08 03:12:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint74.pt (epoch 74 @ 5913 updates, score 56.26) (writing took 22.66074788197875 seconds)
2022-09-08 03:12:03 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-09-08 03:12:03 | INFO | train | epoch 074 | loss 3.55 | nll_loss 0.478 | mask_loss 7.23806 | p_2 0.03518 | mask_ave 0.491 | ppl 1.39 | wps 3277.6 | ups 0.59 | wpb 5523.2 | bsz 358 | num_updates 5913 | lr 0.000411241 | gnorm 0.453 | train_wall 100 | gb_free 9.2 | wall 11155
2022-09-08 03:12:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 03:12:03 | INFO | fairseq.trainer | begin training epoch 75
2022-09-08 03:12:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 03:13:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 03:13:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 03:13:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 03:13:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 03:13:52 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 4.913 | nll_loss 2.114 | mask_loss 8.07039 | p_2 0.04709 | mask_ave 0.653 | ppl 4.33 | bleu 55.73 | wps 1482.1 | wpb 933.5 | bsz 59.6 | num_updates 5994 | best_bleu 56.58
2022-09-08 03:13:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 5994 updates
2022-09-08 03:13:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint75.pt
2022-09-08 03:13:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint75.pt
2022-09-08 03:14:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint75.pt (epoch 75 @ 5994 updates, score 55.73) (writing took 20.87490262836218 seconds)
2022-09-08 03:14:13 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-09-08 03:14:14 | INFO | train | epoch 075 | loss 3.547 | nll_loss 0.475 | mask_loss 7.22724 | p_2 0.03514 | mask_ave 0.493 | ppl 1.39 | wps 3417.9 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 5994 | lr 0.000408453 | gnorm 0.444 | train_wall 97 | gb_free 9.1 | wall 11286
2022-09-08 03:14:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 03:14:14 | INFO | fairseq.trainer | begin training epoch 76
2022-09-08 03:14:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 03:14:22 | INFO | train_inner | epoch 076:      6 / 81 loss=3.547, nll_loss=0.475, mask_loss=7.22213, p_2=0.03524, mask_ave=0.492, ppl=1.39, wps=2892.2, ups=0.53, wpb=5507.5, bsz=358.6, num_updates=6000, lr=0.000408248, gnorm=0.452, train_wall=120, gb_free=9, wall=11295
2022-09-08 03:15:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 03:15:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 03:15:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 03:15:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 03:16:05 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 4.91 | nll_loss 2.126 | mask_loss 8.00702 | p_2 0.04726 | mask_ave 0.646 | ppl 4.37 | bleu 55.77 | wps 1485.5 | wpb 933.5 | bsz 59.6 | num_updates 6075 | best_bleu 56.58
2022-09-08 03:16:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 6075 updates
2022-09-08 03:16:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint76.pt
2022-09-08 03:16:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint76.pt
2022-09-08 03:16:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint76.pt (epoch 76 @ 6075 updates, score 55.77) (writing took 19.558923561125994 seconds)
2022-09-08 03:16:25 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-09-08 03:16:25 | INFO | train | epoch 076 | loss 3.539 | nll_loss 0.467 | mask_loss 7.26805 | p_2 0.03517 | mask_ave 0.492 | ppl 1.38 | wps 3403.4 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 6075 | lr 0.00040572 | gnorm 0.433 | train_wall 98 | gb_free 9.1 | wall 11418
2022-09-08 03:16:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 03:16:25 | INFO | fairseq.trainer | begin training epoch 77
2022-09-08 03:16:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 03:16:57 | INFO | train_inner | epoch 077:     25 / 81 loss=3.54, nll_loss=0.468, mask_loss=7.2677, p_2=0.03521, mask_ave=0.495, ppl=1.38, wps=3554.8, ups=0.65, wpb=5501.7, bsz=355, num_updates=6100, lr=0.000404888, gnorm=0.439, train_wall=122, gb_free=9.1, wall=11450
2022-09-08 03:18:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 03:18:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 03:18:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 03:18:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 03:18:17 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 4.914 | nll_loss 2.133 | mask_loss 8.06039 | p_2 0.04723 | mask_ave 0.648 | ppl 4.39 | bleu 55.81 | wps 1476.1 | wpb 933.5 | bsz 59.6 | num_updates 6156 | best_bleu 56.58
2022-09-08 03:18:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 6156 updates
2022-09-08 03:18:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint77.pt
2022-09-08 03:18:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint77.pt
2022-09-08 03:18:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint77.pt (epoch 77 @ 6156 updates, score 55.81) (writing took 19.65843142941594 seconds)
2022-09-08 03:18:37 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-09-08 03:18:37 | INFO | train | epoch 077 | loss 3.537 | nll_loss 0.466 | mask_loss 7.24592 | p_2 0.03506 | mask_ave 0.495 | ppl 1.38 | wps 3383.9 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 6156 | lr 0.000403042 | gnorm 0.435 | train_wall 99 | gb_free 9.1 | wall 11550
2022-09-08 03:18:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 03:18:37 | INFO | fairseq.trainer | begin training epoch 78
2022-09-08 03:18:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 03:19:33 | INFO | train_inner | epoch 078:     44 / 81 loss=3.532, nll_loss=0.461, mask_loss=7.27769, p_2=0.03488, mask_ave=0.492, ppl=1.38, wps=3565.8, ups=0.64, wpb=5560.4, bsz=360.3, num_updates=6200, lr=0.00040161, gnorm=0.425, train_wall=123, gb_free=9.1, wall=11606
2022-09-08 03:20:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 03:20:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 03:20:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 03:20:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 03:20:30 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 4.911 | nll_loss 2.129 | mask_loss 8.17905 | p_2 0.04782 | mask_ave 0.631 | ppl 4.37 | bleu 56.24 | wps 1472 | wpb 933.5 | bsz 59.6 | num_updates 6237 | best_bleu 56.58
2022-09-08 03:20:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 6237 updates
2022-09-08 03:20:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint78.pt
2022-09-08 03:20:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint78.pt
2022-09-08 03:20:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint78.pt (epoch 78 @ 6237 updates, score 56.24) (writing took 19.80651318654418 seconds)
2022-09-08 03:20:50 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-09-08 03:20:50 | INFO | train | epoch 078 | loss 3.531 | nll_loss 0.459 | mask_loss 7.30119 | p_2 0.0351 | mask_ave 0.494 | ppl 1.37 | wps 3376.5 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 6237 | lr 0.000400417 | gnorm 0.425 | train_wall 99 | gb_free 9 | wall 11683
2022-09-08 03:20:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 03:20:50 | INFO | fairseq.trainer | begin training epoch 79
2022-09-08 03:20:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 03:22:08 | INFO | train_inner | epoch 079:     63 / 81 loss=3.528, nll_loss=0.458, mask_loss=7.28986, p_2=0.03543, mask_ave=0.494, ppl=1.37, wps=3566.2, ups=0.65, wpb=5524.9, bsz=361, num_updates=6300, lr=0.00039841, gnorm=0.42, train_wall=122, gb_free=9.1, wall=11761
2022-09-08 03:22:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 03:22:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 03:22:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 03:22:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 03:22:41 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 4.93 | nll_loss 2.156 | mask_loss 8.31226 | p_2 0.04708 | mask_ave 0.651 | ppl 4.46 | bleu 55.81 | wps 1541.9 | wpb 933.5 | bsz 59.6 | num_updates 6318 | best_bleu 56.58
2022-09-08 03:22:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 6318 updates
2022-09-08 03:22:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint79.pt
2022-09-08 03:22:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint79.pt
2022-09-08 03:22:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint79.pt (epoch 79 @ 6318 updates, score 55.81) (writing took 18.096905432641506 seconds)
2022-09-08 03:22:59 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-09-08 03:22:59 | INFO | train | epoch 079 | loss 3.527 | nll_loss 0.456 | mask_loss 7.33784 | p_2 0.03516 | mask_ave 0.492 | ppl 1.37 | wps 3452.7 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 6318 | lr 0.000397842 | gnorm 0.419 | train_wall 99 | gb_free 9.1 | wall 11812
2022-09-08 03:22:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 03:22:59 | INFO | fairseq.trainer | begin training epoch 80
2022-09-08 03:22:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 03:24:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 03:24:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 03:24:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 03:24:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 03:24:52 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 4.922 | nll_loss 2.167 | mask_loss 8.27198 | p_2 0.04704 | mask_ave 0.653 | ppl 4.49 | bleu 55.4 | wps 1427.5 | wpb 933.5 | bsz 59.6 | num_updates 6399 | best_bleu 56.58
2022-09-08 03:24:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 6399 updates
2022-09-08 03:24:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint80.pt
2022-09-08 03:24:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint80.pt
2022-09-08 03:25:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint80.pt (epoch 80 @ 6399 updates, score 55.4) (writing took 17.33182217553258 seconds)
2022-09-08 03:25:10 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-09-08 03:25:10 | INFO | train | epoch 080 | loss 3.524 | nll_loss 0.454 | mask_loss 7.32889 | p_2 0.03521 | mask_ave 0.49 | ppl 1.37 | wps 3432.8 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 6399 | lr 0.000395316 | gnorm 0.419 | train_wall 100 | gb_free 9.1 | wall 11942
2022-09-08 03:25:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 03:25:10 | INFO | fairseq.trainer | begin training epoch 81
2022-09-08 03:25:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 03:25:12 | INFO | train_inner | epoch 081:      1 / 81 loss=3.526, nll_loss=0.456, mask_loss=7.36878, p_2=0.03498, mask_ave=0.49, ppl=1.37, wps=2989, ups=0.54, wpb=5499.7, bsz=353.6, num_updates=6400, lr=0.000395285, gnorm=0.422, train_wall=123, gb_free=9.1, wall=11945
2022-09-08 03:26:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 03:26:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 03:26:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 03:26:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 03:27:02 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 4.932 | nll_loss 2.158 | mask_loss 8.41864 | p_2 0.0472 | mask_ave 0.649 | ppl 4.46 | bleu 55.73 | wps 1569.6 | wpb 933.5 | bsz 59.6 | num_updates 6480 | best_bleu 56.58
2022-09-08 03:27:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 6480 updates
2022-09-08 03:27:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint81.pt
2022-09-08 03:27:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint81.pt
2022-09-08 03:27:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint81.pt (epoch 81 @ 6480 updates, score 55.73) (writing took 44.08174393698573 seconds)
2022-09-08 03:27:46 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-09-08 03:27:46 | INFO | train | epoch 081 | loss 3.52 | nll_loss 0.45 | mask_loss 7.3076 | p_2 0.03505 | mask_ave 0.495 | ppl 1.37 | wps 2862.5 | ups 0.52 | wpb 5523.2 | bsz 358 | num_updates 6480 | lr 0.000392837 | gnorm 0.413 | train_wall 100 | gb_free 9.1 | wall 12099
2022-09-08 03:27:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 03:27:46 | INFO | fairseq.trainer | begin training epoch 82
2022-09-08 03:27:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 03:28:11 | INFO | train_inner | epoch 082:     20 / 81 loss=3.519, nll_loss=0.448, mask_loss=7.30309, p_2=0.03502, mask_ave=0.493, ppl=1.36, wps=3101.6, ups=0.56, wpb=5560, bsz=360.1, num_updates=6500, lr=0.000392232, gnorm=0.412, train_wall=122, gb_free=9.1, wall=12124
2022-09-08 03:29:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 03:29:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 03:29:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 03:29:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 03:29:35 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 4.906 | nll_loss 2.145 | mask_loss 8.23752 | p_2 0.0476 | mask_ave 0.638 | ppl 4.42 | bleu 56.11 | wps 1521.9 | wpb 933.5 | bsz 59.6 | num_updates 6561 | best_bleu 56.58
2022-09-08 03:29:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 6561 updates
2022-09-08 03:29:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint82.pt
2022-09-08 03:29:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint82.pt
2022-09-08 03:29:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint82.pt (epoch 82 @ 6561 updates, score 56.11) (writing took 20.73531137034297 seconds)
2022-09-08 03:29:56 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-09-08 03:29:56 | INFO | train | epoch 082 | loss 3.52 | nll_loss 0.451 | mask_loss 7.32237 | p_2 0.0351 | mask_ave 0.494 | ppl 1.37 | wps 3432.2 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 6561 | lr 0.000390405 | gnorm 0.422 | train_wall 97 | gb_free 9 | wall 12229
2022-09-08 03:29:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 03:29:56 | INFO | fairseq.trainer | begin training epoch 83
2022-09-08 03:29:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 03:30:44 | INFO | train_inner | epoch 083:     39 / 81 loss=3.516, nll_loss=0.448, mask_loss=7.34883, p_2=0.03504, mask_ave=0.493, ppl=1.36, wps=3598.2, ups=0.65, wpb=5509.2, bsz=359.4, num_updates=6600, lr=0.000389249, gnorm=0.413, train_wall=119, gb_free=9.1, wall=12277
2022-09-08 03:31:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 03:31:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 03:31:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 03:31:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 03:31:47 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 4.939 | nll_loss 2.18 | mask_loss 8.28893 | p_2 0.04732 | mask_ave 0.645 | ppl 4.53 | bleu 56.22 | wps 1509.1 | wpb 933.5 | bsz 59.6 | num_updates 6642 | best_bleu 56.58
2022-09-08 03:31:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 6642 updates
2022-09-08 03:31:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint83.pt
2022-09-08 03:31:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint83.pt
2022-09-08 03:32:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint83.pt (epoch 83 @ 6642 updates, score 56.22) (writing took 21.37814435735345 seconds)
2022-09-08 03:32:09 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-09-08 03:32:09 | INFO | train | epoch 083 | loss 3.513 | nll_loss 0.444 | mask_loss 7.35256 | p_2 0.03513 | mask_ave 0.493 | ppl 1.36 | wps 3378.7 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 6642 | lr 0.000388017 | gnorm 0.405 | train_wall 98 | gb_free 9.2 | wall 12362
2022-09-08 03:32:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 03:32:09 | INFO | fairseq.trainer | begin training epoch 84
2022-09-08 03:32:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 03:33:20 | INFO | train_inner | epoch 084:     58 / 81 loss=3.514, nll_loss=0.445, mask_loss=7.34932, p_2=0.03507, mask_ave=0.494, ppl=1.36, wps=3554.6, ups=0.64, wpb=5543.3, bsz=358, num_updates=6700, lr=0.000386334, gnorm=0.411, train_wall=121, gb_free=9.1, wall=12433
2022-09-08 03:33:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 03:33:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 03:33:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 03:33:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 03:33:59 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 4.931 | nll_loss 2.173 | mask_loss 8.15364 | p_2 0.04722 | mask_ave 0.647 | ppl 4.51 | bleu 56.35 | wps 1513.1 | wpb 933.5 | bsz 59.6 | num_updates 6723 | best_bleu 56.58
2022-09-08 03:33:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 6723 updates
2022-09-08 03:33:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint84.pt
2022-09-08 03:34:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint84.pt
2022-09-08 03:34:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint84.pt (epoch 84 @ 6723 updates, score 56.35) (writing took 22.170258954167366 seconds)
2022-09-08 03:34:21 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-09-08 03:34:21 | INFO | train | epoch 084 | loss 3.513 | nll_loss 0.445 | mask_loss 7.36669 | p_2 0.03509 | mask_ave 0.494 | ppl 1.36 | wps 3373.1 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 6723 | lr 0.000385672 | gnorm 0.412 | train_wall 98 | gb_free 9.1 | wall 12494
2022-09-08 03:34:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 03:34:21 | INFO | fairseq.trainer | begin training epoch 85
2022-09-08 03:34:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 03:35:57 | INFO | train_inner | epoch 085:     77 / 81 loss=3.51, nll_loss=0.441, mask_loss=7.3728, p_2=0.03529, mask_ave=0.498, ppl=1.36, wps=3505.8, ups=0.64, wpb=5497.1, bsz=356, num_updates=6800, lr=0.000383482, gnorm=0.409, train_wall=121, gb_free=9.1, wall=12590
2022-09-08 03:36:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 03:36:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 03:36:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 03:36:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 03:36:13 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 4.925 | nll_loss 2.173 | mask_loss 8.29854 | p_2 0.04736 | mask_ave 0.644 | ppl 4.51 | bleu 55.75 | wps 1510.2 | wpb 933.5 | bsz 59.6 | num_updates 6804 | best_bleu 56.58
2022-09-08 03:36:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 6804 updates
2022-09-08 03:36:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint85.pt
2022-09-08 03:36:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint85.pt
2022-09-08 03:36:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint85.pt (epoch 85 @ 6804 updates, score 55.75) (writing took 19.610925260931253 seconds)
2022-09-08 03:36:33 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-09-08 03:36:33 | INFO | train | epoch 085 | loss 3.507 | nll_loss 0.438 | mask_loss 7.37005 | p_2 0.03507 | mask_ave 0.494 | ppl 1.36 | wps 3406.5 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 6804 | lr 0.00038337 | gnorm 0.405 | train_wall 98 | gb_free 9.2 | wall 12625
2022-09-08 03:36:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 03:36:33 | INFO | fairseq.trainer | begin training epoch 86
2022-09-08 03:36:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 03:38:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 03:38:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 03:38:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 03:38:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 03:38:26 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 4.92 | nll_loss 2.164 | mask_loss 8.17643 | p_2 0.04772 | mask_ave 0.634 | ppl 4.48 | bleu 56.44 | wps 1492.3 | wpb 933.5 | bsz 59.6 | num_updates 6885 | best_bleu 56.58
2022-09-08 03:38:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 6885 updates
2022-09-08 03:38:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint86.pt
2022-09-08 03:38:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint86.pt
2022-09-08 03:38:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint86.pt (epoch 86 @ 6885 updates, score 56.44) (writing took 26.232949253171682 seconds)
2022-09-08 03:38:53 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-09-08 03:38:53 | INFO | train | epoch 086 | loss 3.506 | nll_loss 0.438 | mask_loss 7.3751 | p_2 0.03519 | mask_ave 0.491 | ppl 1.35 | wps 3194.7 | ups 0.58 | wpb 5523.2 | bsz 358 | num_updates 6885 | lr 0.000381108 | gnorm 0.407 | train_wall 100 | gb_free 9.1 | wall 12766
2022-09-08 03:38:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 03:38:53 | INFO | fairseq.trainer | begin training epoch 87
2022-09-08 03:38:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 03:39:12 | INFO | train_inner | epoch 087:     15 / 81 loss=3.504, nll_loss=0.436, mask_loss=7.34774, p_2=0.0352, mask_ave=0.491, ppl=1.35, wps=2828.8, ups=0.51, wpb=5504.7, bsz=359.4, num_updates=6900, lr=0.000380693, gnorm=0.403, train_wall=123, gb_free=9, wall=12784
2022-09-08 03:40:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 03:40:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 03:40:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 03:40:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 03:40:42 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 4.946 | nll_loss 2.208 | mask_loss 8.32523 | p_2 0.04783 | mask_ave 0.631 | ppl 4.62 | bleu 56.06 | wps 1578.7 | wpb 933.5 | bsz 59.6 | num_updates 6966 | best_bleu 56.58
2022-09-08 03:40:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 6966 updates
2022-09-08 03:40:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint87.pt
2022-09-08 03:40:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint87.pt
2022-09-08 03:40:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint87.pt (epoch 87 @ 6966 updates, score 56.06) (writing took 17.447651490569115 seconds)
2022-09-08 03:40:59 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-09-08 03:40:59 | INFO | train | epoch 087 | loss 3.5 | nll_loss 0.432 | mask_loss 7.37218 | p_2 0.03521 | mask_ave 0.491 | ppl 1.35 | wps 3531.1 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 6966 | lr 0.000378886 | gnorm 0.393 | train_wall 97 | gb_free 9.1 | wall 12892
2022-09-08 03:41:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 03:41:00 | INFO | fairseq.trainer | begin training epoch 88
2022-09-08 03:41:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 03:41:42 | INFO | train_inner | epoch 088:     34 / 81 loss=3.499, nll_loss=0.432, mask_loss=7.39831, p_2=0.03518, mask_ave=0.49, ppl=1.35, wps=3708.4, ups=0.67, wpb=5563, bsz=361.8, num_updates=7000, lr=0.000377964, gnorm=0.396, train_wall=120, gb_free=9, wall=12934
2022-09-08 03:42:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 03:42:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 03:42:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 03:42:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 03:42:50 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 4.915 | nll_loss 2.157 | mask_loss 8.2052 | p_2 0.04734 | mask_ave 0.645 | ppl 4.46 | bleu 56.31 | wps 1523.4 | wpb 933.5 | bsz 59.6 | num_updates 7047 | best_bleu 56.58
2022-09-08 03:42:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 7047 updates
2022-09-08 03:42:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint88.pt
2022-09-08 03:42:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint88.pt
2022-09-08 03:43:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint88.pt (epoch 88 @ 7047 updates, score 56.31) (writing took 19.121330458670855 seconds)
2022-09-08 03:43:09 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-09-08 03:43:09 | INFO | train | epoch 088 | loss 3.502 | nll_loss 0.436 | mask_loss 7.38765 | p_2 0.03519 | mask_ave 0.492 | ppl 1.35 | wps 3442.5 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 7047 | lr 0.000376702 | gnorm 0.409 | train_wall 98 | gb_free 9.1 | wall 13022
2022-09-08 03:43:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 03:43:09 | INFO | fairseq.trainer | begin training epoch 89
2022-09-08 03:43:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 03:44:14 | INFO | train_inner | epoch 089:     53 / 81 loss=3.503, nll_loss=0.437, mask_loss=7.41095, p_2=0.03504, mask_ave=0.493, ppl=1.35, wps=3609.3, ups=0.66, wpb=5497.6, bsz=349.3, num_updates=7100, lr=0.000375293, gnorm=0.405, train_wall=120, gb_free=9, wall=13087
2022-09-08 03:44:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 03:44:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 03:44:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 03:44:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 03:44:59 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 4.932 | nll_loss 2.178 | mask_loss 8.40076 | p_2 0.04747 | mask_ave 0.642 | ppl 4.52 | bleu 56.51 | wps 1508.8 | wpb 933.5 | bsz 59.6 | num_updates 7128 | best_bleu 56.58
2022-09-08 03:44:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 7128 updates
2022-09-08 03:44:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint89.pt
2022-09-08 03:45:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint89.pt
2022-09-08 03:45:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint89.pt (epoch 89 @ 7128 updates, score 56.51) (writing took 18.729092583060265 seconds)
2022-09-08 03:45:18 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-09-08 03:45:18 | INFO | train | epoch 089 | loss 3.498 | nll_loss 0.431 | mask_loss 7.42247 | p_2 0.03519 | mask_ave 0.492 | ppl 1.35 | wps 3478.6 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 7128 | lr 0.000374555 | gnorm 0.393 | train_wall 97 | gb_free 9.2 | wall 13151
2022-09-08 03:45:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 03:45:18 | INFO | fairseq.trainer | begin training epoch 90
2022-09-08 03:45:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 03:46:48 | INFO | train_inner | epoch 090:     72 / 81 loss=3.493, nll_loss=0.427, mask_loss=7.41185, p_2=0.03533, mask_ave=0.494, ppl=1.34, wps=3581.6, ups=0.65, wpb=5527.4, bsz=363.2, num_updates=7200, lr=0.000372678, gnorm=0.382, train_wall=122, gb_free=9, wall=13241
2022-09-08 03:46:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 03:47:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 03:47:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 03:47:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 03:47:11 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 4.926 | nll_loss 2.175 | mask_loss 8.34441 | p_2 0.04742 | mask_ave 0.644 | ppl 4.51 | bleu 55.64 | wps 1466.9 | wpb 933.5 | bsz 59.6 | num_updates 7209 | best_bleu 56.58
2022-09-08 03:47:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 7209 updates
2022-09-08 03:47:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint90.pt
2022-09-08 03:47:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint90.pt
2022-09-08 03:47:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint90.pt (epoch 90 @ 7209 updates, score 55.64) (writing took 22.732078559696674 seconds)
2022-09-08 03:47:34 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-09-08 03:47:34 | INFO | train | epoch 090 | loss 3.492 | nll_loss 0.426 | mask_loss 7.41362 | p_2 0.0351 | mask_ave 0.494 | ppl 1.34 | wps 3295.3 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 7209 | lr 0.000372445 | gnorm 0.38 | train_wall 99 | gb_free 9.2 | wall 13287
2022-09-08 03:47:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 03:47:34 | INFO | fairseq.trainer | begin training epoch 91
2022-09-08 03:47:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 03:49:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 03:49:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 03:49:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 03:49:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 03:49:25 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 4.939 | nll_loss 2.199 | mask_loss 8.36007 | p_2 0.04794 | mask_ave 0.628 | ppl 4.59 | bleu 55.88 | wps 1465.1 | wpb 933.5 | bsz 59.6 | num_updates 7290 | best_bleu 56.58
2022-09-08 03:49:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 7290 updates
2022-09-08 03:49:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint91.pt
2022-09-08 03:49:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint91.pt
2022-09-08 03:49:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint91.pt (epoch 91 @ 7290 updates, score 55.88) (writing took 15.528727248311043 seconds)
2022-09-08 03:49:41 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-09-08 03:49:41 | INFO | train | epoch 091 | loss 3.493 | nll_loss 0.428 | mask_loss 7.44246 | p_2 0.03509 | mask_ave 0.494 | ppl 1.34 | wps 3521.1 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 7290 | lr 0.00037037 | gnorm 0.39 | train_wall 98 | gb_free 9 | wall 13414
2022-09-08 03:49:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 03:49:41 | INFO | fairseq.trainer | begin training epoch 92
2022-09-08 03:49:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 03:49:54 | INFO | train_inner | epoch 092:     10 / 81 loss=3.493, nll_loss=0.427, mask_loss=7.44424, p_2=0.03497, mask_ave=0.494, ppl=1.34, wps=2964.3, ups=0.54, wpb=5501.8, bsz=354.5, num_updates=7300, lr=0.000370117, gnorm=0.391, train_wall=121, gb_free=9, wall=13427
2022-09-08 03:51:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 03:51:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 03:51:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 03:51:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 03:51:32 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 4.94 | nll_loss 2.194 | mask_loss 8.25983 | p_2 0.04748 | mask_ave 0.641 | ppl 4.58 | bleu 56.56 | wps 1515.2 | wpb 933.5 | bsz 59.6 | num_updates 7371 | best_bleu 56.58
2022-09-08 03:51:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 7371 updates
2022-09-08 03:51:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint92.pt
2022-09-08 03:51:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint92.pt
2022-09-08 03:51:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint92.pt (epoch 92 @ 7371 updates, score 56.56) (writing took 22.581756729632616 seconds)
2022-09-08 03:51:54 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-09-08 03:51:54 | INFO | train | epoch 092 | loss 3.489 | nll_loss 0.423 | mask_loss 7.42685 | p_2 0.03519 | mask_ave 0.491 | ppl 1.34 | wps 3350.6 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 7371 | lr 0.00036833 | gnorm 0.382 | train_wall 98 | gb_free 9.2 | wall 13547
2022-09-08 03:51:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 03:51:55 | INFO | fairseq.trainer | begin training epoch 93
2022-09-08 03:51:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 03:52:31 | INFO | train_inner | epoch 093:     29 / 81 loss=3.488, nll_loss=0.422, mask_loss=7.39615, p_2=0.0353, mask_ave=0.491, ppl=1.34, wps=3537.9, ups=0.64, wpb=5559.5, bsz=364.3, num_updates=7400, lr=0.000367607, gnorm=0.379, train_wall=122, gb_free=9, wall=13584
2022-09-08 03:53:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 03:53:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 03:53:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 03:53:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 03:53:45 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 4.949 | nll_loss 2.214 | mask_loss 8.22377 | p_2 0.04773 | mask_ave 0.634 | ppl 4.64 | bleu 55.66 | wps 1541.2 | wpb 933.5 | bsz 59.6 | num_updates 7452 | best_bleu 56.58
2022-09-08 03:53:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 7452 updates
2022-09-08 03:53:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint93.pt
2022-09-08 03:53:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint93.pt
2022-09-08 03:54:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint93.pt (epoch 93 @ 7452 updates, score 55.66) (writing took 22.121560148894787 seconds)
2022-09-08 03:54:07 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-09-08 03:54:07 | INFO | train | epoch 093 | loss 3.489 | nll_loss 0.424 | mask_loss 7.37459 | p_2 0.03513 | mask_ave 0.493 | ppl 1.34 | wps 3359.1 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 7452 | lr 0.000366322 | gnorm 0.384 | train_wall 98 | gb_free 9 | wall 13680
2022-09-08 03:54:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 03:54:08 | INFO | fairseq.trainer | begin training epoch 94
2022-09-08 03:54:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 03:55:07 | INFO | train_inner | epoch 094:     48 / 81 loss=3.488, nll_loss=0.423, mask_loss=7.38795, p_2=0.03502, mask_ave=0.493, ppl=1.34, wps=3530.5, ups=0.64, wpb=5507.7, bsz=352.3, num_updates=7500, lr=0.000365148, gnorm=0.385, train_wall=121, gb_free=8.9, wall=13740
2022-09-08 03:55:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 03:55:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 03:55:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 03:55:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 03:55:59 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 4.933 | nll_loss 2.187 | mask_loss 8.39741 | p_2 0.04738 | mask_ave 0.644 | ppl 4.55 | bleu 56.52 | wps 1536.7 | wpb 933.5 | bsz 59.6 | num_updates 7533 | best_bleu 56.58
2022-09-08 03:55:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 7533 updates
2022-09-08 03:55:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint94.pt
2022-09-08 03:56:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint94.pt
2022-09-08 03:56:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint94.pt (epoch 94 @ 7533 updates, score 56.52) (writing took 21.825217485427856 seconds)
2022-09-08 03:56:21 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-09-08 03:56:21 | INFO | train | epoch 094 | loss 3.485 | nll_loss 0.421 | mask_loss 7.41949 | p_2 0.0351 | mask_ave 0.494 | ppl 1.34 | wps 3359.4 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 7533 | lr 0.000364348 | gnorm 0.384 | train_wall 98 | gb_free 9.3 | wall 13814
2022-09-08 03:56:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 03:56:21 | INFO | fairseq.trainer | begin training epoch 95
2022-09-08 03:56:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 03:57:45 | INFO | train_inner | epoch 095:     67 / 81 loss=3.486, nll_loss=0.423, mask_loss=7.45653, p_2=0.03518, mask_ave=0.495, ppl=1.34, wps=3492.4, ups=0.63, wpb=5527, bsz=359.4, num_updates=7600, lr=0.000362738, gnorm=0.39, train_wall=123, gb_free=9.1, wall=13898
2022-09-08 03:58:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 03:58:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 03:58:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 03:58:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 03:58:14 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 4.947 | nll_loss 2.211 | mask_loss 8.41421 | p_2 0.04732 | mask_ave 0.646 | ppl 4.63 | bleu 55.95 | wps 1519.4 | wpb 933.5 | bsz 59.6 | num_updates 7614 | best_bleu 56.58
2022-09-08 03:58:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 7614 updates
2022-09-08 03:58:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint95.pt
2022-09-08 03:58:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint95.pt
2022-09-08 03:58:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint95.pt (epoch 95 @ 7614 updates, score 55.95) (writing took 18.83742282167077 seconds)
2022-09-08 03:58:33 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-09-08 03:58:33 | INFO | train | epoch 095 | loss 3.486 | nll_loss 0.422 | mask_loss 7.43757 | p_2 0.03512 | mask_ave 0.493 | ppl 1.34 | wps 3390.5 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 7614 | lr 0.000362404 | gnorm 0.388 | train_wall 100 | gb_free 9.2 | wall 13945
2022-09-08 03:58:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 03:58:33 | INFO | fairseq.trainer | begin training epoch 96
2022-09-08 03:58:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:00:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:00:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:00:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:00:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:00:24 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 4.938 | nll_loss 2.193 | mask_loss 8.18735 | p_2 0.04742 | mask_ave 0.642 | ppl 4.57 | bleu 56.44 | wps 1549.4 | wpb 933.5 | bsz 59.6 | num_updates 7695 | best_bleu 56.58
2022-09-08 04:00:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 7695 updates
2022-09-08 04:00:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint96.pt
2022-09-08 04:00:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint96.pt
2022-09-08 04:00:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint96.pt (epoch 96 @ 7695 updates, score 56.44) (writing took 19.39877735823393 seconds)
2022-09-08 04:00:44 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-09-08 04:00:44 | INFO | train | epoch 096 | loss 3.482 | nll_loss 0.418 | mask_loss 7.39381 | p_2 0.03513 | mask_ave 0.493 | ppl 1.34 | wps 3414.9 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 7695 | lr 0.000360492 | gnorm 0.373 | train_wall 99 | gb_free 9.2 | wall 14077
2022-09-08 04:00:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 04:00:44 | INFO | fairseq.trainer | begin training epoch 97
2022-09-08 04:00:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:00:51 | INFO | train_inner | epoch 097:      5 / 81 loss=3.482, nll_loss=0.418, mask_loss=7.39011, p_2=0.03506, mask_ave=0.492, ppl=1.34, wps=2973.8, ups=0.54, wpb=5512, bsz=357.5, num_updates=7700, lr=0.000360375, gnorm=0.374, train_wall=121, gb_free=9, wall=14083
2022-09-08 04:02:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:02:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:02:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:02:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:02:37 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 4.958 | nll_loss 2.222 | mask_loss 8.30895 | p_2 0.04779 | mask_ave 0.632 | ppl 4.66 | bleu 56.08 | wps 1421.4 | wpb 933.5 | bsz 59.6 | num_updates 7776 | best_bleu 56.58
2022-09-08 04:02:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 7776 updates
2022-09-08 04:02:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint97.pt
2022-09-08 04:02:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint97.pt
2022-09-08 04:02:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint97.pt (epoch 97 @ 7776 updates, score 56.08) (writing took 20.605260498821735 seconds)
2022-09-08 04:02:57 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-09-08 04:02:57 | INFO | train | epoch 097 | loss 3.479 | nll_loss 0.416 | mask_loss 7.44572 | p_2 0.03516 | mask_ave 0.492 | ppl 1.33 | wps 3345.7 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 7776 | lr 0.00035861 | gnorm 0.369 | train_wall 100 | gb_free 9 | wall 14210
2022-09-08 04:02:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 04:02:58 | INFO | fairseq.trainer | begin training epoch 98
2022-09-08 04:02:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:03:26 | INFO | train_inner | epoch 098:     24 / 81 loss=3.479, nll_loss=0.415, mask_loss=7.44905, p_2=0.03524, mask_ave=0.494, ppl=1.33, wps=3540.1, ups=0.64, wpb=5501.8, bsz=356.2, num_updates=7800, lr=0.000358057, gnorm=0.371, train_wall=121, gb_free=9.1, wall=14239
2022-09-08 04:04:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:04:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:04:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:04:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:04:47 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 4.94 | nll_loss 2.207 | mask_loss 8.54661 | p_2 0.0478 | mask_ave 0.633 | ppl 4.62 | bleu 56.05 | wps 1505.3 | wpb 933.5 | bsz 59.6 | num_updates 7857 | best_bleu 56.58
2022-09-08 04:04:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 7857 updates
2022-09-08 04:04:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint98.pt
2022-09-08 04:04:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint98.pt
2022-09-08 04:04:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint98.pt (epoch 98 @ 7857 updates, score 56.05) (writing took 2.8163128159940243 seconds)
2022-09-08 04:04:50 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-09-08 04:04:50 | INFO | train | epoch 098 | loss 3.479 | nll_loss 0.416 | mask_loss 7.46029 | p_2 0.03515 | mask_ave 0.493 | ppl 1.33 | wps 3956.6 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 7857 | lr 0.000356756 | gnorm 0.379 | train_wall 97 | gb_free 9.1 | wall 14323
2022-09-08 04:04:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 04:04:51 | INFO | fairseq.trainer | begin training epoch 99
2022-09-08 04:04:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:05:43 | INFO | train_inner | epoch 099:     43 / 81 loss=3.478, nll_loss=0.415, mask_loss=7.52063, p_2=0.035, mask_ave=0.493, ppl=1.33, wps=4044.4, ups=0.73, wpb=5541.7, bsz=357, num_updates=7900, lr=0.000355784, gnorm=0.375, train_wall=121, gb_free=9.2, wall=14376
2022-09-08 04:06:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:06:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:06:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:06:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:06:41 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 4.924 | nll_loss 2.2 | mask_loss 8.2982 | p_2 0.04724 | mask_ave 0.648 | ppl 4.59 | bleu 55.95 | wps 1522.2 | wpb 933.5 | bsz 59.6 | num_updates 7938 | best_bleu 56.58
2022-09-08 04:06:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 7938 updates
2022-09-08 04:06:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint99.pt
2022-09-08 04:06:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint99.pt
2022-09-08 04:07:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint99.pt (epoch 99 @ 7938 updates, score 55.95) (writing took 20.488146133720875 seconds)
2022-09-08 04:07:01 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-09-08 04:07:02 | INFO | train | epoch 099 | loss 3.475 | nll_loss 0.412 | mask_loss 7.48724 | p_2 0.03505 | mask_ave 0.496 | ppl 1.33 | wps 3413.7 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 7938 | lr 0.000354931 | gnorm 0.361 | train_wall 98 | gb_free 9 | wall 14454
2022-09-08 04:07:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 04:07:02 | INFO | fairseq.trainer | begin training epoch 100
2022-09-08 04:07:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:08:18 | INFO | train_inner | epoch 100:     62 / 81 loss=3.474, nll_loss=0.412, mask_loss=7.42369, p_2=0.035, mask_ave=0.495, ppl=1.33, wps=3571.6, ups=0.65, wpb=5532.6, bsz=359.9, num_updates=8000, lr=0.000353553, gnorm=0.366, train_wall=121, gb_free=9.1, wall=14531
2022-09-08 04:08:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:08:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:08:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:08:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:08:51 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 4.946 | nll_loss 2.22 | mask_loss 8.41902 | p_2 0.04733 | mask_ave 0.646 | ppl 4.66 | bleu 55.21 | wps 1534.5 | wpb 933.5 | bsz 59.6 | num_updates 8019 | best_bleu 56.58
2022-09-08 04:08:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 8019 updates
2022-09-08 04:08:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint100.pt
2022-09-08 04:08:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint100.pt
2022-09-08 04:09:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint100.pt (epoch 100 @ 8019 updates, score 55.21) (writing took 19.42064844444394 seconds)
2022-09-08 04:09:11 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-09-08 04:09:11 | INFO | train | epoch 100 | loss 3.474 | nll_loss 0.412 | mask_loss 7.44691 | p_2 0.03507 | mask_ave 0.495 | ppl 1.33 | wps 3457.2 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 8019 | lr 0.000353134 | gnorm 0.375 | train_wall 97 | gb_free 9 | wall 14584
2022-09-08 04:09:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 04:09:11 | INFO | fairseq.trainer | begin training epoch 101
2022-09-08 04:09:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:10:51 | INFO | train_inner | epoch 101:     81 / 81 loss=3.474, nll_loss=0.412, mask_loss=7.54205, p_2=0.03516, mask_ave=0.495, ppl=1.33, wps=3592.1, ups=0.65, wpb=5508.8, bsz=358, num_updates=8100, lr=0.000351364, gnorm=0.372, train_wall=121, gb_free=9.2, wall=14684
2022-09-08 04:10:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:10:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:10:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:10:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:11:03 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 4.943 | nll_loss 2.219 | mask_loss 8.47049 | p_2 0.04769 | mask_ave 0.635 | ppl 4.66 | bleu 56.26 | wps 1461.6 | wpb 933.5 | bsz 59.6 | num_updates 8100 | best_bleu 56.58
2022-09-08 04:11:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 8100 updates
2022-09-08 04:11:03 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint101.pt
2022-09-08 04:11:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint101.pt
2022-09-08 04:11:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint101.pt (epoch 101 @ 8100 updates, score 56.26) (writing took 21.675187595188618 seconds)
2022-09-08 04:11:25 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-09-08 04:11:25 | INFO | train | epoch 101 | loss 3.473 | nll_loss 0.412 | mask_loss 7.56247 | p_2 0.03508 | mask_ave 0.495 | ppl 1.33 | wps 3343.4 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 8100 | lr 0.000351364 | gnorm 0.37 | train_wall 98 | gb_free 9.2 | wall 14718
2022-09-08 04:11:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 04:11:25 | INFO | fairseq.trainer | begin training epoch 102
2022-09-08 04:11:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:13:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:13:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:13:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:13:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:13:16 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 4.924 | nll_loss 2.189 | mask_loss 8.50457 | p_2 0.04785 | mask_ave 0.632 | ppl 4.56 | bleu 56.43 | wps 1520.3 | wpb 933.5 | bsz 59.6 | num_updates 8181 | best_bleu 56.58
2022-09-08 04:13:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 8181 updates
2022-09-08 04:13:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint102.pt
2022-09-08 04:13:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint102.pt
2022-09-08 04:13:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint102.pt (epoch 102 @ 8181 updates, score 56.43) (writing took 2.746184792369604 seconds)
2022-09-08 04:13:19 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-09-08 04:13:19 | INFO | train | epoch 102 | loss 3.472 | nll_loss 0.411 | mask_loss 7.51523 | p_2 0.03515 | mask_ave 0.493 | ppl 1.33 | wps 3918.4 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 8181 | lr 0.00034962 | gnorm 0.371 | train_wall 99 | gb_free 9.2 | wall 14832
2022-09-08 04:13:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 04:13:19 | INFO | fairseq.trainer | begin training epoch 103
2022-09-08 04:13:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:13:44 | INFO | train_inner | epoch 103:     19 / 81 loss=3.471, nll_loss=0.409, mask_loss=7.50021, p_2=0.0353, mask_ave=0.494, ppl=1.33, wps=3206, ups=0.58, wpb=5537, bsz=361.2, num_updates=8200, lr=0.000349215, gnorm=0.367, train_wall=123, gb_free=9.1, wall=14857
2022-09-08 04:15:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:15:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:15:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:15:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:15:11 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 4.935 | nll_loss 2.209 | mask_loss 8.44712 | p_2 0.04779 | mask_ave 0.632 | ppl 4.62 | bleu 55.56 | wps 1487.4 | wpb 933.5 | bsz 59.6 | num_updates 8262 | best_bleu 56.58
2022-09-08 04:15:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 8262 updates
2022-09-08 04:15:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint103.pt
2022-09-08 04:15:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint103.pt
2022-09-08 04:15:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint103.pt (epoch 103 @ 8262 updates, score 55.56) (writing took 20.563877157866955 seconds)
2022-09-08 04:15:32 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-09-08 04:15:32 | INFO | train | epoch 103 | loss 3.47 | nll_loss 0.408 | mask_loss 7.53553 | p_2 0.03517 | mask_ave 0.492 | ppl 1.33 | wps 3370.9 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 8262 | lr 0.000347902 | gnorm 0.358 | train_wall 99 | gb_free 9 | wall 14964
2022-09-08 04:15:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 04:15:32 | INFO | fairseq.trainer | begin training epoch 104
2022-09-08 04:15:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:16:18 | INFO | train_inner | epoch 104:     38 / 81 loss=3.469, nll_loss=0.408, mask_loss=7.5495, p_2=0.03489, mask_ave=0.492, ppl=1.33, wps=3573.3, ups=0.65, wpb=5517.6, bsz=351.2, num_updates=8300, lr=0.000347105, gnorm=0.358, train_wall=121, gb_free=9.2, wall=15011
2022-09-08 04:17:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:17:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:17:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:17:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:17:23 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 4.936 | nll_loss 2.211 | mask_loss 8.39443 | p_2 0.04741 | mask_ave 0.643 | ppl 4.63 | bleu 56.13 | wps 1488.2 | wpb 933.5 | bsz 59.6 | num_updates 8343 | best_bleu 56.58
2022-09-08 04:17:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 8343 updates
2022-09-08 04:17:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint104.pt
2022-09-08 04:17:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint104.pt
2022-09-08 04:17:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint104.pt (epoch 104 @ 8343 updates, score 56.13) (writing took 16.273209184408188 seconds)
2022-09-08 04:17:40 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-09-08 04:17:40 | INFO | train | epoch 104 | loss 3.466 | nll_loss 0.405 | mask_loss 7.5325 | p_2 0.03512 | mask_ave 0.494 | ppl 1.32 | wps 3486.5 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 8343 | lr 0.000346209 | gnorm 0.35 | train_wall 99 | gb_free 9 | wall 15093
2022-09-08 04:17:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 04:17:40 | INFO | fairseq.trainer | begin training epoch 105
2022-09-08 04:17:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:18:52 | INFO | train_inner | epoch 105:     57 / 81 loss=3.466, nll_loss=0.405, mask_loss=7.554, p_2=0.03536, mask_ave=0.497, ppl=1.32, wps=3594.6, ups=0.65, wpb=5511, bsz=363.8, num_updates=8400, lr=0.000345033, gnorm=0.357, train_wall=124, gb_free=9, wall=15165
2022-09-08 04:19:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:19:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:19:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:19:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:19:32 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 4.943 | nll_loss 2.222 | mask_loss 8.52662 | p_2 0.04762 | mask_ave 0.638 | ppl 4.66 | bleu 55.81 | wps 1529.7 | wpb 933.5 | bsz 59.6 | num_updates 8424 | best_bleu 56.58
2022-09-08 04:19:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 8424 updates
2022-09-08 04:19:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint105.pt
2022-09-08 04:19:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint105.pt
2022-09-08 04:19:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint105.pt (epoch 105 @ 8424 updates, score 55.81) (writing took 25.31489757820964 seconds)
2022-09-08 04:19:58 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-09-08 04:19:58 | INFO | train | epoch 105 | loss 3.467 | nll_loss 0.407 | mask_loss 7.58489 | p_2 0.03509 | mask_ave 0.495 | ppl 1.33 | wps 3247.1 | ups 0.59 | wpb 5523.2 | bsz 358 | num_updates 8424 | lr 0.000344541 | gnorm 0.366 | train_wall 100 | gb_free 9.1 | wall 15231
2022-09-08 04:19:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 04:19:58 | INFO | fairseq.trainer | begin training epoch 106
2022-09-08 04:19:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:21:32 | INFO | train_inner | epoch 106:     76 / 81 loss=3.466, nll_loss=0.405, mask_loss=7.58408, p_2=0.03494, mask_ave=0.493, ppl=1.32, wps=3465.8, ups=0.62, wpb=5553.8, bsz=357.8, num_updates=8500, lr=0.000342997, gnorm=0.354, train_wall=122, gb_free=9.2, wall=15325
2022-09-08 04:21:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:21:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:21:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:21:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:21:49 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 4.941 | nll_loss 2.224 | mask_loss 8.59646 | p_2 0.04756 | mask_ave 0.64 | ppl 4.67 | bleu 56.13 | wps 1531.1 | wpb 933.5 | bsz 59.6 | num_updates 8505 | best_bleu 56.58
2022-09-08 04:21:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 8505 updates
2022-09-08 04:21:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint106.pt
2022-09-08 04:21:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint106.pt
2022-09-08 04:22:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint106.pt (epoch 106 @ 8505 updates, score 56.13) (writing took 17.590347919613123 seconds)
2022-09-08 04:22:06 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-09-08 04:22:06 | INFO | train | epoch 106 | loss 3.464 | nll_loss 0.403 | mask_loss 7.56863 | p_2 0.0351 | mask_ave 0.495 | ppl 1.32 | wps 3475.4 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 8505 | lr 0.000342896 | gnorm 0.352 | train_wall 98 | gb_free 9.3 | wall 15359
2022-09-08 04:22:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 04:22:07 | INFO | fairseq.trainer | begin training epoch 107
2022-09-08 04:22:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:23:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:23:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:23:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:23:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:23:56 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 4.925 | nll_loss 2.202 | mask_loss 8.35003 | p_2 0.04774 | mask_ave 0.634 | ppl 4.6 | bleu 56.36 | wps 1552.5 | wpb 933.5 | bsz 59.6 | num_updates 8586 | best_bleu 56.58
2022-09-08 04:23:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 8586 updates
2022-09-08 04:23:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint107.pt
2022-09-08 04:23:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint107.pt
2022-09-08 04:23:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint107.pt (epoch 107 @ 8586 updates, score 56.36) (writing took 2.8020893298089504 seconds)
2022-09-08 04:23:59 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-09-08 04:23:59 | INFO | train | epoch 107 | loss 3.464 | nll_loss 0.404 | mask_loss 7.60509 | p_2 0.03508 | mask_ave 0.495 | ppl 1.32 | wps 3973.5 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 8586 | lr 0.000341275 | gnorm 0.354 | train_wall 97 | gb_free 9.1 | wall 15472
2022-09-08 04:23:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 04:23:59 | INFO | fairseq.trainer | begin training epoch 108
2022-09-08 04:23:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:24:17 | INFO | train_inner | epoch 108:     14 / 81 loss=3.463, nll_loss=0.403, mask_loss=7.58212, p_2=0.03518, mask_ave=0.496, ppl=1.32, wps=3341, ups=0.61, wpb=5500.7, bsz=356.6, num_updates=8600, lr=0.000340997, gnorm=0.355, train_wall=119, gb_free=9, wall=15490
2022-09-08 04:25:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:25:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:25:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:25:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:25:50 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 4.941 | nll_loss 2.221 | mask_loss 8.33765 | p_2 0.04777 | mask_ave 0.633 | ppl 4.66 | bleu 55.69 | wps 1513.8 | wpb 933.5 | bsz 59.6 | num_updates 8667 | best_bleu 56.58
2022-09-08 04:25:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 8667 updates
2022-09-08 04:25:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint108.pt
2022-09-08 04:25:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint108.pt
2022-09-08 04:25:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint108.pt (epoch 108 @ 8667 updates, score 55.69) (writing took 2.814992692321539 seconds)
2022-09-08 04:25:53 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-09-08 04:25:53 | INFO | train | epoch 108 | loss 3.462 | nll_loss 0.402 | mask_loss 7.54418 | p_2 0.03514 | mask_ave 0.493 | ppl 1.32 | wps 3938.1 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 8667 | lr 0.000339677 | gnorm 0.356 | train_wall 98 | gb_free 9.2 | wall 15586
2022-09-08 04:25:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 04:25:53 | INFO | fairseq.trainer | begin training epoch 109
2022-09-08 04:25:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:26:34 | INFO | train_inner | epoch 109:     33 / 81 loss=3.463, nll_loss=0.403, mask_loss=7.57124, p_2=0.03504, mask_ave=0.492, ppl=1.32, wps=4014.5, ups=0.73, wpb=5516.4, bsz=355.7, num_updates=8700, lr=0.000339032, gnorm=0.357, train_wall=121, gb_free=9.2, wall=15627
2022-09-08 04:27:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:27:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:27:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:27:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:27:44 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 4.96 | nll_loss 2.239 | mask_loss 8.4361 | p_2 0.04703 | mask_ave 0.653 | ppl 4.72 | bleu 56.55 | wps 1479.2 | wpb 933.5 | bsz 59.6 | num_updates 8748 | best_bleu 56.58
2022-09-08 04:27:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 8748 updates
2022-09-08 04:27:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint109.pt
2022-09-08 04:27:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint109.pt
2022-09-08 04:28:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint109.pt (epoch 109 @ 8748 updates, score 56.55) (writing took 17.622223787009716 seconds)
2022-09-08 04:28:02 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-09-08 04:28:02 | INFO | train | epoch 109 | loss 3.461 | nll_loss 0.401 | mask_loss 7.56117 | p_2 0.03512 | mask_ave 0.494 | ppl 1.32 | wps 3460.6 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 8748 | lr 0.0003381 | gnorm 0.348 | train_wall 98 | gb_free 9.1 | wall 15715
2022-09-08 04:28:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 04:28:02 | INFO | fairseq.trainer | begin training epoch 110
2022-09-08 04:28:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:29:07 | INFO | train_inner | epoch 110:     52 / 81 loss=3.458, nll_loss=0.399, mask_loss=7.50589, p_2=0.03528, mask_ave=0.497, ppl=1.32, wps=3618.2, ups=0.65, wpb=5543.1, bsz=363.8, num_updates=8800, lr=0.0003371, gnorm=0.342, train_wall=122, gb_free=9.1, wall=15780
2022-09-08 04:29:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:29:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:29:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:29:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:29:55 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 4.957 | nll_loss 2.246 | mask_loss 8.51258 | p_2 0.04767 | mask_ave 0.636 | ppl 4.74 | bleu 55.71 | wps 1550.8 | wpb 933.5 | bsz 59.6 | num_updates 8829 | best_bleu 56.58
2022-09-08 04:29:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 8829 updates
2022-09-08 04:29:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint110.pt
2022-09-08 04:29:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint110.pt
2022-09-08 04:30:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint110.pt (epoch 110 @ 8829 updates, score 55.71) (writing took 18.717597287148237 seconds)
2022-09-08 04:30:14 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-09-08 04:30:14 | INFO | train | epoch 110 | loss 3.459 | nll_loss 0.399 | mask_loss 7.51065 | p_2 0.03511 | mask_ave 0.494 | ppl 1.32 | wps 3398.5 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 8829 | lr 0.000336546 | gnorm 0.346 | train_wall 100 | gb_free 9.1 | wall 15846
2022-09-08 04:30:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 04:30:14 | INFO | fairseq.trainer | begin training epoch 111
2022-09-08 04:30:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:31:42 | INFO | train_inner | epoch 111:     71 / 81 loss=3.46, nll_loss=0.401, mask_loss=7.57232, p_2=0.03496, mask_ave=0.497, ppl=1.32, wps=3555.8, ups=0.64, wpb=5513.6, bsz=352.8, num_updates=8900, lr=0.000335201, gnorm=0.348, train_wall=124, gb_free=9.1, wall=15935
2022-09-08 04:31:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:31:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:31:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:31:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:32:06 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 4.956 | nll_loss 2.245 | mask_loss 8.38412 | p_2 0.04807 | mask_ave 0.624 | ppl 4.74 | bleu 56.24 | wps 1535.6 | wpb 933.5 | bsz 59.6 | num_updates 8910 | best_bleu 56.58
2022-09-08 04:32:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 8910 updates
2022-09-08 04:32:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint111.pt
2022-09-08 04:32:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint111.pt
2022-09-08 04:32:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint111.pt (epoch 111 @ 8910 updates, score 56.24) (writing took 18.10204527154565 seconds)
2022-09-08 04:32:24 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-09-08 04:32:24 | INFO | train | epoch 111 | loss 3.458 | nll_loss 0.399 | mask_loss 7.56025 | p_2 0.03504 | mask_ave 0.497 | ppl 1.32 | wps 3434.1 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 8910 | lr 0.000335013 | gnorm 0.343 | train_wall 99 | gb_free 9.1 | wall 15977
2022-09-08 04:32:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 04:32:24 | INFO | fairseq.trainer | begin training epoch 112
2022-09-08 04:32:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:34:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:34:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:34:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:34:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:34:17 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 4.953 | nll_loss 2.239 | mask_loss 8.58504 | p_2 0.04774 | mask_ave 0.634 | ppl 4.72 | bleu 55.39 | wps 1503.6 | wpb 933.5 | bsz 59.6 | num_updates 8991 | best_bleu 56.58
2022-09-08 04:34:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 8991 updates
2022-09-08 04:34:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint112.pt
2022-09-08 04:34:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint112.pt
2022-09-08 04:34:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint112.pt (epoch 112 @ 8991 updates, score 55.39) (writing took 18.6165181286633 seconds)
2022-09-08 04:34:36 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-09-08 04:34:36 | INFO | train | epoch 112 | loss 3.458 | nll_loss 0.399 | mask_loss 7.60892 | p_2 0.03513 | mask_ave 0.494 | ppl 1.32 | wps 3398.8 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 8991 | lr 0.0003335 | gnorm 0.356 | train_wall 100 | gb_free 9.1 | wall 16108
2022-09-08 04:34:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 04:34:36 | INFO | fairseq.trainer | begin training epoch 113
2022-09-08 04:34:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:34:48 | INFO | train_inner | epoch 113:      9 / 81 loss=3.457, nll_loss=0.398, mask_loss=7.59077, p_2=0.03521, mask_ave=0.493, ppl=1.32, wps=2973.1, ups=0.54, wpb=5516.3, bsz=361.8, num_updates=9000, lr=0.000333333, gnorm=0.351, train_wall=123, gb_free=9.1, wall=16121
2022-09-08 04:36:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:36:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:36:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:36:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:36:27 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 4.96 | nll_loss 2.248 | mask_loss 8.56471 | p_2 0.04758 | mask_ave 0.639 | ppl 4.75 | bleu 55.96 | wps 1531.7 | wpb 933.5 | bsz 59.6 | num_updates 9072 | best_bleu 56.58
2022-09-08 04:36:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 9072 updates
2022-09-08 04:36:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint113.pt
2022-09-08 04:36:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint113.pt
2022-09-08 04:36:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint113.pt (epoch 113 @ 9072 updates, score 55.96) (writing took 30.43228981643915 seconds)
2022-09-08 04:36:58 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-09-08 04:36:58 | INFO | train | epoch 113 | loss 3.457 | nll_loss 0.399 | mask_loss 7.60298 | p_2 0.03509 | mask_ave 0.495 | ppl 1.32 | wps 3137 | ups 0.57 | wpb 5523.2 | bsz 358 | num_updates 9072 | lr 0.000332008 | gnorm 0.363 | train_wall 99 | gb_free 9.1 | wall 16251
2022-09-08 04:36:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 04:36:58 | INFO | fairseq.trainer | begin training epoch 114
2022-09-08 04:36:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:37:33 | INFO | train_inner | epoch 114:     28 / 81 loss=3.458, nll_loss=0.399, mask_loss=7.6381, p_2=0.03478, mask_ave=0.495, ppl=1.32, wps=3334.4, ups=0.6, wpb=5521.9, bsz=351, num_updates=9100, lr=0.000331497, gnorm=0.362, train_wall=122, gb_free=9, wall=16286
2022-09-08 04:38:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:38:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:38:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:38:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:38:52 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 4.96 | nll_loss 2.245 | mask_loss 8.42124 | p_2 0.04739 | mask_ave 0.643 | ppl 4.74 | bleu 56.2 | wps 1565.7 | wpb 933.5 | bsz 59.6 | num_updates 9153 | best_bleu 56.58
2022-09-08 04:38:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 9153 updates
2022-09-08 04:38:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint114.pt
2022-09-08 04:38:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint114.pt
2022-09-08 04:39:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint114.pt (epoch 114 @ 9153 updates, score 56.2) (writing took 18.70837762579322 seconds)
2022-09-08 04:39:11 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-09-08 04:39:11 | INFO | train | epoch 114 | loss 3.454 | nll_loss 0.396 | mask_loss 7.57515 | p_2 0.03505 | mask_ave 0.496 | ppl 1.32 | wps 3378.2 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 9153 | lr 0.000330536 | gnorm 0.339 | train_wall 101 | gb_free 9.1 | wall 16383
2022-09-08 04:39:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 04:39:11 | INFO | fairseq.trainer | begin training epoch 115
2022-09-08 04:39:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:40:09 | INFO | train_inner | epoch 115:     47 / 81 loss=3.452, nll_loss=0.393, mask_loss=7.54267, p_2=0.03504, mask_ave=0.495, ppl=1.31, wps=3574, ups=0.64, wpb=5552.3, bsz=362.9, num_updates=9200, lr=0.00032969, gnorm=0.336, train_wall=124, gb_free=9.1, wall=16442
2022-09-08 04:40:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:40:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:40:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:40:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:41:01 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 4.953 | nll_loss 2.247 | mask_loss 8.56607 | p_2 0.04728 | mask_ave 0.647 | ppl 4.75 | bleu 56.43 | wps 1522.6 | wpb 933.5 | bsz 59.6 | num_updates 9234 | best_bleu 56.58
2022-09-08 04:41:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 9234 updates
2022-09-08 04:41:01 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint115.pt
2022-09-08 04:41:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint115.pt
2022-09-08 04:41:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint115.pt (epoch 115 @ 9234 updates, score 56.43) (writing took 21.317346781492233 seconds)
2022-09-08 04:41:23 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-09-08 04:41:23 | INFO | train | epoch 115 | loss 3.452 | nll_loss 0.394 | mask_loss 7.57449 | p_2 0.03504 | mask_ave 0.497 | ppl 1.31 | wps 3377.4 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 9234 | lr 0.000329083 | gnorm 0.348 | train_wall 98 | gb_free 9 | wall 16516
2022-09-08 04:41:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 04:41:23 | INFO | fairseq.trainer | begin training epoch 116
2022-09-08 04:41:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:42:44 | INFO | train_inner | epoch 116:     66 / 81 loss=3.454, nll_loss=0.396, mask_loss=7.57064, p_2=0.03535, mask_ave=0.5, ppl=1.32, wps=3543.8, ups=0.64, wpb=5505.9, bsz=361.1, num_updates=9300, lr=0.000327913, gnorm=0.351, train_wall=121, gb_free=9.1, wall=16597
2022-09-08 04:43:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:43:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:43:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:43:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:43:13 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 4.958 | nll_loss 2.252 | mask_loss 8.58919 | p_2 0.04765 | mask_ave 0.638 | ppl 4.76 | bleu 56.46 | wps 1478.8 | wpb 933.5 | bsz 59.6 | num_updates 9315 | best_bleu 56.58
2022-09-08 04:43:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 9315 updates
2022-09-08 04:43:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint116.pt
2022-09-08 04:43:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint116.pt
2022-09-08 04:43:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint116.pt (epoch 116 @ 9315 updates, score 56.46) (writing took 24.99907313287258 seconds)
2022-09-08 04:43:38 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-09-08 04:43:38 | INFO | train | epoch 116 | loss 3.453 | nll_loss 0.396 | mask_loss 7.60229 | p_2 0.03499 | mask_ave 0.498 | ppl 1.32 | wps 3308.7 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 9315 | lr 0.000327649 | gnorm 0.344 | train_wall 97 | gb_free 9.2 | wall 16651
2022-09-08 04:43:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 04:43:38 | INFO | fairseq.trainer | begin training epoch 117
2022-09-08 04:43:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:45:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:45:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:45:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:45:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:45:30 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 4.958 | nll_loss 2.243 | mask_loss 8.3957 | p_2 0.04799 | mask_ave 0.627 | ppl 4.73 | bleu 56.19 | wps 1470.7 | wpb 933.5 | bsz 59.6 | num_updates 9396 | best_bleu 56.58
2022-09-08 04:45:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 9396 updates
2022-09-08 04:45:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint117.pt
2022-09-08 04:45:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint117.pt
2022-09-08 04:45:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint117.pt (epoch 117 @ 9396 updates, score 56.19) (writing took 20.434935495257378 seconds)
2022-09-08 04:45:51 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-09-08 04:45:51 | INFO | train | epoch 117 | loss 3.449 | nll_loss 0.391 | mask_loss 7.65164 | p_2 0.03516 | mask_ave 0.494 | ppl 1.31 | wps 3378.3 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 9396 | lr 0.000326233 | gnorm 0.336 | train_wall 99 | gb_free 9 | wall 16784
2022-09-08 04:45:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 04:45:51 | INFO | fairseq.trainer | begin training epoch 118
2022-09-08 04:45:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:45:56 | INFO | train_inner | epoch 118:      4 / 81 loss=3.45, nll_loss=0.392, mask_loss=7.65514, p_2=0.03503, mask_ave=0.493, ppl=1.31, wps=2869.6, ups=0.52, wpb=5513.6, bsz=354.6, num_updates=9400, lr=0.000326164, gnorm=0.337, train_wall=121, gb_free=9.2, wall=16789
2022-09-08 04:47:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:47:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:47:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:47:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:47:43 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 4.95 | nll_loss 2.239 | mask_loss 8.59531 | p_2 0.0478 | mask_ave 0.633 | ppl 4.72 | bleu 56.45 | wps 1450.4 | wpb 933.5 | bsz 59.6 | num_updates 9477 | best_bleu 56.58
2022-09-08 04:47:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 9477 updates
2022-09-08 04:47:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint118.pt
2022-09-08 04:47:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint118.pt
2022-09-08 04:48:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint118.pt (epoch 118 @ 9477 updates, score 56.45) (writing took 20.271697249263525 seconds)
2022-09-08 04:48:03 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-09-08 04:48:03 | INFO | train | epoch 118 | loss 3.45 | nll_loss 0.393 | mask_loss 7.62026 | p_2 0.03521 | mask_ave 0.492 | ppl 1.31 | wps 3371.6 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 9477 | lr 0.000324836 | gnorm 0.341 | train_wall 99 | gb_free 9.1 | wall 16916
2022-09-08 04:48:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 04:48:04 | INFO | fairseq.trainer | begin training epoch 119
2022-09-08 04:48:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:48:32 | INFO | train_inner | epoch 119:     23 / 81 loss=3.449, nll_loss=0.392, mask_loss=7.65043, p_2=0.03529, mask_ave=0.493, ppl=1.31, wps=3535.2, ups=0.64, wpb=5510.8, bsz=356.3, num_updates=9500, lr=0.000324443, gnorm=0.337, train_wall=122, gb_free=9.1, wall=16945
2022-09-08 04:49:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:49:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:49:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:49:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:49:55 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 4.954 | nll_loss 2.25 | mask_loss 8.51423 | p_2 0.04705 | mask_ave 0.653 | ppl 4.76 | bleu 56.79 | wps 1508.4 | wpb 933.5 | bsz 59.6 | num_updates 9558 | best_bleu 56.79
2022-09-08 04:49:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 9558 updates
2022-09-08 04:49:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint119.pt
2022-09-08 04:49:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint119.pt
2022-09-08 04:51:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint119.pt (epoch 119 @ 9558 updates, score 56.79) (writing took 72.64875205233693 seconds)
2022-09-08 04:51:08 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-09-08 04:51:08 | INFO | train | epoch 119 | loss 3.449 | nll_loss 0.392 | mask_loss 7.65482 | p_2 0.03516 | mask_ave 0.494 | ppl 1.31 | wps 2424.3 | ups 0.44 | wpb 5523.2 | bsz 358 | num_updates 9558 | lr 0.000323457 | gnorm 0.33 | train_wall 99 | gb_free 9 | wall 17101
2022-09-08 04:51:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 04:51:08 | INFO | fairseq.trainer | begin training epoch 120
2022-09-08 04:51:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:52:01 | INFO | train_inner | epoch 120:     42 / 81 loss=3.449, nll_loss=0.393, mask_loss=7.71579, p_2=0.03443, mask_ave=0.492, ppl=1.31, wps=2665.6, ups=0.48, wpb=5565.9, bsz=354.7, num_updates=9600, lr=0.000322749, gnorm=0.335, train_wall=123, gb_free=9, wall=17154
2022-09-08 04:52:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:52:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:52:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:52:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:52:59 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 4.956 | nll_loss 2.251 | mask_loss 8.53622 | p_2 0.04753 | mask_ave 0.64 | ppl 4.76 | bleu 56.09 | wps 1515.2 | wpb 933.5 | bsz 59.6 | num_updates 9639 | best_bleu 56.79
2022-09-08 04:52:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 9639 updates
2022-09-08 04:52:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint120.pt
2022-09-08 04:53:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint120.pt
2022-09-08 04:53:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint120.pt (epoch 120 @ 9639 updates, score 56.09) (writing took 37.90001920610666 seconds)
2022-09-08 04:53:37 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-09-08 04:53:37 | INFO | train | epoch 120 | loss 3.448 | nll_loss 0.391 | mask_loss 7.66845 | p_2 0.03505 | mask_ave 0.497 | ppl 1.31 | wps 2996.6 | ups 0.54 | wpb 5523.2 | bsz 358 | num_updates 9639 | lr 0.000322095 | gnorm 0.34 | train_wall 98 | gb_free 9.2 | wall 17250
2022-09-08 04:53:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 04:53:37 | INFO | fairseq.trainer | begin training epoch 121
2022-09-08 04:53:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:54:54 | INFO | train_inner | epoch 121:     61 / 81 loss=3.447, nll_loss=0.39, mask_loss=7.59328, p_2=0.03567, mask_ave=0.498, ppl=1.31, wps=3188.2, ups=0.58, wpb=5522.6, bsz=367.1, num_updates=9700, lr=0.000321081, gnorm=0.337, train_wall=122, gb_free=9, wall=17327
2022-09-08 04:55:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:55:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:55:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:55:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:55:29 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 4.949 | nll_loss 2.245 | mask_loss 8.60065 | p_2 0.04774 | mask_ave 0.634 | ppl 4.74 | bleu 56.52 | wps 1469.8 | wpb 933.5 | bsz 59.6 | num_updates 9720 | best_bleu 56.79
2022-09-08 04:55:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 9720 updates
2022-09-08 04:55:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint121.pt
2022-09-08 04:55:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint121.pt
2022-09-08 04:55:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint121.pt (epoch 121 @ 9720 updates, score 56.52) (writing took 17.109963200986385 seconds)
2022-09-08 04:55:47 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-09-08 04:55:47 | INFO | train | epoch 121 | loss 3.447 | nll_loss 0.391 | mask_loss 7.68828 | p_2 0.03507 | mask_ave 0.496 | ppl 1.31 | wps 3453.4 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 9720 | lr 0.00032075 | gnorm 0.339 | train_wall 99 | gb_free 9.3 | wall 17380
2022-09-08 04:55:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 04:55:47 | INFO | fairseq.trainer | begin training epoch 122
2022-09-08 04:55:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:57:26 | INFO | train_inner | epoch 122:     80 / 81 loss=3.449, nll_loss=0.393, mask_loss=7.73688, p_2=0.03501, mask_ave=0.497, ppl=1.31, wps=3629.1, ups=0.66, wpb=5520.4, bsz=355.8, num_updates=9800, lr=0.000319438, gnorm=0.353, train_wall=121, gb_free=9.1, wall=17479
2022-09-08 04:57:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:57:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:57:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:57:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:57:39 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 4.955 | nll_loss 2.244 | mask_loss 8.71781 | p_2 0.04793 | mask_ave 0.629 | ppl 4.74 | bleu 56.28 | wps 1436.9 | wpb 933.5 | bsz 59.6 | num_updates 9801 | best_bleu 56.79
2022-09-08 04:57:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 9801 updates
2022-09-08 04:57:39 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint122.pt
2022-09-08 04:57:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint122.pt
2022-09-08 04:57:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint122.pt (epoch 122 @ 9801 updates, score 56.28) (writing took 14.890956711024046 seconds)
2022-09-08 04:57:54 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-09-08 04:57:54 | INFO | train | epoch 122 | loss 3.448 | nll_loss 0.392 | mask_loss 7.72933 | p_2 0.03506 | mask_ave 0.496 | ppl 1.31 | wps 3518.5 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 9801 | lr 0.000319422 | gnorm 0.354 | train_wall 98 | gb_free 9.1 | wall 17507
2022-09-08 04:57:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 04:57:54 | INFO | fairseq.trainer | begin training epoch 123
2022-09-08 04:57:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 04:59:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 04:59:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 04:59:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 04:59:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 04:59:46 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 4.955 | nll_loss 2.245 | mask_loss 8.44964 | p_2 0.04764 | mask_ave 0.638 | ppl 4.74 | bleu 56.54 | wps 1543.4 | wpb 933.5 | bsz 59.6 | num_updates 9882 | best_bleu 56.79
2022-09-08 04:59:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 9882 updates
2022-09-08 04:59:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint123.pt
2022-09-08 04:59:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint123.pt
2022-09-08 05:00:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint123.pt (epoch 123 @ 9882 updates, score 56.54) (writing took 16.195735033601522 seconds)
2022-09-08 05:00:02 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-09-08 05:00:02 | INFO | train | epoch 123 | loss 3.444 | nll_loss 0.387 | mask_loss 7.66625 | p_2 0.03511 | mask_ave 0.495 | ppl 1.31 | wps 3491.2 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 9882 | lr 0.00031811 | gnorm 0.33 | train_wall 99 | gb_free 9.2 | wall 17635
2022-09-08 05:00:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:00:02 | INFO | fairseq.trainer | begin training epoch 124
2022-09-08 05:00:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:00:25 | INFO | train_inner | epoch 124:     18 / 81 loss=3.444, nll_loss=0.387, mask_loss=7.65875, p_2=0.03504, mask_ave=0.496, ppl=1.31, wps=3087.7, ups=0.56, wpb=5499.2, bsz=355.5, num_updates=9900, lr=0.000317821, gnorm=0.332, train_wall=121, gb_free=9, wall=17657
2022-09-08 05:01:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 05:01:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 05:01:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 05:01:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 05:01:53 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 4.965 | nll_loss 2.264 | mask_loss 8.51127 | p_2 0.04764 | mask_ave 0.637 | ppl 4.8 | bleu 56.5 | wps 1529 | wpb 933.5 | bsz 59.6 | num_updates 9963 | best_bleu 56.79
2022-09-08 05:01:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 9963 updates
2022-09-08 05:01:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint124.pt
2022-09-08 05:01:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint124.pt
2022-09-08 05:02:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint124.pt (epoch 124 @ 9963 updates, score 56.5) (writing took 18.181687965989113 seconds)
2022-09-08 05:02:12 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-09-08 05:02:12 | INFO | train | epoch 124 | loss 3.445 | nll_loss 0.389 | mask_loss 7.65705 | p_2 0.03498 | mask_ave 0.499 | ppl 1.31 | wps 3455.3 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 9963 | lr 0.000316814 | gnorm 0.342 | train_wall 98 | gb_free 9.2 | wall 17764
2022-09-08 05:02:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:02:12 | INFO | fairseq.trainer | begin training epoch 125
2022-09-08 05:02:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:02:59 | INFO | train_inner | epoch 125:     37 / 81 loss=3.446, nll_loss=0.39, mask_loss=7.66411, p_2=0.03508, mask_ave=0.499, ppl=1.31, wps=3578.8, ups=0.65, wpb=5522.6, bsz=355.9, num_updates=10000, lr=0.000316228, gnorm=0.343, train_wall=123, gb_free=9, wall=17812
2022-09-08 05:03:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 05:03:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 05:03:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 05:03:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 05:04:07 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 4.973 | nll_loss 2.275 | mask_loss 8.60973 | p_2 0.04786 | mask_ave 0.633 | ppl 4.84 | bleu 56.42 | wps 1381.7 | wpb 933.5 | bsz 59.6 | num_updates 10044 | best_bleu 56.79
2022-09-08 05:04:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 10044 updates
2022-09-08 05:04:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint125.pt
2022-09-08 05:04:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint125.pt
2022-09-08 05:04:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint125.pt (epoch 125 @ 10044 updates, score 56.42) (writing took 21.1643377058208 seconds)
2022-09-08 05:04:28 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-09-08 05:04:28 | INFO | train | epoch 125 | loss 3.444 | nll_loss 0.388 | mask_loss 7.62188 | p_2 0.03509 | mask_ave 0.496 | ppl 1.31 | wps 3268.1 | ups 0.59 | wpb 5523.2 | bsz 358 | num_updates 10044 | lr 0.000315534 | gnorm 0.332 | train_wall 102 | gb_free 9.2 | wall 17901
2022-09-08 05:04:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:04:29 | INFO | fairseq.trainer | begin training epoch 126
2022-09-08 05:04:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:05:42 | INFO | train_inner | epoch 126:     56 / 81 loss=3.441, nll_loss=0.385, mask_loss=7.59905, p_2=0.03518, mask_ave=0.495, ppl=1.31, wps=3398.2, ups=0.61, wpb=5547.7, bsz=365, num_updates=10100, lr=0.000314658, gnorm=0.325, train_wall=128, gb_free=9, wall=17975
2022-09-08 05:06:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 05:06:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 05:06:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 05:06:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 05:06:25 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 4.953 | nll_loss 2.248 | mask_loss 8.36656 | p_2 0.04794 | mask_ave 0.629 | ppl 4.75 | bleu 56.56 | wps 1544 | wpb 933.5 | bsz 59.6 | num_updates 10125 | best_bleu 56.79
2022-09-08 05:06:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 10125 updates
2022-09-08 05:06:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint126.pt
2022-09-08 05:06:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint126.pt
2022-09-08 05:06:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint126.pt (epoch 126 @ 10125 updates, score 56.56) (writing took 21.17020360007882 seconds)
2022-09-08 05:06:46 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-09-08 05:06:46 | INFO | train | epoch 126 | loss 3.442 | nll_loss 0.386 | mask_loss 7.6102 | p_2 0.0351 | mask_ave 0.496 | ppl 1.31 | wps 3247.2 | ups 0.59 | wpb 5523.2 | bsz 358 | num_updates 10125 | lr 0.00031427 | gnorm 0.332 | train_wall 104 | gb_free 9.2 | wall 18039
2022-09-08 05:06:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:06:46 | INFO | fairseq.trainer | begin training epoch 127
2022-09-08 05:06:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:08:21 | INFO | train_inner | epoch 127:     75 / 81 loss=3.442, nll_loss=0.387, mask_loss=7.66232, p_2=0.0348, mask_ave=0.495, ppl=1.31, wps=3478.3, ups=0.63, wpb=5528.7, bsz=354.5, num_updates=10200, lr=0.000313112, gnorm=0.332, train_wall=125, gb_free=9.1, wall=18134
2022-09-08 05:08:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 05:08:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 05:08:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 05:08:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 05:08:40 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 4.957 | nll_loss 2.254 | mask_loss 8.60955 | p_2 0.0481 | mask_ave 0.626 | ppl 4.77 | bleu 56.19 | wps 1492.5 | wpb 933.5 | bsz 59.6 | num_updates 10206 | best_bleu 56.79
2022-09-08 05:08:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 10206 updates
2022-09-08 05:08:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint127.pt
2022-09-08 05:08:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint127.pt
2022-09-08 05:09:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint127.pt (epoch 127 @ 10206 updates, score 56.19) (writing took 21.9165166169405 seconds)
2022-09-08 05:09:02 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-09-08 05:09:02 | INFO | train | epoch 127 | loss 3.442 | nll_loss 0.386 | mask_loss 7.65644 | p_2 0.03508 | mask_ave 0.496 | ppl 1.31 | wps 3302.8 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 10206 | lr 0.00031302 | gnorm 0.333 | train_wall 100 | gb_free 9.2 | wall 18175
2022-09-08 05:09:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:09:02 | INFO | fairseq.trainer | begin training epoch 128
2022-09-08 05:09:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:10:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 05:10:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 05:10:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 05:10:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 05:10:57 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 4.967 | nll_loss 2.264 | mask_loss 8.65435 | p_2 0.04829 | mask_ave 0.619 | ppl 4.8 | bleu 55.99 | wps 1529.5 | wpb 933.5 | bsz 59.6 | num_updates 10287 | best_bleu 56.79
2022-09-08 05:10:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 10287 updates
2022-09-08 05:10:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint128.pt
2022-09-08 05:10:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint128.pt
2022-09-08 05:11:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint128.pt (epoch 128 @ 10287 updates, score 55.99) (writing took 2.7891960479319096 seconds)
2022-09-08 05:11:00 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-09-08 05:11:00 | INFO | train | epoch 128 | loss 3.442 | nll_loss 0.387 | mask_loss 7.72661 | p_2 0.03512 | mask_ave 0.495 | ppl 1.31 | wps 3776.7 | ups 0.68 | wpb 5523.2 | bsz 358 | num_updates 10287 | lr 0.000311785 | gnorm 0.337 | train_wall 103 | gb_free 9.2 | wall 18293
2022-09-08 05:11:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:11:00 | INFO | fairseq.trainer | begin training epoch 129
2022-09-08 05:11:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:11:17 | INFO | train_inner | epoch 129:     13 / 81 loss=3.441, nll_loss=0.386, mask_loss=7.68249, p_2=0.03539, mask_ave=0.494, ppl=1.31, wps=3128, ups=0.57, wpb=5504.3, bsz=361.1, num_updates=10300, lr=0.000311588, gnorm=0.338, train_wall=125, gb_free=9.1, wall=18310
2022-09-08 05:12:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 05:12:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 05:12:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 05:12:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 05:12:52 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 4.952 | nll_loss 2.253 | mask_loss 8.61665 | p_2 0.04779 | mask_ave 0.633 | ppl 4.77 | bleu 56.52 | wps 1521.9 | wpb 933.5 | bsz 59.6 | num_updates 10368 | best_bleu 56.79
2022-09-08 05:12:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 10368 updates
2022-09-08 05:12:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint129.pt
2022-09-08 05:12:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint129.pt
2022-09-08 05:13:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint129.pt (epoch 129 @ 10368 updates, score 56.52) (writing took 20.405379988253117 seconds)
2022-09-08 05:13:13 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-09-08 05:13:13 | INFO | train | epoch 129 | loss 3.44 | nll_loss 0.385 | mask_loss 7.75741 | p_2 0.03511 | mask_ave 0.495 | ppl 1.31 | wps 3375.7 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 10368 | lr 0.000310565 | gnorm 0.319 | train_wall 99 | gb_free 9.1 | wall 18426
2022-09-08 05:13:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:13:13 | INFO | fairseq.trainer | begin training epoch 130
2022-09-08 05:13:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:13:52 | INFO | train_inner | epoch 130:     32 / 81 loss=3.44, nll_loss=0.384, mask_loss=7.79367, p_2=0.03484, mask_ave=0.496, ppl=1.31, wps=3565.6, ups=0.65, wpb=5524.9, bsz=351.4, num_updates=10400, lr=0.000310087, gnorm=0.321, train_wall=122, gb_free=9, wall=18465
2022-09-08 05:14:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 05:14:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 05:14:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 05:14:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 05:15:05 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 4.975 | nll_loss 2.282 | mask_loss 8.67378 | p_2 0.0481 | mask_ave 0.624 | ppl 4.86 | bleu 55.63 | wps 1515.3 | wpb 933.5 | bsz 59.6 | num_updates 10449 | best_bleu 56.79
2022-09-08 05:15:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 10449 updates
2022-09-08 05:15:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint130.pt
2022-09-08 05:15:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint130.pt
2022-09-08 05:15:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint130.pt (epoch 130 @ 10449 updates, score 55.63) (writing took 2.7757008895277977 seconds)
2022-09-08 05:15:08 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-09-08 05:15:08 | INFO | train | epoch 130 | loss 3.439 | nll_loss 0.384 | mask_loss 7.72656 | p_2 0.03509 | mask_ave 0.496 | ppl 1.31 | wps 3893 | ups 0.7 | wpb 5523.2 | bsz 358 | num_updates 10449 | lr 0.000309359 | gnorm 0.325 | train_wall 99 | gb_free 9.1 | wall 18541
2022-09-08 05:15:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:15:08 | INFO | fairseq.trainer | begin training epoch 131
2022-09-08 05:15:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:16:11 | INFO | train_inner | epoch 131:     51 / 81 loss=3.439, nll_loss=0.384, mask_loss=7.75805, p_2=0.03524, mask_ave=0.498, ppl=1.3, wps=3964.8, ups=0.72, wpb=5522.8, bsz=359.1, num_updates=10500, lr=0.000308607, gnorm=0.323, train_wall=123, gb_free=9, wall=18604
2022-09-08 05:16:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 05:16:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 05:16:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 05:16:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 05:16:59 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 4.956 | nll_loss 2.263 | mask_loss 8.71776 | p_2 0.04796 | mask_ave 0.631 | ppl 4.8 | bleu 55.89 | wps 1480.1 | wpb 933.5 | bsz 59.6 | num_updates 10530 | best_bleu 56.79
2022-09-08 05:16:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 10530 updates
2022-09-08 05:16:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint131.pt
2022-09-08 05:17:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint131.pt
2022-09-08 05:17:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint131.pt (epoch 131 @ 10530 updates, score 55.89) (writing took 20.40163355320692 seconds)
2022-09-08 05:17:20 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-09-08 05:17:20 | INFO | train | epoch 131 | loss 3.438 | nll_loss 0.384 | mask_loss 7.77212 | p_2 0.03508 | mask_ave 0.497 | ppl 1.3 | wps 3385.1 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 10530 | lr 0.000308167 | gnorm 0.327 | train_wall 99 | gb_free 9.2 | wall 18673
2022-09-08 05:17:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:17:20 | INFO | fairseq.trainer | begin training epoch 132
2022-09-08 05:17:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:18:46 | INFO | train_inner | epoch 132:     70 / 81 loss=3.438, nll_loss=0.384, mask_loss=7.73856, p_2=0.03511, mask_ave=0.495, ppl=1.31, wps=3558.1, ups=0.64, wpb=5518.6, bsz=358.1, num_updates=10600, lr=0.000307148, gnorm=0.329, train_wall=121, gb_free=9, wall=18759
2022-09-08 05:18:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 05:19:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 05:19:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 05:19:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 05:19:11 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 4.938 | nll_loss 2.241 | mask_loss 8.74986 | p_2 0.04794 | mask_ave 0.631 | ppl 4.73 | bleu 55.75 | wps 1510.4 | wpb 933.5 | bsz 59.6 | num_updates 10611 | best_bleu 56.79
2022-09-08 05:19:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 10611 updates
2022-09-08 05:19:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint132.pt
2022-09-08 05:19:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint132.pt
2022-09-08 05:19:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint132.pt (epoch 132 @ 10611 updates, score 55.75) (writing took 18.15905986353755 seconds)
2022-09-08 05:19:29 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-09-08 05:19:29 | INFO | train | epoch 132 | loss 3.438 | nll_loss 0.383 | mask_loss 7.73128 | p_2 0.03514 | mask_ave 0.494 | ppl 1.3 | wps 3461.6 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 10611 | lr 0.000306988 | gnorm 0.323 | train_wall 98 | gb_free 9.1 | wall 18802
2022-09-08 05:19:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:19:29 | INFO | fairseq.trainer | begin training epoch 133
2022-09-08 05:19:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:21:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 05:21:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 05:21:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 05:21:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 05:21:20 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 4.983 | nll_loss 2.289 | mask_loss 8.71136 | p_2 0.04787 | mask_ave 0.632 | ppl 4.89 | bleu 56.04 | wps 1513 | wpb 933.5 | bsz 59.6 | num_updates 10692 | best_bleu 56.79
2022-09-08 05:21:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 10692 updates
2022-09-08 05:21:20 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint133.pt
2022-09-08 05:21:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint133.pt
2022-09-08 05:21:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint133.pt (epoch 133 @ 10692 updates, score 56.04) (writing took 2.862118359655142 seconds)
2022-09-08 05:21:23 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-09-08 05:21:23 | INFO | train | epoch 133 | loss 3.437 | nll_loss 0.382 | mask_loss 7.73727 | p_2 0.03508 | mask_ave 0.497 | ppl 1.3 | wps 3914.3 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 10692 | lr 0.000305823 | gnorm 0.325 | train_wall 98 | gb_free 9.1 | wall 18916
2022-09-08 05:21:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:21:24 | INFO | fairseq.trainer | begin training epoch 134
2022-09-08 05:21:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:21:34 | INFO | train_inner | epoch 134:      8 / 81 loss=3.437, nll_loss=0.383, mask_loss=7.72676, p_2=0.03509, mask_ave=0.496, ppl=1.3, wps=3289.3, ups=0.6, wpb=5512.1, bsz=358.6, num_updates=10700, lr=0.000305709, gnorm=0.324, train_wall=121, gb_free=9.1, wall=18927
2022-09-08 05:23:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 05:23:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 05:23:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 05:23:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 05:23:14 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 4.975 | nll_loss 2.285 | mask_loss 8.7183 | p_2 0.04781 | mask_ave 0.634 | ppl 4.87 | bleu 55.78 | wps 1552.2 | wpb 933.5 | bsz 59.6 | num_updates 10773 | best_bleu 56.79
2022-09-08 05:23:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 10773 updates
2022-09-08 05:23:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint134.pt
2022-09-08 05:23:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint134.pt
2022-09-08 05:23:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint134.pt (epoch 134 @ 10773 updates, score 55.78) (writing took 18.383676152676344 seconds)
2022-09-08 05:23:33 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-09-08 05:23:33 | INFO | train | epoch 134 | loss 3.435 | nll_loss 0.381 | mask_loss 7.74743 | p_2 0.03515 | mask_ave 0.495 | ppl 1.3 | wps 3453.7 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 10773 | lr 0.000304671 | gnorm 0.321 | train_wall 98 | gb_free 9.1 | wall 19046
2022-09-08 05:23:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:23:33 | INFO | fairseq.trainer | begin training epoch 135
2022-09-08 05:23:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:24:05 | INFO | train_inner | epoch 135:     27 / 81 loss=3.435, nll_loss=0.381, mask_loss=7.72392, p_2=0.0351, mask_ave=0.496, ppl=1.3, wps=3645.6, ups=0.66, wpb=5522.8, bsz=357.4, num_updates=10800, lr=0.00030429, gnorm=0.319, train_wall=120, gb_free=9, wall=19078
2022-09-08 05:25:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 05:25:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 05:25:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 05:25:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 05:25:22 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 4.959 | nll_loss 2.27 | mask_loss 8.81272 | p_2 0.0479 | mask_ave 0.631 | ppl 4.82 | bleu 56.55 | wps 1548.2 | wpb 933.5 | bsz 59.6 | num_updates 10854 | best_bleu 56.79
2022-09-08 05:25:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 10854 updates
2022-09-08 05:25:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint135.pt
2022-09-08 05:25:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint135.pt
2022-09-08 05:25:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint135.pt (epoch 135 @ 10854 updates, score 56.55) (writing took 19.663659043610096 seconds)
2022-09-08 05:25:42 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-09-08 05:25:42 | INFO | train | epoch 135 | loss 3.435 | nll_loss 0.381 | mask_loss 7.65243 | p_2 0.03503 | mask_ave 0.498 | ppl 1.3 | wps 3470.3 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 10854 | lr 0.000303532 | gnorm 0.319 | train_wall 97 | gb_free 9.1 | wall 19175
2022-09-08 05:25:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:25:42 | INFO | fairseq.trainer | begin training epoch 136
2022-09-08 05:25:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:26:41 | INFO | train_inner | epoch 136:     46 / 81 loss=3.433, nll_loss=0.38, mask_loss=7.67124, p_2=0.03502, mask_ave=0.496, ppl=1.3, wps=3562.6, ups=0.64, wpb=5558.8, bsz=364, num_updates=10900, lr=0.000302891, gnorm=0.313, train_wall=123, gb_free=9, wall=19234
2022-09-08 05:27:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 05:27:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 05:27:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 05:27:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 05:27:36 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 4.977 | nll_loss 2.29 | mask_loss 8.66942 | p_2 0.04787 | mask_ave 0.632 | ppl 4.89 | bleu 56.35 | wps 1507.6 | wpb 933.5 | bsz 59.6 | num_updates 10935 | best_bleu 56.79
2022-09-08 05:27:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 10935 updates
2022-09-08 05:27:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint136.pt
2022-09-08 05:27:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint136.pt
2022-09-08 05:27:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint136.pt (epoch 136 @ 10935 updates, score 56.35) (writing took 2.605726506561041 seconds)
2022-09-08 05:27:39 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-09-08 05:27:39 | INFO | train | epoch 136 | loss 3.434 | nll_loss 0.38 | mask_loss 7.75937 | p_2 0.03502 | mask_ave 0.498 | ppl 1.3 | wps 3831.6 | ups 0.69 | wpb 5523.2 | bsz 358 | num_updates 10935 | lr 0.000302406 | gnorm 0.314 | train_wall 101 | gb_free 9.1 | wall 19291
2022-09-08 05:27:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:27:39 | INFO | fairseq.trainer | begin training epoch 137
2022-09-08 05:27:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:29:00 | INFO | train_inner | epoch 137:     65 / 81 loss=3.434, nll_loss=0.381, mask_loss=7.70808, p_2=0.03531, mask_ave=0.497, ppl=1.3, wps=3968.5, ups=0.72, wpb=5507.3, bsz=357.4, num_updates=11000, lr=0.000301511, gnorm=0.323, train_wall=123, gb_free=9.2, wall=19373
2022-09-08 05:29:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 05:29:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 05:29:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 05:29:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 05:29:31 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 4.978 | nll_loss 2.284 | mask_loss 8.67911 | p_2 0.0476 | mask_ave 0.64 | ppl 4.87 | bleu 56.45 | wps 1471.2 | wpb 933.5 | bsz 59.6 | num_updates 11016 | best_bleu 56.79
2022-09-08 05:29:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 11016 updates
2022-09-08 05:29:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint137.pt
2022-09-08 05:29:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint137.pt
2022-09-08 05:29:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint137.pt (epoch 137 @ 11016 updates, score 56.45) (writing took 18.235297366976738 seconds)
2022-09-08 05:29:50 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-09-08 05:29:50 | INFO | train | epoch 137 | loss 3.434 | nll_loss 0.381 | mask_loss 7.69274 | p_2 0.03516 | mask_ave 0.494 | ppl 1.3 | wps 3413.1 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 11016 | lr 0.000301292 | gnorm 0.325 | train_wall 100 | gb_free 9.3 | wall 19423
2022-09-08 05:29:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:29:50 | INFO | fairseq.trainer | begin training epoch 138
2022-09-08 05:29:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:31:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 05:31:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 05:31:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 05:31:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 05:31:42 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 4.964 | nll_loss 2.275 | mask_loss 8.52874 | p_2 0.04783 | mask_ave 0.634 | ppl 4.84 | bleu 56.63 | wps 1471.3 | wpb 933.5 | bsz 59.6 | num_updates 11097 | best_bleu 56.79
2022-09-08 05:31:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 11097 updates
2022-09-08 05:31:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint138.pt
2022-09-08 05:31:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint138.pt
2022-09-08 05:32:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint138.pt (epoch 138 @ 11097 updates, score 56.63) (writing took 18.31805719062686 seconds)
2022-09-08 05:32:00 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-09-08 05:32:00 | INFO | train | epoch 138 | loss 3.434 | nll_loss 0.381 | mask_loss 7.67459 | p_2 0.03501 | mask_ave 0.499 | ppl 1.3 | wps 3420.8 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 11097 | lr 0.000300191 | gnorm 0.323 | train_wall 99 | gb_free 9 | wall 19553
2022-09-08 05:32:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:32:01 | INFO | fairseq.trainer | begin training epoch 139
2022-09-08 05:32:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:32:05 | INFO | train_inner | epoch 139:      3 / 81 loss=3.434, nll_loss=0.381, mask_loss=7.69854, p_2=0.03494, mask_ave=0.498, ppl=1.3, wps=2976.1, ups=0.54, wpb=5506.2, bsz=355.2, num_updates=11100, lr=0.00030015, gnorm=0.325, train_wall=122, gb_free=9.1, wall=19558
2022-09-08 05:33:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 05:33:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 05:33:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 05:33:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 05:33:52 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 4.971 | nll_loss 2.289 | mask_loss 8.65725 | p_2 0.04791 | mask_ave 0.633 | ppl 4.89 | bleu 56.65 | wps 1457.7 | wpb 933.5 | bsz 59.6 | num_updates 11178 | best_bleu 56.79
2022-09-08 05:33:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 11178 updates
2022-09-08 05:33:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint139.pt
2022-09-08 05:33:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint139.pt
2022-09-08 05:33:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint139.pt (epoch 139 @ 11178 updates, score 56.65) (writing took 2.905742958188057 seconds)
2022-09-08 05:33:55 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-09-08 05:33:55 | INFO | train | epoch 139 | loss 3.433 | nll_loss 0.38 | mask_loss 7.71626 | p_2 0.03511 | mask_ave 0.496 | ppl 1.3 | wps 3902.7 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 11178 | lr 0.000299101 | gnorm 0.321 | train_wall 98 | gb_free 9.1 | wall 19668
2022-09-08 05:33:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:33:55 | INFO | fairseq.trainer | begin training epoch 140
2022-09-08 05:33:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:34:23 | INFO | train_inner | epoch 140:     22 / 81 loss=3.432, nll_loss=0.379, mask_loss=7.72369, p_2=0.03502, mask_ave=0.496, ppl=1.3, wps=4020.1, ups=0.73, wpb=5528.9, bsz=358.6, num_updates=11200, lr=0.000298807, gnorm=0.319, train_wall=121, gb_free=9.1, wall=19696
2022-09-08 05:35:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 05:35:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 05:35:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 05:35:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 05:35:48 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 4.968 | nll_loss 2.284 | mask_loss 8.39924 | p_2 0.04787 | mask_ave 0.633 | ppl 4.87 | bleu 57.01 | wps 1463.9 | wpb 933.5 | bsz 59.6 | num_updates 11259 | best_bleu 57.01
2022-09-08 05:35:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 11259 updates
2022-09-08 05:35:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint140.pt
2022-09-08 05:35:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint140.pt
2022-09-08 05:36:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint140.pt (epoch 140 @ 11259 updates, score 57.01) (writing took 53.1786476187408 seconds)
2022-09-08 05:36:41 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-09-08 05:36:41 | INFO | train | epoch 140 | loss 3.432 | nll_loss 0.38 | mask_loss 7.65593 | p_2 0.03505 | mask_ave 0.497 | ppl 1.3 | wps 2689.4 | ups 0.49 | wpb 5523.2 | bsz 358 | num_updates 11259 | lr 0.000298023 | gnorm 0.321 | train_wall 100 | gb_free 9 | wall 19834
2022-09-08 05:36:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:36:42 | INFO | fairseq.trainer | begin training epoch 141
2022-09-08 05:36:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:37:32 | INFO | train_inner | epoch 141:     41 / 81 loss=3.431, nll_loss=0.378, mask_loss=7.66287, p_2=0.03519, mask_ave=0.498, ppl=1.3, wps=2917.6, ups=0.53, wpb=5530.7, bsz=359.9, num_updates=11300, lr=0.000297482, gnorm=0.316, train_wall=123, gb_free=9, wall=19885
2022-09-08 05:38:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 05:38:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 05:38:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 05:38:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 05:38:33 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 4.97 | nll_loss 2.281 | mask_loss 8.54423 | p_2 0.04765 | mask_ave 0.639 | ppl 4.86 | bleu 56.34 | wps 1495.4 | wpb 933.5 | bsz 59.6 | num_updates 11340 | best_bleu 57.01
2022-09-08 05:38:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 11340 updates
2022-09-08 05:38:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint141.pt
2022-09-08 05:38:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint141.pt
2022-09-08 05:39:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint141.pt (epoch 141 @ 11340 updates, score 56.34) (writing took 27.589568104594946 seconds)
2022-09-08 05:39:01 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-09-08 05:39:01 | INFO | train | epoch 141 | loss 3.43 | nll_loss 0.377 | mask_loss 7.73065 | p_2 0.03501 | mask_ave 0.499 | ppl 1.3 | wps 3202.3 | ups 0.58 | wpb 5523.2 | bsz 358 | num_updates 11340 | lr 0.000296957 | gnorm 0.306 | train_wall 99 | gb_free 9.1 | wall 19974
2022-09-08 05:39:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:39:01 | INFO | fairseq.trainer | begin training epoch 142
2022-09-08 05:39:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:40:16 | INFO | train_inner | epoch 142:     60 / 81 loss=3.43, nll_loss=0.378, mask_loss=7.76025, p_2=0.03501, mask_ave=0.497, ppl=1.3, wps=3387.4, ups=0.61, wpb=5531.9, bsz=357.7, num_updates=11400, lr=0.000296174, gnorm=0.313, train_wall=122, gb_free=9.1, wall=20049
2022-09-08 05:40:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 05:40:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 05:40:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 05:40:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 05:40:52 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 4.973 | nll_loss 2.286 | mask_loss 8.65988 | p_2 0.0482 | mask_ave 0.623 | ppl 4.88 | bleu 55.88 | wps 1594.5 | wpb 933.5 | bsz 59.6 | num_updates 11421 | best_bleu 57.01
2022-09-08 05:40:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 11421 updates
2022-09-08 05:40:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint142.pt
2022-09-08 05:40:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint142.pt
2022-09-08 05:41:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint142.pt (epoch 142 @ 11421 updates, score 55.88) (writing took 15.938042599707842 seconds)
2022-09-08 05:41:08 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-09-08 05:41:08 | INFO | train | epoch 142 | loss 3.43 | nll_loss 0.378 | mask_loss 7.77634 | p_2 0.03508 | mask_ave 0.497 | ppl 1.3 | wps 3520.8 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 11421 | lr 0.000295902 | gnorm 0.318 | train_wall 98 | gb_free 9.2 | wall 20101
2022-09-08 05:41:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:41:08 | INFO | fairseq.trainer | begin training epoch 143
2022-09-08 05:41:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:42:47 | INFO | train_inner | epoch 143:     79 / 81 loss=3.43, nll_loss=0.378, mask_loss=7.80303, p_2=0.03504, mask_ave=0.495, ppl=1.3, wps=3647, ups=0.66, wpb=5528.7, bsz=356.4, num_updates=11500, lr=0.000294884, gnorm=0.312, train_wall=123, gb_free=9.2, wall=20200
2022-09-08 05:42:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 05:42:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 05:42:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 05:42:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 05:43:00 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 4.976 | nll_loss 2.291 | mask_loss 8.57139 | p_2 0.04828 | mask_ave 0.62 | ppl 4.89 | bleu 56.12 | wps 1500.6 | wpb 933.5 | bsz 59.6 | num_updates 11502 | best_bleu 57.01
2022-09-08 05:43:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 11502 updates
2022-09-08 05:43:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint143.pt
2022-09-08 05:43:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint143.pt
2022-09-08 05:43:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint143.pt (epoch 143 @ 11502 updates, score 56.12) (writing took 15.850931577384472 seconds)
2022-09-08 05:43:17 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-09-08 05:43:17 | INFO | train | epoch 143 | loss 3.43 | nll_loss 0.377 | mask_loss 7.79507 | p_2 0.03516 | mask_ave 0.494 | ppl 1.3 | wps 3488.5 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 11502 | lr 0.000294858 | gnorm 0.309 | train_wall 99 | gb_free 9.1 | wall 20229
2022-09-08 05:43:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:43:17 | INFO | fairseq.trainer | begin training epoch 144
2022-09-08 05:43:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:44:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 05:44:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 05:44:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 05:44:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 05:45:07 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 4.972 | nll_loss 2.292 | mask_loss 8.56527 | p_2 0.04834 | mask_ave 0.619 | ppl 4.9 | bleu 56.32 | wps 1483.1 | wpb 933.5 | bsz 59.6 | num_updates 11583 | best_bleu 57.01
2022-09-08 05:45:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 11583 updates
2022-09-08 05:45:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint144.pt
2022-09-08 05:45:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint144.pt
2022-09-08 05:45:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint144.pt (epoch 144 @ 11583 updates, score 56.32) (writing took 21.763876654207706 seconds)
2022-09-08 05:45:29 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-09-08 05:45:29 | INFO | train | epoch 144 | loss 3.429 | nll_loss 0.377 | mask_loss 7.74137 | p_2 0.0352 | mask_ave 0.493 | ppl 1.3 | wps 3388.7 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 11583 | lr 0.000293825 | gnorm 0.315 | train_wall 97 | gb_free 9.2 | wall 20361
2022-09-08 05:45:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:45:29 | INFO | fairseq.trainer | begin training epoch 145
2022-09-08 05:45:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:45:51 | INFO | train_inner | epoch 145:     17 / 81 loss=3.429, nll_loss=0.376, mask_loss=7.75274, p_2=0.03521, mask_ave=0.493, ppl=1.3, wps=2997.5, ups=0.55, wpb=5495.5, bsz=357.4, num_updates=11600, lr=0.00029361, gnorm=0.314, train_wall=120, gb_free=9.1, wall=20384
2022-09-08 05:47:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 05:47:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 05:47:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 05:47:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 05:47:21 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 4.977 | nll_loss 2.295 | mask_loss 8.643 | p_2 0.04803 | mask_ave 0.629 | ppl 4.91 | bleu 56.51 | wps 1521 | wpb 933.5 | bsz 59.6 | num_updates 11664 | best_bleu 57.01
2022-09-08 05:47:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 11664 updates
2022-09-08 05:47:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint145.pt
2022-09-08 05:47:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint145.pt
2022-09-08 05:47:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint145.pt (epoch 145 @ 11664 updates, score 56.51) (writing took 15.834488708525896 seconds)
2022-09-08 05:47:37 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-09-08 05:47:37 | INFO | train | epoch 145 | loss 3.429 | nll_loss 0.377 | mask_loss 7.74811 | p_2 0.03505 | mask_ave 0.497 | ppl 1.3 | wps 3481 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 11664 | lr 0.000292803 | gnorm 0.314 | train_wall 100 | gb_free 9 | wall 20490
2022-09-08 05:47:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:47:37 | INFO | fairseq.trainer | begin training epoch 146
2022-09-08 05:47:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:48:23 | INFO | train_inner | epoch 146:     36 / 81 loss=3.428, nll_loss=0.377, mask_loss=7.71044, p_2=0.0351, mask_ave=0.498, ppl=1.3, wps=3625.3, ups=0.66, wpb=5520, bsz=357.9, num_updates=11700, lr=0.000292353, gnorm=0.313, train_wall=123, gb_free=9, wall=20536
2022-09-08 05:49:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 05:49:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 05:49:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 05:49:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 05:49:30 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 4.966 | nll_loss 2.282 | mask_loss 8.29324 | p_2 0.04826 | mask_ave 0.621 | ppl 4.87 | bleu 57.09 | wps 1545.7 | wpb 933.5 | bsz 59.6 | num_updates 11745 | best_bleu 57.09
2022-09-08 05:49:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 11745 updates
2022-09-08 05:49:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint146.pt
2022-09-08 05:49:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint146.pt
2022-09-08 05:50:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint146.pt (epoch 146 @ 11745 updates, score 57.09) (writing took 39.6284274533391 seconds)
2022-09-08 05:50:10 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-09-08 05:50:10 | INFO | train | epoch 146 | loss 3.428 | nll_loss 0.376 | mask_loss 7.66529 | p_2 0.03508 | mask_ave 0.497 | ppl 1.3 | wps 2931.4 | ups 0.53 | wpb 5523.2 | bsz 358 | num_updates 11745 | lr 0.000291792 | gnorm 0.306 | train_wall 100 | gb_free 9.1 | wall 20643
2022-09-08 05:50:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:50:10 | INFO | fairseq.trainer | begin training epoch 147
2022-09-08 05:50:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:51:18 | INFO | train_inner | epoch 147:     55 / 81 loss=3.427, nll_loss=0.375, mask_loss=7.61987, p_2=0.035, mask_ave=0.495, ppl=1.3, wps=3181.2, ups=0.57, wpb=5564.8, bsz=359.8, num_updates=11800, lr=0.000291111, gnorm=0.309, train_wall=122, gb_free=9, wall=20711
2022-09-08 05:51:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 05:51:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 05:51:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 05:51:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 05:52:04 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 4.981 | nll_loss 2.305 | mask_loss 8.64753 | p_2 0.04832 | mask_ave 0.621 | ppl 4.94 | bleu 56.64 | wps 1140.9 | wpb 933.5 | bsz 59.6 | num_updates 11826 | best_bleu 57.09
2022-09-08 05:52:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 11826 updates
2022-09-08 05:52:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint147.pt
2022-09-08 05:52:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint147.pt
2022-09-08 05:52:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint147.pt (epoch 147 @ 11826 updates, score 56.64) (writing took 23.50357548892498 seconds)
2022-09-08 05:52:27 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-09-08 05:52:27 | INFO | train | epoch 147 | loss 3.425 | nll_loss 0.373 | mask_loss 7.65553 | p_2 0.03515 | mask_ave 0.494 | ppl 1.3 | wps 3251.1 | ups 0.59 | wpb 5523.2 | bsz 358 | num_updates 11826 | lr 0.000290791 | gnorm 0.309 | train_wall 98 | gb_free 9.2 | wall 20780
2022-09-08 05:52:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:52:27 | INFO | fairseq.trainer | begin training epoch 148
2022-09-08 05:52:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:54:23 | INFO | train_inner | epoch 148:     74 / 81 loss=3.426, nll_loss=0.375, mask_loss=7.77637, p_2=0.03524, mask_ave=0.495, ppl=1.3, wps=2969.6, ups=0.54, wpb=5501.1, bsz=356.8, num_updates=11900, lr=0.000289886, gnorm=0.311, train_wall=146, gb_free=9.1, wall=20896
2022-09-08 05:54:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 05:54:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 05:54:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 05:54:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 05:54:42 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 4.992 | nll_loss 2.311 | mask_loss 8.62551 | p_2 0.04866 | mask_ave 0.61 | ppl 4.96 | bleu 56.11 | wps 1519.6 | wpb 933.5 | bsz 59.6 | num_updates 11907 | best_bleu 57.09
2022-09-08 05:54:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 148 @ 11907 updates
2022-09-08 05:54:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint148.pt
2022-09-08 05:54:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint148.pt
2022-09-08 05:55:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint148.pt (epoch 148 @ 11907 updates, score 56.11) (writing took 35.14673275128007 seconds)
2022-09-08 05:55:18 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2022-09-08 05:55:18 | INFO | train | epoch 148 | loss 3.427 | nll_loss 0.375 | mask_loss 7.75913 | p_2 0.03516 | mask_ave 0.495 | ppl 1.3 | wps 2626.2 | ups 0.48 | wpb 5523.2 | bsz 358 | num_updates 11907 | lr 0.0002898 | gnorm 0.313 | train_wall 122 | gb_free 9.1 | wall 20951
2022-09-08 05:55:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:55:18 | INFO | fairseq.trainer | begin training epoch 149
2022-09-08 05:55:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:56:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 05:56:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 05:56:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 05:56:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 05:57:08 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 4.974 | nll_loss 2.296 | mask_loss 8.74573 | p_2 0.0486 | mask_ave 0.613 | ppl 4.91 | bleu 56.45 | wps 1530.5 | wpb 933.5 | bsz 59.6 | num_updates 11988 | best_bleu 57.09
2022-09-08 05:57:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 11988 updates
2022-09-08 05:57:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint149.pt
2022-09-08 05:57:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint149.pt
2022-09-08 05:57:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint149.pt (epoch 149 @ 11988 updates, score 56.45) (writing took 2.7768388241529465 seconds)
2022-09-08 05:57:11 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2022-09-08 05:57:11 | INFO | train | epoch 149 | loss 3.427 | nll_loss 0.376 | mask_loss 7.7727 | p_2 0.03524 | mask_ave 0.492 | ppl 1.3 | wps 3937.7 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 11988 | lr 0.00028882 | gnorm 0.314 | train_wall 98 | gb_free 9.1 | wall 21064
2022-09-08 05:57:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:57:11 | INFO | fairseq.trainer | begin training epoch 150
2022-09-08 05:57:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:57:27 | INFO | train_inner | epoch 150:     12 / 81 loss=3.427, nll_loss=0.375, mask_loss=7.79512, p_2=0.03513, mask_ave=0.492, ppl=1.3, wps=3005.5, ups=0.55, wpb=5512.9, bsz=356.2, num_updates=12000, lr=0.000288675, gnorm=0.311, train_wall=120, gb_free=9.1, wall=21079
2022-09-08 05:58:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 05:58:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 05:58:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 05:58:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 05:59:00 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 4.993 | nll_loss 2.325 | mask_loss 8.74231 | p_2 0.04862 | mask_ave 0.611 | ppl 5.01 | bleu 56 | wps 1511.9 | wpb 933.5 | bsz 59.6 | num_updates 12069 | best_bleu 57.09
2022-09-08 05:59:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 150 @ 12069 updates
2022-09-08 05:59:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint150.pt
2022-09-08 05:59:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint150.pt
2022-09-08 05:59:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint150.pt (epoch 150 @ 12069 updates, score 56.0) (writing took 18.66458997130394 seconds)
2022-09-08 05:59:19 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2022-09-08 05:59:19 | INFO | train | epoch 150 | loss 3.425 | nll_loss 0.374 | mask_loss 7.84379 | p_2 0.03517 | mask_ave 0.494 | ppl 1.3 | wps 3511.6 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 12069 | lr 0.000287849 | gnorm 0.299 | train_wall 96 | gb_free 9 | wall 21192
2022-09-08 05:59:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 05:59:19 | INFO | fairseq.trainer | begin training epoch 151
2022-09-08 05:59:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 05:59:57 | INFO | train_inner | epoch 151:     31 / 81 loss=3.425, nll_loss=0.374, mask_loss=7.79668, p_2=0.03522, mask_ave=0.494, ppl=1.3, wps=3677.8, ups=0.66, wpb=5540.2, bsz=358.1, num_updates=12100, lr=0.00028748, gnorm=0.303, train_wall=119, gb_free=9, wall=21230
2022-09-08 06:00:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 06:00:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 06:00:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 06:00:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 06:01:08 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 4.976 | nll_loss 2.296 | mask_loss 8.67523 | p_2 0.04819 | mask_ave 0.625 | ppl 4.91 | bleu 56.85 | wps 1503.4 | wpb 933.5 | bsz 59.6 | num_updates 12150 | best_bleu 57.09
2022-09-08 06:01:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 151 @ 12150 updates
2022-09-08 06:01:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint151.pt
2022-09-08 06:01:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint151.pt
2022-09-08 06:01:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint151.pt (epoch 151 @ 12150 updates, score 56.85) (writing took 39.812973253428936 seconds)
2022-09-08 06:01:48 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2022-09-08 06:01:48 | INFO | train | epoch 151 | loss 3.426 | nll_loss 0.375 | mask_loss 7.78082 | p_2 0.03515 | mask_ave 0.495 | ppl 1.3 | wps 3000.4 | ups 0.54 | wpb 5523.2 | bsz 358 | num_updates 12150 | lr 0.000286888 | gnorm 0.32 | train_wall 96 | gb_free 9.2 | wall 21341
2022-09-08 06:01:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 06:01:48 | INFO | fairseq.trainer | begin training epoch 152
2022-09-08 06:01:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 06:02:52 | INFO | train_inner | epoch 152:     50 / 81 loss=3.426, nll_loss=0.375, mask_loss=7.78137, p_2=0.03522, mask_ave=0.495, ppl=1.3, wps=3169.4, ups=0.57, wpb=5526.1, bsz=359.8, num_updates=12200, lr=0.000286299, gnorm=0.325, train_wall=121, gb_free=9.2, wall=21404
2022-09-08 06:03:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 06:03:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 06:03:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 06:03:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 06:03:40 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 4.993 | nll_loss 2.32 | mask_loss 8.73981 | p_2 0.04863 | mask_ave 0.613 | ppl 4.99 | bleu 55.92 | wps 1566.2 | wpb 933.5 | bsz 59.6 | num_updates 12231 | best_bleu 57.09
2022-09-08 06:03:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 152 @ 12231 updates
2022-09-08 06:03:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint152.pt
2022-09-08 06:03:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint152.pt
2022-09-08 06:04:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint152.pt (epoch 152 @ 12231 updates, score 55.92) (writing took 20.89509131759405 seconds)
2022-09-08 06:04:01 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2022-09-08 06:04:02 | INFO | train | epoch 152 | loss 3.425 | nll_loss 0.375 | mask_loss 7.8365 | p_2 0.03519 | mask_ave 0.494 | ppl 1.3 | wps 3346.1 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 12231 | lr 0.000285936 | gnorm 0.318 | train_wall 100 | gb_free 9.2 | wall 21474
2022-09-08 06:04:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 06:04:02 | INFO | fairseq.trainer | begin training epoch 153
2022-09-08 06:04:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 06:05:26 | INFO | train_inner | epoch 153:     69 / 81 loss=3.424, nll_loss=0.373, mask_loss=7.84058, p_2=0.0353, mask_ave=0.493, ppl=1.3, wps=3577.2, ups=0.65, wpb=5513.2, bsz=359, num_updates=12300, lr=0.000285133, gnorm=0.306, train_wall=120, gb_free=9.1, wall=21559
2022-09-08 06:05:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 06:05:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 06:05:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 06:05:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 06:05:51 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 5.001 | nll_loss 2.322 | mask_loss 8.73093 | p_2 0.04852 | mask_ave 0.616 | ppl 5 | bleu 56.26 | wps 1520.8 | wpb 933.5 | bsz 59.6 | num_updates 12312 | best_bleu 57.09
2022-09-08 06:05:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 153 @ 12312 updates
2022-09-08 06:05:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint153.pt
2022-09-08 06:05:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint153.pt
2022-09-08 06:06:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint153.pt (epoch 153 @ 12312 updates, score 56.26) (writing took 26.937481567263603 seconds)
2022-09-08 06:06:18 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2022-09-08 06:06:18 | INFO | train | epoch 153 | loss 3.423 | nll_loss 0.373 | mask_loss 7.82015 | p_2 0.03525 | mask_ave 0.492 | ppl 1.29 | wps 3286.5 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 12312 | lr 0.000284994 | gnorm 0.304 | train_wall 96 | gb_free 9.3 | wall 21611
2022-09-08 06:06:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 06:06:18 | INFO | fairseq.trainer | begin training epoch 154
2022-09-08 06:06:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 06:07:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 06:07:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 06:07:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 06:07:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 06:08:09 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 4.976 | nll_loss 2.3 | mask_loss 8.75053 | p_2 0.04821 | mask_ave 0.624 | ppl 4.93 | bleu 56.77 | wps 1400.6 | wpb 933.5 | bsz 59.6 | num_updates 12393 | best_bleu 57.09
2022-09-08 06:08:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 154 @ 12393 updates
2022-09-08 06:08:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint154.pt
2022-09-08 06:08:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint154.pt
2022-09-08 06:08:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint154.pt (epoch 154 @ 12393 updates, score 56.77) (writing took 22.57019166275859 seconds)
2022-09-08 06:08:32 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2022-09-08 06:08:32 | INFO | train | epoch 154 | loss 3.422 | nll_loss 0.371 | mask_loss 7.88299 | p_2 0.03516 | mask_ave 0.495 | ppl 1.29 | wps 3335.8 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 12393 | lr 0.000284061 | gnorm 0.302 | train_wall 98 | gb_free 9.1 | wall 21745
2022-09-08 06:08:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 06:08:32 | INFO | fairseq.trainer | begin training epoch 155
2022-09-08 06:08:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 06:08:41 | INFO | train_inner | epoch 155:      7 / 81 loss=3.422, nll_loss=0.371, mask_loss=7.89267, p_2=0.03509, mask_ave=0.494, ppl=1.29, wps=2821.6, ups=0.51, wpb=5514.2, bsz=356.9, num_updates=12400, lr=0.000283981, gnorm=0.302, train_wall=120, gb_free=9.1, wall=21754
2022-09-08 06:10:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 06:10:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 06:10:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 06:10:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 06:10:24 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 4.98 | nll_loss 2.305 | mask_loss 8.79997 | p_2 0.04833 | mask_ave 0.62 | ppl 4.94 | bleu 55.65 | wps 1461.9 | wpb 933.5 | bsz 59.6 | num_updates 12474 | best_bleu 57.09
2022-09-08 06:10:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 155 @ 12474 updates
2022-09-08 06:10:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint155.pt
2022-09-08 06:10:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint155.pt
2022-09-08 06:10:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint155.pt (epoch 155 @ 12474 updates, score 55.65) (writing took 28.93496411666274 seconds)
2022-09-08 06:10:53 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2022-09-08 06:10:53 | INFO | train | epoch 155 | loss 3.423 | nll_loss 0.373 | mask_loss 7.8741 | p_2 0.03511 | mask_ave 0.496 | ppl 1.29 | wps 3168.6 | ups 0.57 | wpb 5523.2 | bsz 358 | num_updates 12474 | lr 0.000283137 | gnorm 0.309 | train_wall 99 | gb_free 9 | wall 21886
2022-09-08 06:10:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 06:10:53 | INFO | fairseq.trainer | begin training epoch 156
2022-09-08 06:10:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 06:11:25 | INFO | train_inner | epoch 156:     26 / 81 loss=3.423, nll_loss=0.373, mask_loss=7.85859, p_2=0.03513, mask_ave=0.496, ppl=1.29, wps=3365.1, ups=0.61, wpb=5517.8, bsz=357.2, num_updates=12500, lr=0.000282843, gnorm=0.309, train_wall=121, gb_free=9, wall=21918
2022-09-08 06:12:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 06:12:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 06:12:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 06:12:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 06:12:44 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 4.979 | nll_loss 2.302 | mask_loss 8.7586 | p_2 0.04832 | mask_ave 0.621 | ppl 4.93 | bleu 55.84 | wps 1485.9 | wpb 933.5 | bsz 59.6 | num_updates 12555 | best_bleu 57.09
2022-09-08 06:12:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 156 @ 12555 updates
2022-09-08 06:12:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint156.pt
2022-09-08 06:12:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint156.pt
2022-09-08 06:13:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint156.pt (epoch 156 @ 12555 updates, score 55.84) (writing took 36.50400744006038 seconds)
2022-09-08 06:13:20 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2022-09-08 06:13:20 | INFO | train | epoch 156 | loss 3.423 | nll_loss 0.372 | mask_loss 7.81836 | p_2 0.03516 | mask_ave 0.495 | ppl 1.29 | wps 3037.3 | ups 0.55 | wpb 5523.2 | bsz 358 | num_updates 12555 | lr 0.000282223 | gnorm 0.308 | train_wall 98 | gb_free 9 | wall 22033
2022-09-08 06:13:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 06:13:20 | INFO | fairseq.trainer | begin training epoch 157
2022-09-08 06:13:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 06:14:15 | INFO | train_inner | epoch 157:     45 / 81 loss=3.422, nll_loss=0.372, mask_loss=7.89083, p_2=0.03517, mask_ave=0.496, ppl=1.29, wps=3249.9, ups=0.59, wpb=5508.6, bsz=354.6, num_updates=12600, lr=0.000281718, gnorm=0.305, train_wall=120, gb_free=9.2, wall=22087
2022-09-08 06:14:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 06:14:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 06:14:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 06:14:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 06:15:09 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 4.986 | nll_loss 2.315 | mask_loss 8.86263 | p_2 0.04815 | mask_ave 0.626 | ppl 4.97 | bleu 56.23 | wps 1516.1 | wpb 933.5 | bsz 59.6 | num_updates 12636 | best_bleu 57.09
2022-09-08 06:15:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 157 @ 12636 updates
2022-09-08 06:15:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint157.pt
2022-09-08 06:15:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint157.pt
2022-09-08 06:15:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint157.pt (epoch 157 @ 12636 updates, score 56.23) (writing took 15.26773040741682 seconds)
2022-09-08 06:15:24 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2022-09-08 06:15:24 | INFO | train | epoch 157 | loss 3.421 | nll_loss 0.371 | mask_loss 7.92868 | p_2 0.03515 | mask_ave 0.495 | ppl 1.29 | wps 3612.3 | ups 0.65 | wpb 5523.2 | bsz 358 | num_updates 12636 | lr 0.000281316 | gnorm 0.299 | train_wall 95 | gb_free 9.1 | wall 22157
2022-09-08 06:15:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 06:15:24 | INFO | fairseq.trainer | begin training epoch 158
2022-09-08 06:15:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 06:16:44 | INFO | train_inner | epoch 158:     64 / 81 loss=3.422, nll_loss=0.372, mask_loss=7.89588, p_2=0.03501, mask_ave=0.494, ppl=1.29, wps=3739, ups=0.67, wpb=5574.9, bsz=362.6, num_updates=12700, lr=0.000280607, gnorm=0.306, train_wall=121, gb_free=9.1, wall=22237
2022-09-08 06:17:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 06:17:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 06:17:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 06:17:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 06:17:15 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 4.983 | nll_loss 2.307 | mask_loss 8.89101 | p_2 0.04809 | mask_ave 0.627 | ppl 4.95 | bleu 56.47 | wps 1563.4 | wpb 933.5 | bsz 59.6 | num_updates 12717 | best_bleu 57.09
2022-09-08 06:17:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 158 @ 12717 updates
2022-09-08 06:17:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint158.pt
2022-09-08 06:17:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint158.pt
2022-09-08 06:17:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint158.pt (epoch 158 @ 12717 updates, score 56.47) (writing took 21.696372270584106 seconds)
2022-09-08 06:17:37 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2022-09-08 06:17:37 | INFO | train | epoch 158 | loss 3.421 | nll_loss 0.371 | mask_loss 7.89355 | p_2 0.03514 | mask_ave 0.495 | ppl 1.29 | wps 3366.2 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 12717 | lr 0.000280419 | gnorm 0.312 | train_wall 99 | gb_free 9 | wall 22290
2022-09-08 06:17:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 06:17:37 | INFO | fairseq.trainer | begin training epoch 159
2022-09-08 06:17:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 06:19:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 06:19:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 06:19:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 06:19:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 06:19:27 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 4.985 | nll_loss 2.319 | mask_loss 8.74758 | p_2 0.04827 | mask_ave 0.621 | ppl 4.99 | bleu 56.46 | wps 1542.8 | wpb 933.5 | bsz 59.6 | num_updates 12798 | best_bleu 57.09
2022-09-08 06:19:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 159 @ 12798 updates
2022-09-08 06:19:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint159.pt
2022-09-08 06:19:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint159.pt
2022-09-08 06:20:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint159.pt (epoch 159 @ 12798 updates, score 56.46) (writing took 37.4448077455163 seconds)
2022-09-08 06:20:05 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2022-09-08 06:20:05 | INFO | train | epoch 159 | loss 3.421 | nll_loss 0.371 | mask_loss 7.81494 | p_2 0.03518 | mask_ave 0.494 | ppl 1.29 | wps 3021.6 | ups 0.55 | wpb 5523.2 | bsz 358 | num_updates 12798 | lr 0.00027953 | gnorm 0.299 | train_wall 98 | gb_free 9.1 | wall 22438
2022-09-08 06:20:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 06:20:05 | INFO | fairseq.trainer | begin training epoch 160
2022-09-08 06:20:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 06:20:09 | INFO | train_inner | epoch 160:      2 / 81 loss=3.421, nll_loss=0.371, mask_loss=7.8122, p_2=0.03537, mask_ave=0.495, ppl=1.29, wps=2674.2, ups=0.49, wpb=5478.1, bsz=357, num_updates=12800, lr=0.000279508, gnorm=0.304, train_wall=121, gb_free=9, wall=22441
2022-09-08 06:21:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 06:21:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 06:21:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 06:21:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 06:21:58 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 4.984 | nll_loss 2.307 | mask_loss 8.75709 | p_2 0.04848 | mask_ave 0.616 | ppl 4.95 | bleu 56.14 | wps 1556.1 | wpb 933.5 | bsz 59.6 | num_updates 12879 | best_bleu 57.09
2022-09-08 06:21:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 12879 updates
2022-09-08 06:21:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint160.pt
2022-09-08 06:22:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint160.pt
2022-09-08 06:22:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint160.pt (epoch 160 @ 12879 updates, score 56.14) (writing took 20.99874472245574 seconds)
2022-09-08 06:22:19 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2022-09-08 06:22:19 | INFO | train | epoch 160 | loss 3.42 | nll_loss 0.37 | mask_loss 7.8754 | p_2 0.03525 | mask_ave 0.492 | ppl 1.29 | wps 3339.1 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 12879 | lr 0.00027865 | gnorm 0.305 | train_wall 100 | gb_free 9 | wall 22572
2022-09-08 06:22:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 06:22:19 | INFO | fairseq.trainer | begin training epoch 161
2022-09-08 06:22:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 06:22:45 | INFO | train_inner | epoch 161:     21 / 81 loss=3.42, nll_loss=0.37, mask_loss=7.90193, p_2=0.03515, mask_ave=0.492, ppl=1.29, wps=3539, ups=0.64, wpb=5543.6, bsz=358.9, num_updates=12900, lr=0.000278423, gnorm=0.297, train_wall=123, gb_free=9.1, wall=22598
2022-09-08 06:23:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 06:24:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 06:24:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 06:24:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 06:24:10 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 4.988 | nll_loss 2.315 | mask_loss 8.63116 | p_2 0.0487 | mask_ave 0.61 | ppl 4.98 | bleu 56.29 | wps 1454.9 | wpb 933.5 | bsz 59.6 | num_updates 12960 | best_bleu 57.09
2022-09-08 06:24:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 161 @ 12960 updates
2022-09-08 06:24:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint161.pt
2022-09-08 06:24:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint161.pt
2022-09-08 06:24:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint161.pt (epoch 161 @ 12960 updates, score 56.29) (writing took 27.79897117614746 seconds)
2022-09-08 06:24:38 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2022-09-08 06:24:38 | INFO | train | epoch 161 | loss 3.419 | nll_loss 0.37 | mask_loss 7.8812 | p_2 0.03517 | mask_ave 0.494 | ppl 1.29 | wps 3217.3 | ups 0.58 | wpb 5523.2 | bsz 358 | num_updates 12960 | lr 0.000277778 | gnorm 0.298 | train_wall 98 | gb_free 9.1 | wall 22711
2022-09-08 06:24:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 06:24:38 | INFO | fairseq.trainer | begin training epoch 162
2022-09-08 06:24:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 06:25:29 | INFO | train_inner | epoch 162:     40 / 81 loss=3.42, nll_loss=0.37, mask_loss=7.82746, p_2=0.03548, mask_ave=0.497, ppl=1.29, wps=3352.4, ups=0.61, wpb=5498.1, bsz=358.8, num_updates=13000, lr=0.00027735, gnorm=0.302, train_wall=123, gb_free=9.1, wall=22762
2022-09-08 06:26:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 06:26:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 06:26:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 06:26:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 06:26:31 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 4.98 | nll_loss 2.307 | mask_loss 8.77989 | p_2 0.04811 | mask_ave 0.627 | ppl 4.95 | bleu 56.2 | wps 1453.4 | wpb 933.5 | bsz 59.6 | num_updates 13041 | best_bleu 57.09
2022-09-08 06:26:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 162 @ 13041 updates
2022-09-08 06:26:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint162.pt
2022-09-08 06:26:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint162.pt
2022-09-08 06:26:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint162.pt (epoch 162 @ 13041 updates, score 56.2) (writing took 2.9539430923759937 seconds)
2022-09-08 06:26:35 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2022-09-08 06:26:35 | INFO | train | epoch 162 | loss 3.419 | nll_loss 0.37 | mask_loss 7.83211 | p_2 0.03516 | mask_ave 0.495 | ppl 1.29 | wps 3840.5 | ups 0.7 | wpb 5523.2 | bsz 358 | num_updates 13041 | lr 0.000276914 | gnorm 0.3 | train_wall 100 | gb_free 9.1 | wall 22828
2022-09-08 06:26:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 06:26:35 | INFO | fairseq.trainer | begin training epoch 163
2022-09-08 06:26:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 06:27:47 | INFO | train_inner | epoch 163:     59 / 81 loss=3.419, nll_loss=0.37, mask_loss=7.89746, p_2=0.03476, mask_ave=0.494, ppl=1.29, wps=4022.7, ups=0.72, wpb=5552.4, bsz=357.4, num_updates=13100, lr=0.000276289, gnorm=0.303, train_wall=121, gb_free=9.2, wall=22900
2022-09-08 06:28:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 06:28:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 06:28:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 06:28:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 06:28:25 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 4.973 | nll_loss 2.309 | mask_loss 8.70732 | p_2 0.04824 | mask_ave 0.624 | ppl 4.95 | bleu 56.55 | wps 1537.2 | wpb 933.5 | bsz 59.6 | num_updates 13122 | best_bleu 57.09
2022-09-08 06:28:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 163 @ 13122 updates
2022-09-08 06:28:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint163.pt
2022-09-08 06:28:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint163.pt
2022-09-08 06:28:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint163.pt (epoch 163 @ 13122 updates, score 56.55) (writing took 15.957513514906168 seconds)
2022-09-08 06:28:41 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2022-09-08 06:28:41 | INFO | train | epoch 163 | loss 3.418 | nll_loss 0.369 | mask_loss 7.90379 | p_2 0.03508 | mask_ave 0.498 | ppl 1.29 | wps 3538.9 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 13122 | lr 0.000276058 | gnorm 0.305 | train_wall 97 | gb_free 9.2 | wall 22954
2022-09-08 06:28:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 06:28:41 | INFO | fairseq.trainer | begin training epoch 164
2022-09-08 06:28:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 06:30:17 | INFO | train_inner | epoch 164:     78 / 81 loss=3.419, nll_loss=0.37, mask_loss=7.91579, p_2=0.03522, mask_ave=0.495, ppl=1.29, wps=3683.4, ups=0.67, wpb=5529.3, bsz=357.7, num_updates=13200, lr=0.000275241, gnorm=0.304, train_wall=121, gb_free=9, wall=23050
2022-09-08 06:30:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 06:30:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 06:30:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 06:30:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 06:30:32 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 4.998 | nll_loss 2.337 | mask_loss 8.7805 | p_2 0.04791 | mask_ave 0.633 | ppl 5.05 | bleu 56.63 | wps 1544.9 | wpb 933.5 | bsz 59.6 | num_updates 13203 | best_bleu 57.09
2022-09-08 06:30:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 164 @ 13203 updates
2022-09-08 06:30:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint164.pt
2022-09-08 06:30:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint164.pt
2022-09-08 06:30:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint164.pt (epoch 164 @ 13203 updates, score 56.63) (writing took 19.940907519310713 seconds)
2022-09-08 06:30:52 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2022-09-08 06:30:52 | INFO | train | epoch 164 | loss 3.419 | nll_loss 0.37 | mask_loss 7.9324 | p_2 0.03519 | mask_ave 0.494 | ppl 1.29 | wps 3421.9 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 13203 | lr 0.00027521 | gnorm 0.301 | train_wall 98 | gb_free 9.2 | wall 23085
2022-09-08 06:30:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 06:30:52 | INFO | fairseq.trainer | begin training epoch 165
2022-09-08 06:30:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 06:32:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 06:32:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 06:32:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 06:32:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 06:32:43 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 4.99 | nll_loss 2.32 | mask_loss 8.79158 | p_2 0.04827 | mask_ave 0.623 | ppl 4.99 | bleu 56.62 | wps 1511.9 | wpb 933.5 | bsz 59.6 | num_updates 13284 | best_bleu 57.09
2022-09-08 06:32:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 165 @ 13284 updates
2022-09-08 06:32:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint165.pt
2022-09-08 06:32:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint165.pt
2022-09-08 06:33:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint165.pt (epoch 165 @ 13284 updates, score 56.62) (writing took 21.864155814051628 seconds)
2022-09-08 06:33:05 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2022-09-08 06:33:05 | INFO | train | epoch 165 | loss 3.418 | nll_loss 0.369 | mask_loss 7.90106 | p_2 0.03504 | mask_ave 0.499 | ppl 1.29 | wps 3359 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 13284 | lr 0.000274369 | gnorm 0.296 | train_wall 98 | gb_free 9.1 | wall 23218
2022-09-08 06:33:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 06:33:05 | INFO | fairseq.trainer | begin training epoch 166
2022-09-08 06:33:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 06:33:26 | INFO | train_inner | epoch 166:     16 / 81 loss=3.418, nll_loss=0.369, mask_loss=7.90602, p_2=0.03502, mask_ave=0.498, ppl=1.29, wps=2919.9, ups=0.53, wpb=5509.9, bsz=357.2, num_updates=13300, lr=0.000274204, gnorm=0.296, train_wall=121, gb_free=9.1, wall=23239
2022-09-08 06:34:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 06:34:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 06:34:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 06:34:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 06:34:56 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 5.005 | nll_loss 2.324 | mask_loss 8.94759 | p_2 0.04849 | mask_ave 0.616 | ppl 5.01 | bleu 56.27 | wps 1560.7 | wpb 933.5 | bsz 59.6 | num_updates 13365 | best_bleu 57.09
2022-09-08 06:34:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 166 @ 13365 updates
2022-09-08 06:34:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint166.pt
2022-09-08 06:34:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint166.pt
2022-09-08 06:35:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint166.pt (epoch 166 @ 13365 updates, score 56.27) (writing took 24.02820661664009 seconds)
2022-09-08 06:35:21 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2022-09-08 06:35:21 | INFO | train | epoch 166 | loss 3.417 | nll_loss 0.368 | mask_loss 7.86333 | p_2 0.03511 | mask_ave 0.496 | ppl 1.29 | wps 3299 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 13365 | lr 0.000273537 | gnorm 0.29 | train_wall 99 | gb_free 9.2 | wall 23353
2022-09-08 06:35:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 06:35:21 | INFO | fairseq.trainer | begin training epoch 167
2022-09-08 06:35:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 06:36:04 | INFO | train_inner | epoch 167:     35 / 81 loss=3.416, nll_loss=0.367, mask_loss=7.87617, p_2=0.03493, mask_ave=0.495, ppl=1.29, wps=3512.9, ups=0.63, wpb=5545.9, bsz=357.7, num_updates=13400, lr=0.000273179, gnorm=0.292, train_wall=121, gb_free=9.3, wall=23397
2022-09-08 06:37:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 06:37:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 06:37:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 06:37:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 06:37:11 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 4.999 | nll_loss 2.329 | mask_loss 8.78882 | p_2 0.04786 | mask_ave 0.635 | ppl 5.02 | bleu 55.9 | wps 1510.8 | wpb 933.5 | bsz 59.6 | num_updates 13446 | best_bleu 57.09
2022-09-08 06:37:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 167 @ 13446 updates
2022-09-08 06:37:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint167.pt
2022-09-08 06:37:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint167.pt
2022-09-08 06:37:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint167.pt (epoch 167 @ 13446 updates, score 55.9) (writing took 23.616959661245346 seconds)
2022-09-08 06:37:35 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2022-09-08 06:37:35 | INFO | train | epoch 167 | loss 3.418 | nll_loss 0.369 | mask_loss 7.85684 | p_2 0.03509 | mask_ave 0.497 | ppl 1.29 | wps 3331.9 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 13446 | lr 0.000272711 | gnorm 0.305 | train_wall 98 | gb_free 9.1 | wall 23488
2022-09-08 06:37:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 06:37:35 | INFO | fairseq.trainer | begin training epoch 168
2022-09-08 06:37:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 06:38:42 | INFO | train_inner | epoch 168:     54 / 81 loss=3.418, nll_loss=0.369, mask_loss=7.83594, p_2=0.03509, mask_ave=0.498, ppl=1.29, wps=3484.6, ups=0.63, wpb=5518.4, bsz=356.1, num_updates=13500, lr=0.000272166, gnorm=0.305, train_wall=122, gb_free=9.1, wall=23555
2022-09-08 06:39:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 06:39:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 06:39:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 06:39:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 06:39:26 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 4.992 | nll_loss 2.327 | mask_loss 8.71957 | p_2 0.04791 | mask_ave 0.633 | ppl 5.02 | bleu 56.19 | wps 1531 | wpb 933.5 | bsz 59.6 | num_updates 13527 | best_bleu 57.09
2022-09-08 06:39:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 168 @ 13527 updates
2022-09-08 06:39:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint168.pt
2022-09-08 06:39:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint168.pt
2022-09-08 06:39:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint168.pt (epoch 168 @ 13527 updates, score 56.19) (writing took 18.56858390197158 seconds)
2022-09-08 06:39:45 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2022-09-08 06:39:45 | INFO | train | epoch 168 | loss 3.417 | nll_loss 0.369 | mask_loss 7.83327 | p_2 0.03507 | mask_ave 0.498 | ppl 1.29 | wps 3437.4 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 13527 | lr 0.000271894 | gnorm 0.304 | train_wall 99 | gb_free 9 | wall 23618
2022-09-08 06:39:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 06:39:45 | INFO | fairseq.trainer | begin training epoch 169
2022-09-08 06:39:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 06:41:15 | INFO | train_inner | epoch 169:     73 / 81 loss=3.417, nll_loss=0.368, mask_loss=7.85311, p_2=0.03528, mask_ave=0.501, ppl=1.29, wps=3611.1, ups=0.66, wpb=5511.4, bsz=361, num_updates=13600, lr=0.000271163, gnorm=0.305, train_wall=121, gb_free=8.9, wall=23708
2022-09-08 06:41:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 06:41:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 06:41:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 06:41:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 06:41:35 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 4.99 | nll_loss 2.323 | mask_loss 8.98859 | p_2 0.0482 | mask_ave 0.624 | ppl 5 | bleu 56 | wps 1475.6 | wpb 933.5 | bsz 59.6 | num_updates 13608 | best_bleu 57.09
2022-09-08 06:41:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 169 @ 13608 updates
2022-09-08 06:41:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint169.pt
2022-09-08 06:41:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint169.pt
2022-09-08 06:41:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint169.pt (epoch 169 @ 13608 updates, score 56.0) (writing took 19.9475401006639 seconds)
2022-09-08 06:41:56 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2022-09-08 06:41:56 | INFO | train | epoch 169 | loss 3.417 | nll_loss 0.368 | mask_loss 7.89267 | p_2 0.035 | mask_ave 0.5 | ppl 1.29 | wps 3425.6 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 13608 | lr 0.000271083 | gnorm 0.302 | train_wall 98 | gb_free 9.1 | wall 23749
2022-09-08 06:41:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 06:41:56 | INFO | fairseq.trainer | begin training epoch 170
2022-09-08 06:41:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 06:43:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 06:43:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 06:43:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 06:43:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 06:43:47 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 4.994 | nll_loss 2.326 | mask_loss 8.86097 | p_2 0.0482 | mask_ave 0.625 | ppl 5.01 | bleu 56.33 | wps 1522.9 | wpb 933.5 | bsz 59.6 | num_updates 13689 | best_bleu 57.09
2022-09-08 06:43:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 170 @ 13689 updates
2022-09-08 06:43:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint170.pt
2022-09-08 06:43:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint170.pt
2022-09-08 06:43:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint170.pt (epoch 170 @ 13689 updates, score 56.33) (writing took 2.7230283357203007 seconds)
2022-09-08 06:43:50 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2022-09-08 06:43:50 | INFO | train | epoch 170 | loss 3.415 | nll_loss 0.367 | mask_loss 7.93127 | p_2 0.03511 | mask_ave 0.497 | ppl 1.29 | wps 3915 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 13689 | lr 0.00027028 | gnorm 0.29 | train_wall 98 | gb_free 9 | wall 23863
2022-09-08 06:43:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 06:43:50 | INFO | fairseq.trainer | begin training epoch 171
2022-09-08 06:43:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 06:44:05 | INFO | train_inner | epoch 171:     11 / 81 loss=3.415, nll_loss=0.367, mask_loss=7.93858, p_2=0.03488, mask_ave=0.495, ppl=1.29, wps=3245.3, ups=0.59, wpb=5526.6, bsz=356.7, num_updates=13700, lr=0.000270172, gnorm=0.289, train_wall=121, gb_free=9, wall=23878
2022-09-08 06:45:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 06:45:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 06:45:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 06:45:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 06:45:41 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 4.973 | nll_loss 2.296 | mask_loss 8.81204 | p_2 0.04785 | mask_ave 0.634 | ppl 4.91 | bleu 56.27 | wps 1554 | wpb 933.5 | bsz 59.6 | num_updates 13770 | best_bleu 57.09
2022-09-08 06:45:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 171 @ 13770 updates
2022-09-08 06:45:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint171.pt
2022-09-08 06:45:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint171.pt
2022-09-08 06:46:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint171.pt (epoch 171 @ 13770 updates, score 56.27) (writing took 18.534565214067698 seconds)
2022-09-08 06:46:00 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2022-09-08 06:46:00 | INFO | train | epoch 171 | loss 3.415 | nll_loss 0.367 | mask_loss 7.91132 | p_2 0.0351 | mask_ave 0.497 | ppl 1.29 | wps 3447.3 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 13770 | lr 0.000269484 | gnorm 0.29 | train_wall 99 | gb_free 9.1 | wall 23993
2022-09-08 06:46:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 06:46:00 | INFO | fairseq.trainer | begin training epoch 172
2022-09-08 06:46:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 06:46:38 | INFO | train_inner | epoch 172:     30 / 81 loss=3.415, nll_loss=0.366, mask_loss=7.85499, p_2=0.0356, mask_ave=0.5, ppl=1.29, wps=3615.6, ups=0.66, wpb=5507.1, bsz=363.4, num_updates=13800, lr=0.000269191, gnorm=0.294, train_wall=121, gb_free=9.1, wall=24030
2022-09-08 06:47:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 06:47:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 06:47:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 06:47:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 06:47:51 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 5.008 | nll_loss 2.342 | mask_loss 8.68976 | p_2 0.04831 | mask_ave 0.621 | ppl 5.07 | bleu 56.28 | wps 1562.9 | wpb 933.5 | bsz 59.6 | num_updates 13851 | best_bleu 57.09
2022-09-08 06:47:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 172 @ 13851 updates
2022-09-08 06:47:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint172.pt
2022-09-08 06:47:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint172.pt
2022-09-08 06:48:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint172.pt (epoch 172 @ 13851 updates, score 56.28) (writing took 19.2406798414886 seconds)
2022-09-08 06:48:10 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2022-09-08 06:48:11 | INFO | train | epoch 172 | loss 3.416 | nll_loss 0.368 | mask_loss 7.86397 | p_2 0.03517 | mask_ave 0.495 | ppl 1.29 | wps 3421.1 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 13851 | lr 0.000268695 | gnorm 0.296 | train_wall 98 | gb_free 9.1 | wall 24123
2022-09-08 06:48:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 06:48:11 | INFO | fairseq.trainer | begin training epoch 173
2022-09-08 06:48:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 06:49:11 | INFO | train_inner | epoch 173:     49 / 81 loss=3.416, nll_loss=0.368, mask_loss=7.84947, p_2=0.03505, mask_ave=0.493, ppl=1.29, wps=3602.1, ups=0.65, wpb=5515.2, bsz=354, num_updates=13900, lr=0.000268221, gnorm=0.295, train_wall=121, gb_free=9.1, wall=24184
2022-09-08 06:49:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 06:49:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 06:49:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 06:49:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 06:50:00 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 4.989 | nll_loss 2.323 | mask_loss 8.79209 | p_2 0.0484 | mask_ave 0.618 | ppl 5.01 | bleu 56.16 | wps 1547.6 | wpb 933.5 | bsz 59.6 | num_updates 13932 | best_bleu 57.09
2022-09-08 06:50:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 173 @ 13932 updates
2022-09-08 06:50:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint173.pt
2022-09-08 06:50:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint173.pt
2022-09-08 06:50:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint173.pt (epoch 173 @ 13932 updates, score 56.16) (writing took 20.769739508628845 seconds)
2022-09-08 06:50:21 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2022-09-08 06:50:21 | INFO | train | epoch 173 | loss 3.414 | nll_loss 0.366 | mask_loss 7.86724 | p_2 0.03512 | mask_ave 0.496 | ppl 1.29 | wps 3436.4 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 13932 | lr 0.000267913 | gnorm 0.297 | train_wall 97 | gb_free 9 | wall 24254
2022-09-08 06:50:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 06:50:21 | INFO | fairseq.trainer | begin training epoch 174
2022-09-08 06:50:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 06:51:45 | INFO | train_inner | epoch 174:     68 / 81 loss=3.414, nll_loss=0.367, mask_loss=7.93727, p_2=0.03491, mask_ave=0.496, ppl=1.29, wps=3591.5, ups=0.65, wpb=5542.2, bsz=357.8, num_updates=14000, lr=0.000267261, gnorm=0.298, train_wall=121, gb_free=9.1, wall=24338
2022-09-08 06:52:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 06:52:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 06:52:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 06:52:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 06:52:11 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 4.996 | nll_loss 2.327 | mask_loss 8.76618 | p_2 0.04826 | mask_ave 0.622 | ppl 5.02 | bleu 56.5 | wps 1514.8 | wpb 933.5 | bsz 59.6 | num_updates 14013 | best_bleu 57.09
2022-09-08 06:52:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 174 @ 14013 updates
2022-09-08 06:52:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint174.pt
2022-09-08 06:52:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint174.pt
2022-09-08 06:52:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint174.pt (epoch 174 @ 14013 updates, score 56.5) (writing took 19.96227379143238 seconds)
2022-09-08 06:52:32 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2022-09-08 06:52:32 | INFO | train | epoch 174 | loss 3.415 | nll_loss 0.367 | mask_loss 7.8967 | p_2 0.03514 | mask_ave 0.496 | ppl 1.29 | wps 3417.6 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 14013 | lr 0.000267137 | gnorm 0.3 | train_wall 98 | gb_free 9.2 | wall 24384
2022-09-08 06:52:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 06:52:32 | INFO | fairseq.trainer | begin training epoch 175
2022-09-08 06:52:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 06:54:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 06:54:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 06:54:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 06:54:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 06:54:22 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 5.007 | nll_loss 2.338 | mask_loss 9.04829 | p_2 0.04809 | mask_ave 0.629 | ppl 5.06 | bleu 55.42 | wps 1561.1 | wpb 933.5 | bsz 59.6 | num_updates 14094 | best_bleu 57.09
2022-09-08 06:54:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 175 @ 14094 updates
2022-09-08 06:54:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint175.pt
2022-09-08 06:54:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint175.pt
2022-09-08 06:54:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint175.pt (epoch 175 @ 14094 updates, score 55.42) (writing took 19.244899552315474 seconds)
2022-09-08 06:54:41 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2022-09-08 06:54:41 | INFO | train | epoch 175 | loss 3.414 | nll_loss 0.367 | mask_loss 7.88259 | p_2 0.03512 | mask_ave 0.496 | ppl 1.29 | wps 3455 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 14094 | lr 0.000266369 | gnorm 0.288 | train_wall 98 | gb_free 9 | wall 24514
2022-09-08 06:54:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 06:54:41 | INFO | fairseq.trainer | begin training epoch 176
2022-09-08 06:54:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 06:54:49 | INFO | train_inner | epoch 176:      6 / 81 loss=3.414, nll_loss=0.366, mask_loss=7.90176, p_2=0.03509, mask_ave=0.496, ppl=1.29, wps=2989.7, ups=0.54, wpb=5509.2, bsz=356.6, num_updates=14100, lr=0.000266312, gnorm=0.291, train_wall=120, gb_free=9.1, wall=24522
2022-09-08 06:56:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 06:56:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 06:56:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 06:56:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 06:56:32 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 4.98 | nll_loss 2.31 | mask_loss 8.80343 | p_2 0.04855 | mask_ave 0.613 | ppl 4.96 | bleu 55.75 | wps 1521 | wpb 933.5 | bsz 59.6 | num_updates 14175 | best_bleu 57.09
2022-09-08 06:56:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 176 @ 14175 updates
2022-09-08 06:56:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint176.pt
2022-09-08 06:56:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint176.pt
2022-09-08 06:56:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint176.pt (epoch 176 @ 14175 updates, score 55.75) (writing took 22.058876775205135 seconds)
2022-09-08 06:56:54 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2022-09-08 06:56:54 | INFO | train | epoch 176 | loss 3.413 | nll_loss 0.366 | mask_loss 7.94642 | p_2 0.03504 | mask_ave 0.498 | ppl 1.29 | wps 3364.9 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 14175 | lr 0.000265606 | gnorm 0.286 | train_wall 98 | gb_free 9 | wall 24647
2022-09-08 06:56:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 06:56:54 | INFO | fairseq.trainer | begin training epoch 177
2022-09-08 06:56:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 06:57:26 | INFO | train_inner | epoch 177:     25 / 81 loss=3.414, nll_loss=0.366, mask_loss=7.91281, p_2=0.03499, mask_ave=0.497, ppl=1.29, wps=3535, ups=0.64, wpb=5537.1, bsz=358.6, num_updates=14200, lr=0.000265372, gnorm=0.286, train_wall=122, gb_free=9.1, wall=24679
2022-09-08 06:58:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 06:58:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 06:58:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 06:58:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 06:58:45 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 4.983 | nll_loss 2.307 | mask_loss 8.78031 | p_2 0.04834 | mask_ave 0.621 | ppl 4.95 | bleu 56.49 | wps 1560.3 | wpb 933.5 | bsz 59.6 | num_updates 14256 | best_bleu 57.09
2022-09-08 06:58:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 177 @ 14256 updates
2022-09-08 06:58:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint177.pt
2022-09-08 06:58:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint177.pt
2022-09-08 06:59:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint177.pt (epoch 177 @ 14256 updates, score 56.49) (writing took 19.437505200505257 seconds)
2022-09-08 06:59:05 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2022-09-08 06:59:05 | INFO | train | epoch 177 | loss 3.414 | nll_loss 0.366 | mask_loss 7.91609 | p_2 0.03516 | mask_ave 0.495 | ppl 1.29 | wps 3419.9 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 14256 | lr 0.000264851 | gnorm 0.296 | train_wall 99 | gb_free 9.1 | wall 24778
2022-09-08 06:59:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 06:59:05 | INFO | fairseq.trainer | begin training epoch 178
2022-09-08 06:59:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 06:59:59 | INFO | train_inner | epoch 178:     44 / 81 loss=3.413, nll_loss=0.366, mask_loss=7.93272, p_2=0.03514, mask_ave=0.494, ppl=1.29, wps=3615.4, ups=0.65, wpb=5526.9, bsz=355.3, num_updates=14300, lr=0.000264443, gnorm=0.295, train_wall=121, gb_free=9.2, wall=24832
2022-09-08 07:00:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 07:00:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 07:00:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 07:00:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 07:00:55 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 5.009 | nll_loss 2.344 | mask_loss 8.64666 | p_2 0.04827 | mask_ave 0.622 | ppl 5.08 | bleu 55.75 | wps 1416.1 | wpb 933.5 | bsz 59.6 | num_updates 14337 | best_bleu 57.09
2022-09-08 07:00:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 178 @ 14337 updates
2022-09-08 07:00:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint178.pt
2022-09-08 07:00:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint178.pt
2022-09-08 07:01:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint178.pt (epoch 178 @ 14337 updates, score 55.75) (writing took 18.10700089111924 seconds)
2022-09-08 07:01:14 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2022-09-08 07:01:14 | INFO | train | epoch 178 | loss 3.413 | nll_loss 0.366 | mask_loss 7.86003 | p_2 0.03527 | mask_ave 0.492 | ppl 1.29 | wps 3474.3 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 14337 | lr 0.000264101 | gnorm 0.295 | train_wall 97 | gb_free 9 | wall 24907
2022-09-08 07:01:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 07:01:14 | INFO | fairseq.trainer | begin training epoch 179
2022-09-08 07:01:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 07:02:33 | INFO | train_inner | epoch 179:     63 / 81 loss=3.413, nll_loss=0.365, mask_loss=7.85341, p_2=0.03537, mask_ave=0.495, ppl=1.29, wps=3581.4, ups=0.65, wpb=5507.4, bsz=357.6, num_updates=14400, lr=0.000263523, gnorm=0.294, train_wall=122, gb_free=9.1, wall=24985
2022-09-08 07:02:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 07:02:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 07:02:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 07:02:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 07:03:06 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 4.984 | nll_loss 2.316 | mask_loss 8.85602 | p_2 0.04811 | mask_ave 0.628 | ppl 4.98 | bleu 57.1 | wps 1523 | wpb 933.5 | bsz 59.6 | num_updates 14418 | best_bleu 57.1
2022-09-08 07:03:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 179 @ 14418 updates
2022-09-08 07:03:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint179.pt
2022-09-08 07:03:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint179.pt
2022-09-08 07:04:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint179.pt (epoch 179 @ 14418 updates, score 57.1) (writing took 69.51613076403737 seconds)
2022-09-08 07:04:15 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)
2022-09-08 07:04:15 | INFO | train | epoch 179 | loss 3.412 | nll_loss 0.364 | mask_loss 7.88161 | p_2 0.03515 | mask_ave 0.495 | ppl 1.29 | wps 2461.8 | ups 0.45 | wpb 5523.2 | bsz 358 | num_updates 14418 | lr 0.000263359 | gnorm 0.292 | train_wall 99 | gb_free 9.1 | wall 25088
2022-09-08 07:04:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 07:04:16 | INFO | fairseq.trainer | begin training epoch 180
2022-09-08 07:04:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 07:05:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 07:05:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 07:05:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 07:05:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 07:06:07 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 4.997 | nll_loss 2.334 | mask_loss 8.96614 | p_2 0.04821 | mask_ave 0.625 | ppl 5.04 | bleu 56.39 | wps 1467.5 | wpb 933.5 | bsz 59.6 | num_updates 14499 | best_bleu 57.1
2022-09-08 07:06:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 180 @ 14499 updates
2022-09-08 07:06:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint180.pt
2022-09-08 07:06:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint180.pt
2022-09-08 07:06:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint180.pt (epoch 180 @ 14499 updates, score 56.39) (writing took 36.473743576556444 seconds)
2022-09-08 07:06:44 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)
2022-09-08 07:06:44 | INFO | train | epoch 180 | loss 3.411 | nll_loss 0.364 | mask_loss 7.97523 | p_2 0.03511 | mask_ave 0.496 | ppl 1.29 | wps 3017.1 | ups 0.55 | wpb 5523.2 | bsz 358 | num_updates 14499 | lr 0.000262622 | gnorm 0.278 | train_wall 98 | gb_free 9 | wall 25237
2022-09-08 07:06:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 07:06:44 | INFO | fairseq.trainer | begin training epoch 181
2022-09-08 07:06:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 07:06:46 | INFO | train_inner | epoch 181:      1 / 81 loss=3.411, nll_loss=0.364, mask_loss=7.96081, p_2=0.03505, mask_ave=0.495, ppl=1.29, wps=2178.6, ups=0.39, wpb=5522.2, bsz=360.4, num_updates=14500, lr=0.000262613, gnorm=0.282, train_wall=121, gb_free=9.1, wall=25239
2022-09-08 07:08:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 07:08:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 07:08:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 07:08:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 07:08:37 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 4.998 | nll_loss 2.334 | mask_loss 8.84174 | p_2 0.04847 | mask_ave 0.617 | ppl 5.04 | bleu 56.39 | wps 1532.7 | wpb 933.5 | bsz 59.6 | num_updates 14580 | best_bleu 57.1
2022-09-08 07:08:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 181 @ 14580 updates
2022-09-08 07:08:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint181.pt
2022-09-08 07:08:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint181.pt
2022-09-08 07:08:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint181.pt (epoch 181 @ 14580 updates, score 56.39) (writing took 20.218480449169874 seconds)
2022-09-08 07:08:57 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)
2022-09-08 07:08:57 | INFO | train | epoch 181 | loss 3.411 | nll_loss 0.364 | mask_loss 8.01792 | p_2 0.0352 | mask_ave 0.493 | ppl 1.29 | wps 3352.3 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 14580 | lr 0.000261891 | gnorm 0.287 | train_wall 100 | gb_free 9 | wall 25370
2022-09-08 07:08:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 07:08:57 | INFO | fairseq.trainer | begin training epoch 182
2022-09-08 07:08:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 07:09:21 | INFO | train_inner | epoch 182:     20 / 81 loss=3.411, nll_loss=0.364, mask_loss=7.99101, p_2=0.03548, mask_ave=0.494, ppl=1.29, wps=3554.2, ups=0.64, wpb=5516.1, bsz=360.2, num_updates=14600, lr=0.000261712, gnorm=0.292, train_wall=122, gb_free=9.1, wall=25394
2022-09-08 07:10:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 07:10:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 07:10:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 07:10:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 07:10:47 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 5.005 | nll_loss 2.348 | mask_loss 8.94386 | p_2 0.04816 | mask_ave 0.626 | ppl 5.09 | bleu 56.77 | wps 1549.1 | wpb 933.5 | bsz 59.6 | num_updates 14661 | best_bleu 57.1
2022-09-08 07:10:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 182 @ 14661 updates
2022-09-08 07:10:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint182.pt
2022-09-08 07:10:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint182.pt
2022-09-08 07:11:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint182.pt (epoch 182 @ 14661 updates, score 56.77) (writing took 18.392083182930946 seconds)
2022-09-08 07:11:06 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)
2022-09-08 07:11:06 | INFO | train | epoch 182 | loss 3.412 | nll_loss 0.365 | mask_loss 8.00725 | p_2 0.03522 | mask_ave 0.493 | ppl 1.29 | wps 3482.6 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 14661 | lr 0.000261167 | gnorm 0.301 | train_wall 97 | gb_free 9.1 | wall 25498
2022-09-08 07:11:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 07:11:06 | INFO | fairseq.trainer | begin training epoch 183
2022-09-08 07:11:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 07:11:54 | INFO | train_inner | epoch 183:     39 / 81 loss=3.412, nll_loss=0.365, mask_loss=8.01473, p_2=0.03494, mask_ave=0.493, ppl=1.29, wps=3629.6, ups=0.65, wpb=5545.2, bsz=356.9, num_updates=14700, lr=0.00026082, gnorm=0.3, train_wall=122, gb_free=9.1, wall=25547
2022-09-08 07:12:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 07:12:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 07:12:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 07:12:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 07:12:57 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 4.988 | nll_loss 2.324 | mask_loss 8.79536 | p_2 0.04876 | mask_ave 0.608 | ppl 5.01 | bleu 56.12 | wps 1497.8 | wpb 933.5 | bsz 59.6 | num_updates 14742 | best_bleu 57.1
2022-09-08 07:12:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 183 @ 14742 updates
2022-09-08 07:12:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint183.pt
2022-09-08 07:12:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint183.pt
2022-09-08 07:13:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint183.pt (epoch 183 @ 14742 updates, score 56.12) (writing took 19.526636477559805 seconds)
2022-09-08 07:13:16 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)
2022-09-08 07:13:16 | INFO | train | epoch 183 | loss 3.412 | nll_loss 0.365 | mask_loss 7.92695 | p_2 0.03526 | mask_ave 0.492 | ppl 1.29 | wps 3421.7 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 14742 | lr 0.000260448 | gnorm 0.302 | train_wall 98 | gb_free 9.2 | wall 25629
2022-09-08 07:13:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 07:13:17 | INFO | fairseq.trainer | begin training epoch 184
2022-09-08 07:13:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 07:14:28 | INFO | train_inner | epoch 184:     58 / 81 loss=3.411, nll_loss=0.364, mask_loss=7.91601, p_2=0.03538, mask_ave=0.49, ppl=1.29, wps=3581.8, ups=0.65, wpb=5529.7, bsz=360, num_updates=14800, lr=0.000259938, gnorm=0.291, train_wall=122, gb_free=9.1, wall=25701
2022-09-08 07:14:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 07:14:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 07:14:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 07:14:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 07:15:07 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 5.006 | nll_loss 2.347 | mask_loss 9.06003 | p_2 0.0486 | mask_ave 0.613 | ppl 5.09 | bleu 56.14 | wps 1486.6 | wpb 933.5 | bsz 59.6 | num_updates 14823 | best_bleu 57.1
2022-09-08 07:15:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 184 @ 14823 updates
2022-09-08 07:15:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint184.pt
2022-09-08 07:15:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint184.pt
2022-09-08 07:15:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint184.pt (epoch 184 @ 14823 updates, score 56.14) (writing took 19.416126064956188 seconds)
2022-09-08 07:15:27 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)
2022-09-08 07:15:27 | INFO | train | epoch 184 | loss 3.41 | nll_loss 0.363 | mask_loss 7.95527 | p_2 0.03529 | mask_ave 0.491 | ppl 1.29 | wps 3424.1 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 14823 | lr 0.000259736 | gnorm 0.285 | train_wall 98 | gb_free 9.1 | wall 25760
2022-09-08 07:15:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 07:15:27 | INFO | fairseq.trainer | begin training epoch 185
2022-09-08 07:15:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 07:17:03 | INFO | train_inner | epoch 185:     77 / 81 loss=3.411, nll_loss=0.364, mask_loss=7.92223, p_2=0.03517, mask_ave=0.493, ppl=1.29, wps=3573.3, ups=0.65, wpb=5537, bsz=359, num_updates=14900, lr=0.000259064, gnorm=0.289, train_wall=122, gb_free=9.2, wall=25856
2022-09-08 07:17:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 07:17:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 07:17:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 07:17:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 07:17:19 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 4.996 | nll_loss 2.331 | mask_loss 8.74476 | p_2 0.04857 | mask_ave 0.613 | ppl 5.03 | bleu 56.22 | wps 1498.1 | wpb 933.5 | bsz 59.6 | num_updates 14904 | best_bleu 57.1
2022-09-08 07:17:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 185 @ 14904 updates
2022-09-08 07:17:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint185.pt
2022-09-08 07:17:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint185.pt
2022-09-08 07:17:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint185.pt (epoch 185 @ 14904 updates, score 56.22) (writing took 18.906804405152798 seconds)
2022-09-08 07:17:38 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)
2022-09-08 07:17:38 | INFO | train | epoch 185 | loss 3.41 | nll_loss 0.364 | mask_loss 7.91273 | p_2 0.03521 | mask_ave 0.494 | ppl 1.29 | wps 3408.6 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 14904 | lr 0.000259029 | gnorm 0.29 | train_wall 99 | gb_free 9.2 | wall 25891
2022-09-08 07:17:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 07:17:38 | INFO | fairseq.trainer | begin training epoch 186
2022-09-08 07:17:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 07:19:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 07:19:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 07:19:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 07:19:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 07:19:30 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 4.998 | nll_loss 2.334 | mask_loss 8.75575 | p_2 0.04885 | mask_ave 0.605 | ppl 5.04 | bleu 56.46 | wps 1518.8 | wpb 933.5 | bsz 59.6 | num_updates 14985 | best_bleu 57.1
2022-09-08 07:19:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 186 @ 14985 updates
2022-09-08 07:19:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint186.pt
2022-09-08 07:19:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint186.pt
2022-09-08 07:19:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint186.pt (epoch 186 @ 14985 updates, score 56.46) (writing took 19.945878762751818 seconds)
2022-09-08 07:19:50 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)
2022-09-08 07:19:50 | INFO | train | epoch 186 | loss 3.41 | nll_loss 0.363 | mask_loss 7.88667 | p_2 0.03521 | mask_ave 0.494 | ppl 1.29 | wps 3403.3 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 14985 | lr 0.000258328 | gnorm 0.292 | train_wall 99 | gb_free 9.1 | wall 26023
2022-09-08 07:19:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 07:19:50 | INFO | fairseq.trainer | begin training epoch 187
2022-09-08 07:19:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 07:20:08 | INFO | train_inner | epoch 187:     15 / 81 loss=3.41, nll_loss=0.363, mask_loss=7.90424, p_2=0.03522, mask_ave=0.492, ppl=1.29, wps=2974.9, ups=0.54, wpb=5505.5, bsz=357.2, num_updates=15000, lr=0.000258199, gnorm=0.287, train_wall=120, gb_free=8.9, wall=26041
2022-09-08 07:21:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 07:21:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 07:21:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 07:21:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 07:21:39 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 4.996 | nll_loss 2.327 | mask_loss 8.95837 | p_2 0.04861 | mask_ave 0.613 | ppl 5.02 | bleu 56.95 | wps 1548.5 | wpb 933.5 | bsz 59.6 | num_updates 15066 | best_bleu 57.1
2022-09-08 07:21:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 187 @ 15066 updates
2022-09-08 07:21:39 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint187.pt
2022-09-08 07:21:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint187.pt
2022-09-08 07:22:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint187.pt (epoch 187 @ 15066 updates, score 56.95) (writing took 21.316938638687134 seconds)
2022-09-08 07:22:01 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)
2022-09-08 07:22:01 | INFO | train | epoch 187 | loss 3.41 | nll_loss 0.363 | mask_loss 7.93026 | p_2 0.03531 | mask_ave 0.49 | ppl 1.29 | wps 3414.6 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 15066 | lr 0.000257633 | gnorm 0.295 | train_wall 97 | gb_free 9.1 | wall 26154
2022-09-08 07:22:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 07:22:01 | INFO | fairseq.trainer | begin training epoch 188
2022-09-08 07:22:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 07:22:43 | INFO | train_inner | epoch 188:     34 / 81 loss=3.41, nll_loss=0.364, mask_loss=7.92081, p_2=0.03547, mask_ave=0.493, ppl=1.29, wps=3566.2, ups=0.65, wpb=5522.1, bsz=358, num_updates=15100, lr=0.000257343, gnorm=0.306, train_wall=121, gb_free=9.1, wall=26196
2022-09-08 07:23:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 07:23:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 07:23:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 07:23:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 07:23:51 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 4.989 | nll_loss 2.321 | mask_loss 8.79228 | p_2 0.04857 | mask_ave 0.613 | ppl 5 | bleu 55.89 | wps 1494.7 | wpb 933.5 | bsz 59.6 | num_updates 15147 | best_bleu 57.1
2022-09-08 07:23:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 188 @ 15147 updates
2022-09-08 07:23:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint188.pt
2022-09-08 07:23:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint188.pt
2022-09-08 07:24:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint188.pt (epoch 188 @ 15147 updates, score 55.89) (writing took 18.66767293587327 seconds)
2022-09-08 07:24:09 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)
2022-09-08 07:24:09 | INFO | train | epoch 188 | loss 3.41 | nll_loss 0.363 | mask_loss 7.93386 | p_2 0.03522 | mask_ave 0.493 | ppl 1.29 | wps 3479 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 15147 | lr 0.000256943 | gnorm 0.293 | train_wall 97 | gb_free 9.1 | wall 26282
2022-09-08 07:24:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 07:24:10 | INFO | fairseq.trainer | begin training epoch 189
2022-09-08 07:24:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 07:25:15 | INFO | train_inner | epoch 189:     53 / 81 loss=3.408, nll_loss=0.362, mask_loss=7.91983, p_2=0.03549, mask_ave=0.493, ppl=1.29, wps=3646.6, ups=0.66, wpb=5525.4, bsz=361.4, num_updates=15200, lr=0.000256495, gnorm=0.276, train_wall=120, gb_free=9, wall=26348
2022-09-08 07:25:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 07:25:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 07:25:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 07:25:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 07:25:59 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 4.998 | nll_loss 2.34 | mask_loss 8.72004 | p_2 0.04873 | mask_ave 0.609 | ppl 5.06 | bleu 57.29 | wps 1502.4 | wpb 933.5 | bsz 59.6 | num_updates 15228 | best_bleu 57.29
2022-09-08 07:25:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 189 @ 15228 updates
2022-09-08 07:25:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint189.pt
2022-09-08 07:26:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint189.pt
2022-09-08 07:26:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint189.pt (epoch 189 @ 15228 updates, score 57.29) (writing took 39.61747011169791 seconds)
2022-09-08 07:26:39 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)
2022-09-08 07:26:39 | INFO | train | epoch 189 | loss 3.409 | nll_loss 0.362 | mask_loss 7.95915 | p_2 0.03528 | mask_ave 0.492 | ppl 1.29 | wps 2986.6 | ups 0.54 | wpb 5523.2 | bsz 358 | num_updates 15228 | lr 0.000256259 | gnorm 0.279 | train_wall 97 | gb_free 9.1 | wall 26432
2022-09-08 07:26:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 07:26:39 | INFO | fairseq.trainer | begin training epoch 190
2022-09-08 07:26:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 07:28:08 | INFO | train_inner | epoch 190:     72 / 81 loss=3.409, nll_loss=0.363, mask_loss=8.02274, p_2=0.03478, mask_ave=0.491, ppl=1.29, wps=3199.7, ups=0.58, wpb=5526.9, bsz=352.2, num_updates=15300, lr=0.000255655, gnorm=0.286, train_wall=120, gb_free=9.1, wall=26520
2022-09-08 07:28:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 07:28:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 07:28:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 07:28:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 07:28:29 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 5.003 | nll_loss 2.341 | mask_loss 8.93826 | p_2 0.04835 | mask_ave 0.621 | ppl 5.07 | bleu 56.24 | wps 1504.6 | wpb 933.5 | bsz 59.6 | num_updates 15309 | best_bleu 57.29
2022-09-08 07:28:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 190 @ 15309 updates
2022-09-08 07:28:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint190.pt
2022-09-08 07:28:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint190.pt
2022-09-08 07:28:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint190.pt (epoch 190 @ 15309 updates, score 56.24) (writing took 21.363841470330954 seconds)
2022-09-08 07:28:50 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)
2022-09-08 07:28:50 | INFO | train | epoch 190 | loss 3.408 | nll_loss 0.362 | mask_loss 7.97178 | p_2 0.03526 | mask_ave 0.493 | ppl 1.29 | wps 3408.7 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 15309 | lr 0.00025558 | gnorm 0.287 | train_wall 97 | gb_free 8.9 | wall 26563
2022-09-08 07:28:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 07:28:51 | INFO | fairseq.trainer | begin training epoch 191
2022-09-08 07:28:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 07:30:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 07:30:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 07:30:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 07:30:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 07:30:44 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 4.995 | nll_loss 2.337 | mask_loss 8.98477 | p_2 0.04874 | mask_ave 0.609 | ppl 5.05 | bleu 56.27 | wps 1463.2 | wpb 933.5 | bsz 59.6 | num_updates 15390 | best_bleu 57.29
2022-09-08 07:30:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 191 @ 15390 updates
2022-09-08 07:30:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint191.pt
2022-09-08 07:30:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint191.pt
2022-09-08 07:31:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint191.pt (epoch 191 @ 15390 updates, score 56.27) (writing took 17.717002421617508 seconds)
2022-09-08 07:31:02 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)
2022-09-08 07:31:02 | INFO | train | epoch 191 | loss 3.409 | nll_loss 0.362 | mask_loss 8.01991 | p_2 0.0351 | mask_ave 0.497 | ppl 1.29 | wps 3412.2 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 15390 | lr 0.000254906 | gnorm 0.287 | train_wall 100 | gb_free 9.1 | wall 26694
2022-09-08 07:31:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 07:31:02 | INFO | fairseq.trainer | begin training epoch 192
2022-09-08 07:31:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 07:31:15 | INFO | train_inner | epoch 192:     10 / 81 loss=3.408, nll_loss=0.362, mask_loss=8.00399, p_2=0.03516, mask_ave=0.496, ppl=1.29, wps=2947.2, ups=0.53, wpb=5519.4, bsz=359.1, num_updates=15400, lr=0.000254824, gnorm=0.284, train_wall=122, gb_free=9.1, wall=26708
2022-09-08 07:32:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 07:32:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 07:32:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 07:32:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 07:32:52 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 4.999 | nll_loss 2.342 | mask_loss 8.82237 | p_2 0.04859 | mask_ave 0.613 | ppl 5.07 | bleu 56.18 | wps 1516 | wpb 933.5 | bsz 59.6 | num_updates 15471 | best_bleu 57.29
2022-09-08 07:32:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 192 @ 15471 updates
2022-09-08 07:32:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint192.pt
2022-09-08 07:32:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint192.pt
2022-09-08 07:33:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint192.pt (epoch 192 @ 15471 updates, score 56.18) (writing took 26.62960333377123 seconds)
2022-09-08 07:33:19 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)
2022-09-08 07:33:19 | INFO | train | epoch 192 | loss 3.407 | nll_loss 0.361 | mask_loss 8.06929 | p_2 0.03515 | mask_ave 0.495 | ppl 1.28 | wps 3250.8 | ups 0.59 | wpb 5523.2 | bsz 358 | num_updates 15471 | lr 0.000254238 | gnorm 0.272 | train_wall 98 | gb_free 9.1 | wall 26832
2022-09-08 07:33:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 07:33:19 | INFO | fairseq.trainer | begin training epoch 193
2022-09-08 07:33:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 07:33:56 | INFO | train_inner | epoch 193:     29 / 81 loss=3.408, nll_loss=0.362, mask_loss=8.08991, p_2=0.03512, mask_ave=0.497, ppl=1.28, wps=3419.5, ups=0.62, wpb=5493.5, bsz=352.2, num_updates=15500, lr=0.000254, gnorm=0.286, train_wall=121, gb_free=9.1, wall=26868
2022-09-08 07:34:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 07:35:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 07:35:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 07:35:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 07:35:10 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 5.002 | nll_loss 2.343 | mask_loss 8.82208 | p_2 0.04865 | mask_ave 0.612 | ppl 5.07 | bleu 56.54 | wps 1577.1 | wpb 933.5 | bsz 59.6 | num_updates 15552 | best_bleu 57.29
2022-09-08 07:35:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 193 @ 15552 updates
2022-09-08 07:35:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint193.pt
2022-09-08 07:35:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint193.pt
2022-09-08 07:35:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint193.pt (epoch 193 @ 15552 updates, score 56.54) (writing took 21.39183997735381 seconds)
2022-09-08 07:35:31 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)
2022-09-08 07:35:31 | INFO | train | epoch 193 | loss 3.408 | nll_loss 0.362 | mask_loss 8.04064 | p_2 0.03518 | mask_ave 0.494 | ppl 1.28 | wps 3385 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 15552 | lr 0.000253575 | gnorm 0.289 | train_wall 98 | gb_free 9.2 | wall 26964
2022-09-08 07:35:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 07:35:32 | INFO | fairseq.trainer | begin training epoch 194
2022-09-08 07:35:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 07:36:30 | INFO | train_inner | epoch 194:     48 / 81 loss=3.407, nll_loss=0.361, mask_loss=7.97215, p_2=0.03533, mask_ave=0.492, ppl=1.28, wps=3582.2, ups=0.65, wpb=5544.4, bsz=365.3, num_updates=15600, lr=0.000253185, gnorm=0.281, train_wall=121, gb_free=9.1, wall=27023
2022-09-08 07:37:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 07:37:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 07:37:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 07:37:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 07:37:20 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 5.001 | nll_loss 2.34 | mask_loss 8.92034 | p_2 0.04852 | mask_ave 0.615 | ppl 5.06 | bleu 56.57 | wps 1585.8 | wpb 933.5 | bsz 59.6 | num_updates 15633 | best_bleu 57.29
2022-09-08 07:37:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 194 @ 15633 updates
2022-09-08 07:37:20 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint194.pt
2022-09-08 07:37:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint194.pt
2022-09-08 07:37:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint194.pt (epoch 194 @ 15633 updates, score 56.57) (writing took 20.624560624361038 seconds)
2022-09-08 07:37:41 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)
2022-09-08 07:37:41 | INFO | train | epoch 194 | loss 3.408 | nll_loss 0.362 | mask_loss 7.99742 | p_2 0.03526 | mask_ave 0.492 | ppl 1.29 | wps 3446.3 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 15633 | lr 0.000252917 | gnorm 0.283 | train_wall 97 | gb_free 9.1 | wall 27094
2022-09-08 07:37:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 07:37:41 | INFO | fairseq.trainer | begin training epoch 195
2022-09-08 07:37:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 07:39:03 | INFO | train_inner | epoch 195:     67 / 81 loss=3.408, nll_loss=0.362, mask_loss=8.04526, p_2=0.03514, mask_ave=0.496, ppl=1.29, wps=3621.9, ups=0.66, wpb=5528.2, bsz=352.9, num_updates=15700, lr=0.000252377, gnorm=0.289, train_wall=120, gb_free=9.1, wall=27176
2022-09-08 07:39:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 07:39:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 07:39:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 07:39:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 07:39:31 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 5.027 | nll_loss 2.372 | mask_loss 8.90223 | p_2 0.04857 | mask_ave 0.614 | ppl 5.18 | bleu 56.19 | wps 1560.2 | wpb 933.5 | bsz 59.6 | num_updates 15714 | best_bleu 57.29
2022-09-08 07:39:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 195 @ 15714 updates
2022-09-08 07:39:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint195.pt
2022-09-08 07:39:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint195.pt
2022-09-08 07:39:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint195.pt (epoch 195 @ 15714 updates, score 56.19) (writing took 18.889047522097826 seconds)
2022-09-08 07:39:50 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)
2022-09-08 07:39:50 | INFO | train | epoch 195 | loss 3.407 | nll_loss 0.362 | mask_loss 8.02007 | p_2 0.03513 | mask_ave 0.496 | ppl 1.29 | wps 3476 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 15714 | lr 0.000252265 | gnorm 0.287 | train_wall 97 | gb_free 9.1 | wall 27223
2022-09-08 07:39:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 07:39:50 | INFO | fairseq.trainer | begin training epoch 196
2022-09-08 07:39:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 07:41:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 07:41:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 07:41:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 07:41:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 07:41:40 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 5.016 | nll_loss 2.366 | mask_loss 9.09035 | p_2 0.04839 | mask_ave 0.62 | ppl 5.15 | bleu 56.03 | wps 1475.6 | wpb 933.5 | bsz 59.6 | num_updates 15795 | best_bleu 57.29
2022-09-08 07:41:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 196 @ 15795 updates
2022-09-08 07:41:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint196.pt
2022-09-08 07:41:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint196.pt
2022-09-08 07:41:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint196.pt (epoch 196 @ 15795 updates, score 56.03) (writing took 18.055455047637224 seconds)
2022-09-08 07:41:58 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)
2022-09-08 07:41:58 | INFO | train | epoch 196 | loss 3.407 | nll_loss 0.361 | mask_loss 7.99127 | p_2 0.03519 | mask_ave 0.495 | ppl 1.28 | wps 3483.1 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 15795 | lr 0.000251617 | gnorm 0.292 | train_wall 97 | gb_free 9.2 | wall 27351
2022-09-08 07:41:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 07:41:58 | INFO | fairseq.trainer | begin training epoch 197
2022-09-08 07:41:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 07:42:05 | INFO | train_inner | epoch 197:      5 / 81 loss=3.406, nll_loss=0.361, mask_loss=7.98892, p_2=0.03516, mask_ave=0.494, ppl=1.28, wps=3022.4, ups=0.55, wpb=5512.8, bsz=361, num_updates=15800, lr=0.000251577, gnorm=0.29, train_wall=120, gb_free=9, wall=27358
2022-09-08 07:43:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 07:43:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 07:43:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 07:43:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 07:43:48 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 5.021 | nll_loss 2.37 | mask_loss 8.92816 | p_2 0.04856 | mask_ave 0.615 | ppl 5.17 | bleu 56.81 | wps 1509.9 | wpb 933.5 | bsz 59.6 | num_updates 15876 | best_bleu 57.29
2022-09-08 07:43:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 197 @ 15876 updates
2022-09-08 07:43:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint197.pt
2022-09-08 07:43:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint197.pt
2022-09-08 07:44:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint197.pt (epoch 197 @ 15876 updates, score 56.81) (writing took 20.57574850693345 seconds)
2022-09-08 07:44:09 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)
2022-09-08 07:44:09 | INFO | train | epoch 197 | loss 3.407 | nll_loss 0.361 | mask_loss 8.10635 | p_2 0.03531 | mask_ave 0.491 | ppl 1.28 | wps 3423 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 15876 | lr 0.000250974 | gnorm 0.29 | train_wall 97 | gb_free 9 | wall 27482
2022-09-08 07:44:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 07:44:09 | INFO | fairseq.trainer | begin training epoch 198
2022-09-08 07:44:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 07:44:39 | INFO | train_inner | epoch 198:     24 / 81 loss=3.407, nll_loss=0.361, mask_loss=8.09839, p_2=0.03521, mask_ave=0.493, ppl=1.28, wps=3589, ups=0.65, wpb=5519, bsz=356.9, num_updates=15900, lr=0.000250785, gnorm=0.285, train_wall=120, gb_free=9.1, wall=27512
2022-09-08 07:45:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 07:45:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 07:45:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 07:45:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 07:46:00 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 5.028 | nll_loss 2.378 | mask_loss 8.95522 | p_2 0.04832 | mask_ave 0.621 | ppl 5.2 | bleu 56.17 | wps 1495.9 | wpb 933.5 | bsz 59.6 | num_updates 15957 | best_bleu 57.29
2022-09-08 07:46:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 198 @ 15957 updates
2022-09-08 07:46:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint198.pt
2022-09-08 07:46:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint198.pt
2022-09-08 07:46:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint198.pt (epoch 198 @ 15957 updates, score 56.17) (writing took 19.306932233273983 seconds)
2022-09-08 07:46:19 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)
2022-09-08 07:46:19 | INFO | train | epoch 198 | loss 3.407 | nll_loss 0.362 | mask_loss 8.11409 | p_2 0.03512 | mask_ave 0.496 | ppl 1.29 | wps 3437.7 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 15957 | lr 0.000250337 | gnorm 0.291 | train_wall 98 | gb_free 9 | wall 27612
2022-09-08 07:46:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 07:46:19 | INFO | fairseq.trainer | begin training epoch 199
2022-09-08 07:46:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 07:47:13 | INFO | train_inner | epoch 199:     43 / 81 loss=3.406, nll_loss=0.361, mask_loss=8.10761, p_2=0.03522, mask_ave=0.495, ppl=1.28, wps=3601.4, ups=0.65, wpb=5539.7, bsz=360.6, num_updates=16000, lr=0.00025, gnorm=0.287, train_wall=121, gb_free=9, wall=27666
2022-09-08 07:47:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 07:48:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 07:48:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 07:48:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 07:48:10 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 5.025 | nll_loss 2.378 | mask_loss 8.92057 | p_2 0.04881 | mask_ave 0.607 | ppl 5.2 | bleu 55.76 | wps 1507.3 | wpb 933.5 | bsz 59.6 | num_updates 16038 | best_bleu 57.29
2022-09-08 07:48:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 199 @ 16038 updates
2022-09-08 07:48:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint199.pt
2022-09-08 07:48:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint199.pt
2022-09-08 07:48:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint199.pt (epoch 199 @ 16038 updates, score 55.76) (writing took 20.65555165335536 seconds)
2022-09-08 07:48:31 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)
2022-09-08 07:48:31 | INFO | train | epoch 199 | loss 3.405 | nll_loss 0.359 | mask_loss 8.12692 | p_2 0.03518 | mask_ave 0.494 | ppl 1.28 | wps 3385.6 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 16038 | lr 0.000249704 | gnorm 0.275 | train_wall 98 | gb_free 9.2 | wall 27744
2022-09-08 07:48:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 07:48:32 | INFO | fairseq.trainer | begin training epoch 200
2022-09-08 07:48:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 07:49:48 | INFO | train_inner | epoch 200:     62 / 81 loss=3.405, nll_loss=0.36, mask_loss=8.16376, p_2=0.03511, mask_ave=0.496, ppl=1.28, wps=3569.2, ups=0.65, wpb=5526.6, bsz=353.4, num_updates=16100, lr=0.000249222, gnorm=0.279, train_wall=121, gb_free=9, wall=27821
2022-09-08 07:50:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 07:50:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 07:50:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 07:50:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 07:50:22 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 5.015 | nll_loss 2.367 | mask_loss 9.05558 | p_2 0.04875 | mask_ave 0.608 | ppl 5.16 | bleu 55.64 | wps 1532.4 | wpb 933.5 | bsz 59.6 | num_updates 16119 | best_bleu 57.29
2022-09-08 07:50:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 200 @ 16119 updates
2022-09-08 07:50:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint200.pt
2022-09-08 07:50:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint200.pt
2022-09-08 07:50:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint200.pt (epoch 200 @ 16119 updates, score 55.64) (writing took 18.741583615541458 seconds)
2022-09-08 07:50:41 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)
2022-09-08 07:50:41 | INFO | train | epoch 200 | loss 3.405 | nll_loss 0.36 | mask_loss 8.13943 | p_2 0.0352 | mask_ave 0.494 | ppl 1.28 | wps 3458.2 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 16119 | lr 0.000249075 | gnorm 0.282 | train_wall 98 | gb_free 9 | wall 27874
2022-09-08 07:50:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 07:50:41 | INFO | fairseq.trainer | begin training epoch 201
2022-09-08 07:50:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 07:52:20 | INFO | train_inner | epoch 201:     81 / 81 loss=3.405, nll_loss=0.361, mask_loss=8.11121, p_2=0.03537, mask_ave=0.491, ppl=1.28, wps=3620.8, ups=0.66, wpb=5502.2, bsz=361.1, num_updates=16200, lr=0.000248452, gnorm=0.284, train_wall=120, gb_free=9.2, wall=27973
2022-09-08 07:52:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 07:52:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 07:52:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 07:52:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 07:52:31 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 5.017 | nll_loss 2.365 | mask_loss 9.10875 | p_2 0.04871 | mask_ave 0.61 | ppl 5.15 | bleu 55.84 | wps 1553.7 | wpb 933.5 | bsz 59.6 | num_updates 16200 | best_bleu 57.29
2022-09-08 07:52:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 201 @ 16200 updates
2022-09-08 07:52:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint201.pt
2022-09-08 07:52:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint201.pt
2022-09-08 07:52:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint201.pt (epoch 201 @ 16200 updates, score 55.84) (writing took 21.5247214846313 seconds)
2022-09-08 07:52:52 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)
2022-09-08 07:52:52 | INFO | train | epoch 201 | loss 3.406 | nll_loss 0.361 | mask_loss 8.12021 | p_2 0.03526 | mask_ave 0.492 | ppl 1.28 | wps 3399 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 16200 | lr 0.000248452 | gnorm 0.282 | train_wall 97 | gb_free 9.2 | wall 28005
2022-09-08 07:52:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 07:52:53 | INFO | fairseq.trainer | begin training epoch 202
2022-09-08 07:52:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 07:54:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 07:54:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 07:54:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 07:54:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 07:54:43 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 5.022 | nll_loss 2.364 | mask_loss 9.03429 | p_2 0.04852 | mask_ave 0.615 | ppl 5.15 | bleu 56.32 | wps 1618 | wpb 933.5 | bsz 59.6 | num_updates 16281 | best_bleu 57.29
2022-09-08 07:54:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 202 @ 16281 updates
2022-09-08 07:54:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint202.pt
2022-09-08 07:54:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint202.pt
2022-09-08 07:55:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint202.pt (epoch 202 @ 16281 updates, score 56.32) (writing took 24.51311632990837 seconds)
2022-09-08 07:55:08 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)
2022-09-08 07:55:08 | INFO | train | epoch 202 | loss 3.404 | nll_loss 0.359 | mask_loss 8.12188 | p_2 0.03521 | mask_ave 0.494 | ppl 1.28 | wps 3300.3 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 16281 | lr 0.000247833 | gnorm 0.284 | train_wall 99 | gb_free 9.1 | wall 28141
2022-09-08 07:55:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 07:55:08 | INFO | fairseq.trainer | begin training epoch 203
2022-09-08 07:55:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 07:55:32 | INFO | train_inner | epoch 203:     19 / 81 loss=3.404, nll_loss=0.359, mask_loss=8.11249, p_2=0.03519, mask_ave=0.494, ppl=1.28, wps=2883.9, ups=0.52, wpb=5547.4, bsz=359.8, num_updates=16300, lr=0.000247689, gnorm=0.284, train_wall=122, gb_free=9.1, wall=28165
2022-09-08 07:56:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 07:56:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 07:56:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 07:56:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 07:57:02 | INFO | valid | epoch 203 | valid on 'valid' subset | loss 5.023 | nll_loss 2.377 | mask_loss 9.05672 | p_2 0.04874 | mask_ave 0.609 | ppl 5.19 | bleu 56 | wps 1516.3 | wpb 933.5 | bsz 59.6 | num_updates 16362 | best_bleu 57.29
2022-09-08 07:57:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 203 @ 16362 updates
2022-09-08 07:57:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint203.pt
2022-09-08 07:57:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint203.pt
2022-09-08 07:57:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint203.pt (epoch 203 @ 16362 updates, score 56.0) (writing took 21.40713708475232 seconds)
2022-09-08 07:57:23 | INFO | fairseq_cli.train | end of epoch 203 (average epoch stats below)
2022-09-08 07:57:23 | INFO | train | epoch 203 | loss 3.405 | nll_loss 0.36 | mask_loss 8.14065 | p_2 0.03519 | mask_ave 0.494 | ppl 1.28 | wps 3308.8 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 16362 | lr 0.000247219 | gnorm 0.282 | train_wall 101 | gb_free 9.1 | wall 28276
2022-09-08 07:57:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 07:57:23 | INFO | fairseq.trainer | begin training epoch 204
2022-09-08 07:57:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 07:58:11 | INFO | train_inner | epoch 204:     38 / 81 loss=3.404, nll_loss=0.359, mask_loss=8.13463, p_2=0.03525, mask_ave=0.493, ppl=1.28, wps=3463.9, ups=0.63, wpb=5518.1, bsz=357.9, num_updates=16400, lr=0.000246932, gnorm=0.283, train_wall=125, gb_free=9.1, wall=28324
2022-09-08 07:59:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 07:59:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 07:59:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 07:59:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 07:59:15 | INFO | valid | epoch 204 | valid on 'valid' subset | loss 5.026 | nll_loss 2.383 | mask_loss 9.14194 | p_2 0.04855 | mask_ave 0.614 | ppl 5.22 | bleu 55.83 | wps 1513.7 | wpb 933.5 | bsz 59.6 | num_updates 16443 | best_bleu 57.29
2022-09-08 07:59:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 204 @ 16443 updates
2022-09-08 07:59:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint204.pt
2022-09-08 07:59:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint204.pt
2022-09-08 07:59:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint204.pt (epoch 204 @ 16443 updates, score 55.83) (writing took 19.089301135390997 seconds)
2022-09-08 07:59:34 | INFO | fairseq_cli.train | end of epoch 204 (average epoch stats below)
2022-09-08 07:59:34 | INFO | train | epoch 204 | loss 3.405 | nll_loss 0.36 | mask_loss 8.11633 | p_2 0.03524 | mask_ave 0.493 | ppl 1.28 | wps 3416.3 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 16443 | lr 0.000246609 | gnorm 0.292 | train_wall 99 | gb_free 9.1 | wall 28407
2022-09-08 07:59:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 07:59:34 | INFO | fairseq.trainer | begin training epoch 205
2022-09-08 07:59:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:00:45 | INFO | train_inner | epoch 205:     57 / 81 loss=3.405, nll_loss=0.36, mask_loss=8.15512, p_2=0.03517, mask_ave=0.493, ppl=1.28, wps=3602.8, ups=0.65, wpb=5534.9, bsz=358.2, num_updates=16500, lr=0.000246183, gnorm=0.284, train_wall=122, gb_free=9.1, wall=28478
2022-09-08 08:01:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:01:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:01:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:01:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:01:25 | INFO | valid | epoch 205 | valid on 'valid' subset | loss 5.006 | nll_loss 2.359 | mask_loss 8.97324 | p_2 0.04871 | mask_ave 0.609 | ppl 5.13 | bleu 56.68 | wps 1571.2 | wpb 933.5 | bsz 59.6 | num_updates 16524 | best_bleu 57.29
2022-09-08 08:01:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 205 @ 16524 updates
2022-09-08 08:01:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint205.pt
2022-09-08 08:01:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint205.pt
2022-09-08 08:01:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint205.pt (epoch 205 @ 16524 updates, score 56.68) (writing took 19.532023765146732 seconds)
2022-09-08 08:01:44 | INFO | fairseq_cli.train | end of epoch 205 (average epoch stats below)
2022-09-08 08:01:44 | INFO | train | epoch 205 | loss 3.404 | nll_loss 0.359 | mask_loss 8.1843 | p_2 0.03524 | mask_ave 0.493 | ppl 1.28 | wps 3432.5 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 16524 | lr 0.000246004 | gnorm 0.273 | train_wall 98 | gb_free 9.1 | wall 28537
2022-09-08 08:01:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 08:01:45 | INFO | fairseq.trainer | begin training epoch 206
2022-09-08 08:01:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:03:18 | INFO | train_inner | epoch 206:     76 / 81 loss=3.404, nll_loss=0.359, mask_loss=8.13611, p_2=0.03536, mask_ave=0.493, ppl=1.28, wps=3603.6, ups=0.65, wpb=5515.6, bsz=359, num_updates=16600, lr=0.00024544, gnorm=0.277, train_wall=121, gb_free=9, wall=28631
2022-09-08 08:03:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:03:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:03:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:03:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:03:35 | INFO | valid | epoch 206 | valid on 'valid' subset | loss 4.996 | nll_loss 2.343 | mask_loss 9.09301 | p_2 0.04881 | mask_ave 0.606 | ppl 5.07 | bleu 56.11 | wps 1570.4 | wpb 933.5 | bsz 59.6 | num_updates 16605 | best_bleu 57.29
2022-09-08 08:03:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 206 @ 16605 updates
2022-09-08 08:03:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint206.pt
2022-09-08 08:03:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint206.pt
2022-09-08 08:03:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint206.pt (epoch 206 @ 16605 updates, score 56.11) (writing took 19.568330265581608 seconds)
2022-09-08 08:03:54 | INFO | fairseq_cli.train | end of epoch 206 (average epoch stats below)
2022-09-08 08:03:54 | INFO | train | epoch 206 | loss 3.404 | nll_loss 0.36 | mask_loss 8.12155 | p_2 0.03529 | mask_ave 0.491 | ppl 1.28 | wps 3444.2 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 16605 | lr 0.000245403 | gnorm 0.281 | train_wall 98 | gb_free 9 | wall 28667
2022-09-08 08:03:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 08:03:55 | INFO | fairseq.trainer | begin training epoch 207
2022-09-08 08:03:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:05:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:05:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:05:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:05:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:05:44 | INFO | valid | epoch 207 | valid on 'valid' subset | loss 4.993 | nll_loss 2.336 | mask_loss 8.94095 | p_2 0.0484 | mask_ave 0.618 | ppl 5.05 | bleu 56.2 | wps 1552.3 | wpb 933.5 | bsz 59.6 | num_updates 16686 | best_bleu 57.29
2022-09-08 08:05:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 207 @ 16686 updates
2022-09-08 08:05:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint207.pt
2022-09-08 08:05:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint207.pt
2022-09-08 08:06:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint207.pt (epoch 207 @ 16686 updates, score 56.2) (writing took 19.585951630026102 seconds)
2022-09-08 08:06:04 | INFO | fairseq_cli.train | end of epoch 207 (average epoch stats below)
2022-09-08 08:06:04 | INFO | train | epoch 207 | loss 3.404 | nll_loss 0.359 | mask_loss 8.16163 | p_2 0.0353 | mask_ave 0.491 | ppl 1.28 | wps 3444.1 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 16686 | lr 0.000244807 | gnorm 0.279 | train_wall 98 | gb_free 9.2 | wall 28797
2022-09-08 08:06:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 08:06:04 | INFO | fairseq.trainer | begin training epoch 208
2022-09-08 08:06:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:06:22 | INFO | train_inner | epoch 208:     14 / 81 loss=3.404, nll_loss=0.359, mask_loss=8.15697, p_2=0.03528, mask_ave=0.492, ppl=1.28, wps=2989.5, ups=0.54, wpb=5493.3, bsz=353.8, num_updates=16700, lr=0.000244704, gnorm=0.278, train_wall=120, gb_free=9, wall=28815
2022-09-08 08:07:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:07:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:07:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:07:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:07:55 | INFO | valid | epoch 208 | valid on 'valid' subset | loss 5.022 | nll_loss 2.374 | mask_loss 9.2045 | p_2 0.0484 | mask_ave 0.618 | ppl 5.18 | bleu 56.39 | wps 1496.4 | wpb 933.5 | bsz 59.6 | num_updates 16767 | best_bleu 57.29
2022-09-08 08:07:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 208 @ 16767 updates
2022-09-08 08:07:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint208.pt
2022-09-08 08:07:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint208.pt
2022-09-08 08:07:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint208.pt (epoch 208 @ 16767 updates, score 56.39) (writing took 2.840752799063921 seconds)
2022-09-08 08:07:58 | INFO | fairseq_cli.train | end of epoch 208 (average epoch stats below)
2022-09-08 08:07:58 | INFO | train | epoch 208 | loss 3.403 | nll_loss 0.358 | mask_loss 8.07527 | p_2 0.03521 | mask_ave 0.494 | ppl 1.28 | wps 3919.3 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 16767 | lr 0.000244215 | gnorm 0.267 | train_wall 98 | gb_free 9.1 | wall 28911
2022-09-08 08:07:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 08:07:58 | INFO | fairseq.trainer | begin training epoch 209
2022-09-08 08:07:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:08:38 | INFO | train_inner | epoch 209:     33 / 81 loss=3.403, nll_loss=0.359, mask_loss=8.11119, p_2=0.03514, mask_ave=0.491, ppl=1.28, wps=4077.7, ups=0.73, wpb=5559.7, bsz=360.8, num_updates=16800, lr=0.000243975, gnorm=0.271, train_wall=121, gb_free=9.1, wall=28951
2022-09-08 08:09:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:09:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:09:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:09:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:09:46 | INFO | valid | epoch 209 | valid on 'valid' subset | loss 5.02 | nll_loss 2.365 | mask_loss 9.24587 | p_2 0.04865 | mask_ave 0.612 | ppl 5.15 | bleu 55.87 | wps 1587 | wpb 933.5 | bsz 59.6 | num_updates 16848 | best_bleu 57.29
2022-09-08 08:09:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 209 @ 16848 updates
2022-09-08 08:09:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint209.pt
2022-09-08 08:09:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint209.pt
2022-09-08 08:10:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint209.pt (epoch 209 @ 16848 updates, score 55.87) (writing took 19.492586061358452 seconds)
2022-09-08 08:10:06 | INFO | fairseq_cli.train | end of epoch 209 (average epoch stats below)
2022-09-08 08:10:06 | INFO | train | epoch 209 | loss 3.403 | nll_loss 0.359 | mask_loss 8.2274 | p_2 0.03528 | mask_ave 0.492 | ppl 1.28 | wps 3506.9 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 16848 | lr 0.000243627 | gnorm 0.284 | train_wall 96 | gb_free 8.9 | wall 29039
2022-09-08 08:10:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 08:10:06 | INFO | fairseq.trainer | begin training epoch 210
2022-09-08 08:10:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:11:10 | INFO | train_inner | epoch 210:     52 / 81 loss=3.403, nll_loss=0.358, mask_loss=8.24847, p_2=0.03551, mask_ave=0.493, ppl=1.28, wps=3626.3, ups=0.66, wpb=5495.9, bsz=359.2, num_updates=16900, lr=0.000243252, gnorm=0.281, train_wall=120, gb_free=9, wall=29103
2022-09-08 08:11:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:11:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:11:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:11:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:11:57 | INFO | valid | epoch 210 | valid on 'valid' subset | loss 5.026 | nll_loss 2.373 | mask_loss 9.03484 | p_2 0.0486 | mask_ave 0.614 | ppl 5.18 | bleu 56.13 | wps 1544.3 | wpb 933.5 | bsz 59.6 | num_updates 16929 | best_bleu 57.29
2022-09-08 08:11:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 210 @ 16929 updates
2022-09-08 08:11:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint210.pt
2022-09-08 08:11:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint210.pt
2022-09-08 08:12:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint210.pt (epoch 210 @ 16929 updates, score 56.13) (writing took 25.25951498374343 seconds)
2022-09-08 08:12:22 | INFO | fairseq_cli.train | end of epoch 210 (average epoch stats below)
2022-09-08 08:12:22 | INFO | train | epoch 210 | loss 3.403 | nll_loss 0.359 | mask_loss 8.2102 | p_2 0.03528 | mask_ave 0.492 | ppl 1.28 | wps 3279.6 | ups 0.59 | wpb 5523.2 | bsz 358 | num_updates 16929 | lr 0.000243044 | gnorm 0.279 | train_wall 98 | gb_free 9 | wall 29175
2022-09-08 08:12:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 08:12:23 | INFO | fairseq.trainer | begin training epoch 211
2022-09-08 08:12:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:13:53 | INFO | train_inner | epoch 211:     71 / 81 loss=3.403, nll_loss=0.359, mask_loss=8.12462, p_2=0.03524, mask_ave=0.492, ppl=1.28, wps=3400.9, ups=0.61, wpb=5543.6, bsz=360.6, num_updates=17000, lr=0.000242536, gnorm=0.293, train_wall=125, gb_free=8.9, wall=29266
2022-09-08 08:14:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:14:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:14:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:14:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:14:16 | INFO | valid | epoch 211 | valid on 'valid' subset | loss 5.003 | nll_loss 2.354 | mask_loss 9.08911 | p_2 0.04865 | mask_ave 0.611 | ppl 5.11 | bleu 55.6 | wps 1509.5 | wpb 933.5 | bsz 59.6 | num_updates 17010 | best_bleu 57.29
2022-09-08 08:14:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 211 @ 17010 updates
2022-09-08 08:14:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint211.pt
2022-09-08 08:14:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint211.pt
2022-09-08 08:14:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint211.pt (epoch 211 @ 17010 updates, score 55.6) (writing took 20.106643091887236 seconds)
2022-09-08 08:14:37 | INFO | fairseq_cli.train | end of epoch 211 (average epoch stats below)
2022-09-08 08:14:37 | INFO | train | epoch 211 | loss 3.402 | nll_loss 0.358 | mask_loss 8.13154 | p_2 0.03527 | mask_ave 0.492 | ppl 1.28 | wps 3329.8 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 17010 | lr 0.000242464 | gnorm 0.296 | train_wall 101 | gb_free 9.3 | wall 29310
2022-09-08 08:14:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 08:14:37 | INFO | fairseq.trainer | begin training epoch 212
2022-09-08 08:14:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:16:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:16:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:16:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:16:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:16:31 | INFO | valid | epoch 212 | valid on 'valid' subset | loss 5.02 | nll_loss 2.373 | mask_loss 9.37749 | p_2 0.04853 | mask_ave 0.615 | ppl 5.18 | bleu 56.06 | wps 1463.7 | wpb 933.5 | bsz 59.6 | num_updates 17091 | best_bleu 57.29
2022-09-08 08:16:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 212 @ 17091 updates
2022-09-08 08:16:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint212.pt
2022-09-08 08:16:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint212.pt
2022-09-08 08:16:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint212.pt (epoch 212 @ 17091 updates, score 56.06) (writing took 18.366099156439304 seconds)
2022-09-08 08:16:49 | INFO | fairseq_cli.train | end of epoch 212 (average epoch stats below)
2022-09-08 08:16:49 | INFO | train | epoch 212 | loss 3.402 | nll_loss 0.358 | mask_loss 8.15871 | p_2 0.03531 | mask_ave 0.491 | ppl 1.28 | wps 3378.4 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 17091 | lr 0.000241889 | gnorm 0.281 | train_wall 101 | gb_free 9.2 | wall 29442
2022-09-08 08:16:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 08:16:49 | INFO | fairseq.trainer | begin training epoch 213
2022-09-08 08:16:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:17:02 | INFO | train_inner | epoch 213:      9 / 81 loss=3.403, nll_loss=0.358, mask_loss=8.19259, p_2=0.03511, mask_ave=0.491, ppl=1.28, wps=2914.8, ups=0.53, wpb=5500.2, bsz=351, num_updates=17100, lr=0.000241825, gnorm=0.286, train_wall=124, gb_free=9.1, wall=29454
2022-09-08 08:18:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:18:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:18:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:18:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:18:43 | INFO | valid | epoch 213 | valid on 'valid' subset | loss 5.027 | nll_loss 2.38 | mask_loss 9.10743 | p_2 0.04885 | mask_ave 0.606 | ppl 5.2 | bleu 56.02 | wps 1536.3 | wpb 933.5 | bsz 59.6 | num_updates 17172 | best_bleu 57.29
2022-09-08 08:18:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 213 @ 17172 updates
2022-09-08 08:18:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint213.pt
2022-09-08 08:18:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint213.pt
2022-09-08 08:19:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint213.pt (epoch 213 @ 17172 updates, score 56.02) (writing took 18.76038207858801 seconds)
2022-09-08 08:19:02 | INFO | fairseq_cli.train | end of epoch 213 (average epoch stats below)
2022-09-08 08:19:02 | INFO | train | epoch 213 | loss 3.403 | nll_loss 0.358 | mask_loss 8.14093 | p_2 0.03533 | mask_ave 0.491 | ppl 1.28 | wps 3366.1 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 17172 | lr 0.000241318 | gnorm 0.275 | train_wall 101 | gb_free 9.1 | wall 29575
2022-09-08 08:19:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 08:19:02 | INFO | fairseq.trainer | begin training epoch 214
2022-09-08 08:19:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:19:37 | INFO | train_inner | epoch 214:     28 / 81 loss=3.402, nll_loss=0.358, mask_loss=8.08396, p_2=0.03563, mask_ave=0.492, ppl=1.28, wps=3566, ups=0.65, wpb=5527, bsz=364.5, num_updates=17200, lr=0.000241121, gnorm=0.274, train_wall=123, gb_free=9.1, wall=29609
2022-09-08 08:20:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:20:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:20:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:20:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:20:57 | INFO | valid | epoch 214 | valid on 'valid' subset | loss 5.024 | nll_loss 2.375 | mask_loss 8.91671 | p_2 0.0487 | mask_ave 0.61 | ppl 5.19 | bleu 56.68 | wps 1359.6 | wpb 933.5 | bsz 59.6 | num_updates 17253 | best_bleu 57.29
2022-09-08 08:20:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 214 @ 17253 updates
2022-09-08 08:20:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint214.pt
2022-09-08 08:20:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint214.pt
2022-09-08 08:21:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint214.pt (epoch 214 @ 17253 updates, score 56.68) (writing took 17.96824824810028 seconds)
2022-09-08 08:21:15 | INFO | fairseq_cli.train | end of epoch 214 (average epoch stats below)
2022-09-08 08:21:15 | INFO | train | epoch 214 | loss 3.402 | nll_loss 0.358 | mask_loss 8.09181 | p_2 0.03533 | mask_ave 0.491 | ppl 1.28 | wps 3368.4 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 17253 | lr 0.000240751 | gnorm 0.291 | train_wall 101 | gb_free 9.1 | wall 29708
2022-09-08 08:21:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 08:21:15 | INFO | fairseq.trainer | begin training epoch 215
2022-09-08 08:21:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:22:12 | INFO | train_inner | epoch 215:     47 / 81 loss=3.402, nll_loss=0.358, mask_loss=8.09535, p_2=0.03518, mask_ave=0.489, ppl=1.28, wps=3552.2, ups=0.64, wpb=5526.8, bsz=354.7, num_updates=17300, lr=0.000240424, gnorm=0.284, train_wall=124, gb_free=9.1, wall=29765
2022-09-08 08:22:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:22:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:22:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:22:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:23:09 | INFO | valid | epoch 215 | valid on 'valid' subset | loss 5.018 | nll_loss 2.37 | mask_loss 8.89127 | p_2 0.04863 | mask_ave 0.612 | ppl 5.17 | bleu 56.15 | wps 1372.6 | wpb 933.5 | bsz 59.6 | num_updates 17334 | best_bleu 57.29
2022-09-08 08:23:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 215 @ 17334 updates
2022-09-08 08:23:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint215.pt
2022-09-08 08:23:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint215.pt
2022-09-08 08:23:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint215.pt (epoch 215 @ 17334 updates, score 56.15) (writing took 19.889759857207537 seconds)
2022-09-08 08:23:29 | INFO | fairseq_cli.train | end of epoch 215 (average epoch stats below)
2022-09-08 08:23:29 | INFO | train | epoch 215 | loss 3.402 | nll_loss 0.358 | mask_loss 8.07295 | p_2 0.03534 | mask_ave 0.49 | ppl 1.28 | wps 3334.4 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 17334 | lr 0.000240188 | gnorm 0.28 | train_wall 100 | gb_free 9.1 | wall 29842
2022-09-08 08:23:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 08:23:29 | INFO | fairseq.trainer | begin training epoch 216
2022-09-08 08:23:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:24:55 | INFO | train_inner | epoch 216:     66 / 81 loss=3.402, nll_loss=0.358, mask_loss=8.04324, p_2=0.03515, mask_ave=0.49, ppl=1.28, wps=3389.4, ups=0.61, wpb=5532.1, bsz=356.2, num_updates=17400, lr=0.000239732, gnorm=0.284, train_wall=129, gb_free=9.1, wall=29928
2022-09-08 08:25:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:25:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:25:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:25:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:25:25 | INFO | valid | epoch 216 | valid on 'valid' subset | loss 5.024 | nll_loss 2.381 | mask_loss 8.93319 | p_2 0.0488 | mask_ave 0.608 | ppl 5.21 | bleu 56.55 | wps 1531 | wpb 933.5 | bsz 59.6 | num_updates 17415 | best_bleu 57.29
2022-09-08 08:25:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 216 @ 17415 updates
2022-09-08 08:25:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint216.pt
2022-09-08 08:25:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint216.pt
2022-09-08 08:25:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint216.pt (epoch 216 @ 17415 updates, score 56.55) (writing took 21.16005438566208 seconds)
2022-09-08 08:25:46 | INFO | fairseq_cli.train | end of epoch 216 (average epoch stats below)
2022-09-08 08:25:46 | INFO | train | epoch 216 | loss 3.402 | nll_loss 0.358 | mask_loss 7.99028 | p_2 0.03533 | mask_ave 0.49 | ppl 1.28 | wps 3257.1 | ups 0.59 | wpb 5523.2 | bsz 358 | num_updates 17415 | lr 0.000239628 | gnorm 0.283 | train_wall 103 | gb_free 9.1 | wall 29979
2022-09-08 08:25:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 08:25:47 | INFO | fairseq.trainer | begin training epoch 217
2022-09-08 08:25:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:27:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:27:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:27:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:27:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:27:37 | INFO | valid | epoch 217 | valid on 'valid' subset | loss 5.007 | nll_loss 2.36 | mask_loss 8.97077 | p_2 0.04867 | mask_ave 0.612 | ppl 5.14 | bleu 55.43 | wps 1554.8 | wpb 933.5 | bsz 59.6 | num_updates 17496 | best_bleu 57.29
2022-09-08 08:27:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 217 @ 17496 updates
2022-09-08 08:27:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint217.pt
2022-09-08 08:27:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint217.pt
2022-09-08 08:27:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint217.pt (epoch 217 @ 17496 updates, score 55.43) (writing took 19.299620490521193 seconds)
2022-09-08 08:27:57 | INFO | fairseq_cli.train | end of epoch 217 (average epoch stats below)
2022-09-08 08:27:57 | INFO | train | epoch 217 | loss 3.402 | nll_loss 0.358 | mask_loss 8.10418 | p_2 0.03531 | mask_ave 0.491 | ppl 1.28 | wps 3435.8 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 17496 | lr 0.000239073 | gnorm 0.289 | train_wall 98 | gb_free 9.1 | wall 30109
2022-09-08 08:27:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 08:27:57 | INFO | fairseq.trainer | begin training epoch 218
2022-09-08 08:27:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:28:03 | INFO | train_inner | epoch 218:      4 / 81 loss=3.402, nll_loss=0.358, mask_loss=8.06764, p_2=0.0354, mask_ave=0.491, ppl=1.28, wps=2942.6, ups=0.53, wpb=5513.9, bsz=358.5, num_updates=17500, lr=0.000239046, gnorm=0.288, train_wall=121, gb_free=9.1, wall=30116
2022-09-08 08:29:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:29:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:29:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:29:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:29:49 | INFO | valid | epoch 218 | valid on 'valid' subset | loss 5.006 | nll_loss 2.348 | mask_loss 9.04984 | p_2 0.04869 | mask_ave 0.61 | ppl 5.09 | bleu 56.12 | wps 1501 | wpb 933.5 | bsz 59.6 | num_updates 17577 | best_bleu 57.29
2022-09-08 08:29:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 218 @ 17577 updates
2022-09-08 08:29:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint218.pt
2022-09-08 08:29:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint218.pt
2022-09-08 08:30:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint218.pt (epoch 218 @ 17577 updates, score 56.12) (writing took 20.35343310981989 seconds)
2022-09-08 08:30:09 | INFO | fairseq_cli.train | end of epoch 218 (average epoch stats below)
2022-09-08 08:30:09 | INFO | train | epoch 218 | loss 3.401 | nll_loss 0.357 | mask_loss 8.17097 | p_2 0.03523 | mask_ave 0.493 | ppl 1.28 | wps 3367.7 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 17577 | lr 0.000238522 | gnorm 0.285 | train_wall 99 | gb_free 9.1 | wall 30242
2022-09-08 08:30:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 08:30:10 | INFO | fairseq.trainer | begin training epoch 219
2022-09-08 08:30:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:30:39 | INFO | train_inner | epoch 219:     23 / 81 loss=3.4, nll_loss=0.356, mask_loss=8.2011, p_2=0.03517, mask_ave=0.492, ppl=1.28, wps=3531.1, ups=0.64, wpb=5536, bsz=360.6, num_updates=17600, lr=0.000238366, gnorm=0.278, train_wall=123, gb_free=9.1, wall=30272
2022-09-08 08:31:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:31:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:31:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:31:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:32:00 | INFO | valid | epoch 219 | valid on 'valid' subset | loss 5.033 | nll_loss 2.39 | mask_loss 9.05347 | p_2 0.04829 | mask_ave 0.622 | ppl 5.24 | bleu 56.01 | wps 1542.4 | wpb 933.5 | bsz 59.6 | num_updates 17658 | best_bleu 57.29
2022-09-08 08:32:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 219 @ 17658 updates
2022-09-08 08:32:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint219.pt
2022-09-08 08:32:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint219.pt
2022-09-08 08:32:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint219.pt (epoch 219 @ 17658 updates, score 56.01) (writing took 18.212496012449265 seconds)
2022-09-08 08:32:18 | INFO | fairseq_cli.train | end of epoch 219 (average epoch stats below)
2022-09-08 08:32:18 | INFO | train | epoch 219 | loss 3.4 | nll_loss 0.356 | mask_loss 8.23825 | p_2 0.03528 | mask_ave 0.492 | ppl 1.28 | wps 3468.6 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 17658 | lr 0.000237974 | gnorm 0.276 | train_wall 98 | gb_free 9.1 | wall 30371
2022-09-08 08:32:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 08:32:19 | INFO | fairseq.trainer | begin training epoch 220
2022-09-08 08:32:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:33:10 | INFO | train_inner | epoch 220:     42 / 81 loss=3.401, nll_loss=0.357, mask_loss=8.1741, p_2=0.03544, mask_ave=0.492, ppl=1.28, wps=3676.9, ups=0.66, wpb=5531.5, bsz=360.4, num_updates=17700, lr=0.000237691, gnorm=0.285, train_wall=120, gb_free=9.1, wall=30423
2022-09-08 08:33:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:33:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:33:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:33:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:34:08 | INFO | valid | epoch 220 | valid on 'valid' subset | loss 5.011 | nll_loss 2.364 | mask_loss 9.10906 | p_2 0.04854 | mask_ave 0.615 | ppl 5.15 | bleu 56.38 | wps 1501.3 | wpb 933.5 | bsz 59.6 | num_updates 17739 | best_bleu 57.29
2022-09-08 08:34:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 220 @ 17739 updates
2022-09-08 08:34:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint220.pt
2022-09-08 08:34:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint220.pt
2022-09-08 08:34:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint220.pt (epoch 220 @ 17739 updates, score 56.38) (writing took 2.432276137173176 seconds)
2022-09-08 08:34:10 | INFO | fairseq_cli.train | end of epoch 220 (average epoch stats below)
2022-09-08 08:34:10 | INFO | train | epoch 220 | loss 3.401 | nll_loss 0.357 | mask_loss 8.148 | p_2 0.03529 | mask_ave 0.492 | ppl 1.28 | wps 3996.4 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 17739 | lr 0.00023743 | gnorm 0.279 | train_wall 97 | gb_free 9.2 | wall 30483
2022-09-08 08:34:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 08:34:11 | INFO | fairseq.trainer | begin training epoch 221
2022-09-08 08:34:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:35:25 | INFO | train_inner | epoch 221:     61 / 81 loss=3.4, nll_loss=0.357, mask_loss=8.24527, p_2=0.03523, mask_ave=0.493, ppl=1.28, wps=4079, ups=0.74, wpb=5502.6, bsz=353.5, num_updates=17800, lr=0.000237023, gnorm=0.288, train_wall=119, gb_free=9.1, wall=30558
2022-09-08 08:35:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:35:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:35:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:35:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:36:00 | INFO | valid | epoch 221 | valid on 'valid' subset | loss 5.014 | nll_loss 2.362 | mask_loss 9.0529 | p_2 0.04875 | mask_ave 0.608 | ppl 5.14 | bleu 56.14 | wps 1538.2 | wpb 933.5 | bsz 59.6 | num_updates 17820 | best_bleu 57.29
2022-09-08 08:36:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 221 @ 17820 updates
2022-09-08 08:36:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint221.pt
2022-09-08 08:36:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint221.pt
2022-09-08 08:36:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint221.pt (epoch 221 @ 17820 updates, score 56.14) (writing took 19.495974961668253 seconds)
2022-09-08 08:36:20 | INFO | fairseq_cli.train | end of epoch 221 (average epoch stats below)
2022-09-08 08:36:20 | INFO | train | epoch 221 | loss 3.401 | nll_loss 0.357 | mask_loss 8.25021 | p_2 0.03531 | mask_ave 0.491 | ppl 1.28 | wps 3462.1 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 17820 | lr 0.00023689 | gnorm 0.298 | train_wall 97 | gb_free 9 | wall 30612
2022-09-08 08:36:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 08:36:20 | INFO | fairseq.trainer | begin training epoch 222
2022-09-08 08:36:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:37:58 | INFO | train_inner | epoch 222:     80 / 81 loss=3.401, nll_loss=0.358, mask_loss=8.21579, p_2=0.03541, mask_ave=0.486, ppl=1.28, wps=3616.7, ups=0.65, wpb=5542.6, bsz=359.2, num_updates=17900, lr=0.00023636, gnorm=0.278, train_wall=121, gb_free=9.2, wall=30711
2022-09-08 08:37:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:38:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:38:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:38:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:38:10 | INFO | valid | epoch 222 | valid on 'valid' subset | loss 5.019 | nll_loss 2.37 | mask_loss 9.06755 | p_2 0.04873 | mask_ave 0.609 | ppl 5.17 | bleu 56.15 | wps 1515.6 | wpb 933.5 | bsz 59.6 | num_updates 17901 | best_bleu 57.29
2022-09-08 08:38:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 222 @ 17901 updates
2022-09-08 08:38:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint222.pt
2022-09-08 08:38:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint222.pt
2022-09-08 08:38:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint222.pt (epoch 222 @ 17901 updates, score 56.15) (writing took 17.671322911977768 seconds)
2022-09-08 08:38:28 | INFO | fairseq_cli.train | end of epoch 222 (average epoch stats below)
2022-09-08 08:38:28 | INFO | train | epoch 222 | loss 3.401 | nll_loss 0.357 | mask_loss 8.22446 | p_2 0.03546 | mask_ave 0.486 | ppl 1.28 | wps 3490.8 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 17901 | lr 0.000236353 | gnorm 0.275 | train_wall 97 | gb_free 9 | wall 30741
2022-09-08 08:38:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 08:38:28 | INFO | fairseq.trainer | begin training epoch 223
2022-09-08 08:38:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:40:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:40:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:40:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:40:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:40:18 | INFO | valid | epoch 223 | valid on 'valid' subset | loss 5.02 | nll_loss 2.378 | mask_loss 9.17661 | p_2 0.04863 | mask_ave 0.612 | ppl 5.2 | bleu 55.96 | wps 1582 | wpb 933.5 | bsz 59.6 | num_updates 17982 | best_bleu 57.29
2022-09-08 08:40:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 223 @ 17982 updates
2022-09-08 08:40:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint223.pt
2022-09-08 08:40:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint223.pt
2022-09-08 08:40:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint223.pt (epoch 223 @ 17982 updates, score 55.96) (writing took 20.4384882748127 seconds)
2022-09-08 08:40:38 | INFO | fairseq_cli.train | end of epoch 223 (average epoch stats below)
2022-09-08 08:40:38 | INFO | train | epoch 223 | loss 3.4 | nll_loss 0.356 | mask_loss 8.28131 | p_2 0.03539 | mask_ave 0.489 | ppl 1.28 | wps 3427.4 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 17982 | lr 0.00023582 | gnorm 0.272 | train_wall 98 | gb_free 9 | wall 30871
2022-09-08 08:40:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 08:40:38 | INFO | fairseq.trainer | begin training epoch 224
2022-09-08 08:40:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:41:00 | INFO | train_inner | epoch 224:     18 / 81 loss=3.4, nll_loss=0.356, mask_loss=8.2933, p_2=0.03534, mask_ave=0.489, ppl=1.28, wps=3019.9, ups=0.55, wpb=5493.5, bsz=354.5, num_updates=18000, lr=0.000235702, gnorm=0.274, train_wall=119, gb_free=9.2, wall=30893
2022-09-08 08:42:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:42:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:42:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:42:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:42:27 | INFO | valid | epoch 224 | valid on 'valid' subset | loss 5.018 | nll_loss 2.368 | mask_loss 9.19101 | p_2 0.04896 | mask_ave 0.603 | ppl 5.16 | bleu 56.03 | wps 1560.6 | wpb 933.5 | bsz 59.6 | num_updates 18063 | best_bleu 57.29
2022-09-08 08:42:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 224 @ 18063 updates
2022-09-08 08:42:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint224.pt
2022-09-08 08:42:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint224.pt
2022-09-08 08:42:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint224.pt (epoch 224 @ 18063 updates, score 56.03) (writing took 18.303521864116192 seconds)
2022-09-08 08:42:46 | INFO | fairseq_cli.train | end of epoch 224 (average epoch stats below)
2022-09-08 08:42:46 | INFO | train | epoch 224 | loss 3.399 | nll_loss 0.356 | mask_loss 8.25769 | p_2 0.03543 | mask_ave 0.488 | ppl 1.28 | wps 3505 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 18063 | lr 0.000235291 | gnorm 0.269 | train_wall 97 | gb_free 9.1 | wall 30999
2022-09-08 08:42:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 08:42:46 | INFO | fairseq.trainer | begin training epoch 225
2022-09-08 08:42:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:43:33 | INFO | train_inner | epoch 225:     37 / 81 loss=3.399, nll_loss=0.356, mask_loss=8.20765, p_2=0.03574, mask_ave=0.489, ppl=1.28, wps=3607.4, ups=0.66, wpb=5507.4, bsz=362.8, num_updates=18100, lr=0.00023505, gnorm=0.267, train_wall=122, gb_free=9, wall=31046
2022-09-08 08:44:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:44:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:44:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:44:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:44:37 | INFO | valid | epoch 225 | valid on 'valid' subset | loss 5.019 | nll_loss 2.372 | mask_loss 9.10352 | p_2 0.04881 | mask_ave 0.607 | ppl 5.18 | bleu 56.42 | wps 1605.3 | wpb 933.5 | bsz 59.6 | num_updates 18144 | best_bleu 57.29
2022-09-08 08:44:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 225 @ 18144 updates
2022-09-08 08:44:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint225.pt
2022-09-08 08:44:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint225.pt
2022-09-08 08:44:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint225.pt (epoch 225 @ 18144 updates, score 56.42) (writing took 18.422727581113577 seconds)
2022-09-08 08:44:56 | INFO | fairseq_cli.train | end of epoch 225 (average epoch stats below)
2022-09-08 08:44:56 | INFO | train | epoch 225 | loss 3.4 | nll_loss 0.356 | mask_loss 8.23193 | p_2 0.03541 | mask_ave 0.488 | ppl 1.28 | wps 3438.9 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 18144 | lr 0.000234765 | gnorm 0.279 | train_wall 99 | gb_free 9.2 | wall 31129
2022-09-08 08:44:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 08:44:56 | INFO | fairseq.trainer | begin training epoch 226
2022-09-08 08:44:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:46:06 | INFO | train_inner | epoch 226:     56 / 81 loss=3.399, nll_loss=0.356, mask_loss=8.22224, p_2=0.03543, mask_ave=0.489, ppl=1.28, wps=3627.1, ups=0.65, wpb=5543.4, bsz=360, num_updates=18200, lr=0.000234404, gnorm=0.284, train_wall=121, gb_free=9.1, wall=31198
2022-09-08 08:46:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:46:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:46:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:46:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:46:47 | INFO | valid | epoch 226 | valid on 'valid' subset | loss 5.024 | nll_loss 2.374 | mask_loss 9.22429 | p_2 0.04835 | mask_ave 0.62 | ppl 5.18 | bleu 56.77 | wps 1572.8 | wpb 933.5 | bsz 59.6 | num_updates 18225 | best_bleu 57.29
2022-09-08 08:46:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 226 @ 18225 updates
2022-09-08 08:46:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint226.pt
2022-09-08 08:46:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint226.pt
2022-09-08 08:47:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint226.pt (epoch 226 @ 18225 updates, score 56.77) (writing took 17.632422886788845 seconds)
2022-09-08 08:47:05 | INFO | fairseq_cli.train | end of epoch 226 (average epoch stats below)
2022-09-08 08:47:05 | INFO | train | epoch 226 | loss 3.399 | nll_loss 0.356 | mask_loss 8.24981 | p_2 0.03533 | mask_ave 0.49 | ppl 1.28 | wps 3482.9 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 18225 | lr 0.000234243 | gnorm 0.279 | train_wall 98 | gb_free 9.1 | wall 31257
2022-09-08 08:47:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 08:47:05 | INFO | fairseq.trainer | begin training epoch 227
2022-09-08 08:47:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:48:37 | INFO | train_inner | epoch 227:     75 / 81 loss=3.4, nll_loss=0.356, mask_loss=8.3326, p_2=0.03499, mask_ave=0.491, ppl=1.28, wps=3642.1, ups=0.66, wpb=5534.4, bsz=352.4, num_updates=18300, lr=0.000233762, gnorm=0.273, train_wall=122, gb_free=9, wall=31350
2022-09-08 08:48:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:48:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:48:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:48:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:48:55 | INFO | valid | epoch 227 | valid on 'valid' subset | loss 5.006 | nll_loss 2.353 | mask_loss 9.07786 | p_2 0.04858 | mask_ave 0.614 | ppl 5.11 | bleu 57.12 | wps 1541.6 | wpb 933.5 | bsz 59.6 | num_updates 18306 | best_bleu 57.29
2022-09-08 08:48:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 227 @ 18306 updates
2022-09-08 08:48:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint227.pt
2022-09-08 08:48:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint227.pt
2022-09-08 08:48:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint227.pt (epoch 227 @ 18306 updates, score 57.12) (writing took 2.3977431170642376 seconds)
2022-09-08 08:48:58 | INFO | fairseq_cli.train | end of epoch 227 (average epoch stats below)
2022-09-08 08:48:58 | INFO | train | epoch 227 | loss 3.399 | nll_loss 0.355 | mask_loss 8.29295 | p_2 0.03529 | mask_ave 0.491 | ppl 1.28 | wps 3942.4 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 18306 | lr 0.000233724 | gnorm 0.27 | train_wall 98 | gb_free 9.1 | wall 31371
2022-09-08 08:48:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 08:48:58 | INFO | fairseq.trainer | begin training epoch 228
2022-09-08 08:48:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:50:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:50:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:50:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:50:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:50:49 | INFO | valid | epoch 228 | valid on 'valid' subset | loss 5.031 | nll_loss 2.397 | mask_loss 9.09743 | p_2 0.04895 | mask_ave 0.603 | ppl 5.27 | bleu 55.84 | wps 1488 | wpb 933.5 | bsz 59.6 | num_updates 18387 | best_bleu 57.29
2022-09-08 08:50:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 228 @ 18387 updates
2022-09-08 08:50:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint228.pt
2022-09-08 08:50:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint228.pt
2022-09-08 08:51:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint228.pt (epoch 228 @ 18387 updates, score 55.84) (writing took 16.14717123657465 seconds)
2022-09-08 08:51:05 | INFO | fairseq_cli.train | end of epoch 228 (average epoch stats below)
2022-09-08 08:51:05 | INFO | train | epoch 228 | loss 3.398 | nll_loss 0.355 | mask_loss 8.20359 | p_2 0.03544 | mask_ave 0.487 | ppl 1.28 | wps 3521 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 18387 | lr 0.000233209 | gnorm 0.265 | train_wall 98 | gb_free 9.1 | wall 31498
2022-09-08 08:51:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 08:51:05 | INFO | fairseq.trainer | begin training epoch 229
2022-09-08 08:51:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:51:22 | INFO | train_inner | epoch 229:     13 / 81 loss=3.398, nll_loss=0.355, mask_loss=8.23355, p_2=0.0351, mask_ave=0.485, ppl=1.28, wps=3373.6, ups=0.61, wpb=5540.1, bsz=355.8, num_updates=18400, lr=0.000233126, gnorm=0.266, train_wall=120, gb_free=9, wall=31515
2022-09-08 08:52:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:52:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:52:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:52:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:52:56 | INFO | valid | epoch 229 | valid on 'valid' subset | loss 5.027 | nll_loss 2.391 | mask_loss 9.07362 | p_2 0.04878 | mask_ave 0.607 | ppl 5.25 | bleu 55.65 | wps 1516.2 | wpb 933.5 | bsz 59.6 | num_updates 18468 | best_bleu 57.29
2022-09-08 08:52:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 229 @ 18468 updates
2022-09-08 08:52:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint229.pt
2022-09-08 08:52:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint229.pt
2022-09-08 08:53:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint229.pt (epoch 229 @ 18468 updates, score 55.65) (writing took 19.52385252714157 seconds)
2022-09-08 08:53:16 | INFO | fairseq_cli.train | end of epoch 229 (average epoch stats below)
2022-09-08 08:53:16 | INFO | train | epoch 229 | loss 3.399 | nll_loss 0.356 | mask_loss 8.24769 | p_2 0.03528 | mask_ave 0.492 | ppl 1.28 | wps 3415 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 18468 | lr 0.000232697 | gnorm 0.292 | train_wall 99 | gb_free 9.2 | wall 31629
2022-09-08 08:53:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 08:53:16 | INFO | fairseq.trainer | begin training epoch 230
2022-09-08 08:53:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:53:57 | INFO | train_inner | epoch 230:     32 / 81 loss=3.399, nll_loss=0.356, mask_loss=8.19511, p_2=0.0359, mask_ave=0.494, ppl=1.28, wps=3538.2, ups=0.65, wpb=5481.1, bsz=364.3, num_updates=18500, lr=0.000232495, gnorm=0.285, train_wall=122, gb_free=9.1, wall=31669
2022-09-08 08:54:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:54:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:54:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:54:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:55:08 | INFO | valid | epoch 230 | valid on 'valid' subset | loss 5.019 | nll_loss 2.371 | mask_loss 9.10081 | p_2 0.04852 | mask_ave 0.616 | ppl 5.17 | bleu 56.39 | wps 1482.9 | wpb 933.5 | bsz 59.6 | num_updates 18549 | best_bleu 57.29
2022-09-08 08:55:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 230 @ 18549 updates
2022-09-08 08:55:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint230.pt
2022-09-08 08:55:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint230.pt
2022-09-08 08:55:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint230.pt (epoch 230 @ 18549 updates, score 56.39) (writing took 42.44923581182957 seconds)
2022-09-08 08:55:51 | INFO | fairseq_cli.train | end of epoch 230 (average epoch stats below)
2022-09-08 08:55:51 | INFO | train | epoch 230 | loss 3.398 | nll_loss 0.355 | mask_loss 8.26541 | p_2 0.03538 | mask_ave 0.489 | ppl 1.28 | wps 2896.2 | ups 0.52 | wpb 5523.2 | bsz 358 | num_updates 18549 | lr 0.000232188 | gnorm 0.26 | train_wall 99 | gb_free 9.3 | wall 31783
2022-09-08 08:55:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 08:55:51 | INFO | fairseq.trainer | begin training epoch 231
2022-09-08 08:55:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:56:54 | INFO | train_inner | epoch 231:     51 / 81 loss=3.398, nll_loss=0.355, mask_loss=8.33387, p_2=0.03511, mask_ave=0.489, ppl=1.28, wps=3133.1, ups=0.57, wpb=5542.6, bsz=354.8, num_updates=18600, lr=0.000231869, gnorm=0.264, train_wall=121, gb_free=9.1, wall=31846
2022-09-08 08:57:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:57:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:57:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:57:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:57:41 | INFO | valid | epoch 231 | valid on 'valid' subset | loss 5.022 | nll_loss 2.374 | mask_loss 9.075 | p_2 0.04867 | mask_ave 0.611 | ppl 5.19 | bleu 55.54 | wps 1439.3 | wpb 933.5 | bsz 59.6 | num_updates 18630 | best_bleu 57.29
2022-09-08 08:57:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 231 @ 18630 updates
2022-09-08 08:57:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint231.pt
2022-09-08 08:57:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint231.pt
2022-09-08 08:58:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint231.pt (epoch 231 @ 18630 updates, score 55.54) (writing took 18.52897559478879 seconds)
2022-09-08 08:58:00 | INFO | fairseq_cli.train | end of epoch 231 (average epoch stats below)
2022-09-08 08:58:00 | INFO | train | epoch 231 | loss 3.398 | nll_loss 0.355 | mask_loss 8.28674 | p_2 0.03537 | mask_ave 0.489 | ppl 1.28 | wps 3451.6 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 18630 | lr 0.000231683 | gnorm 0.269 | train_wall 98 | gb_free 9.1 | wall 31913
2022-09-08 08:58:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 08:58:00 | INFO | fairseq.trainer | begin training epoch 232
2022-09-08 08:58:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 08:59:27 | INFO | train_inner | epoch 232:     70 / 81 loss=3.398, nll_loss=0.355, mask_loss=8.19063, p_2=0.03537, mask_ave=0.489, ppl=1.28, wps=3613.3, ups=0.65, wpb=5538.9, bsz=359.6, num_updates=18700, lr=0.000231249, gnorm=0.271, train_wall=121, gb_free=9.1, wall=32000
2022-09-08 08:59:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 08:59:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 08:59:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 08:59:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 08:59:51 | INFO | valid | epoch 232 | valid on 'valid' subset | loss 5.019 | nll_loss 2.374 | mask_loss 9.16215 | p_2 0.04874 | mask_ave 0.609 | ppl 5.18 | bleu 56.35 | wps 1473.7 | wpb 933.5 | bsz 59.6 | num_updates 18711 | best_bleu 57.29
2022-09-08 08:59:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 232 @ 18711 updates
2022-09-08 08:59:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint232.pt
2022-09-08 08:59:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint232.pt
2022-09-08 09:00:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint232.pt (epoch 232 @ 18711 updates, score 56.35) (writing took 35.93957086279988 seconds)
2022-09-08 09:00:28 | INFO | fairseq_cli.train | end of epoch 232 (average epoch stats below)
2022-09-08 09:00:28 | INFO | train | epoch 232 | loss 3.398 | nll_loss 0.355 | mask_loss 8.19802 | p_2 0.03537 | mask_ave 0.489 | ppl 1.28 | wps 3036.1 | ups 0.55 | wpb 5523.2 | bsz 358 | num_updates 18711 | lr 0.000231181 | gnorm 0.273 | train_wall 98 | gb_free 9.1 | wall 32060
2022-09-08 09:00:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:00:28 | INFO | fairseq.trainer | begin training epoch 233
2022-09-08 09:00:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:02:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 09:02:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 09:02:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 09:02:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 09:02:17 | INFO | valid | epoch 233 | valid on 'valid' subset | loss 5.02 | nll_loss 2.374 | mask_loss 9.13643 | p_2 0.04837 | mask_ave 0.62 | ppl 5.18 | bleu 56.05 | wps 1543 | wpb 933.5 | bsz 59.6 | num_updates 18792 | best_bleu 57.29
2022-09-08 09:02:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 233 @ 18792 updates
2022-09-08 09:02:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint233.pt
2022-09-08 09:02:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint233.pt
2022-09-08 09:02:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint233.pt (epoch 233 @ 18792 updates, score 56.05) (writing took 18.559396173805 seconds)
2022-09-08 09:02:36 | INFO | fairseq_cli.train | end of epoch 233 (average epoch stats below)
2022-09-08 09:02:36 | INFO | train | epoch 233 | loss 3.398 | nll_loss 0.355 | mask_loss 8.27549 | p_2 0.03525 | mask_ave 0.493 | ppl 1.28 | wps 3490.4 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 18792 | lr 0.000230682 | gnorm 0.269 | train_wall 97 | gb_free 9.1 | wall 32189
2022-09-08 09:02:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:02:36 | INFO | fairseq.trainer | begin training epoch 234
2022-09-08 09:02:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:02:46 | INFO | train_inner | epoch 234:      8 / 81 loss=3.398, nll_loss=0.355, mask_loss=8.27661, p_2=0.03519, mask_ave=0.493, ppl=1.28, wps=2761, ups=0.5, wpb=5505.2, bsz=354.8, num_updates=18800, lr=0.000230633, gnorm=0.27, train_wall=119, gb_free=9.1, wall=32199
2022-09-08 09:04:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 09:04:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 09:04:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 09:04:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 09:04:26 | INFO | valid | epoch 234 | valid on 'valid' subset | loss 5.024 | nll_loss 2.384 | mask_loss 9.18458 | p_2 0.04877 | mask_ave 0.608 | ppl 5.22 | bleu 56.33 | wps 1544 | wpb 933.5 | bsz 59.6 | num_updates 18873 | best_bleu 57.29
2022-09-08 09:04:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 234 @ 18873 updates
2022-09-08 09:04:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint234.pt
2022-09-08 09:04:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint234.pt
2022-09-08 09:04:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint234.pt (epoch 234 @ 18873 updates, score 56.33) (writing took 19.319297656416893 seconds)
2022-09-08 09:04:46 | INFO | fairseq_cli.train | end of epoch 234 (average epoch stats below)
2022-09-08 09:04:46 | INFO | train | epoch 234 | loss 3.397 | nll_loss 0.355 | mask_loss 8.17928 | p_2 0.03522 | mask_ave 0.493 | ppl 1.28 | wps 3442.7 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 18873 | lr 0.000230186 | gnorm 0.269 | train_wall 98 | gb_free 9.1 | wall 32319
2022-09-08 09:04:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:04:46 | INFO | fairseq.trainer | begin training epoch 235
2022-09-08 09:04:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:05:19 | INFO | train_inner | epoch 235:     27 / 81 loss=3.397, nll_loss=0.354, mask_loss=8.17812, p_2=0.03522, mask_ave=0.491, ppl=1.28, wps=3637.4, ups=0.66, wpb=5544.5, bsz=359.9, num_updates=18900, lr=0.000230022, gnorm=0.268, train_wall=120, gb_free=9.1, wall=32352
2022-09-08 09:06:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 09:06:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 09:06:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 09:06:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 09:06:35 | INFO | valid | epoch 235 | valid on 'valid' subset | loss 5.013 | nll_loss 2.371 | mask_loss 8.98146 | p_2 0.04851 | mask_ave 0.616 | ppl 5.17 | bleu 56.16 | wps 1581.3 | wpb 933.5 | bsz 59.6 | num_updates 18954 | best_bleu 57.29
2022-09-08 09:06:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 235 @ 18954 updates
2022-09-08 09:06:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint235.pt
2022-09-08 09:06:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint235.pt
2022-09-08 09:06:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint235.pt (epoch 235 @ 18954 updates, score 56.16) (writing took 19.158047281205654 seconds)
2022-09-08 09:06:55 | INFO | fairseq_cli.train | end of epoch 235 (average epoch stats below)
2022-09-08 09:06:55 | INFO | train | epoch 235 | loss 3.397 | nll_loss 0.354 | mask_loss 8.21729 | p_2 0.0353 | mask_ave 0.491 | ppl 1.28 | wps 3463.2 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 18954 | lr 0.000229694 | gnorm 0.264 | train_wall 97 | gb_free 9.1 | wall 32448
2022-09-08 09:06:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:06:55 | INFO | fairseq.trainer | begin training epoch 236
2022-09-08 09:06:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:07:52 | INFO | train_inner | epoch 236:     46 / 81 loss=3.396, nll_loss=0.354, mask_loss=8.15549, p_2=0.03553, mask_ave=0.492, ppl=1.28, wps=3592.4, ups=0.65, wpb=5514.8, bsz=361.1, num_updates=19000, lr=0.000229416, gnorm=0.267, train_wall=122, gb_free=9, wall=32505
2022-09-08 09:08:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 09:08:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 09:08:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 09:08:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 09:08:46 | INFO | valid | epoch 236 | valid on 'valid' subset | loss 5.015 | nll_loss 2.372 | mask_loss 9.12853 | p_2 0.04886 | mask_ave 0.606 | ppl 5.18 | bleu 56.83 | wps 1529.1 | wpb 933.5 | bsz 59.6 | num_updates 19035 | best_bleu 57.29
2022-09-08 09:08:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 236 @ 19035 updates
2022-09-08 09:08:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint236.pt
2022-09-08 09:08:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint236.pt
2022-09-08 09:09:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint236.pt (epoch 236 @ 19035 updates, score 56.83) (writing took 22.89375325292349 seconds)
2022-09-08 09:09:09 | INFO | fairseq_cli.train | end of epoch 236 (average epoch stats below)
2022-09-08 09:09:09 | INFO | train | epoch 236 | loss 3.397 | nll_loss 0.355 | mask_loss 8.1351 | p_2 0.03537 | mask_ave 0.489 | ppl 1.28 | wps 3325.3 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 19035 | lr 0.000229205 | gnorm 0.278 | train_wall 99 | gb_free 9.3 | wall 32582
2022-09-08 09:09:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:09:10 | INFO | fairseq.trainer | begin training epoch 237
2022-09-08 09:09:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:10:30 | INFO | train_inner | epoch 237:     65 / 81 loss=3.398, nll_loss=0.356, mask_loss=8.13624, p_2=0.03536, mask_ave=0.487, ppl=1.28, wps=3518.6, ups=0.64, wpb=5536.9, bsz=359.8, num_updates=19100, lr=0.000228814, gnorm=0.283, train_wall=122, gb_free=9.1, wall=32662
2022-09-08 09:10:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 09:10:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 09:10:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 09:10:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 09:10:59 | INFO | valid | epoch 237 | valid on 'valid' subset | loss 5.027 | nll_loss 2.382 | mask_loss 9.08989 | p_2 0.04858 | mask_ave 0.613 | ppl 5.21 | bleu 56.35 | wps 1525.1 | wpb 933.5 | bsz 59.6 | num_updates 19116 | best_bleu 57.29
2022-09-08 09:10:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 237 @ 19116 updates
2022-09-08 09:10:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint237.pt
2022-09-08 09:11:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint237.pt
2022-09-08 09:11:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint237.pt (epoch 237 @ 19116 updates, score 56.35) (writing took 19.05039271339774 seconds)
2022-09-08 09:11:19 | INFO | fairseq_cli.train | end of epoch 237 (average epoch stats below)
2022-09-08 09:11:19 | INFO | train | epoch 237 | loss 3.398 | nll_loss 0.355 | mask_loss 8.1564 | p_2 0.03543 | mask_ave 0.487 | ppl 1.28 | wps 3462.3 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 19116 | lr 0.000228719 | gnorm 0.279 | train_wall 97 | gb_free 9.2 | wall 32711
2022-09-08 09:11:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:11:19 | INFO | fairseq.trainer | begin training epoch 238
2022-09-08 09:11:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:12:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 09:12:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 09:12:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 09:12:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 09:13:08 | INFO | valid | epoch 238 | valid on 'valid' subset | loss 5.004 | nll_loss 2.352 | mask_loss 9.11158 | p_2 0.04862 | mask_ave 0.612 | ppl 5.11 | bleu 56.77 | wps 1566.7 | wpb 933.5 | bsz 59.6 | num_updates 19197 | best_bleu 57.29
2022-09-08 09:13:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 238 @ 19197 updates
2022-09-08 09:13:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint238.pt
2022-09-08 09:13:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint238.pt
2022-09-08 09:13:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint238.pt (epoch 238 @ 19197 updates, score 56.77) (writing took 19.993399690836668 seconds)
2022-09-08 09:13:28 | INFO | fairseq_cli.train | end of epoch 238 (average epoch stats below)
2022-09-08 09:13:28 | INFO | train | epoch 238 | loss 3.397 | nll_loss 0.354 | mask_loss 8.17014 | p_2 0.03534 | mask_ave 0.49 | ppl 1.28 | wps 3459.6 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 19197 | lr 0.000228236 | gnorm 0.293 | train_wall 97 | gb_free 9.3 | wall 32841
2022-09-08 09:13:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:13:28 | INFO | fairseq.trainer | begin training epoch 239
2022-09-08 09:13:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:13:33 | INFO | train_inner | epoch 239:      3 / 81 loss=3.397, nll_loss=0.355, mask_loss=8.20889, p_2=0.03531, mask_ave=0.49, ppl=1.28, wps=3000.6, ups=0.55, wpb=5491.8, bsz=352.7, num_updates=19200, lr=0.000228218, gnorm=0.292, train_wall=119, gb_free=9.1, wall=32845
2022-09-08 09:15:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 09:15:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 09:15:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 09:15:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 09:15:19 | INFO | valid | epoch 239 | valid on 'valid' subset | loss 5.015 | nll_loss 2.368 | mask_loss 9.04238 | p_2 0.04863 | mask_ave 0.612 | ppl 5.16 | bleu 56.34 | wps 1444.3 | wpb 933.5 | bsz 59.6 | num_updates 19278 | best_bleu 57.29
2022-09-08 09:15:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 239 @ 19278 updates
2022-09-08 09:15:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint239.pt
2022-09-08 09:15:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint239.pt
2022-09-08 09:15:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint239.pt (epoch 239 @ 19278 updates, score 56.34) (writing took 20.989816807210445 seconds)
2022-09-08 09:15:40 | INFO | fairseq_cli.train | end of epoch 239 (average epoch stats below)
2022-09-08 09:15:40 | INFO | train | epoch 239 | loss 3.397 | nll_loss 0.354 | mask_loss 8.18201 | p_2 0.03525 | mask_ave 0.493 | ppl 1.28 | wps 3379.5 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 19278 | lr 0.000227756 | gnorm 0.274 | train_wall 98 | gb_free 9.1 | wall 32973
2022-09-08 09:15:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:15:41 | INFO | fairseq.trainer | begin training epoch 240
2022-09-08 09:15:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:16:08 | INFO | train_inner | epoch 240:     22 / 81 loss=3.396, nll_loss=0.354, mask_loss=8.16104, p_2=0.03511, mask_ave=0.492, ppl=1.28, wps=3577, ups=0.64, wpb=5548, bsz=361.1, num_updates=19300, lr=0.000227626, gnorm=0.27, train_wall=121, gb_free=9, wall=33001
2022-09-08 09:17:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 09:17:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 09:17:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 09:17:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 09:17:30 | INFO | valid | epoch 240 | valid on 'valid' subset | loss 5.033 | nll_loss 2.397 | mask_loss 9.08436 | p_2 0.04841 | mask_ave 0.619 | ppl 5.27 | bleu 56.43 | wps 1569.6 | wpb 933.5 | bsz 59.6 | num_updates 19359 | best_bleu 57.29
2022-09-08 09:17:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 240 @ 19359 updates
2022-09-08 09:17:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint240.pt
2022-09-08 09:17:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint240.pt
2022-09-08 09:17:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint240.pt (epoch 240 @ 19359 updates, score 56.43) (writing took 17.742088489234447 seconds)
2022-09-08 09:17:48 | INFO | fairseq_cli.train | end of epoch 240 (average epoch stats below)
2022-09-08 09:17:48 | INFO | train | epoch 240 | loss 3.396 | nll_loss 0.354 | mask_loss 8.16605 | p_2 0.03521 | mask_ave 0.494 | ppl 1.28 | wps 3516.9 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 19359 | lr 0.000227279 | gnorm 0.273 | train_wall 97 | gb_free 9.2 | wall 33100
2022-09-08 09:17:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:17:48 | INFO | fairseq.trainer | begin training epoch 241
2022-09-08 09:17:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:18:38 | INFO | train_inner | epoch 241:     41 / 81 loss=3.396, nll_loss=0.353, mask_loss=8.17219, p_2=0.03518, mask_ave=0.495, ppl=1.28, wps=3667.4, ups=0.66, wpb=5520.8, bsz=354.2, num_updates=19400, lr=0.000227038, gnorm=0.264, train_wall=120, gb_free=9, wall=33151
2022-09-08 09:19:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 09:19:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 09:19:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 09:19:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 09:19:37 | INFO | valid | epoch 241 | valid on 'valid' subset | loss 5.023 | nll_loss 2.385 | mask_loss 9.02756 | p_2 0.04848 | mask_ave 0.617 | ppl 5.22 | bleu 55.65 | wps 1570 | wpb 933.5 | bsz 59.6 | num_updates 19440 | best_bleu 57.29
2022-09-08 09:19:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 241 @ 19440 updates
2022-09-08 09:19:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint241.pt
2022-09-08 09:19:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint241.pt
2022-09-08 09:19:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint241.pt (epoch 241 @ 19440 updates, score 55.65) (writing took 18.432847760617733 seconds)
2022-09-08 09:19:56 | INFO | fairseq_cli.train | end of epoch 241 (average epoch stats below)
2022-09-08 09:19:56 | INFO | train | epoch 241 | loss 3.395 | nll_loss 0.353 | mask_loss 8.11475 | p_2 0.03517 | mask_ave 0.495 | ppl 1.28 | wps 3486.4 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 19440 | lr 0.000226805 | gnorm 0.255 | train_wall 97 | gb_free 9.1 | wall 33229
2022-09-08 09:19:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:19:56 | INFO | fairseq.trainer | begin training epoch 242
2022-09-08 09:19:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:21:10 | INFO | train_inner | epoch 242:     60 / 81 loss=3.396, nll_loss=0.354, mask_loss=8.13338, p_2=0.03546, mask_ave=0.493, ppl=1.28, wps=3622.4, ups=0.66, wpb=5508.3, bsz=360.7, num_updates=19500, lr=0.000226455, gnorm=0.271, train_wall=121, gb_free=9, wall=33303
2022-09-08 09:21:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 09:21:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 09:21:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 09:21:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 09:21:46 | INFO | valid | epoch 242 | valid on 'valid' subset | loss 5.043 | nll_loss 2.397 | mask_loss 9.03519 | p_2 0.04848 | mask_ave 0.616 | ppl 5.27 | bleu 56.17 | wps 1523.2 | wpb 933.5 | bsz 59.6 | num_updates 19521 | best_bleu 57.29
2022-09-08 09:21:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 242 @ 19521 updates
2022-09-08 09:21:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint242.pt
2022-09-08 09:21:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint242.pt
2022-09-08 09:22:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint242.pt (epoch 242 @ 19521 updates, score 56.17) (writing took 19.290192987769842 seconds)
2022-09-08 09:22:06 | INFO | fairseq_cli.train | end of epoch 242 (average epoch stats below)
2022-09-08 09:22:06 | INFO | train | epoch 242 | loss 3.396 | nll_loss 0.354 | mask_loss 8.18351 | p_2 0.03528 | mask_ave 0.492 | ppl 1.28 | wps 3451.1 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 19521 | lr 0.000226334 | gnorm 0.275 | train_wall 97 | gb_free 9.2 | wall 33358
2022-09-08 09:22:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:22:06 | INFO | fairseq.trainer | begin training epoch 243
2022-09-08 09:22:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:23:43 | INFO | train_inner | epoch 243:     79 / 81 loss=3.396, nll_loss=0.354, mask_loss=8.25518, p_2=0.03506, mask_ave=0.497, ppl=1.28, wps=3634, ups=0.66, wpb=5543.2, bsz=358, num_updates=19600, lr=0.000225877, gnorm=0.269, train_wall=120, gb_free=9.1, wall=33456
2022-09-08 09:23:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 09:23:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 09:23:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 09:23:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 09:23:56 | INFO | valid | epoch 243 | valid on 'valid' subset | loss 5.028 | nll_loss 2.39 | mask_loss 9.08073 | p_2 0.04848 | mask_ave 0.616 | ppl 5.24 | bleu 56.28 | wps 1543.5 | wpb 933.5 | bsz 59.6 | num_updates 19602 | best_bleu 57.29
2022-09-08 09:23:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 243 @ 19602 updates
2022-09-08 09:23:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint243.pt
2022-09-08 09:23:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint243.pt
2022-09-08 09:24:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint243.pt (epoch 243 @ 19602 updates, score 56.28) (writing took 21.51153977960348 seconds)
2022-09-08 09:24:17 | INFO | fairseq_cli.train | end of epoch 243 (average epoch stats below)
2022-09-08 09:24:17 | INFO | train | epoch 243 | loss 3.396 | nll_loss 0.354 | mask_loss 8.26387 | p_2 0.03509 | mask_ave 0.498 | ppl 1.28 | wps 3393.7 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 19602 | lr 0.000225865 | gnorm 0.268 | train_wall 98 | gb_free 9.1 | wall 33490
2022-09-08 09:24:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:24:18 | INFO | fairseq.trainer | begin training epoch 244
2022-09-08 09:24:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:25:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 09:25:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 09:25:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 09:25:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 09:26:08 | INFO | valid | epoch 244 | valid on 'valid' subset | loss 5.033 | nll_loss 2.398 | mask_loss 8.87345 | p_2 0.04896 | mask_ave 0.601 | ppl 5.27 | bleu 56.14 | wps 1580.1 | wpb 933.5 | bsz 59.6 | num_updates 19683 | best_bleu 57.29
2022-09-08 09:26:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 244 @ 19683 updates
2022-09-08 09:26:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint244.pt
2022-09-08 09:26:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint244.pt
2022-09-08 09:26:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint244.pt (epoch 244 @ 19683 updates, score 56.14) (writing took 20.179318476468325 seconds)
2022-09-08 09:26:28 | INFO | fairseq_cli.train | end of epoch 244 (average epoch stats below)
2022-09-08 09:26:28 | INFO | train | epoch 244 | loss 3.395 | nll_loss 0.354 | mask_loss 8.22016 | p_2 0.03526 | mask_ave 0.493 | ppl 1.28 | wps 3424.2 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 19683 | lr 0.0002254 | gnorm 0.27 | train_wall 98 | gb_free 9.2 | wall 33621
2022-09-08 09:26:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:26:28 | INFO | fairseq.trainer | begin training epoch 245
2022-09-08 09:26:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:26:49 | INFO | train_inner | epoch 245:     17 / 81 loss=3.395, nll_loss=0.353, mask_loss=8.22377, p_2=0.03501, mask_ave=0.491, ppl=1.28, wps=2961.7, ups=0.54, wpb=5503.2, bsz=351.9, num_updates=19700, lr=0.000225303, gnorm=0.265, train_wall=119, gb_free=9.2, wall=33642
2022-09-08 09:28:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 09:28:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 09:28:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 09:28:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 09:28:18 | INFO | valid | epoch 245 | valid on 'valid' subset | loss 5.037 | nll_loss 2.399 | mask_loss 9.15821 | p_2 0.04862 | mask_ave 0.613 | ppl 5.27 | bleu 56.24 | wps 1537.3 | wpb 933.5 | bsz 59.6 | num_updates 19764 | best_bleu 57.29
2022-09-08 09:28:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 245 @ 19764 updates
2022-09-08 09:28:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint245.pt
2022-09-08 09:28:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint245.pt
2022-09-08 09:28:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint245.pt (epoch 245 @ 19764 updates, score 56.24) (writing took 18.05763192102313 seconds)
2022-09-08 09:28:37 | INFO | fairseq_cli.train | end of epoch 245 (average epoch stats below)
2022-09-08 09:28:37 | INFO | train | epoch 245 | loss 3.396 | nll_loss 0.354 | mask_loss 8.14096 | p_2 0.03525 | mask_ave 0.493 | ppl 1.28 | wps 3481.3 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 19764 | lr 0.000224938 | gnorm 0.268 | train_wall 97 | gb_free 9.1 | wall 33749
2022-09-08 09:28:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:28:37 | INFO | fairseq.trainer | begin training epoch 246
2022-09-08 09:28:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:29:21 | INFO | train_inner | epoch 246:     36 / 81 loss=3.395, nll_loss=0.353, mask_loss=8.18498, p_2=0.03549, mask_ave=0.493, ppl=1.28, wps=3634.7, ups=0.66, wpb=5528, bsz=362.9, num_updates=19800, lr=0.000224733, gnorm=0.268, train_wall=121, gb_free=9.1, wall=33794
2022-09-08 09:30:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 09:30:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 09:30:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 09:30:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 09:30:26 | INFO | valid | epoch 246 | valid on 'valid' subset | loss 5.019 | nll_loss 2.373 | mask_loss 8.99197 | p_2 0.04844 | mask_ave 0.617 | ppl 5.18 | bleu 56.82 | wps 1486.9 | wpb 933.5 | bsz 59.6 | num_updates 19845 | best_bleu 57.29
2022-09-08 09:30:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 246 @ 19845 updates
2022-09-08 09:30:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint246.pt
2022-09-08 09:30:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint246.pt
2022-09-08 09:30:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint246.pt (epoch 246 @ 19845 updates, score 56.82) (writing took 14.923578605055809 seconds)
2022-09-08 09:30:41 | INFO | fairseq_cli.train | end of epoch 246 (average epoch stats below)
2022-09-08 09:30:41 | INFO | train | epoch 246 | loss 3.396 | nll_loss 0.354 | mask_loss 8.24708 | p_2 0.03529 | mask_ave 0.492 | ppl 1.28 | wps 3584.2 | ups 0.65 | wpb 5523.2 | bsz 358 | num_updates 19845 | lr 0.000224478 | gnorm 0.269 | train_wall 97 | gb_free 9.1 | wall 33874
2022-09-08 09:30:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:30:41 | INFO | fairseq.trainer | begin training epoch 247
2022-09-08 09:30:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:31:50 | INFO | train_inner | epoch 247:     55 / 81 loss=3.396, nll_loss=0.354, mask_loss=8.23249, p_2=0.03513, mask_ave=0.494, ppl=1.28, wps=3700.3, ups=0.67, wpb=5538.1, bsz=357.5, num_updates=19900, lr=0.000224168, gnorm=0.264, train_wall=121, gb_free=9.1, wall=33943
2022-09-08 09:32:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 09:32:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 09:32:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 09:32:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 09:32:32 | INFO | valid | epoch 247 | valid on 'valid' subset | loss 5.025 | nll_loss 2.388 | mask_loss 9.25062 | p_2 0.04886 | mask_ave 0.605 | ppl 5.23 | bleu 56.3 | wps 1529.3 | wpb 933.5 | bsz 59.6 | num_updates 19926 | best_bleu 57.29
2022-09-08 09:32:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 247 @ 19926 updates
2022-09-08 09:32:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint247.pt
2022-09-08 09:32:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint247.pt
2022-09-08 09:32:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint247.pt (epoch 247 @ 19926 updates, score 56.3) (writing took 18.990230459719896 seconds)
2022-09-08 09:32:51 | INFO | fairseq_cli.train | end of epoch 247 (average epoch stats below)
2022-09-08 09:32:51 | INFO | train | epoch 247 | loss 3.395 | nll_loss 0.353 | mask_loss 8.22895 | p_2 0.03521 | mask_ave 0.494 | ppl 1.28 | wps 3452 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 19926 | lr 0.000224022 | gnorm 0.264 | train_wall 98 | gb_free 9.1 | wall 34004
2022-09-08 09:32:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:32:51 | INFO | fairseq.trainer | begin training epoch 248
2022-09-08 09:32:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:34:22 | INFO | train_inner | epoch 248:     74 / 81 loss=3.395, nll_loss=0.354, mask_loss=8.25948, p_2=0.0354, mask_ave=0.49, ppl=1.28, wps=3636.8, ups=0.66, wpb=5521.7, bsz=358.9, num_updates=20000, lr=0.000223607, gnorm=0.273, train_wall=120, gb_free=9.1, wall=34095
2022-09-08 09:34:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 09:34:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 09:34:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 09:34:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 09:34:41 | INFO | valid | epoch 248 | valid on 'valid' subset | loss 5.027 | nll_loss 2.39 | mask_loss 9.0803 | p_2 0.0487 | mask_ave 0.61 | ppl 5.24 | bleu 56.45 | wps 1574.1 | wpb 933.5 | bsz 59.6 | num_updates 20007 | best_bleu 57.29
2022-09-08 09:34:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 248 @ 20007 updates
2022-09-08 09:34:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint248.pt
2022-09-08 09:34:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint248.pt
2022-09-08 09:34:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint248.pt (epoch 248 @ 20007 updates, score 56.45) (writing took 2.3669710345566273 seconds)
2022-09-08 09:34:44 | INFO | fairseq_cli.train | end of epoch 248 (average epoch stats below)
2022-09-08 09:34:44 | INFO | train | epoch 248 | loss 3.395 | nll_loss 0.353 | mask_loss 8.29033 | p_2 0.03538 | mask_ave 0.489 | ppl 1.28 | wps 3973.4 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 20007 | lr 0.000223568 | gnorm 0.272 | train_wall 97 | gb_free 9.2 | wall 34116
2022-09-08 09:34:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:34:44 | INFO | fairseq.trainer | begin training epoch 249
2022-09-08 09:34:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:36:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 09:36:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 09:36:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 09:36:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 09:36:34 | INFO | valid | epoch 249 | valid on 'valid' subset | loss 5.039 | nll_loss 2.403 | mask_loss 9.02061 | p_2 0.04849 | mask_ave 0.616 | ppl 5.29 | bleu 56.08 | wps 1514.9 | wpb 933.5 | bsz 59.6 | num_updates 20088 | best_bleu 57.29
2022-09-08 09:36:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 249 @ 20088 updates
2022-09-08 09:36:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint249.pt
2022-09-08 09:36:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint249.pt
2022-09-08 09:36:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint249.pt (epoch 249 @ 20088 updates, score 56.08) (writing took 20.45181478187442 seconds)
2022-09-08 09:36:55 | INFO | fairseq_cli.train | end of epoch 249 (average epoch stats below)
2022-09-08 09:36:55 | INFO | train | epoch 249 | loss 3.395 | nll_loss 0.353 | mask_loss 8.26549 | p_2 0.03525 | mask_ave 0.493 | ppl 1.28 | wps 3413.8 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 20088 | lr 0.000223116 | gnorm 0.27 | train_wall 98 | gb_free 9.1 | wall 34247
2022-09-08 09:36:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:36:55 | INFO | fairseq.trainer | begin training epoch 250
2022-09-08 09:36:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:37:10 | INFO | train_inner | epoch 250:     12 / 81 loss=3.395, nll_loss=0.353, mask_loss=8.23434, p_2=0.03547, mask_ave=0.493, ppl=1.28, wps=3283.8, ups=0.6, wpb=5497.3, bsz=360, num_updates=20100, lr=0.00022305, gnorm=0.273, train_wall=120, gb_free=9.1, wall=34263
2022-09-08 09:38:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 09:38:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 09:38:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 09:38:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 09:38:45 | INFO | valid | epoch 250 | valid on 'valid' subset | loss 5.023 | nll_loss 2.382 | mask_loss 9.27272 | p_2 0.04871 | mask_ave 0.61 | ppl 5.21 | bleu 56.48 | wps 1457.4 | wpb 933.5 | bsz 59.6 | num_updates 20169 | best_bleu 57.29
2022-09-08 09:38:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 250 @ 20169 updates
2022-09-08 09:38:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint250.pt
2022-09-08 09:38:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint250.pt
2022-09-08 09:39:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint250.pt (epoch 250 @ 20169 updates, score 56.48) (writing took 18.07982375845313 seconds)
2022-09-08 09:39:04 | INFO | fairseq_cli.train | end of epoch 250 (average epoch stats below)
2022-09-08 09:39:04 | INFO | train | epoch 250 | loss 3.395 | nll_loss 0.353 | mask_loss 8.27705 | p_2 0.03527 | mask_ave 0.492 | ppl 1.28 | wps 3465.4 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 20169 | lr 0.000222668 | gnorm 0.274 | train_wall 98 | gb_free 9.2 | wall 34377
2022-09-08 09:39:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:39:04 | INFO | fairseq.trainer | begin training epoch 251
2022-09-08 09:39:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:39:42 | INFO | train_inner | epoch 251:     31 / 81 loss=3.394, nll_loss=0.353, mask_loss=8.29386, p_2=0.03544, mask_ave=0.492, ppl=1.28, wps=3614.5, ups=0.65, wpb=5519.3, bsz=358.6, num_updates=20200, lr=0.000222497, gnorm=0.271, train_wall=121, gb_free=9.1, wall=34415
2022-09-08 09:40:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 09:40:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 09:40:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 09:40:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 09:40:53 | INFO | valid | epoch 251 | valid on 'valid' subset | loss 5.037 | nll_loss 2.402 | mask_loss 9.02922 | p_2 0.04841 | mask_ave 0.618 | ppl 5.28 | bleu 56.55 | wps 1472.6 | wpb 933.5 | bsz 59.6 | num_updates 20250 | best_bleu 57.29
2022-09-08 09:40:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 251 @ 20250 updates
2022-09-08 09:40:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint251.pt
2022-09-08 09:40:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint251.pt
2022-09-08 09:41:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint251.pt (epoch 251 @ 20250 updates, score 56.55) (writing took 19.649668585509062 seconds)
2022-09-08 09:41:13 | INFO | fairseq_cli.train | end of epoch 251 (average epoch stats below)
2022-09-08 09:41:13 | INFO | train | epoch 251 | loss 3.394 | nll_loss 0.353 | mask_loss 8.30109 | p_2 0.03529 | mask_ave 0.491 | ppl 1.28 | wps 3473.3 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 20250 | lr 0.000222222 | gnorm 0.271 | train_wall 96 | gb_free 9.2 | wall 34505
2022-09-08 09:41:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:41:13 | INFO | fairseq.trainer | begin training epoch 252
2022-09-08 09:41:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:42:14 | INFO | train_inner | epoch 252:     50 / 81 loss=3.395, nll_loss=0.354, mask_loss=8.28305, p_2=0.03506, mask_ave=0.492, ppl=1.28, wps=3668.7, ups=0.66, wpb=5557, bsz=359.2, num_updates=20300, lr=0.000221948, gnorm=0.278, train_wall=118, gb_free=9, wall=34567
2022-09-08 09:42:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 09:42:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 09:42:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 09:42:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 09:43:02 | INFO | valid | epoch 252 | valid on 'valid' subset | loss 5.045 | nll_loss 2.41 | mask_loss 9.13909 | p_2 0.04852 | mask_ave 0.615 | ppl 5.31 | bleu 56.02 | wps 1590.9 | wpb 933.5 | bsz 59.6 | num_updates 20331 | best_bleu 57.29
2022-09-08 09:43:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 252 @ 20331 updates
2022-09-08 09:43:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint252.pt
2022-09-08 09:43:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint252.pt
2022-09-08 09:43:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint252.pt (epoch 252 @ 20331 updates, score 56.02) (writing took 20.463826943188906 seconds)
2022-09-08 09:43:22 | INFO | fairseq_cli.train | end of epoch 252 (average epoch stats below)
2022-09-08 09:43:22 | INFO | train | epoch 252 | loss 3.395 | nll_loss 0.354 | mask_loss 8.24407 | p_2 0.0352 | mask_ave 0.494 | ppl 1.28 | wps 3446.4 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 20331 | lr 0.000221779 | gnorm 0.279 | train_wall 97 | gb_free 9.2 | wall 34635
2022-09-08 09:43:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:43:23 | INFO | fairseq.trainer | begin training epoch 253
2022-09-08 09:43:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:44:47 | INFO | train_inner | epoch 253:     69 / 81 loss=3.395, nll_loss=0.353, mask_loss=8.30967, p_2=0.03498, mask_ave=0.491, ppl=1.28, wps=3624.9, ups=0.66, wpb=5534.2, bsz=353.8, num_updates=20400, lr=0.000221404, gnorm=0.275, train_wall=120, gb_free=9.2, wall=34719
2022-09-08 09:44:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 09:45:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 09:45:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 09:45:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 09:45:10 | INFO | valid | epoch 253 | valid on 'valid' subset | loss 5.032 | nll_loss 2.393 | mask_loss 9.35462 | p_2 0.04838 | mask_ave 0.619 | ppl 5.25 | bleu 55.96 | wps 1584.1 | wpb 933.5 | bsz 59.6 | num_updates 20412 | best_bleu 57.29
2022-09-08 09:45:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 253 @ 20412 updates
2022-09-08 09:45:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint253.pt
2022-09-08 09:45:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint253.pt
2022-09-08 09:45:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint253.pt (epoch 253 @ 20412 updates, score 55.96) (writing took 16.13706213235855 seconds)
2022-09-08 09:45:27 | INFO | fairseq_cli.train | end of epoch 253 (average epoch stats below)
2022-09-08 09:45:27 | INFO | train | epoch 253 | loss 3.394 | nll_loss 0.353 | mask_loss 8.30349 | p_2 0.03529 | mask_ave 0.492 | ppl 1.28 | wps 3602 | ups 0.65 | wpb 5523.2 | bsz 358 | num_updates 20412 | lr 0.000221339 | gnorm 0.277 | train_wall 96 | gb_free 9.2 | wall 34759
2022-09-08 09:45:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:45:27 | INFO | fairseq.trainer | begin training epoch 254
2022-09-08 09:45:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:47:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 09:47:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 09:47:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 09:47:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 09:47:18 | INFO | valid | epoch 254 | valid on 'valid' subset | loss 5.028 | nll_loss 2.388 | mask_loss 9.32964 | p_2 0.04865 | mask_ave 0.612 | ppl 5.23 | bleu 56.31 | wps 1552.5 | wpb 933.5 | bsz 59.6 | num_updates 20493 | best_bleu 57.29
2022-09-08 09:47:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 254 @ 20493 updates
2022-09-08 09:47:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint254.pt
2022-09-08 09:47:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint254.pt
2022-09-08 09:47:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint254.pt (epoch 254 @ 20493 updates, score 56.31) (writing took 18.120613876730204 seconds)
2022-09-08 09:47:36 | INFO | fairseq_cli.train | end of epoch 254 (average epoch stats below)
2022-09-08 09:47:36 | INFO | train | epoch 254 | loss 3.394 | nll_loss 0.353 | mask_loss 8.33505 | p_2 0.03522 | mask_ave 0.494 | ppl 1.28 | wps 3448.1 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 20493 | lr 0.000220901 | gnorm 0.265 | train_wall 99 | gb_free 9.1 | wall 34889
2022-09-08 09:47:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:47:36 | INFO | fairseq.trainer | begin training epoch 255
2022-09-08 09:47:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:47:45 | INFO | train_inner | epoch 255:      7 / 81 loss=3.394, nll_loss=0.353, mask_loss=8.34766, p_2=0.03532, mask_ave=0.494, ppl=1.28, wps=3072.4, ups=0.56, wpb=5491, bsz=356.6, num_updates=20500, lr=0.000220863, gnorm=0.267, train_wall=120, gb_free=9, wall=34898
2022-09-08 09:49:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 09:49:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 09:49:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 09:49:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 09:49:26 | INFO | valid | epoch 255 | valid on 'valid' subset | loss 5.021 | nll_loss 2.382 | mask_loss 9.23354 | p_2 0.04871 | mask_ave 0.609 | ppl 5.21 | bleu 56.81 | wps 1604.7 | wpb 933.5 | bsz 59.6 | num_updates 20574 | best_bleu 57.29
2022-09-08 09:49:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 255 @ 20574 updates
2022-09-08 09:49:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint255.pt
2022-09-08 09:49:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint255.pt
2022-09-08 09:49:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint255.pt (epoch 255 @ 20574 updates, score 56.81) (writing took 22.746725622564554 seconds)
2022-09-08 09:49:49 | INFO | fairseq_cli.train | end of epoch 255 (average epoch stats below)
2022-09-08 09:49:49 | INFO | train | epoch 255 | loss 3.394 | nll_loss 0.352 | mask_loss 8.38857 | p_2 0.0352 | mask_ave 0.494 | ppl 1.28 | wps 3379 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 20574 | lr 0.000220465 | gnorm 0.262 | train_wall 98 | gb_free 9.1 | wall 35022
2022-09-08 09:49:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:49:49 | INFO | fairseq.trainer | begin training epoch 256
2022-09-08 09:49:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:50:20 | INFO | train_inner | epoch 256:     26 / 81 loss=3.394, nll_loss=0.352, mask_loss=8.33956, p_2=0.03545, mask_ave=0.494, ppl=1.28, wps=3555.2, ups=0.64, wpb=5516.4, bsz=359.3, num_updates=20600, lr=0.000220326, gnorm=0.263, train_wall=120, gb_free=9, wall=35053
2022-09-08 09:51:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 09:51:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 09:51:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 09:51:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 09:51:38 | INFO | valid | epoch 256 | valid on 'valid' subset | loss 5.03 | nll_loss 2.395 | mask_loss 9.25726 | p_2 0.04847 | mask_ave 0.617 | ppl 5.26 | bleu 56.17 | wps 1587.1 | wpb 933.5 | bsz 59.6 | num_updates 20655 | best_bleu 57.29
2022-09-08 09:51:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 256 @ 20655 updates
2022-09-08 09:51:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint256.pt
2022-09-08 09:51:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint256.pt
2022-09-08 09:51:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint256.pt (epoch 256 @ 20655 updates, score 56.17) (writing took 21.13569774851203 seconds)
2022-09-08 09:51:59 | INFO | fairseq_cli.train | end of epoch 256 (average epoch stats below)
2022-09-08 09:51:59 | INFO | train | epoch 256 | loss 3.394 | nll_loss 0.353 | mask_loss 8.35035 | p_2 0.03535 | mask_ave 0.49 | ppl 1.28 | wps 3434.5 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 20655 | lr 0.000220033 | gnorm 0.281 | train_wall 96 | gb_free 9.1 | wall 35152
2022-09-08 09:51:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:51:59 | INFO | fairseq.trainer | begin training epoch 257
2022-09-08 09:51:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:52:55 | INFO | train_inner | epoch 257:     45 / 81 loss=3.394, nll_loss=0.353, mask_loss=8.38082, p_2=0.03531, mask_ave=0.492, ppl=1.28, wps=3579.8, ups=0.65, wpb=5535.3, bsz=363, num_updates=20700, lr=0.000219793, gnorm=0.283, train_wall=121, gb_free=9, wall=35208
2022-09-08 09:53:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 09:53:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 09:53:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 09:53:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 09:53:50 | INFO | valid | epoch 257 | valid on 'valid' subset | loss 5.019 | nll_loss 2.38 | mask_loss 9.07484 | p_2 0.04838 | mask_ave 0.62 | ppl 5.2 | bleu 56.76 | wps 1533.9 | wpb 933.5 | bsz 59.6 | num_updates 20736 | best_bleu 57.29
2022-09-08 09:53:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 257 @ 20736 updates
2022-09-08 09:53:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint257.pt
2022-09-08 09:53:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint257.pt
2022-09-08 09:54:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint257.pt (epoch 257 @ 20736 updates, score 56.76) (writing took 21.563094325363636 seconds)
2022-09-08 09:54:11 | INFO | fairseq_cli.train | end of epoch 257 (average epoch stats below)
2022-09-08 09:54:12 | INFO | train | epoch 257 | loss 3.394 | nll_loss 0.352 | mask_loss 8.48004 | p_2 0.03514 | mask_ave 0.496 | ppl 1.28 | wps 3375.7 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 20736 | lr 0.000219603 | gnorm 0.278 | train_wall 98 | gb_free 9.2 | wall 35284
2022-09-08 09:54:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:54:12 | INFO | fairseq.trainer | begin training epoch 258
2022-09-08 09:54:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:55:31 | INFO | train_inner | epoch 258:     64 / 81 loss=3.394, nll_loss=0.353, mask_loss=8.38122, p_2=0.03521, mask_ave=0.495, ppl=1.28, wps=3539.5, ups=0.64, wpb=5533.2, bsz=356.2, num_updates=20800, lr=0.000219265, gnorm=0.265, train_wall=122, gb_free=9.1, wall=35364
2022-09-08 09:55:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 09:55:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 09:55:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 09:55:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 09:56:03 | INFO | valid | epoch 258 | valid on 'valid' subset | loss 5.027 | nll_loss 2.381 | mask_loss 9.13264 | p_2 0.04847 | mask_ave 0.617 | ppl 5.21 | bleu 56.63 | wps 1555.8 | wpb 933.5 | bsz 59.6 | num_updates 20817 | best_bleu 57.29
2022-09-08 09:56:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 258 @ 20817 updates
2022-09-08 09:56:03 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint258.pt
2022-09-08 09:56:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint258.pt
2022-09-08 09:56:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint258.pt (epoch 258 @ 20817 updates, score 56.63) (writing took 20.027050748467445 seconds)
2022-09-08 09:56:23 | INFO | fairseq_cli.train | end of epoch 258 (average epoch stats below)
2022-09-08 09:56:23 | INFO | train | epoch 258 | loss 3.393 | nll_loss 0.352 | mask_loss 8.31064 | p_2 0.03523 | mask_ave 0.493 | ppl 1.28 | wps 3397 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 20817 | lr 0.000219175 | gnorm 0.26 | train_wall 99 | gb_free 9.1 | wall 35416
2022-09-08 09:56:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:56:23 | INFO | fairseq.trainer | begin training epoch 259
2022-09-08 09:56:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:58:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 09:58:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 09:58:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 09:58:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 09:58:13 | INFO | valid | epoch 259 | valid on 'valid' subset | loss 5.02 | nll_loss 2.374 | mask_loss 9.24295 | p_2 0.04863 | mask_ave 0.612 | ppl 5.18 | bleu 56.75 | wps 1551.7 | wpb 933.5 | bsz 59.6 | num_updates 20898 | best_bleu 57.29
2022-09-08 09:58:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 259 @ 20898 updates
2022-09-08 09:58:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint259.pt
2022-09-08 09:58:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint259.pt
2022-09-08 09:58:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint259.pt (epoch 259 @ 20898 updates, score 56.75) (writing took 19.055422566831112 seconds)
2022-09-08 09:58:32 | INFO | fairseq_cli.train | end of epoch 259 (average epoch stats below)
2022-09-08 09:58:32 | INFO | train | epoch 259 | loss 3.393 | nll_loss 0.352 | mask_loss 8.33863 | p_2 0.03524 | mask_ave 0.493 | ppl 1.28 | wps 3472.6 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 20898 | lr 0.00021875 | gnorm 0.258 | train_wall 97 | gb_free 8.9 | wall 35545
2022-09-08 09:58:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 09:58:32 | INFO | fairseq.trainer | begin training epoch 260
2022-09-08 09:58:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 09:58:36 | INFO | train_inner | epoch 260:      2 / 81 loss=3.393, nll_loss=0.352, mask_loss=8.37226, p_2=0.03499, mask_ave=0.493, ppl=1.28, wps=2986.5, ups=0.54, wpb=5499.8, bsz=354.4, num_updates=20900, lr=0.000218739, gnorm=0.262, train_wall=120, gb_free=9, wall=35548
2022-09-08 10:00:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:00:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:00:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:00:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:00:21 | INFO | valid | epoch 260 | valid on 'valid' subset | loss 5.039 | nll_loss 2.406 | mask_loss 9.18572 | p_2 0.04835 | mask_ave 0.62 | ppl 5.3 | bleu 56.18 | wps 1580.9 | wpb 933.5 | bsz 59.6 | num_updates 20979 | best_bleu 57.29
2022-09-08 10:00:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 260 @ 20979 updates
2022-09-08 10:00:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint260.pt
2022-09-08 10:00:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint260.pt
2022-09-08 10:00:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint260.pt (epoch 260 @ 20979 updates, score 56.18) (writing took 19.26337133347988 seconds)
2022-09-08 10:00:41 | INFO | fairseq_cli.train | end of epoch 260 (average epoch stats below)
2022-09-08 10:00:41 | INFO | train | epoch 260 | loss 3.393 | nll_loss 0.352 | mask_loss 8.36384 | p_2 0.03513 | mask_ave 0.497 | ppl 1.28 | wps 3476.6 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 20979 | lr 0.000218327 | gnorm 0.281 | train_wall 97 | gb_free 9.3 | wall 35674
2022-09-08 10:00:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:00:41 | INFO | fairseq.trainer | begin training epoch 261
2022-09-08 10:00:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:01:08 | INFO | train_inner | epoch 261:     21 / 81 loss=3.393, nll_loss=0.352, mask_loss=8.35397, p_2=0.03527, mask_ave=0.498, ppl=1.28, wps=3634.3, ups=0.66, wpb=5529.7, bsz=358.6, num_updates=21000, lr=0.000218218, gnorm=0.278, train_wall=120, gb_free=9.1, wall=35701
2022-09-08 10:02:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:02:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:02:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:02:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:02:32 | INFO | valid | epoch 261 | valid on 'valid' subset | loss 5.034 | nll_loss 2.4 | mask_loss 9.27564 | p_2 0.04836 | mask_ave 0.62 | ppl 5.28 | bleu 56.26 | wps 1627.5 | wpb 933.5 | bsz 59.6 | num_updates 21060 | best_bleu 57.29
2022-09-08 10:02:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 261 @ 21060 updates
2022-09-08 10:02:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint261.pt
2022-09-08 10:02:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint261.pt
2022-09-08 10:02:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint261.pt (epoch 261 @ 21060 updates, score 56.26) (writing took 20.177959505468607 seconds)
2022-09-08 10:02:52 | INFO | fairseq_cli.train | end of epoch 261 (average epoch stats below)
2022-09-08 10:02:52 | INFO | train | epoch 261 | loss 3.393 | nll_loss 0.352 | mask_loss 8.35399 | p_2 0.03514 | mask_ave 0.496 | ppl 1.28 | wps 3407.4 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 21060 | lr 0.000217907 | gnorm 0.263 | train_wall 99 | gb_free 9 | wall 35805
2022-09-08 10:02:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:02:52 | INFO | fairseq.trainer | begin training epoch 262
2022-09-08 10:02:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:03:41 | INFO | train_inner | epoch 262:     40 / 81 loss=3.393, nll_loss=0.352, mask_loss=8.39044, p_2=0.03484, mask_ave=0.495, ppl=1.28, wps=3591.7, ups=0.65, wpb=5522.2, bsz=355.9, num_updates=21100, lr=0.0002177, gnorm=0.266, train_wall=121, gb_free=9.1, wall=35854
2022-09-08 10:04:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:04:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:04:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:04:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:04:42 | INFO | valid | epoch 262 | valid on 'valid' subset | loss 5.038 | nll_loss 2.399 | mask_loss 9.07847 | p_2 0.04834 | mask_ave 0.621 | ppl 5.28 | bleu 56.66 | wps 1557.5 | wpb 933.5 | bsz 59.6 | num_updates 21141 | best_bleu 57.29
2022-09-08 10:04:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 262 @ 21141 updates
2022-09-08 10:04:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint262.pt
2022-09-08 10:04:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint262.pt
2022-09-08 10:04:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint262.pt (epoch 262 @ 21141 updates, score 56.66) (writing took 17.548859179019928 seconds)
2022-09-08 10:04:59 | INFO | fairseq_cli.train | end of epoch 262 (average epoch stats below)
2022-09-08 10:04:59 | INFO | train | epoch 262 | loss 3.393 | nll_loss 0.352 | mask_loss 8.32861 | p_2 0.03513 | mask_ave 0.496 | ppl 1.28 | wps 3509.9 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 21141 | lr 0.000217489 | gnorm 0.271 | train_wall 97 | gb_free 9.1 | wall 35932
2022-09-08 10:05:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:05:00 | INFO | fairseq.trainer | begin training epoch 263
2022-09-08 10:05:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:06:12 | INFO | train_inner | epoch 263:     59 / 81 loss=3.393, nll_loss=0.352, mask_loss=8.3542, p_2=0.03502, mask_ave=0.492, ppl=1.28, wps=3701.3, ups=0.66, wpb=5580.4, bsz=362.2, num_updates=21200, lr=0.000217186, gnorm=0.268, train_wall=120, gb_free=9.1, wall=36005
2022-09-08 10:06:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:06:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:06:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:06:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:06:49 | INFO | valid | epoch 263 | valid on 'valid' subset | loss 5.021 | nll_loss 2.384 | mask_loss 9.21126 | p_2 0.04871 | mask_ave 0.61 | ppl 5.22 | bleu 56.84 | wps 1533.5 | wpb 933.5 | bsz 59.6 | num_updates 21222 | best_bleu 57.29
2022-09-08 10:06:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 263 @ 21222 updates
2022-09-08 10:06:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint263.pt
2022-09-08 10:06:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint263.pt
2022-09-08 10:07:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint263.pt (epoch 263 @ 21222 updates, score 56.84) (writing took 18.933999188244343 seconds)
2022-09-08 10:07:08 | INFO | fairseq_cli.train | end of epoch 263 (average epoch stats below)
2022-09-08 10:07:08 | INFO | train | epoch 263 | loss 3.393 | nll_loss 0.352 | mask_loss 8.40447 | p_2 0.03525 | mask_ave 0.493 | ppl 1.28 | wps 3469.6 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 21222 | lr 0.000217074 | gnorm 0.273 | train_wall 97 | gb_free 9.2 | wall 36061
2022-09-08 10:07:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:07:09 | INFO | fairseq.trainer | begin training epoch 264
2022-09-08 10:07:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:08:44 | INFO | train_inner | epoch 264:     78 / 81 loss=3.393, nll_loss=0.352, mask_loss=8.34644, p_2=0.03565, mask_ave=0.494, ppl=1.28, wps=3619.1, ups=0.66, wpb=5487.4, bsz=356.5, num_updates=21300, lr=0.000216676, gnorm=0.275, train_wall=120, gb_free=9, wall=36157
2022-09-08 10:08:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:08:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:08:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:08:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:08:58 | INFO | valid | epoch 264 | valid on 'valid' subset | loss 5.039 | nll_loss 2.404 | mask_loss 9.39976 | p_2 0.04837 | mask_ave 0.621 | ppl 5.29 | bleu 56.67 | wps 1570.3 | wpb 933.5 | bsz 59.6 | num_updates 21303 | best_bleu 57.29
2022-09-08 10:08:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 264 @ 21303 updates
2022-09-08 10:08:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint264.pt
2022-09-08 10:08:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint264.pt
2022-09-08 10:09:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint264.pt (epoch 264 @ 21303 updates, score 56.67) (writing took 2.4290247559547424 seconds)
2022-09-08 10:09:00 | INFO | fairseq_cli.train | end of epoch 264 (average epoch stats below)
2022-09-08 10:09:00 | INFO | train | epoch 264 | loss 3.392 | nll_loss 0.352 | mask_loss 8.36231 | p_2 0.03527 | mask_ave 0.492 | ppl 1.28 | wps 4004.3 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 21303 | lr 0.00021666 | gnorm 0.269 | train_wall 97 | gb_free 9.2 | wall 36173
2022-09-08 10:09:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:09:00 | INFO | fairseq.trainer | begin training epoch 265
2022-09-08 10:09:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:10:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:10:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:10:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:10:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:10:50 | INFO | valid | epoch 265 | valid on 'valid' subset | loss 5.04 | nll_loss 2.403 | mask_loss 9.26981 | p_2 0.04848 | mask_ave 0.617 | ppl 5.29 | bleu 56.35 | wps 1609.6 | wpb 933.5 | bsz 59.6 | num_updates 21384 | best_bleu 57.29
2022-09-08 10:10:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 265 @ 21384 updates
2022-09-08 10:10:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint265.pt
2022-09-08 10:10:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint265.pt
2022-09-08 10:11:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint265.pt (epoch 265 @ 21384 updates, score 56.35) (writing took 21.074721600860357 seconds)
2022-09-08 10:11:12 | INFO | fairseq_cli.train | end of epoch 265 (average epoch stats below)
2022-09-08 10:11:12 | INFO | train | epoch 265 | loss 3.393 | nll_loss 0.352 | mask_loss 8.37224 | p_2 0.03526 | mask_ave 0.493 | ppl 1.28 | wps 3405.1 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 21384 | lr 0.00021625 | gnorm 0.27 | train_wall 98 | gb_free 9.1 | wall 36304
2022-09-08 10:11:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:11:12 | INFO | fairseq.trainer | begin training epoch 266
2022-09-08 10:11:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:11:32 | INFO | train_inner | epoch 266:     16 / 81 loss=3.393, nll_loss=0.352, mask_loss=8.38223, p_2=0.03521, mask_ave=0.493, ppl=1.28, wps=3270.5, ups=0.6, wpb=5490.1, bsz=355.7, num_updates=21400, lr=0.000216169, gnorm=0.268, train_wall=120, gb_free=9, wall=36325
2022-09-08 10:12:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:12:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:12:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:12:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:13:02 | INFO | valid | epoch 266 | valid on 'valid' subset | loss 5.03 | nll_loss 2.391 | mask_loss 9.36504 | p_2 0.04865 | mask_ave 0.612 | ppl 5.25 | bleu 56.57 | wps 1634.4 | wpb 933.5 | bsz 59.6 | num_updates 21465 | best_bleu 57.29
2022-09-08 10:13:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 266 @ 21465 updates
2022-09-08 10:13:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint266.pt
2022-09-08 10:13:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint266.pt
2022-09-08 10:13:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint266.pt (epoch 266 @ 21465 updates, score 56.57) (writing took 18.715112142264843 seconds)
2022-09-08 10:13:21 | INFO | fairseq_cli.train | end of epoch 266 (average epoch stats below)
2022-09-08 10:13:21 | INFO | train | epoch 266 | loss 3.392 | nll_loss 0.351 | mask_loss 8.41513 | p_2 0.03529 | mask_ave 0.492 | ppl 1.28 | wps 3456.3 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 21465 | lr 0.000215841 | gnorm 0.258 | train_wall 98 | gb_free 9.2 | wall 36434
2022-09-08 10:13:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:13:21 | INFO | fairseq.trainer | begin training epoch 267
2022-09-08 10:13:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:14:04 | INFO | train_inner | epoch 267:     35 / 81 loss=3.392, nll_loss=0.351, mask_loss=8.37859, p_2=0.03544, mask_ave=0.491, ppl=1.28, wps=3632.6, ups=0.66, wpb=5529.1, bsz=360.8, num_updates=21500, lr=0.000215666, gnorm=0.264, train_wall=121, gb_free=9.1, wall=36477
2022-09-08 10:14:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:15:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:15:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:15:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:15:10 | INFO | valid | epoch 267 | valid on 'valid' subset | loss 5.038 | nll_loss 2.403 | mask_loss 9.40624 | p_2 0.04885 | mask_ave 0.605 | ppl 5.29 | bleu 56.14 | wps 1619.5 | wpb 933.5 | bsz 59.6 | num_updates 21546 | best_bleu 57.29
2022-09-08 10:15:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 267 @ 21546 updates
2022-09-08 10:15:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint267.pt
2022-09-08 10:15:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint267.pt
2022-09-08 10:15:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint267.pt (epoch 267 @ 21546 updates, score 56.14) (writing took 17.836466550827026 seconds)
2022-09-08 10:15:28 | INFO | fairseq_cli.train | end of epoch 267 (average epoch stats below)
2022-09-08 10:15:28 | INFO | train | epoch 267 | loss 3.393 | nll_loss 0.352 | mask_loss 8.40551 | p_2 0.03521 | mask_ave 0.494 | ppl 1.28 | wps 3529.4 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 21546 | lr 0.000215435 | gnorm 0.275 | train_wall 97 | gb_free 9.2 | wall 36561
2022-09-08 10:15:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:15:28 | INFO | fairseq.trainer | begin training epoch 268
2022-09-08 10:15:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:16:34 | INFO | train_inner | epoch 268:     54 / 81 loss=3.393, nll_loss=0.352, mask_loss=8.47424, p_2=0.035, mask_ave=0.494, ppl=1.28, wps=3698.8, ups=0.67, wpb=5536.5, bsz=353.9, num_updates=21600, lr=0.000215166, gnorm=0.265, train_wall=119, gb_free=9.1, wall=36626
2022-09-08 10:17:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:17:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:17:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:17:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:17:17 | INFO | valid | epoch 268 | valid on 'valid' subset | loss 5.03 | nll_loss 2.396 | mask_loss 9.27529 | p_2 0.0489 | mask_ave 0.604 | ppl 5.26 | bleu 56.25 | wps 1567.7 | wpb 933.5 | bsz 59.6 | num_updates 21627 | best_bleu 57.29
2022-09-08 10:17:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 268 @ 21627 updates
2022-09-08 10:17:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint268.pt
2022-09-08 10:17:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint268.pt
2022-09-08 10:17:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint268.pt (epoch 268 @ 21627 updates, score 56.25) (writing took 21.649056401103735 seconds)
2022-09-08 10:17:39 | INFO | fairseq_cli.train | end of epoch 268 (average epoch stats below)
2022-09-08 10:17:39 | INFO | train | epoch 268 | loss 3.392 | nll_loss 0.351 | mask_loss 8.42512 | p_2 0.03529 | mask_ave 0.492 | ppl 1.28 | wps 3412.9 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 21627 | lr 0.000215031 | gnorm 0.261 | train_wall 97 | gb_free 9.1 | wall 36692
2022-09-08 10:17:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:17:39 | INFO | fairseq.trainer | begin training epoch 269
2022-09-08 10:17:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:19:08 | INFO | train_inner | epoch 269:     73 / 81 loss=3.391, nll_loss=0.351, mask_loss=8.37133, p_2=0.03533, mask_ave=0.494, ppl=1.28, wps=3580.6, ups=0.65, wpb=5537.5, bsz=362.3, num_updates=21700, lr=0.000214669, gnorm=0.261, train_wall=120, gb_free=9, wall=36781
2022-09-08 10:19:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:19:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:19:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:19:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:19:28 | INFO | valid | epoch 269 | valid on 'valid' subset | loss 5.029 | nll_loss 2.396 | mask_loss 9.27993 | p_2 0.04853 | mask_ave 0.615 | ppl 5.26 | bleu 56.2 | wps 1531.4 | wpb 933.5 | bsz 59.6 | num_updates 21708 | best_bleu 57.29
2022-09-08 10:19:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 269 @ 21708 updates
2022-09-08 10:19:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint269.pt
2022-09-08 10:19:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint269.pt
2022-09-08 10:19:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint269.pt (epoch 269 @ 21708 updates, score 56.2) (writing took 25.66777803003788 seconds)
2022-09-08 10:19:54 | INFO | fairseq_cli.train | end of epoch 269 (average epoch stats below)
2022-09-08 10:19:54 | INFO | train | epoch 269 | loss 3.392 | nll_loss 0.351 | mask_loss 8.396 | p_2 0.03515 | mask_ave 0.496 | ppl 1.28 | wps 3307.7 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 21708 | lr 0.00021463 | gnorm 0.258 | train_wall 97 | gb_free 9 | wall 36827
2022-09-08 10:19:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:19:54 | INFO | fairseq.trainer | begin training epoch 270
2022-09-08 10:19:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:21:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:21:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:21:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:21:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:21:44 | INFO | valid | epoch 270 | valid on 'valid' subset | loss 5.038 | nll_loss 2.402 | mask_loss 9.20626 | p_2 0.0487 | mask_ave 0.611 | ppl 5.28 | bleu 56.54 | wps 1507.4 | wpb 933.5 | bsz 59.6 | num_updates 21789 | best_bleu 57.29
2022-09-08 10:21:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 270 @ 21789 updates
2022-09-08 10:21:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint270.pt
2022-09-08 10:21:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint270.pt
2022-09-08 10:22:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint270.pt (epoch 270 @ 21789 updates, score 56.54) (writing took 20.60101268067956 seconds)
2022-09-08 10:22:05 | INFO | fairseq_cli.train | end of epoch 270 (average epoch stats below)
2022-09-08 10:22:05 | INFO | train | epoch 270 | loss 3.391 | nll_loss 0.35 | mask_loss 8.45743 | p_2 0.0352 | mask_ave 0.494 | ppl 1.27 | wps 3424.7 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 21789 | lr 0.000214231 | gnorm 0.263 | train_wall 97 | gb_free 9 | wall 36958
2022-09-08 10:22:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:22:05 | INFO | fairseq.trainer | begin training epoch 271
2022-09-08 10:22:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:22:19 | INFO | train_inner | epoch 271:     11 / 81 loss=3.391, nll_loss=0.35, mask_loss=8.44711, p_2=0.03533, mask_ave=0.496, ppl=1.27, wps=2880.5, ups=0.53, wpb=5483.4, bsz=356.2, num_updates=21800, lr=0.000214176, gnorm=0.264, train_wall=119, gb_free=9.1, wall=36972
2022-09-08 10:23:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:23:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:23:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:23:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:23:55 | INFO | valid | epoch 271 | valid on 'valid' subset | loss 5.032 | nll_loss 2.39 | mask_loss 9.22803 | p_2 0.04854 | mask_ave 0.615 | ppl 5.24 | bleu 56.23 | wps 1538.4 | wpb 933.5 | bsz 59.6 | num_updates 21870 | best_bleu 57.29
2022-09-08 10:23:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 271 @ 21870 updates
2022-09-08 10:23:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint271.pt
2022-09-08 10:23:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint271.pt
2022-09-08 10:24:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint271.pt (epoch 271 @ 21870 updates, score 56.23) (writing took 20.244579553604126 seconds)
2022-09-08 10:24:15 | INFO | fairseq_cli.train | end of epoch 271 (average epoch stats below)
2022-09-08 10:24:15 | INFO | train | epoch 271 | loss 3.392 | nll_loss 0.351 | mask_loss 8.35647 | p_2 0.03514 | mask_ave 0.496 | ppl 1.28 | wps 3435.2 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 21870 | lr 0.000213833 | gnorm 0.277 | train_wall 97 | gb_free 9.2 | wall 37088
2022-09-08 10:24:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:24:15 | INFO | fairseq.trainer | begin training epoch 272
2022-09-08 10:24:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:24:52 | INFO | train_inner | epoch 272:     30 / 81 loss=3.392, nll_loss=0.351, mask_loss=8.38666, p_2=0.03479, mask_ave=0.495, ppl=1.28, wps=3619, ups=0.65, wpb=5558.4, bsz=357.8, num_updates=21900, lr=0.000213687, gnorm=0.272, train_wall=121, gb_free=9, wall=37125
2022-09-08 10:25:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:25:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:25:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:25:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:26:06 | INFO | valid | epoch 272 | valid on 'valid' subset | loss 5.035 | nll_loss 2.398 | mask_loss 9.25673 | p_2 0.0484 | mask_ave 0.62 | ppl 5.27 | bleu 56.54 | wps 1535.1 | wpb 933.5 | bsz 59.6 | num_updates 21951 | best_bleu 57.29
2022-09-08 10:26:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 272 @ 21951 updates
2022-09-08 10:26:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint272.pt
2022-09-08 10:26:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint272.pt
2022-09-08 10:26:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint272.pt (epoch 272 @ 21951 updates, score 56.54) (writing took 17.329923804849386 seconds)
2022-09-08 10:26:23 | INFO | fairseq_cli.train | end of epoch 272 (average epoch stats below)
2022-09-08 10:26:23 | INFO | train | epoch 272 | loss 3.391 | nll_loss 0.351 | mask_loss 8.41495 | p_2 0.03507 | mask_ave 0.498 | ppl 1.28 | wps 3489.1 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 21951 | lr 0.000213439 | gnorm 0.273 | train_wall 98 | gb_free 9.1 | wall 37216
2022-09-08 10:26:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:26:23 | INFO | fairseq.trainer | begin training epoch 273
2022-09-08 10:26:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:27:25 | INFO | train_inner | epoch 273:     49 / 81 loss=3.392, nll_loss=0.351, mask_loss=8.38135, p_2=0.03536, mask_ave=0.498, ppl=1.28, wps=3594.4, ups=0.65, wpb=5500.6, bsz=357.7, num_updates=22000, lr=0.000213201, gnorm=0.275, train_wall=123, gb_free=9.2, wall=37278
2022-09-08 10:28:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:28:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:28:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:28:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:28:17 | INFO | valid | epoch 273 | valid on 'valid' subset | loss 5.029 | nll_loss 2.395 | mask_loss 9.14537 | p_2 0.0486 | mask_ave 0.613 | ppl 5.26 | bleu 55.92 | wps 1438 | wpb 933.5 | bsz 59.6 | num_updates 22032 | best_bleu 57.29
2022-09-08 10:28:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 273 @ 22032 updates
2022-09-08 10:28:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint273.pt
2022-09-08 10:28:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint273.pt
2022-09-08 10:28:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint273.pt (epoch 273 @ 22032 updates, score 55.92) (writing took 17.93564221262932 seconds)
2022-09-08 10:28:35 | INFO | fairseq_cli.train | end of epoch 273 (average epoch stats below)
2022-09-08 10:28:35 | INFO | train | epoch 273 | loss 3.392 | nll_loss 0.351 | mask_loss 8.34572 | p_2 0.03529 | mask_ave 0.492 | ppl 1.28 | wps 3396.3 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 22032 | lr 0.000213046 | gnorm 0.268 | train_wall 100 | gb_free 9.1 | wall 37348
2022-09-08 10:28:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:28:35 | INFO | fairseq.trainer | begin training epoch 274
2022-09-08 10:28:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:30:00 | INFO | train_inner | epoch 274:     68 / 81 loss=3.391, nll_loss=0.351, mask_loss=8.33808, p_2=0.03514, mask_ave=0.493, ppl=1.28, wps=3581.4, ups=0.64, wpb=5558.4, bsz=363, num_updates=22100, lr=0.000212718, gnorm=0.262, train_wall=124, gb_free=9.1, wall=37433
2022-09-08 10:30:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:30:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:30:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:30:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:30:27 | INFO | valid | epoch 274 | valid on 'valid' subset | loss 5.032 | nll_loss 2.397 | mask_loss 9.32236 | p_2 0.04801 | mask_ave 0.631 | ppl 5.27 | bleu 56.38 | wps 1507.8 | wpb 933.5 | bsz 59.6 | num_updates 22113 | best_bleu 57.29
2022-09-08 10:30:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 274 @ 22113 updates
2022-09-08 10:30:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint274.pt
2022-09-08 10:30:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint274.pt
2022-09-08 10:30:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint274.pt (epoch 274 @ 22113 updates, score 56.38) (writing took 16.344739984720945 seconds)
2022-09-08 10:30:44 | INFO | fairseq_cli.train | end of epoch 274 (average epoch stats below)
2022-09-08 10:30:44 | INFO | train | epoch 274 | loss 3.391 | nll_loss 0.351 | mask_loss 8.36458 | p_2 0.03514 | mask_ave 0.496 | ppl 1.28 | wps 3477.3 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 22113 | lr 0.000212655 | gnorm 0.262 | train_wall 99 | gb_free 9 | wall 37476
2022-09-08 10:30:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:30:44 | INFO | fairseq.trainer | begin training epoch 275
2022-09-08 10:30:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:32:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:32:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:32:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:32:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:32:34 | INFO | valid | epoch 275 | valid on 'valid' subset | loss 5.039 | nll_loss 2.408 | mask_loss 9.20129 | p_2 0.04844 | mask_ave 0.619 | ppl 5.31 | bleu 56.44 | wps 1513.5 | wpb 933.5 | bsz 59.6 | num_updates 22194 | best_bleu 57.29
2022-09-08 10:32:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 275 @ 22194 updates
2022-09-08 10:32:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint275.pt
2022-09-08 10:32:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint275.pt
2022-09-08 10:32:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint275.pt (epoch 275 @ 22194 updates, score 56.44) (writing took 23.272484589368105 seconds)
2022-09-08 10:32:57 | INFO | fairseq_cli.train | end of epoch 275 (average epoch stats below)
2022-09-08 10:32:57 | INFO | train | epoch 275 | loss 3.391 | nll_loss 0.351 | mask_loss 8.39588 | p_2 0.03512 | mask_ave 0.497 | ppl 1.28 | wps 3353.1 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 22194 | lr 0.000212267 | gnorm 0.267 | train_wall 97 | gb_free 9.1 | wall 37610
2022-09-08 10:32:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:32:57 | INFO | fairseq.trainer | begin training epoch 276
2022-09-08 10:32:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:33:05 | INFO | train_inner | epoch 276:      6 / 81 loss=3.391, nll_loss=0.351, mask_loss=8.40441, p_2=0.03504, mask_ave=0.497, ppl=1.28, wps=2977.9, ups=0.54, wpb=5498.7, bsz=354.1, num_updates=22200, lr=0.000212238, gnorm=0.268, train_wall=119, gb_free=9.1, wall=37618
2022-09-08 10:34:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:34:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:34:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:34:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:34:46 | INFO | valid | epoch 276 | valid on 'valid' subset | loss 5.041 | nll_loss 2.406 | mask_loss 9.28452 | p_2 0.04816 | mask_ave 0.628 | ppl 5.3 | bleu 55.94 | wps 1535.2 | wpb 933.5 | bsz 59.6 | num_updates 22275 | best_bleu 57.29
2022-09-08 10:34:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 276 @ 22275 updates
2022-09-08 10:34:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint276.pt
2022-09-08 10:34:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint276.pt
2022-09-08 10:35:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint276.pt (epoch 276 @ 22275 updates, score 55.94) (writing took 23.297022435814142 seconds)
2022-09-08 10:35:10 | INFO | fairseq_cli.train | end of epoch 276 (average epoch stats below)
2022-09-08 10:35:10 | INFO | train | epoch 276 | loss 3.391 | nll_loss 0.35 | mask_loss 8.33535 | p_2 0.03516 | mask_ave 0.496 | ppl 1.27 | wps 3368.7 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 22275 | lr 0.000211881 | gnorm 0.256 | train_wall 97 | gb_free 9.1 | wall 37743
2022-09-08 10:35:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:35:10 | INFO | fairseq.trainer | begin training epoch 277
2022-09-08 10:35:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:35:41 | INFO | train_inner | epoch 277:     25 / 81 loss=3.39, nll_loss=0.35, mask_loss=8.3436, p_2=0.0354, mask_ave=0.499, ppl=1.27, wps=3547.4, ups=0.64, wpb=5513.7, bsz=361.4, num_updates=22300, lr=0.000211762, gnorm=0.252, train_wall=119, gb_free=9, wall=37773
2022-09-08 10:36:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:36:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:36:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:36:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:36:59 | INFO | valid | epoch 277 | valid on 'valid' subset | loss 5.031 | nll_loss 2.391 | mask_loss 9.30064 | p_2 0.0485 | mask_ave 0.618 | ppl 5.25 | bleu 56.19 | wps 1535.6 | wpb 933.5 | bsz 59.6 | num_updates 22356 | best_bleu 57.29
2022-09-08 10:36:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 277 @ 22356 updates
2022-09-08 10:36:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint277.pt
2022-09-08 10:37:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint277.pt
2022-09-08 10:37:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint277.pt (epoch 277 @ 22356 updates, score 56.19) (writing took 19.025297168642282 seconds)
2022-09-08 10:37:18 | INFO | fairseq_cli.train | end of epoch 277 (average epoch stats below)
2022-09-08 10:37:18 | INFO | train | epoch 277 | loss 3.391 | nll_loss 0.351 | mask_loss 8.43372 | p_2 0.03502 | mask_ave 0.5 | ppl 1.28 | wps 3491.4 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 22356 | lr 0.000211496 | gnorm 0.257 | train_wall 96 | gb_free 9.1 | wall 37871
2022-09-08 10:37:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:37:18 | INFO | fairseq.trainer | begin training epoch 278
2022-09-08 10:37:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:38:12 | INFO | train_inner | epoch 278:     44 / 81 loss=3.392, nll_loss=0.351, mask_loss=8.43143, p_2=0.03481, mask_ave=0.496, ppl=1.28, wps=3668.3, ups=0.66, wpb=5554.8, bsz=353.5, num_updates=22400, lr=0.000211289, gnorm=0.263, train_wall=120, gb_free=9.1, wall=37925
2022-09-08 10:38:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:38:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:38:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:38:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:39:07 | INFO | valid | epoch 278 | valid on 'valid' subset | loss 5.052 | nll_loss 2.424 | mask_loss 9.1638 | p_2 0.04838 | mask_ave 0.621 | ppl 5.37 | bleu 55.69 | wps 1498.1 | wpb 933.5 | bsz 59.6 | num_updates 22437 | best_bleu 57.29
2022-09-08 10:39:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 278 @ 22437 updates
2022-09-08 10:39:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint278.pt
2022-09-08 10:39:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint278.pt
2022-09-08 10:39:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint278.pt (epoch 278 @ 22437 updates, score 55.69) (writing took 2.455993816256523 seconds)
2022-09-08 10:39:10 | INFO | fairseq_cli.train | end of epoch 278 (average epoch stats below)
2022-09-08 10:39:10 | INFO | train | epoch 278 | loss 3.39 | nll_loss 0.35 | mask_loss 8.38922 | p_2 0.03519 | mask_ave 0.495 | ppl 1.27 | wps 3997.8 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 22437 | lr 0.000211114 | gnorm 0.256 | train_wall 97 | gb_free 9 | wall 37983
2022-09-08 10:39:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:39:10 | INFO | fairseq.trainer | begin training epoch 279
2022-09-08 10:39:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:40:28 | INFO | train_inner | epoch 279:     63 / 81 loss=3.39, nll_loss=0.35, mask_loss=8.33821, p_2=0.0353, mask_ave=0.496, ppl=1.27, wps=4044.6, ups=0.73, wpb=5515.6, bsz=359.2, num_updates=22500, lr=0.000210819, gnorm=0.268, train_wall=121, gb_free=9.1, wall=38061
2022-09-08 10:40:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:40:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:40:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:40:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:41:00 | INFO | valid | epoch 279 | valid on 'valid' subset | loss 5.028 | nll_loss 2.394 | mask_loss 9.36774 | p_2 0.04834 | mask_ave 0.622 | ppl 5.25 | bleu 56.47 | wps 1494 | wpb 933.5 | bsz 59.6 | num_updates 22518 | best_bleu 57.29
2022-09-08 10:41:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 279 @ 22518 updates
2022-09-08 10:41:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint279.pt
2022-09-08 10:41:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint279.pt
2022-09-08 10:41:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint279.pt (epoch 279 @ 22518 updates, score 56.47) (writing took 18.286795251071453 seconds)
2022-09-08 10:41:19 | INFO | fairseq_cli.train | end of epoch 279 (average epoch stats below)
2022-09-08 10:41:19 | INFO | train | epoch 279 | loss 3.39 | nll_loss 0.351 | mask_loss 8.35114 | p_2 0.0352 | mask_ave 0.495 | ppl 1.28 | wps 3474.2 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 22518 | lr 0.000210734 | gnorm 0.277 | train_wall 97 | gb_free 9.2 | wall 38112
2022-09-08 10:41:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:41:19 | INFO | fairseq.trainer | begin training epoch 280
2022-09-08 10:41:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:42:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:43:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:43:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:43:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:43:10 | INFO | valid | epoch 280 | valid on 'valid' subset | loss 5.044 | nll_loss 2.413 | mask_loss 9.47567 | p_2 0.04849 | mask_ave 0.617 | ppl 5.32 | bleu 56.31 | wps 1539.4 | wpb 933.5 | bsz 59.6 | num_updates 22599 | best_bleu 57.29
2022-09-08 10:43:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 280 @ 22599 updates
2022-09-08 10:43:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint280.pt
2022-09-08 10:43:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint280.pt
2022-09-08 10:43:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint280.pt (epoch 280 @ 22599 updates, score 56.31) (writing took 16.846128962934017 seconds)
2022-09-08 10:43:27 | INFO | fairseq_cli.train | end of epoch 280 (average epoch stats below)
2022-09-08 10:43:27 | INFO | train | epoch 280 | loss 3.391 | nll_loss 0.351 | mask_loss 8.47028 | p_2 0.03519 | mask_ave 0.495 | ppl 1.28 | wps 3491.9 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 22599 | lr 0.000210356 | gnorm 0.293 | train_wall 98 | gb_free 9.1 | wall 38240
2022-09-08 10:43:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:43:27 | INFO | fairseq.trainer | begin training epoch 281
2022-09-08 10:43:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:43:29 | INFO | train_inner | epoch 281:      1 / 81 loss=3.391, nll_loss=0.351, mask_loss=8.46812, p_2=0.0352, mask_ave=0.495, ppl=1.28, wps=3045.4, ups=0.55, wpb=5500.6, bsz=357.1, num_updates=22600, lr=0.000210352, gnorm=0.292, train_wall=120, gb_free=9.2, wall=38242
2022-09-08 10:45:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:45:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:45:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:45:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:45:18 | INFO | valid | epoch 281 | valid on 'valid' subset | loss 5.031 | nll_loss 2.395 | mask_loss 9.40623 | p_2 0.04885 | mask_ave 0.607 | ppl 5.26 | bleu 56.49 | wps 1447.4 | wpb 933.5 | bsz 59.6 | num_updates 22680 | best_bleu 57.29
2022-09-08 10:45:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 281 @ 22680 updates
2022-09-08 10:45:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint281.pt
2022-09-08 10:45:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint281.pt
2022-09-08 10:45:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint281.pt (epoch 281 @ 22680 updates, score 56.49) (writing took 20.11586905643344 seconds)
2022-09-08 10:45:38 | INFO | fairseq_cli.train | end of epoch 281 (average epoch stats below)
2022-09-08 10:45:38 | INFO | train | epoch 281 | loss 3.39 | nll_loss 0.35 | mask_loss 8.3837 | p_2 0.03524 | mask_ave 0.494 | ppl 1.27 | wps 3410.6 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 22680 | lr 0.00020998 | gnorm 0.259 | train_wall 98 | gb_free 9.2 | wall 38371
2022-09-08 10:45:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:45:38 | INFO | fairseq.trainer | begin training epoch 282
2022-09-08 10:45:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:46:03 | INFO | train_inner | epoch 282:     20 / 81 loss=3.39, nll_loss=0.35, mask_loss=8.39531, p_2=0.03516, mask_ave=0.492, ppl=1.27, wps=3594.2, ups=0.65, wpb=5540.7, bsz=358.8, num_updates=22700, lr=0.000209888, gnorm=0.262, train_wall=120, gb_free=8.9, wall=38396
2022-09-08 10:47:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:47:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:47:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:47:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:47:29 | INFO | valid | epoch 282 | valid on 'valid' subset | loss 5.035 | nll_loss 2.406 | mask_loss 9.31019 | p_2 0.04875 | mask_ave 0.61 | ppl 5.3 | bleu 55.74 | wps 1515.1 | wpb 933.5 | bsz 59.6 | num_updates 22761 | best_bleu 57.29
2022-09-08 10:47:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 282 @ 22761 updates
2022-09-08 10:47:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint282.pt
2022-09-08 10:47:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint282.pt
2022-09-08 10:47:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint282.pt (epoch 282 @ 22761 updates, score 55.74) (writing took 21.48905983939767 seconds)
2022-09-08 10:47:50 | INFO | fairseq_cli.train | end of epoch 282 (average epoch stats below)
2022-09-08 10:47:50 | INFO | train | epoch 282 | loss 3.39 | nll_loss 0.349 | mask_loss 8.41625 | p_2 0.03536 | mask_ave 0.49 | ppl 1.27 | wps 3383.5 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 22761 | lr 0.000209606 | gnorm 0.272 | train_wall 97 | gb_free 9.1 | wall 38503
2022-09-08 10:47:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:47:50 | INFO | fairseq.trainer | begin training epoch 283
2022-09-08 10:47:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:48:39 | INFO | train_inner | epoch 283:     39 / 81 loss=3.39, nll_loss=0.35, mask_loss=8.41123, p_2=0.03536, mask_ave=0.492, ppl=1.27, wps=3542, ups=0.64, wpb=5514.9, bsz=357.7, num_updates=22800, lr=0.000209427, gnorm=0.27, train_wall=121, gb_free=9, wall=38552
2022-09-08 10:49:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:49:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:49:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:49:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:49:41 | INFO | valid | epoch 283 | valid on 'valid' subset | loss 5.043 | nll_loss 2.412 | mask_loss 9.41511 | p_2 0.04889 | mask_ave 0.605 | ppl 5.32 | bleu 55.23 | wps 1560.8 | wpb 933.5 | bsz 59.6 | num_updates 22842 | best_bleu 57.29
2022-09-08 10:49:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 283 @ 22842 updates
2022-09-08 10:49:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint283.pt
2022-09-08 10:49:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint283.pt
2022-09-08 10:50:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint283.pt (epoch 283 @ 22842 updates, score 55.23) (writing took 34.18870858848095 seconds)
2022-09-08 10:50:16 | INFO | fairseq_cli.train | end of epoch 283 (average epoch stats below)
2022-09-08 10:50:16 | INFO | train | epoch 283 | loss 3.39 | nll_loss 0.35 | mask_loss 8.42008 | p_2 0.03523 | mask_ave 0.494 | ppl 1.27 | wps 3074.4 | ups 0.56 | wpb 5523.2 | bsz 358 | num_updates 22842 | lr 0.000209234 | gnorm 0.265 | train_wall 98 | gb_free 9 | wall 38649
2022-09-08 10:50:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:50:16 | INFO | fairseq.trainer | begin training epoch 284
2022-09-08 10:50:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:51:27 | INFO | train_inner | epoch 284:     58 / 81 loss=3.39, nll_loss=0.35, mask_loss=8.43789, p_2=0.03516, mask_ave=0.494, ppl=1.27, wps=3308.9, ups=0.6, wpb=5556.1, bsz=360.5, num_updates=22900, lr=0.000208969, gnorm=0.254, train_wall=121, gb_free=9, wall=38720
2022-09-08 10:51:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:51:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:51:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:51:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:52:05 | INFO | valid | epoch 284 | valid on 'valid' subset | loss 5.032 | nll_loss 2.403 | mask_loss 9.53069 | p_2 0.0485 | mask_ave 0.617 | ppl 5.29 | bleu 56.2 | wps 1514.1 | wpb 933.5 | bsz 59.6 | num_updates 22923 | best_bleu 57.29
2022-09-08 10:52:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 284 @ 22923 updates
2022-09-08 10:52:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint284.pt
2022-09-08 10:52:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint284.pt
2022-09-08 10:52:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint284.pt (epoch 284 @ 22923 updates, score 56.2) (writing took 31.123156052082777 seconds)
2022-09-08 10:52:36 | INFO | fairseq_cli.train | end of epoch 284 (average epoch stats below)
2022-09-08 10:52:36 | INFO | train | epoch 284 | loss 3.39 | nll_loss 0.35 | mask_loss 8.46619 | p_2 0.03524 | mask_ave 0.494 | ppl 1.27 | wps 3182.4 | ups 0.58 | wpb 5523.2 | bsz 358 | num_updates 22923 | lr 0.000208864 | gnorm 0.255 | train_wall 97 | gb_free 9.1 | wall 38789
2022-09-08 10:52:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:52:36 | INFO | fairseq.trainer | begin training epoch 285
2022-09-08 10:52:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:54:10 | INFO | train_inner | epoch 285:     77 / 81 loss=3.39, nll_loss=0.351, mask_loss=8.50957, p_2=0.03543, mask_ave=0.494, ppl=1.28, wps=3370.2, ups=0.61, wpb=5512.7, bsz=360.1, num_updates=23000, lr=0.000208514, gnorm=0.275, train_wall=120, gb_free=9.1, wall=38883
2022-09-08 10:54:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:54:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:54:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:54:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:54:25 | INFO | valid | epoch 285 | valid on 'valid' subset | loss 5.032 | nll_loss 2.396 | mask_loss 9.19996 | p_2 0.04836 | mask_ave 0.622 | ppl 5.26 | bleu 55.91 | wps 1543.8 | wpb 933.5 | bsz 59.6 | num_updates 23004 | best_bleu 57.29
2022-09-08 10:54:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 285 @ 23004 updates
2022-09-08 10:54:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint285.pt
2022-09-08 10:54:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint285.pt
2022-09-08 10:54:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint285.pt (epoch 285 @ 23004 updates, score 55.91) (writing took 2.54621684551239 seconds)
2022-09-08 10:54:28 | INFO | fairseq_cli.train | end of epoch 285 (average epoch stats below)
2022-09-08 10:54:28 | INFO | train | epoch 285 | loss 3.39 | nll_loss 0.351 | mask_loss 8.52417 | p_2 0.03522 | mask_ave 0.494 | ppl 1.28 | wps 4000.3 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 23004 | lr 0.000208496 | gnorm 0.279 | train_wall 97 | gb_free 9.2 | wall 38901
2022-09-08 10:54:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:54:28 | INFO | fairseq.trainer | begin training epoch 286
2022-09-08 10:54:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:56:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:56:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:56:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:56:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:56:19 | INFO | valid | epoch 286 | valid on 'valid' subset | loss 5.048 | nll_loss 2.421 | mask_loss 9.24475 | p_2 0.04872 | mask_ave 0.61 | ppl 5.36 | bleu 56.46 | wps 1518.5 | wpb 933.5 | bsz 59.6 | num_updates 23085 | best_bleu 57.29
2022-09-08 10:56:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 286 @ 23085 updates
2022-09-08 10:56:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint286.pt
2022-09-08 10:56:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint286.pt
2022-09-08 10:56:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint286.pt (epoch 286 @ 23085 updates, score 56.46) (writing took 2.689516931772232 seconds)
2022-09-08 10:56:21 | INFO | fairseq_cli.train | end of epoch 286 (average epoch stats below)
2022-09-08 10:56:21 | INFO | train | epoch 286 | loss 3.39 | nll_loss 0.35 | mask_loss 8.44091 | p_2 0.0352 | mask_ave 0.495 | ppl 1.27 | wps 3952.3 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 23085 | lr 0.00020813 | gnorm 0.262 | train_wall 98 | gb_free 9.1 | wall 39014
2022-09-08 10:56:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:56:22 | INFO | fairseq.trainer | begin training epoch 287
2022-09-08 10:56:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:56:40 | INFO | train_inner | epoch 287:     15 / 81 loss=3.39, nll_loss=0.35, mask_loss=8.46603, p_2=0.03487, mask_ave=0.495, ppl=1.27, wps=3662.9, ups=0.67, wpb=5498.9, bsz=349, num_updates=23100, lr=0.000208063, gnorm=0.264, train_wall=119, gb_free=8.9, wall=39033
2022-09-08 10:58:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 10:58:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 10:58:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 10:58:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 10:58:11 | INFO | valid | epoch 287 | valid on 'valid' subset | loss 5.036 | nll_loss 2.404 | mask_loss 9.22719 | p_2 0.04876 | mask_ave 0.609 | ppl 5.29 | bleu 55.61 | wps 1542.9 | wpb 933.5 | bsz 59.6 | num_updates 23166 | best_bleu 57.29
2022-09-08 10:58:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 287 @ 23166 updates
2022-09-08 10:58:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint287.pt
2022-09-08 10:58:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint287.pt
2022-09-08 10:58:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint287.pt (epoch 287 @ 23166 updates, score 55.61) (writing took 18.882089786231518 seconds)
2022-09-08 10:58:30 | INFO | fairseq_cli.train | end of epoch 287 (average epoch stats below)
2022-09-08 10:58:30 | INFO | train | epoch 287 | loss 3.389 | nll_loss 0.35 | mask_loss 8.31865 | p_2 0.03531 | mask_ave 0.491 | ppl 1.27 | wps 3486.2 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 23166 | lr 0.000207766 | gnorm 0.262 | train_wall 96 | gb_free 9.1 | wall 39143
2022-09-08 10:58:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 10:58:30 | INFO | fairseq.trainer | begin training epoch 288
2022-09-08 10:58:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 10:59:11 | INFO | train_inner | epoch 288:     34 / 81 loss=3.389, nll_loss=0.349, mask_loss=8.3272, p_2=0.03532, mask_ave=0.491, ppl=1.27, wps=3679.9, ups=0.66, wpb=5540.4, bsz=360.4, num_updates=23200, lr=0.000207614, gnorm=0.262, train_wall=119, gb_free=9.1, wall=39184
2022-09-08 11:00:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:00:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:00:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:00:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:00:19 | INFO | valid | epoch 288 | valid on 'valid' subset | loss 5.054 | nll_loss 2.427 | mask_loss 9.35137 | p_2 0.04867 | mask_ave 0.612 | ppl 5.38 | bleu 56.05 | wps 1522.5 | wpb 933.5 | bsz 59.6 | num_updates 23247 | best_bleu 57.29
2022-09-08 11:00:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 288 @ 23247 updates
2022-09-08 11:00:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint288.pt
2022-09-08 11:00:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint288.pt
2022-09-08 11:00:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint288.pt (epoch 288 @ 23247 updates, score 56.05) (writing took 20.407226160168648 seconds)
2022-09-08 11:00:40 | INFO | fairseq_cli.train | end of epoch 288 (average epoch stats below)
2022-09-08 11:00:40 | INFO | train | epoch 288 | loss 3.389 | nll_loss 0.349 | mask_loss 8.43162 | p_2 0.03525 | mask_ave 0.493 | ppl 1.27 | wps 3432.6 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 23247 | lr 0.000207404 | gnorm 0.255 | train_wall 97 | gb_free 9 | wall 39273
2022-09-08 11:00:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:00:40 | INFO | fairseq.trainer | begin training epoch 289
2022-09-08 11:00:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:01:46 | INFO | train_inner | epoch 289:     53 / 81 loss=3.388, nll_loss=0.348, mask_loss=8.426, p_2=0.0356, mask_ave=0.495, ppl=1.27, wps=3571.9, ups=0.65, wpb=5530.2, bsz=367, num_updates=23300, lr=0.000207168, gnorm=0.242, train_wall=121, gb_free=9.1, wall=39339
2022-09-08 11:02:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:02:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:02:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:02:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:02:31 | INFO | valid | epoch 289 | valid on 'valid' subset | loss 5.042 | nll_loss 2.408 | mask_loss 9.37628 | p_2 0.04846 | mask_ave 0.619 | ppl 5.31 | bleu 56.27 | wps 1505.6 | wpb 933.5 | bsz 59.6 | num_updates 23328 | best_bleu 57.29
2022-09-08 11:02:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 289 @ 23328 updates
2022-09-08 11:02:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint289.pt
2022-09-08 11:02:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint289.pt
2022-09-08 11:02:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint289.pt (epoch 289 @ 23328 updates, score 56.27) (writing took 20.95508274063468 seconds)
2022-09-08 11:02:52 | INFO | fairseq_cli.train | end of epoch 289 (average epoch stats below)
2022-09-08 11:02:52 | INFO | train | epoch 289 | loss 3.388 | nll_loss 0.349 | mask_loss 8.45408 | p_2 0.03524 | mask_ave 0.494 | ppl 1.27 | wps 3388.8 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 23328 | lr 0.000207043 | gnorm 0.244 | train_wall 98 | gb_free 9.1 | wall 39405
2022-09-08 11:02:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:02:52 | INFO | fairseq.trainer | begin training epoch 290
2022-09-08 11:02:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:04:20 | INFO | train_inner | epoch 290:     72 / 81 loss=3.389, nll_loss=0.35, mask_loss=8.48265, p_2=0.035, mask_ave=0.493, ppl=1.27, wps=3584.6, ups=0.65, wpb=5529.3, bsz=353.3, num_updates=23400, lr=0.000206725, gnorm=0.249, train_wall=120, gb_free=9.1, wall=39493
2022-09-08 11:04:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:04:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:04:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:04:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:04:42 | INFO | valid | epoch 290 | valid on 'valid' subset | loss 5.054 | nll_loss 2.424 | mask_loss 9.352 | p_2 0.04865 | mask_ave 0.612 | ppl 5.37 | bleu 55.75 | wps 1534 | wpb 933.5 | bsz 59.6 | num_updates 23409 | best_bleu 57.29
2022-09-08 11:04:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 290 @ 23409 updates
2022-09-08 11:04:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint290.pt
2022-09-08 11:04:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint290.pt
2022-09-08 11:05:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint290.pt (epoch 290 @ 23409 updates, score 55.75) (writing took 20.456539325416088 seconds)
2022-09-08 11:05:02 | INFO | fairseq_cli.train | end of epoch 290 (average epoch stats below)
2022-09-08 11:05:02 | INFO | train | epoch 290 | loss 3.389 | nll_loss 0.349 | mask_loss 8.45341 | p_2 0.03522 | mask_ave 0.494 | ppl 1.27 | wps 3434.1 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 23409 | lr 0.000206685 | gnorm 0.244 | train_wall 97 | gb_free 9 | wall 39535
2022-09-08 11:05:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:05:03 | INFO | fairseq.trainer | begin training epoch 291
2022-09-08 11:05:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:06:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:06:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:06:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:06:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:06:52 | INFO | valid | epoch 291 | valid on 'valid' subset | loss 5.045 | nll_loss 2.414 | mask_loss 9.36457 | p_2 0.04874 | mask_ave 0.61 | ppl 5.33 | bleu 57.01 | wps 1505.3 | wpb 933.5 | bsz 59.6 | num_updates 23490 | best_bleu 57.29
2022-09-08 11:06:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 291 @ 23490 updates
2022-09-08 11:06:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint291.pt
2022-09-08 11:06:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint291.pt
2022-09-08 11:07:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint291.pt (epoch 291 @ 23490 updates, score 57.01) (writing took 19.07652223855257 seconds)
2022-09-08 11:07:11 | INFO | fairseq_cli.train | end of epoch 291 (average epoch stats below)
2022-09-08 11:07:11 | INFO | train | epoch 291 | loss 3.389 | nll_loss 0.349 | mask_loss 8.44295 | p_2 0.03523 | mask_ave 0.494 | ppl 1.27 | wps 3468.4 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 23490 | lr 0.000206328 | gnorm 0.258 | train_wall 97 | gb_free 9.2 | wall 39664
2022-09-08 11:07:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:07:12 | INFO | fairseq.trainer | begin training epoch 292
2022-09-08 11:07:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:07:24 | INFO | train_inner | epoch 292:     10 / 81 loss=3.389, nll_loss=0.349, mask_loss=8.43816, p_2=0.03534, mask_ave=0.495, ppl=1.27, wps=2990, ups=0.54, wpb=5499.5, bsz=356.3, num_updates=23500, lr=0.000206284, gnorm=0.255, train_wall=119, gb_free=9.1, wall=39677
2022-09-08 11:08:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:08:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:08:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:08:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:09:00 | INFO | valid | epoch 292 | valid on 'valid' subset | loss 5.05 | nll_loss 2.416 | mask_loss 9.33214 | p_2 0.04886 | mask_ave 0.606 | ppl 5.34 | bleu 56.06 | wps 1529.4 | wpb 933.5 | bsz 59.6 | num_updates 23571 | best_bleu 57.29
2022-09-08 11:09:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 292 @ 23571 updates
2022-09-08 11:09:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint292.pt
2022-09-08 11:09:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint292.pt
2022-09-08 11:09:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint292.pt (epoch 292 @ 23571 updates, score 56.06) (writing took 2.5078983046114445 seconds)
2022-09-08 11:09:03 | INFO | fairseq_cli.train | end of epoch 292 (average epoch stats below)
2022-09-08 11:09:03 | INFO | train | epoch 292 | loss 3.388 | nll_loss 0.349 | mask_loss 8.47753 | p_2 0.03522 | mask_ave 0.494 | ppl 1.27 | wps 4010.1 | ups 0.73 | wpb 5523.2 | bsz 358 | num_updates 23571 | lr 0.000205973 | gnorm 0.253 | train_wall 96 | gb_free 9.1 | wall 39776
2022-09-08 11:09:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:09:03 | INFO | fairseq.trainer | begin training epoch 293
2022-09-08 11:09:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:09:39 | INFO | train_inner | epoch 293:     29 / 81 loss=3.389, nll_loss=0.349, mask_loss=8.49945, p_2=0.03535, mask_ave=0.495, ppl=1.27, wps=4091.4, ups=0.74, wpb=5511.9, bsz=356.6, num_updates=23600, lr=0.000205847, gnorm=0.255, train_wall=119, gb_free=9.2, wall=39812
2022-09-08 11:10:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:10:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:10:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:10:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:10:53 | INFO | valid | epoch 293 | valid on 'valid' subset | loss 5.038 | nll_loss 2.411 | mask_loss 9.58239 | p_2 0.04857 | mask_ave 0.614 | ppl 5.32 | bleu 56.43 | wps 1514.1 | wpb 933.5 | bsz 59.6 | num_updates 23652 | best_bleu 57.29
2022-09-08 11:10:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 293 @ 23652 updates
2022-09-08 11:10:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint293.pt
2022-09-08 11:10:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint293.pt
2022-09-08 11:11:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint293.pt (epoch 293 @ 23652 updates, score 56.43) (writing took 20.59134365618229 seconds)
2022-09-08 11:11:14 | INFO | fairseq_cli.train | end of epoch 293 (average epoch stats below)
2022-09-08 11:11:14 | INFO | train | epoch 293 | loss 3.389 | nll_loss 0.349 | mask_loss 8.65007 | p_2 0.03526 | mask_ave 0.493 | ppl 1.27 | wps 3413.2 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 23652 | lr 0.00020562 | gnorm 0.263 | train_wall 98 | gb_free 9.1 | wall 39907
2022-09-08 11:11:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:11:14 | INFO | fairseq.trainer | begin training epoch 294
2022-09-08 11:11:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:12:14 | INFO | train_inner | epoch 294:     48 / 81 loss=3.388, nll_loss=0.349, mask_loss=8.61884, p_2=0.03516, mask_ave=0.495, ppl=1.27, wps=3566.2, ups=0.64, wpb=5544.7, bsz=361.4, num_updates=23700, lr=0.000205412, gnorm=0.267, train_wall=122, gb_free=9, wall=39967
2022-09-08 11:12:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:12:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:12:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:12:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:13:05 | INFO | valid | epoch 294 | valid on 'valid' subset | loss 5.041 | nll_loss 2.405 | mask_loss 9.45266 | p_2 0.04836 | mask_ave 0.621 | ppl 5.3 | bleu 55.82 | wps 1503.1 | wpb 933.5 | bsz 59.6 | num_updates 23733 | best_bleu 57.29
2022-09-08 11:13:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 294 @ 23733 updates
2022-09-08 11:13:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint294.pt
2022-09-08 11:13:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint294.pt
2022-09-08 11:13:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint294.pt (epoch 294 @ 23733 updates, score 55.82) (writing took 2.579265784472227 seconds)
2022-09-08 11:13:08 | INFO | fairseq_cli.train | end of epoch 294 (average epoch stats below)
2022-09-08 11:13:08 | INFO | train | epoch 294 | loss 3.388 | nll_loss 0.349 | mask_loss 8.54148 | p_2 0.03509 | mask_ave 0.498 | ppl 1.27 | wps 3923.2 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 23733 | lr 0.000205269 | gnorm 0.258 | train_wall 98 | gb_free 9.2 | wall 40021
2022-09-08 11:13:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:13:08 | INFO | fairseq.trainer | begin training epoch 295
2022-09-08 11:13:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:14:30 | INFO | train_inner | epoch 295:     67 / 81 loss=3.389, nll_loss=0.35, mask_loss=8.56695, p_2=0.03492, mask_ave=0.497, ppl=1.27, wps=4081.6, ups=0.74, wpb=5523.2, bsz=357.2, num_updates=23800, lr=0.00020498, gnorm=0.28, train_wall=120, gb_free=9.1, wall=40102
2022-09-08 11:14:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:14:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:14:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:14:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:14:57 | INFO | valid | epoch 295 | valid on 'valid' subset | loss 5.046 | nll_loss 2.418 | mask_loss 9.39369 | p_2 0.04862 | mask_ave 0.613 | ppl 5.34 | bleu 56.31 | wps 1530.2 | wpb 933.5 | bsz 59.6 | num_updates 23814 | best_bleu 57.29
2022-09-08 11:14:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 295 @ 23814 updates
2022-09-08 11:14:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint295.pt
2022-09-08 11:14:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint295.pt
2022-09-08 11:15:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint295.pt (epoch 295 @ 23814 updates, score 56.31) (writing took 17.642032902687788 seconds)
2022-09-08 11:15:15 | INFO | fairseq_cli.train | end of epoch 295 (average epoch stats below)
2022-09-08 11:15:15 | INFO | train | epoch 295 | loss 3.389 | nll_loss 0.35 | mask_loss 8.5551 | p_2 0.03512 | mask_ave 0.497 | ppl 1.27 | wps 3531.4 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 23814 | lr 0.00020492 | gnorm 0.293 | train_wall 96 | gb_free 9.1 | wall 40148
2022-09-08 11:15:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:15:15 | INFO | fairseq.trainer | begin training epoch 296
2022-09-08 11:15:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:16:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:16:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:16:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:16:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:17:05 | INFO | valid | epoch 296 | valid on 'valid' subset | loss 5.054 | nll_loss 2.424 | mask_loss 9.39241 | p_2 0.04831 | mask_ave 0.623 | ppl 5.37 | bleu 56.67 | wps 1569.7 | wpb 933.5 | bsz 59.6 | num_updates 23895 | best_bleu 57.29
2022-09-08 11:17:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 296 @ 23895 updates
2022-09-08 11:17:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint296.pt
2022-09-08 11:17:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint296.pt
2022-09-08 11:17:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint296.pt (epoch 296 @ 23895 updates, score 56.67) (writing took 20.401436060667038 seconds)
2022-09-08 11:17:26 | INFO | fairseq_cli.train | end of epoch 296 (average epoch stats below)
2022-09-08 11:17:26 | INFO | train | epoch 296 | loss 3.388 | nll_loss 0.349 | mask_loss 8.46636 | p_2 0.03519 | mask_ave 0.495 | ppl 1.27 | wps 3415.2 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 23895 | lr 0.000204572 | gnorm 0.258 | train_wall 98 | gb_free 9.1 | wall 40279
2022-09-08 11:17:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:17:26 | INFO | fairseq.trainer | begin training epoch 297
2022-09-08 11:17:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:17:33 | INFO | train_inner | epoch 297:      5 / 81 loss=3.389, nll_loss=0.35, mask_loss=8.47261, p_2=0.03528, mask_ave=0.497, ppl=1.27, wps=2994.3, ups=0.55, wpb=5494, bsz=355.4, num_updates=23900, lr=0.000204551, gnorm=0.261, train_wall=120, gb_free=9.1, wall=40286
2022-09-08 11:19:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:19:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:19:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:19:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:19:16 | INFO | valid | epoch 297 | valid on 'valid' subset | loss 5.049 | nll_loss 2.418 | mask_loss 9.5246 | p_2 0.04855 | mask_ave 0.616 | ppl 5.34 | bleu 56.79 | wps 1532.5 | wpb 933.5 | bsz 59.6 | num_updates 23976 | best_bleu 57.29
2022-09-08 11:19:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 297 @ 23976 updates
2022-09-08 11:19:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint297.pt
2022-09-08 11:19:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint297.pt
2022-09-08 11:19:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint297.pt (epoch 297 @ 23976 updates, score 56.79) (writing took 22.508658476173878 seconds)
2022-09-08 11:19:39 | INFO | fairseq_cli.train | end of epoch 297 (average epoch stats below)
2022-09-08 11:19:39 | INFO | train | epoch 297 | loss 3.389 | nll_loss 0.349 | mask_loss 8.50641 | p_2 0.03521 | mask_ave 0.495 | ppl 1.27 | wps 3363.3 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 23976 | lr 0.000204226 | gnorm 0.278 | train_wall 97 | gb_free 9.2 | wall 40412
2022-09-08 11:19:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:19:39 | INFO | fairseq.trainer | begin training epoch 298
2022-09-08 11:19:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:20:08 | INFO | train_inner | epoch 298:     24 / 81 loss=3.388, nll_loss=0.349, mask_loss=8.48803, p_2=0.03533, mask_ave=0.494, ppl=1.27, wps=3569.9, ups=0.64, wpb=5535.6, bsz=361.6, num_updates=24000, lr=0.000204124, gnorm=0.28, train_wall=120, gb_free=9.1, wall=40441
2022-09-08 11:21:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:21:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:21:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:21:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:21:28 | INFO | valid | epoch 298 | valid on 'valid' subset | loss 5.041 | nll_loss 2.412 | mask_loss 9.38536 | p_2 0.04846 | mask_ave 0.617 | ppl 5.32 | bleu 56.33 | wps 1539.4 | wpb 933.5 | bsz 59.6 | num_updates 24057 | best_bleu 57.29
2022-09-08 11:21:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 298 @ 24057 updates
2022-09-08 11:21:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint298.pt
2022-09-08 11:21:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint298.pt
2022-09-08 11:21:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint298.pt (epoch 298 @ 24057 updates, score 56.33) (writing took 19.11004861444235 seconds)
2022-09-08 11:21:48 | INFO | fairseq_cli.train | end of epoch 298 (average epoch stats below)
2022-09-08 11:21:48 | INFO | train | epoch 298 | loss 3.388 | nll_loss 0.348 | mask_loss 8.5077 | p_2 0.03517 | mask_ave 0.496 | ppl 1.27 | wps 3469.8 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 24057 | lr 0.000203882 | gnorm 0.264 | train_wall 97 | gb_free 9.2 | wall 40541
2022-09-08 11:21:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:21:48 | INFO | fairseq.trainer | begin training epoch 299
2022-09-08 11:21:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:22:40 | INFO | train_inner | epoch 299:     43 / 81 loss=3.387, nll_loss=0.348, mask_loss=8.49824, p_2=0.03516, mask_ave=0.497, ppl=1.27, wps=3630.6, ups=0.66, wpb=5524.1, bsz=358.4, num_updates=24100, lr=0.0002037, gnorm=0.257, train_wall=120, gb_free=9.1, wall=40593
2022-09-08 11:23:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:23:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:23:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:23:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:23:37 | INFO | valid | epoch 299 | valid on 'valid' subset | loss 5.054 | nll_loss 2.422 | mask_loss 9.40115 | p_2 0.04857 | mask_ave 0.615 | ppl 5.36 | bleu 56.37 | wps 1477 | wpb 933.5 | bsz 59.6 | num_updates 24138 | best_bleu 57.29
2022-09-08 11:23:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 299 @ 24138 updates
2022-09-08 11:23:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint299.pt
2022-09-08 11:23:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint299.pt
2022-09-08 11:24:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint299.pt (epoch 299 @ 24138 updates, score 56.37) (writing took 25.618830882012844 seconds)
2022-09-08 11:24:03 | INFO | fairseq_cli.train | end of epoch 299 (average epoch stats below)
2022-09-08 11:24:03 | INFO | train | epoch 299 | loss 3.388 | nll_loss 0.349 | mask_loss 8.48758 | p_2 0.03518 | mask_ave 0.496 | ppl 1.27 | wps 3306 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 24138 | lr 0.00020354 | gnorm 0.271 | train_wall 96 | gb_free 9.2 | wall 40676
2022-09-08 11:24:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:24:03 | INFO | fairseq.trainer | begin training epoch 300
2022-09-08 11:24:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:25:20 | INFO | train_inner | epoch 300:     62 / 81 loss=3.388, nll_loss=0.349, mask_loss=8.50848, p_2=0.03512, mask_ave=0.496, ppl=1.27, wps=3466.3, ups=0.63, wpb=5536.7, bsz=358.2, num_updates=24200, lr=0.000203279, gnorm=0.275, train_wall=121, gb_free=9, wall=40753
2022-09-08 11:25:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:25:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:25:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:25:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:25:54 | INFO | valid | epoch 300 | valid on 'valid' subset | loss 5.049 | nll_loss 2.422 | mask_loss 9.43213 | p_2 0.04859 | mask_ave 0.614 | ppl 5.36 | bleu 55.83 | wps 1527.3 | wpb 933.5 | bsz 59.6 | num_updates 24219 | best_bleu 57.29
2022-09-08 11:25:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 300 @ 24219 updates
2022-09-08 11:25:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint300.pt
2022-09-08 11:25:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint300.pt
2022-09-08 11:25:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint300.pt (epoch 300 @ 24219 updates, score 55.83) (writing took 2.5773264802992344 seconds)
2022-09-08 11:25:57 | INFO | fairseq_cli.train | end of epoch 300 (average epoch stats below)
2022-09-08 11:25:57 | INFO | train | epoch 300 | loss 3.388 | nll_loss 0.349 | mask_loss 8.49488 | p_2 0.03514 | mask_ave 0.497 | ppl 1.27 | wps 3934.2 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 24219 | lr 0.000203199 | gnorm 0.266 | train_wall 98 | gb_free 9 | wall 40790
2022-09-08 11:25:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:25:57 | INFO | fairseq.trainer | begin training epoch 301
2022-09-08 11:25:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:27:35 | INFO | train_inner | epoch 301:     81 / 81 loss=3.388, nll_loss=0.348, mask_loss=8.47449, p_2=0.03519, mask_ave=0.492, ppl=1.27, wps=4067.3, ups=0.74, wpb=5502.6, bsz=355.4, num_updates=24300, lr=0.00020286, gnorm=0.257, train_wall=120, gb_free=9, wall=40888
2022-09-08 11:27:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:27:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:27:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:27:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:27:47 | INFO | valid | epoch 301 | valid on 'valid' subset | loss 5.04 | nll_loss 2.415 | mask_loss 9.24215 | p_2 0.04868 | mask_ave 0.611 | ppl 5.33 | bleu 56.62 | wps 1501.4 | wpb 933.5 | bsz 59.6 | num_updates 24300 | best_bleu 57.29
2022-09-08 11:27:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 301 @ 24300 updates
2022-09-08 11:27:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint301.pt
2022-09-08 11:27:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint301.pt
2022-09-08 11:28:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint301.pt (epoch 301 @ 24300 updates, score 56.62) (writing took 19.2527426071465 seconds)
2022-09-08 11:28:06 | INFO | fairseq_cli.train | end of epoch 301 (average epoch stats below)
2022-09-08 11:28:06 | INFO | train | epoch 301 | loss 3.387 | nll_loss 0.348 | mask_loss 8.47226 | p_2 0.0353 | mask_ave 0.492 | ppl 1.27 | wps 3457.6 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 24300 | lr 0.00020286 | gnorm 0.256 | train_wall 97 | gb_free 9 | wall 40919
2022-09-08 11:28:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:28:06 | INFO | fairseq.trainer | begin training epoch 302
2022-09-08 11:28:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:29:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:29:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:29:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:29:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:29:58 | INFO | valid | epoch 302 | valid on 'valid' subset | loss 5.031 | nll_loss 2.395 | mask_loss 9.33845 | p_2 0.04832 | mask_ave 0.623 | ppl 5.26 | bleu 56.23 | wps 1481.7 | wpb 933.5 | bsz 59.6 | num_updates 24381 | best_bleu 57.29
2022-09-08 11:29:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 302 @ 24381 updates
2022-09-08 11:29:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint302.pt
2022-09-08 11:29:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint302.pt
2022-09-08 11:30:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint302.pt (epoch 302 @ 24381 updates, score 56.23) (writing took 2.578792192041874 seconds)
2022-09-08 11:30:00 | INFO | fairseq_cli.train | end of epoch 302 (average epoch stats below)
2022-09-08 11:30:00 | INFO | train | epoch 302 | loss 3.387 | nll_loss 0.348 | mask_loss 8.49038 | p_2 0.03528 | mask_ave 0.492 | ppl 1.27 | wps 3918.7 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 24381 | lr 0.000202523 | gnorm 0.262 | train_wall 98 | gb_free 9.3 | wall 41033
2022-09-08 11:30:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:30:00 | INFO | fairseq.trainer | begin training epoch 303
2022-09-08 11:30:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:30:24 | INFO | train_inner | epoch 303:     19 / 81 loss=3.387, nll_loss=0.348, mask_loss=8.49385, p_2=0.03524, mask_ave=0.493, ppl=1.27, wps=3281.3, ups=0.59, wpb=5544.6, bsz=359.3, num_updates=24400, lr=0.000202444, gnorm=0.26, train_wall=121, gb_free=9, wall=41057
2022-09-08 11:31:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:31:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:31:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:31:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:31:51 | INFO | valid | epoch 303 | valid on 'valid' subset | loss 5.056 | nll_loss 2.43 | mask_loss 9.22196 | p_2 0.04861 | mask_ave 0.614 | ppl 5.39 | bleu 56.51 | wps 1492.8 | wpb 933.5 | bsz 59.6 | num_updates 24462 | best_bleu 57.29
2022-09-08 11:31:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 303 @ 24462 updates
2022-09-08 11:31:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint303.pt
2022-09-08 11:31:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint303.pt
2022-09-08 11:31:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint303.pt (epoch 303 @ 24462 updates, score 56.51) (writing took 2.6758287772536278 seconds)
2022-09-08 11:31:54 | INFO | fairseq_cli.train | end of epoch 303 (average epoch stats below)
2022-09-08 11:31:54 | INFO | train | epoch 303 | loss 3.387 | nll_loss 0.348 | mask_loss 8.51044 | p_2 0.0352 | mask_ave 0.495 | ppl 1.27 | wps 3944.8 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 24462 | lr 0.000202187 | gnorm 0.248 | train_wall 97 | gb_free 9.2 | wall 41147
2022-09-08 11:31:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:31:54 | INFO | fairseq.trainer | begin training epoch 304
2022-09-08 11:31:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:32:40 | INFO | train_inner | epoch 304:     38 / 81 loss=3.387, nll_loss=0.348, mask_loss=8.53278, p_2=0.03527, mask_ave=0.497, ppl=1.27, wps=4055.7, ups=0.74, wpb=5504.1, bsz=357.6, num_updates=24500, lr=0.000202031, gnorm=0.246, train_wall=120, gb_free=9, wall=41193
2022-09-08 11:33:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:33:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:33:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:33:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:33:44 | INFO | valid | epoch 304 | valid on 'valid' subset | loss 5.034 | nll_loss 2.399 | mask_loss 9.43149 | p_2 0.04842 | mask_ave 0.62 | ppl 5.27 | bleu 56.47 | wps 1472.6 | wpb 933.5 | bsz 59.6 | num_updates 24543 | best_bleu 57.29
2022-09-08 11:33:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 304 @ 24543 updates
2022-09-08 11:33:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint304.pt
2022-09-08 11:33:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint304.pt
2022-09-08 11:34:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint304.pt (epoch 304 @ 24543 updates, score 56.47) (writing took 20.357799515128136 seconds)
2022-09-08 11:34:04 | INFO | fairseq_cli.train | end of epoch 304 (average epoch stats below)
2022-09-08 11:34:04 | INFO | train | epoch 304 | loss 3.387 | nll_loss 0.348 | mask_loss 8.58896 | p_2 0.03517 | mask_ave 0.496 | ppl 1.27 | wps 3421.4 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 24543 | lr 0.000201853 | gnorm 0.257 | train_wall 97 | gb_free 9.2 | wall 41277
2022-09-08 11:34:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:34:05 | INFO | fairseq.trainer | begin training epoch 305
2022-09-08 11:34:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:35:13 | INFO | train_inner | epoch 305:     57 / 81 loss=3.388, nll_loss=0.349, mask_loss=8.60949, p_2=0.03502, mask_ave=0.495, ppl=1.27, wps=3620.3, ups=0.65, wpb=5554.8, bsz=357, num_updates=24600, lr=0.000201619, gnorm=0.267, train_wall=120, gb_free=9.1, wall=41346
2022-09-08 11:35:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:35:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:35:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:35:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:35:53 | INFO | valid | epoch 305 | valid on 'valid' subset | loss 5.032 | nll_loss 2.4 | mask_loss 9.22965 | p_2 0.0486 | mask_ave 0.614 | ppl 5.28 | bleu 56.95 | wps 1508.1 | wpb 933.5 | bsz 59.6 | num_updates 24624 | best_bleu 57.29
2022-09-08 11:35:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 305 @ 24624 updates
2022-09-08 11:35:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint305.pt
2022-09-08 11:35:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint305.pt
2022-09-08 11:36:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint305.pt (epoch 305 @ 24624 updates, score 56.95) (writing took 15.51132046058774 seconds)
2022-09-08 11:36:09 | INFO | fairseq_cli.train | end of epoch 305 (average epoch stats below)
2022-09-08 11:36:09 | INFO | train | epoch 305 | loss 3.387 | nll_loss 0.348 | mask_loss 8.59273 | p_2 0.03518 | mask_ave 0.496 | ppl 1.27 | wps 3603.9 | ups 0.65 | wpb 5523.2 | bsz 358 | num_updates 24624 | lr 0.000201521 | gnorm 0.263 | train_wall 96 | gb_free 9.3 | wall 41402
2022-09-08 11:36:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:36:09 | INFO | fairseq.trainer | begin training epoch 306
2022-09-08 11:36:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:37:41 | INFO | train_inner | epoch 306:     76 / 81 loss=3.387, nll_loss=0.348, mask_loss=8.52547, p_2=0.03542, mask_ave=0.494, ppl=1.27, wps=3734.4, ups=0.68, wpb=5508.4, bsz=359.7, num_updates=24700, lr=0.000201211, gnorm=0.256, train_wall=119, gb_free=8.9, wall=41494
2022-09-08 11:37:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:37:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:37:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:37:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:37:57 | INFO | valid | epoch 306 | valid on 'valid' subset | loss 5.025 | nll_loss 2.392 | mask_loss 9.50088 | p_2 0.04826 | mask_ave 0.624 | ppl 5.25 | bleu 56.64 | wps 1523.6 | wpb 933.5 | bsz 59.6 | num_updates 24705 | best_bleu 57.29
2022-09-08 11:37:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 306 @ 24705 updates
2022-09-08 11:37:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint306.pt
2022-09-08 11:37:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint306.pt
2022-09-08 11:38:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint306.pt (epoch 306 @ 24705 updates, score 56.64) (writing took 2.6765985302627087 seconds)
2022-09-08 11:38:00 | INFO | fairseq_cli.train | end of epoch 306 (average epoch stats below)
2022-09-08 11:38:00 | INFO | train | epoch 306 | loss 3.387 | nll_loss 0.348 | mask_loss 8.52298 | p_2 0.03527 | mask_ave 0.493 | ppl 1.27 | wps 4010.6 | ups 0.73 | wpb 5523.2 | bsz 358 | num_updates 24705 | lr 0.000201191 | gnorm 0.261 | train_wall 96 | gb_free 9.3 | wall 41513
2022-09-08 11:38:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:38:00 | INFO | fairseq.trainer | begin training epoch 307
2022-09-08 11:38:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:39:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:39:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:39:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:39:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:39:49 | INFO | valid | epoch 307 | valid on 'valid' subset | loss 5.032 | nll_loss 2.397 | mask_loss 9.40614 | p_2 0.04821 | mask_ave 0.626 | ppl 5.27 | bleu 56.51 | wps 1506.7 | wpb 933.5 | bsz 59.6 | num_updates 24786 | best_bleu 57.29
2022-09-08 11:39:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 307 @ 24786 updates
2022-09-08 11:39:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint307.pt
2022-09-08 11:39:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint307.pt
2022-09-08 11:40:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint307.pt (epoch 307 @ 24786 updates, score 56.51) (writing took 19.31841468065977 seconds)
2022-09-08 11:40:09 | INFO | fairseq_cli.train | end of epoch 307 (average epoch stats below)
2022-09-08 11:40:09 | INFO | train | epoch 307 | loss 3.387 | nll_loss 0.349 | mask_loss 8.54684 | p_2 0.0351 | mask_ave 0.498 | ppl 1.27 | wps 3481.3 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 24786 | lr 0.000200862 | gnorm 0.26 | train_wall 96 | gb_free 9.1 | wall 41642
2022-09-08 11:40:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:40:09 | INFO | fairseq.trainer | begin training epoch 308
2022-09-08 11:40:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:40:26 | INFO | train_inner | epoch 308:     14 / 81 loss=3.387, nll_loss=0.348, mask_loss=8.57511, p_2=0.03486, mask_ave=0.497, ppl=1.27, wps=3342.6, ups=0.61, wpb=5524.6, bsz=357.2, num_updates=24800, lr=0.000200805, gnorm=0.264, train_wall=118, gb_free=9.1, wall=41659
2022-09-08 11:41:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:41:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:41:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:41:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:42:00 | INFO | valid | epoch 308 | valid on 'valid' subset | loss 5.033 | nll_loss 2.399 | mask_loss 9.31959 | p_2 0.04851 | mask_ave 0.617 | ppl 5.27 | bleu 56.33 | wps 1424.6 | wpb 933.5 | bsz 59.6 | num_updates 24867 | best_bleu 57.29
2022-09-08 11:42:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 308 @ 24867 updates
2022-09-08 11:42:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint308.pt
2022-09-08 11:42:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint308.pt
2022-09-08 11:42:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint308.pt (epoch 308 @ 24867 updates, score 56.33) (writing took 17.605780635029078 seconds)
2022-09-08 11:42:18 | INFO | fairseq_cli.train | end of epoch 308 (average epoch stats below)
2022-09-08 11:42:18 | INFO | train | epoch 308 | loss 3.387 | nll_loss 0.348 | mask_loss 8.54855 | p_2 0.03503 | mask_ave 0.5 | ppl 1.27 | wps 3457.1 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 24867 | lr 0.000200534 | gnorm 0.264 | train_wall 98 | gb_free 9.1 | wall 41771
2022-09-08 11:42:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:42:18 | INFO | fairseq.trainer | begin training epoch 309
2022-09-08 11:42:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:43:00 | INFO | train_inner | epoch 309:     33 / 81 loss=3.387, nll_loss=0.348, mask_loss=8.51025, p_2=0.03533, mask_ave=0.501, ppl=1.27, wps=3594.2, ups=0.65, wpb=5517.7, bsz=360.6, num_updates=24900, lr=0.000200401, gnorm=0.26, train_wall=122, gb_free=9.1, wall=41813
2022-09-08 11:43:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:44:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:44:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:44:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:44:10 | INFO | valid | epoch 309 | valid on 'valid' subset | loss 5.044 | nll_loss 2.413 | mask_loss 9.38286 | p_2 0.04858 | mask_ave 0.614 | ppl 5.32 | bleu 56.6 | wps 1470.5 | wpb 933.5 | bsz 59.6 | num_updates 24948 | best_bleu 57.29
2022-09-08 11:44:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 309 @ 24948 updates
2022-09-08 11:44:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint309.pt
2022-09-08 11:44:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint309.pt
2022-09-08 11:44:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint309.pt (epoch 309 @ 24948 updates, score 56.6) (writing took 24.23276936635375 seconds)
2022-09-08 11:44:35 | INFO | fairseq_cli.train | end of epoch 309 (average epoch stats below)
2022-09-08 11:44:35 | INFO | train | epoch 309 | loss 3.387 | nll_loss 0.348 | mask_loss 8.50149 | p_2 0.03509 | mask_ave 0.499 | ppl 1.27 | wps 3271.5 | ups 0.59 | wpb 5523.2 | bsz 358 | num_updates 24948 | lr 0.000200208 | gnorm 0.252 | train_wall 99 | gb_free 9.4 | wall 41908
2022-09-08 11:44:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:44:35 | INFO | fairseq.trainer | begin training epoch 310
2022-09-08 11:44:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:45:41 | INFO | train_inner | epoch 310:     52 / 81 loss=3.387, nll_loss=0.348, mask_loss=8.42774, p_2=0.03513, mask_ave=0.496, ppl=1.27, wps=3447.4, ups=0.62, wpb=5540.8, bsz=357.1, num_updates=25000, lr=0.0002, gnorm=0.256, train_wall=123, gb_free=9.1, wall=41973
2022-09-08 11:46:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:46:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:46:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:46:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:46:27 | INFO | valid | epoch 310 | valid on 'valid' subset | loss 5.037 | nll_loss 2.413 | mask_loss 9.17539 | p_2 0.04847 | mask_ave 0.618 | ppl 5.32 | bleu 56.6 | wps 1528.9 | wpb 933.5 | bsz 59.6 | num_updates 25029 | best_bleu 57.29
2022-09-08 11:46:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 310 @ 25029 updates
2022-09-08 11:46:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint310.pt
2022-09-08 11:46:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint310.pt
2022-09-08 11:46:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint310.pt (epoch 310 @ 25029 updates, score 56.6) (writing took 27.196551974862814 seconds)
2022-09-08 11:46:54 | INFO | fairseq_cli.train | end of epoch 310 (average epoch stats below)
2022-09-08 11:46:54 | INFO | train | epoch 310 | loss 3.387 | nll_loss 0.348 | mask_loss 8.41437 | p_2 0.03523 | mask_ave 0.495 | ppl 1.27 | wps 3207.3 | ups 0.58 | wpb 5523.2 | bsz 358 | num_updates 25029 | lr 0.000199884 | gnorm 0.27 | train_wall 99 | gb_free 9 | wall 42047
2022-09-08 11:46:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:46:55 | INFO | fairseq.trainer | begin training epoch 311
2022-09-08 11:46:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:48:22 | INFO | train_inner | epoch 311:     71 / 81 loss=3.387, nll_loss=0.348, mask_loss=8.39057, p_2=0.03507, mask_ave=0.497, ppl=1.27, wps=3401.1, ups=0.62, wpb=5504.3, bsz=355.6, num_updates=25100, lr=0.000199601, gnorm=0.269, train_wall=122, gb_free=9.1, wall=42135
2022-09-08 11:48:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:48:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:48:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:48:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:48:46 | INFO | valid | epoch 311 | valid on 'valid' subset | loss 5.045 | nll_loss 2.414 | mask_loss 9.36338 | p_2 0.04813 | mask_ave 0.628 | ppl 5.33 | bleu 56.56 | wps 1476.2 | wpb 933.5 | bsz 59.6 | num_updates 25110 | best_bleu 57.29
2022-09-08 11:48:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 311 @ 25110 updates
2022-09-08 11:48:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint311.pt
2022-09-08 11:48:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint311.pt
2022-09-08 11:48:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint311.pt (epoch 311 @ 25110 updates, score 56.56) (writing took 2.482631392776966 seconds)
2022-09-08 11:48:48 | INFO | fairseq_cli.train | end of epoch 311 (average epoch stats below)
2022-09-08 11:48:48 | INFO | train | epoch 311 | loss 3.387 | nll_loss 0.348 | mask_loss 8.34344 | p_2 0.03513 | mask_ave 0.497 | ppl 1.27 | wps 3931.4 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 25110 | lr 0.000199561 | gnorm 0.267 | train_wall 98 | gb_free 9.1 | wall 42161
2022-09-08 11:48:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:48:48 | INFO | fairseq.trainer | begin training epoch 312
2022-09-08 11:48:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:50:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:50:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:50:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:50:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:50:38 | INFO | valid | epoch 312 | valid on 'valid' subset | loss 5.038 | nll_loss 2.404 | mask_loss 9.33692 | p_2 0.04822 | mask_ave 0.625 | ppl 5.29 | bleu 56.99 | wps 1564.4 | wpb 933.5 | bsz 59.6 | num_updates 25191 | best_bleu 57.29
2022-09-08 11:50:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 312 @ 25191 updates
2022-09-08 11:50:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint312.pt
2022-09-08 11:50:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint312.pt
2022-09-08 11:50:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint312.pt (epoch 312 @ 25191 updates, score 56.99) (writing took 21.100983519107103 seconds)
2022-09-08 11:50:59 | INFO | fairseq_cli.train | end of epoch 312 (average epoch stats below)
2022-09-08 11:50:59 | INFO | train | epoch 312 | loss 3.386 | nll_loss 0.348 | mask_loss 8.43625 | p_2 0.03506 | mask_ave 0.499 | ppl 1.27 | wps 3417.3 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 25191 | lr 0.00019924 | gnorm 0.26 | train_wall 97 | gb_free 9 | wall 42292
2022-09-08 11:50:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:50:59 | INFO | fairseq.trainer | begin training epoch 313
2022-09-08 11:50:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:51:11 | INFO | train_inner | epoch 313:      9 / 81 loss=3.386, nll_loss=0.348, mask_loss=8.40412, p_2=0.03517, mask_ave=0.499, ppl=1.27, wps=3272.4, ups=0.59, wpb=5511.2, bsz=360.4, num_updates=25200, lr=0.000199205, gnorm=0.271, train_wall=120, gb_free=9.1, wall=42304
2022-09-08 11:52:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:52:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:52:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:52:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:52:49 | INFO | valid | epoch 313 | valid on 'valid' subset | loss 5.038 | nll_loss 2.408 | mask_loss 9.40052 | p_2 0.04836 | mask_ave 0.62 | ppl 5.31 | bleu 57.15 | wps 1459.6 | wpb 933.5 | bsz 59.6 | num_updates 25272 | best_bleu 57.29
2022-09-08 11:52:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 313 @ 25272 updates
2022-09-08 11:52:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint313.pt
2022-09-08 11:52:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint313.pt
2022-09-08 11:53:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint313.pt (epoch 313 @ 25272 updates, score 57.15) (writing took 21.45595360174775 seconds)
2022-09-08 11:53:10 | INFO | fairseq_cli.train | end of epoch 313 (average epoch stats below)
2022-09-08 11:53:10 | INFO | train | epoch 313 | loss 3.386 | nll_loss 0.347 | mask_loss 8.47518 | p_2 0.03499 | mask_ave 0.501 | ppl 1.27 | wps 3407.6 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 25272 | lr 0.000198921 | gnorm 0.26 | train_wall 97 | gb_free 9 | wall 42423
2022-09-08 11:53:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:53:11 | INFO | fairseq.trainer | begin training epoch 314
2022-09-08 11:53:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:53:45 | INFO | train_inner | epoch 314:     28 / 81 loss=3.386, nll_loss=0.347, mask_loss=8.50872, p_2=0.03492, mask_ave=0.498, ppl=1.27, wps=3599.6, ups=0.65, wpb=5537.8, bsz=359.3, num_updates=25300, lr=0.000198811, gnorm=0.259, train_wall=119, gb_free=9.1, wall=42458
2022-09-08 11:54:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:54:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:54:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:54:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:54:59 | INFO | valid | epoch 314 | valid on 'valid' subset | loss 5.045 | nll_loss 2.416 | mask_loss 9.41613 | p_2 0.0486 | mask_ave 0.614 | ppl 5.34 | bleu 56.41 | wps 1493.1 | wpb 933.5 | bsz 59.6 | num_updates 25353 | best_bleu 57.29
2022-09-08 11:54:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 314 @ 25353 updates
2022-09-08 11:54:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint314.pt
2022-09-08 11:55:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint314.pt
2022-09-08 11:55:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint314.pt (epoch 314 @ 25353 updates, score 56.41) (writing took 14.376463253051043 seconds)
2022-09-08 11:55:14 | INFO | fairseq_cli.train | end of epoch 314 (average epoch stats below)
2022-09-08 11:55:14 | INFO | train | epoch 314 | loss 3.386 | nll_loss 0.347 | mask_loss 8.54064 | p_2 0.03514 | mask_ave 0.497 | ppl 1.27 | wps 3623.2 | ups 0.66 | wpb 5523.2 | bsz 358 | num_updates 25353 | lr 0.000198603 | gnorm 0.271 | train_wall 96 | gb_free 9.1 | wall 42547
2022-09-08 11:55:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:55:14 | INFO | fairseq.trainer | begin training epoch 315
2022-09-08 11:55:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:56:12 | INFO | train_inner | epoch 315:     47 / 81 loss=3.387, nll_loss=0.348, mask_loss=8.56756, p_2=0.03528, mask_ave=0.499, ppl=1.27, wps=3744.4, ups=0.68, wpb=5532.4, bsz=357.4, num_updates=25400, lr=0.000198419, gnorm=0.272, train_wall=120, gb_free=9.1, wall=42605
2022-09-08 11:56:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:56:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:56:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:56:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:57:04 | INFO | valid | epoch 315 | valid on 'valid' subset | loss 5.045 | nll_loss 2.418 | mask_loss 9.35692 | p_2 0.04863 | mask_ave 0.613 | ppl 5.34 | bleu 55.99 | wps 1531.6 | wpb 933.5 | bsz 59.6 | num_updates 25434 | best_bleu 57.29
2022-09-08 11:57:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 315 @ 25434 updates
2022-09-08 11:57:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint315.pt
2022-09-08 11:57:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint315.pt
2022-09-08 11:57:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint315.pt (epoch 315 @ 25434 updates, score 55.99) (writing took 28.26642805337906 seconds)
2022-09-08 11:57:33 | INFO | fairseq_cli.train | end of epoch 315 (average epoch stats below)
2022-09-08 11:57:33 | INFO | train | epoch 315 | loss 3.387 | nll_loss 0.349 | mask_loss 8.56286 | p_2 0.0352 | mask_ave 0.495 | ppl 1.27 | wps 3217.8 | ups 0.58 | wpb 5523.2 | bsz 358 | num_updates 25434 | lr 0.000198286 | gnorm 0.269 | train_wall 98 | gb_free 9.3 | wall 42686
2022-09-08 11:57:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:57:33 | INFO | fairseq.trainer | begin training epoch 316
2022-09-08 11:57:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 11:58:55 | INFO | train_inner | epoch 316:     66 / 81 loss=3.387, nll_loss=0.348, mask_loss=8.53155, p_2=0.03514, mask_ave=0.497, ppl=1.27, wps=3386.3, ups=0.61, wpb=5516.4, bsz=357.8, num_updates=25500, lr=0.00019803, gnorm=0.255, train_wall=121, gb_free=9.2, wall=42768
2022-09-08 11:59:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 11:59:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 11:59:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 11:59:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 11:59:24 | INFO | valid | epoch 316 | valid on 'valid' subset | loss 5.053 | nll_loss 2.425 | mask_loss 9.44986 | p_2 0.04835 | mask_ave 0.622 | ppl 5.37 | bleu 56.06 | wps 1536.3 | wpb 933.5 | bsz 59.6 | num_updates 25515 | best_bleu 57.29
2022-09-08 11:59:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 316 @ 25515 updates
2022-09-08 11:59:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint316.pt
2022-09-08 11:59:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint316.pt
2022-09-08 11:59:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint316.pt (epoch 316 @ 25515 updates, score 56.06) (writing took 2.4373323880136013 seconds)
2022-09-08 11:59:27 | INFO | fairseq_cli.train | end of epoch 316 (average epoch stats below)
2022-09-08 11:59:27 | INFO | train | epoch 316 | loss 3.386 | nll_loss 0.347 | mask_loss 8.55146 | p_2 0.03506 | mask_ave 0.499 | ppl 1.27 | wps 3935.1 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 25515 | lr 0.000197971 | gnorm 0.25 | train_wall 98 | gb_free 9.2 | wall 42799
2022-09-08 11:59:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 11:59:27 | INFO | fairseq.trainer | begin training epoch 317
2022-09-08 11:59:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:01:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:01:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:01:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:01:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:01:17 | INFO | valid | epoch 317 | valid on 'valid' subset | loss 5.054 | nll_loss 2.43 | mask_loss 9.48984 | p_2 0.04816 | mask_ave 0.627 | ppl 5.39 | bleu 56.44 | wps 1532.1 | wpb 933.5 | bsz 59.6 | num_updates 25596 | best_bleu 57.29
2022-09-08 12:01:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 317 @ 25596 updates
2022-09-08 12:01:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint317.pt
2022-09-08 12:01:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint317.pt
2022-09-08 12:01:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint317.pt (epoch 317 @ 25596 updates, score 56.44) (writing took 23.20457300916314 seconds)
2022-09-08 12:01:41 | INFO | fairseq_cli.train | end of epoch 317 (average epoch stats below)
2022-09-08 12:01:41 | INFO | train | epoch 317 | loss 3.386 | nll_loss 0.347 | mask_loss 8.53554 | p_2 0.03507 | mask_ave 0.499 | ppl 1.27 | wps 3341.2 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 25596 | lr 0.000197658 | gnorm 0.262 | train_wall 98 | gb_free 9.1 | wall 42933
2022-09-08 12:01:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:01:41 | INFO | fairseq.trainer | begin training epoch 318
2022-09-08 12:01:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:01:47 | INFO | train_inner | epoch 318:      4 / 81 loss=3.386, nll_loss=0.347, mask_loss=8.54541, p_2=0.03502, mask_ave=0.499, ppl=1.27, wps=3207.7, ups=0.58, wpb=5499, bsz=355.8, num_updates=25600, lr=0.000197642, gnorm=0.258, train_wall=120, gb_free=9.1, wall=42940
2022-09-08 12:03:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:03:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:03:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:03:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:03:34 | INFO | valid | epoch 318 | valid on 'valid' subset | loss 5.046 | nll_loss 2.422 | mask_loss 9.49208 | p_2 0.04821 | mask_ave 0.625 | ppl 5.36 | bleu 56.11 | wps 1524 | wpb 933.5 | bsz 59.6 | num_updates 25677 | best_bleu 57.29
2022-09-08 12:03:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 318 @ 25677 updates
2022-09-08 12:03:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint318.pt
2022-09-08 12:03:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint318.pt
2022-09-08 12:03:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint318.pt (epoch 318 @ 25677 updates, score 56.11) (writing took 2.4203965701162815 seconds)
2022-09-08 12:03:36 | INFO | fairseq_cli.train | end of epoch 318 (average epoch stats below)
2022-09-08 12:03:36 | INFO | train | epoch 318 | loss 3.386 | nll_loss 0.348 | mask_loss 8.5373 | p_2 0.03497 | mask_ave 0.502 | ppl 1.27 | wps 3865.3 | ups 0.7 | wpb 5523.2 | bsz 358 | num_updates 25677 | lr 0.000197346 | gnorm 0.257 | train_wall 100 | gb_free 9.1 | wall 43049
2022-09-08 12:03:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:03:36 | INFO | fairseq.trainer | begin training epoch 319
2022-09-08 12:03:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:04:06 | INFO | train_inner | epoch 319:     23 / 81 loss=3.386, nll_loss=0.347, mask_loss=8.54281, p_2=0.0348, mask_ave=0.501, ppl=1.27, wps=3985.2, ups=0.72, wpb=5533.7, bsz=357.8, num_updates=25700, lr=0.000197257, gnorm=0.255, train_wall=124, gb_free=9, wall=43078
2022-09-08 12:05:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:05:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:05:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:05:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:05:27 | INFO | valid | epoch 319 | valid on 'valid' subset | loss 5.043 | nll_loss 2.413 | mask_loss 9.44276 | p_2 0.04842 | mask_ave 0.619 | ppl 5.33 | bleu 56.71 | wps 1504.3 | wpb 933.5 | bsz 59.6 | num_updates 25758 | best_bleu 57.29
2022-09-08 12:05:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 319 @ 25758 updates
2022-09-08 12:05:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint319.pt
2022-09-08 12:05:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint319.pt
2022-09-08 12:05:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint319.pt (epoch 319 @ 25758 updates, score 56.71) (writing took 2.574813276529312 seconds)
2022-09-08 12:05:30 | INFO | fairseq_cli.train | end of epoch 319 (average epoch stats below)
2022-09-08 12:05:30 | INFO | train | epoch 319 | loss 3.385 | nll_loss 0.347 | mask_loss 8.52858 | p_2 0.03508 | mask_ave 0.499 | ppl 1.27 | wps 3948.3 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 25758 | lr 0.000197035 | gnorm 0.259 | train_wall 98 | gb_free 9.1 | wall 43162
2022-09-08 12:05:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:05:30 | INFO | fairseq.trainer | begin training epoch 320
2022-09-08 12:05:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:06:21 | INFO | train_inner | epoch 320:     42 / 81 loss=3.385, nll_loss=0.347, mask_loss=8.5826, p_2=0.03485, mask_ave=0.497, ppl=1.27, wps=4117.1, ups=0.74, wpb=5576, bsz=359.8, num_updates=25800, lr=0.000196875, gnorm=0.261, train_wall=120, gb_free=9.2, wall=43214
2022-09-08 12:07:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:07:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:07:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:07:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:07:19 | INFO | valid | epoch 320 | valid on 'valid' subset | loss 5.043 | nll_loss 2.417 | mask_loss 9.47107 | p_2 0.04818 | mask_ave 0.626 | ppl 5.34 | bleu 56.54 | wps 1531.7 | wpb 933.5 | bsz 59.6 | num_updates 25839 | best_bleu 57.29
2022-09-08 12:07:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 320 @ 25839 updates
2022-09-08 12:07:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint320.pt
2022-09-08 12:07:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint320.pt
2022-09-08 12:07:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint320.pt (epoch 320 @ 25839 updates, score 56.54) (writing took 2.6540054492652416 seconds)
2022-09-08 12:07:22 | INFO | fairseq_cli.train | end of epoch 320 (average epoch stats below)
2022-09-08 12:07:22 | INFO | train | epoch 320 | loss 3.386 | nll_loss 0.348 | mask_loss 8.60617 | p_2 0.03503 | mask_ave 0.5 | ppl 1.27 | wps 3970.3 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 25839 | lr 0.000196726 | gnorm 0.263 | train_wall 97 | gb_free 9.1 | wall 43275
2022-09-08 12:07:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:07:22 | INFO | fairseq.trainer | begin training epoch 321
2022-09-08 12:07:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:08:37 | INFO | train_inner | epoch 321:     61 / 81 loss=3.386, nll_loss=0.348, mask_loss=8.58179, p_2=0.03506, mask_ave=0.501, ppl=1.27, wps=4034.3, ups=0.73, wpb=5497, bsz=353.9, num_updates=25900, lr=0.000196494, gnorm=0.266, train_wall=121, gb_free=9.1, wall=43350
2022-09-08 12:09:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:09:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:09:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:09:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:09:11 | INFO | valid | epoch 321 | valid on 'valid' subset | loss 5.052 | nll_loss 2.429 | mask_loss 9.63401 | p_2 0.04828 | mask_ave 0.624 | ppl 5.38 | bleu 56.3 | wps 1568.7 | wpb 933.5 | bsz 59.6 | num_updates 25920 | best_bleu 57.29
2022-09-08 12:09:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 321 @ 25920 updates
2022-09-08 12:09:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint321.pt
2022-09-08 12:09:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint321.pt
2022-09-08 12:09:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint321.pt (epoch 321 @ 25920 updates, score 56.3) (writing took 19.547905959188938 seconds)
2022-09-08 12:09:31 | INFO | fairseq_cli.train | end of epoch 321 (average epoch stats below)
2022-09-08 12:09:31 | INFO | train | epoch 321 | loss 3.386 | nll_loss 0.347 | mask_loss 8.56568 | p_2 0.035 | mask_ave 0.501 | ppl 1.27 | wps 3475.2 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 25920 | lr 0.000196419 | gnorm 0.262 | train_wall 97 | gb_free 9.1 | wall 43404
2022-09-08 12:09:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:09:31 | INFO | fairseq.trainer | begin training epoch 322
2022-09-08 12:09:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:11:09 | INFO | train_inner | epoch 322:     80 / 81 loss=3.385, nll_loss=0.347, mask_loss=8.45698, p_2=0.03526, mask_ave=0.499, ppl=1.27, wps=3638.4, ups=0.66, wpb=5528.6, bsz=363, num_updates=26000, lr=0.000196116, gnorm=0.243, train_wall=120, gb_free=9, wall=43502
2022-09-08 12:11:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:11:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:11:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:11:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:11:21 | INFO | valid | epoch 322 | valid on 'valid' subset | loss 5.053 | nll_loss 2.43 | mask_loss 9.44469 | p_2 0.04848 | mask_ave 0.618 | ppl 5.39 | bleu 56.11 | wps 1536.5 | wpb 933.5 | bsz 59.6 | num_updates 26001 | best_bleu 57.29
2022-09-08 12:11:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 322 @ 26001 updates
2022-09-08 12:11:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint322.pt
2022-09-08 12:11:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint322.pt
2022-09-08 12:11:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint322.pt (epoch 322 @ 26001 updates, score 56.11) (writing took 18.81370620802045 seconds)
2022-09-08 12:11:40 | INFO | fairseq_cli.train | end of epoch 322 (average epoch stats below)
2022-09-08 12:11:40 | INFO | train | epoch 322 | loss 3.385 | nll_loss 0.347 | mask_loss 8.45683 | p_2 0.03509 | mask_ave 0.498 | ppl 1.27 | wps 3468.4 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 26001 | lr 0.000196112 | gnorm 0.243 | train_wall 97 | gb_free 9.4 | wall 43533
2022-09-08 12:11:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:11:40 | INFO | fairseq.trainer | begin training epoch 323
2022-09-08 12:11:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:13:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:13:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:13:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:13:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:13:31 | INFO | valid | epoch 323 | valid on 'valid' subset | loss 5.054 | nll_loss 2.43 | mask_loss 9.5887 | p_2 0.04846 | mask_ave 0.619 | ppl 5.39 | bleu 56.16 | wps 1464.7 | wpb 933.5 | bsz 59.6 | num_updates 26082 | best_bleu 57.29
2022-09-08 12:13:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 323 @ 26082 updates
2022-09-08 12:13:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint323.pt
2022-09-08 12:13:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint323.pt
2022-09-08 12:13:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint323.pt (epoch 323 @ 26082 updates, score 56.16) (writing took 21.14055335149169 seconds)
2022-09-08 12:13:52 | INFO | fairseq_cli.train | end of epoch 323 (average epoch stats below)
2022-09-08 12:13:52 | INFO | train | epoch 323 | loss 3.386 | nll_loss 0.347 | mask_loss 8.51898 | p_2 0.03511 | mask_ave 0.498 | ppl 1.27 | wps 3381.3 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 26082 | lr 0.000195808 | gnorm 0.26 | train_wall 98 | gb_free 9.2 | wall 43665
2022-09-08 12:13:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:13:52 | INFO | fairseq.trainer | begin training epoch 324
2022-09-08 12:13:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:14:15 | INFO | train_inner | epoch 324:     18 / 81 loss=3.386, nll_loss=0.348, mask_loss=8.51226, p_2=0.03509, mask_ave=0.498, ppl=1.27, wps=2963.3, ups=0.54, wpb=5497.6, bsz=355.8, num_updates=26100, lr=0.00019574, gnorm=0.274, train_wall=119, gb_free=9.1, wall=43688
2022-09-08 12:15:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:15:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:15:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:15:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:15:42 | INFO | valid | epoch 324 | valid on 'valid' subset | loss 5.046 | nll_loss 2.419 | mask_loss 9.13902 | p_2 0.04878 | mask_ave 0.608 | ppl 5.35 | bleu 56.2 | wps 1516.3 | wpb 933.5 | bsz 59.6 | num_updates 26163 | best_bleu 57.29
2022-09-08 12:15:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 324 @ 26163 updates
2022-09-08 12:15:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint324.pt
2022-09-08 12:15:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint324.pt
2022-09-08 12:16:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint324.pt (epoch 324 @ 26163 updates, score 56.2) (writing took 18.642290733754635 seconds)
2022-09-08 12:16:01 | INFO | fairseq_cli.train | end of epoch 324 (average epoch stats below)
2022-09-08 12:16:01 | INFO | train | epoch 324 | loss 3.386 | nll_loss 0.348 | mask_loss 8.49311 | p_2 0.03505 | mask_ave 0.5 | ppl 1.27 | wps 3480.6 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 26163 | lr 0.000195504 | gnorm 0.279 | train_wall 97 | gb_free 9.1 | wall 43794
2022-09-08 12:16:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:16:01 | INFO | fairseq.trainer | begin training epoch 325
2022-09-08 12:16:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:16:45 | INFO | train_inner | epoch 325:     37 / 81 loss=3.385, nll_loss=0.347, mask_loss=8.44627, p_2=0.03532, mask_ave=0.498, ppl=1.27, wps=3664.4, ups=0.66, wpb=5514.7, bsz=357.4, num_updates=26200, lr=0.000195366, gnorm=0.26, train_wall=119, gb_free=9, wall=43838
2022-09-08 12:17:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:17:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:17:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:17:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:17:49 | INFO | valid | epoch 325 | valid on 'valid' subset | loss 5.061 | nll_loss 2.435 | mask_loss 9.58894 | p_2 0.04854 | mask_ave 0.617 | ppl 5.41 | bleu 56.02 | wps 1556.3 | wpb 933.5 | bsz 59.6 | num_updates 26244 | best_bleu 57.29
2022-09-08 12:17:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 325 @ 26244 updates
2022-09-08 12:17:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint325.pt
2022-09-08 12:17:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint325.pt
2022-09-08 12:18:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint325.pt (epoch 325 @ 26244 updates, score 56.02) (writing took 20.288971301168203 seconds)
2022-09-08 12:18:09 | INFO | fairseq_cli.train | end of epoch 325 (average epoch stats below)
2022-09-08 12:18:09 | INFO | train | epoch 325 | loss 3.385 | nll_loss 0.347 | mask_loss 8.52102 | p_2 0.03528 | mask_ave 0.493 | ppl 1.27 | wps 3480.7 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 26244 | lr 0.000195202 | gnorm 0.257 | train_wall 96 | gb_free 9.2 | wall 43922
2022-09-08 12:18:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:18:10 | INFO | fairseq.trainer | begin training epoch 326
2022-09-08 12:18:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:19:20 | INFO | train_inner | epoch 326:     56 / 81 loss=3.385, nll_loss=0.347, mask_loss=8.63761, p_2=0.03519, mask_ave=0.493, ppl=1.27, wps=3590.2, ups=0.65, wpb=5556.6, bsz=364.2, num_updates=26300, lr=0.000194994, gnorm=0.257, train_wall=122, gb_free=9, wall=43993
2022-09-08 12:19:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:19:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:19:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:19:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:20:01 | INFO | valid | epoch 326 | valid on 'valid' subset | loss 5.047 | nll_loss 2.428 | mask_loss 9.45387 | p_2 0.04863 | mask_ave 0.614 | ppl 5.38 | bleu 56.74 | wps 1526.6 | wpb 933.5 | bsz 59.6 | num_updates 26325 | best_bleu 57.29
2022-09-08 12:20:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 326 @ 26325 updates
2022-09-08 12:20:01 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint326.pt
2022-09-08 12:20:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint326.pt
2022-09-08 12:20:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint326.pt (epoch 326 @ 26325 updates, score 56.74) (writing took 18.184713788330555 seconds)
2022-09-08 12:20:20 | INFO | fairseq_cli.train | end of epoch 326 (average epoch stats below)
2022-09-08 12:20:20 | INFO | train | epoch 326 | loss 3.385 | nll_loss 0.347 | mask_loss 8.64437 | p_2 0.03524 | mask_ave 0.494 | ppl 1.27 | wps 3432.5 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 26325 | lr 0.000194902 | gnorm 0.251 | train_wall 99 | gb_free 9.2 | wall 44053
2022-09-08 12:20:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:20:20 | INFO | fairseq.trainer | begin training epoch 327
2022-09-08 12:20:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:21:54 | INFO | train_inner | epoch 327:     75 / 81 loss=3.385, nll_loss=0.346, mask_loss=8.58311, p_2=0.0351, mask_ave=0.495, ppl=1.27, wps=3583.7, ups=0.65, wpb=5504.1, bsz=351.7, num_updates=26400, lr=0.000194625, gnorm=0.246, train_wall=122, gb_free=9, wall=44147
2022-09-08 12:22:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:22:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:22:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:22:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:22:12 | INFO | valid | epoch 327 | valid on 'valid' subset | loss 5.048 | nll_loss 2.416 | mask_loss 9.4363 | p_2 0.0487 | mask_ave 0.612 | ppl 5.34 | bleu 56.66 | wps 1499.5 | wpb 933.5 | bsz 59.6 | num_updates 26406 | best_bleu 57.29
2022-09-08 12:22:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 327 @ 26406 updates
2022-09-08 12:22:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint327.pt
2022-09-08 12:22:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint327.pt
2022-09-08 12:22:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint327.pt (epoch 327 @ 26406 updates, score 56.66) (writing took 21.33338985592127 seconds)
2022-09-08 12:22:33 | INFO | fairseq_cli.train | end of epoch 327 (average epoch stats below)
2022-09-08 12:22:33 | INFO | train | epoch 327 | loss 3.384 | nll_loss 0.346 | mask_loss 8.53706 | p_2 0.03521 | mask_ave 0.495 | ppl 1.27 | wps 3356.5 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 26406 | lr 0.000194603 | gnorm 0.248 | train_wall 99 | gb_free 9 | wall 44186
2022-09-08 12:22:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:22:33 | INFO | fairseq.trainer | begin training epoch 328
2022-09-08 12:22:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:24:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:24:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:24:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:24:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:24:22 | INFO | valid | epoch 328 | valid on 'valid' subset | loss 5.054 | nll_loss 2.434 | mask_loss 9.44972 | p_2 0.0486 | mask_ave 0.614 | ppl 5.41 | bleu 56.45 | wps 1454.1 | wpb 933.5 | bsz 59.6 | num_updates 26487 | best_bleu 57.29
2022-09-08 12:24:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 328 @ 26487 updates
2022-09-08 12:24:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint328.pt
2022-09-08 12:24:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint328.pt
2022-09-08 12:24:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint328.pt (epoch 328 @ 26487 updates, score 56.45) (writing took 18.041770227253437 seconds)
2022-09-08 12:24:40 | INFO | fairseq_cli.train | end of epoch 328 (average epoch stats below)
2022-09-08 12:24:40 | INFO | train | epoch 328 | loss 3.385 | nll_loss 0.347 | mask_loss 8.48982 | p_2 0.03527 | mask_ave 0.493 | ppl 1.27 | wps 3522 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 26487 | lr 0.000194305 | gnorm 0.264 | train_wall 96 | gb_free 9.1 | wall 44313
2022-09-08 12:24:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:24:40 | INFO | fairseq.trainer | begin training epoch 329
2022-09-08 12:24:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:24:56 | INFO | train_inner | epoch 329:     13 / 81 loss=3.385, nll_loss=0.347, mask_loss=8.48244, p_2=0.03548, mask_ave=0.494, ppl=1.27, wps=3013.7, ups=0.55, wpb=5493.8, bsz=360.6, num_updates=26500, lr=0.000194257, gnorm=0.262, train_wall=117, gb_free=9, wall=44329
2022-09-08 12:26:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:26:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:26:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:26:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:26:46 | INFO | valid | epoch 329 | valid on 'valid' subset | loss 5.051 | nll_loss 2.427 | mask_loss 9.40247 | p_2 0.04857 | mask_ave 0.615 | ppl 5.38 | bleu 56.74 | wps 1595.3 | wpb 933.5 | bsz 59.6 | num_updates 26568 | best_bleu 57.29
2022-09-08 12:26:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 329 @ 26568 updates
2022-09-08 12:26:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint329.pt
2022-09-08 12:26:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint329.pt
2022-09-08 12:27:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint329.pt (epoch 329 @ 26568 updates, score 56.74) (writing took 20.728947322815657 seconds)
2022-09-08 12:27:07 | INFO | fairseq_cli.train | end of epoch 329 (average epoch stats below)
2022-09-08 12:27:07 | INFO | train | epoch 329 | loss 3.385 | nll_loss 0.347 | mask_loss 8.59685 | p_2 0.03509 | mask_ave 0.498 | ppl 1.27 | wps 3045.8 | ups 0.55 | wpb 5523.2 | bsz 358 | num_updates 26568 | lr 0.000194008 | gnorm 0.26 | train_wall 114 | gb_free 9.1 | wall 44460
2022-09-08 12:27:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:27:07 | INFO | fairseq.trainer | begin training epoch 330
2022-09-08 12:27:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:27:46 | INFO | train_inner | epoch 330:     32 / 81 loss=3.385, nll_loss=0.347, mask_loss=8.59033, p_2=0.03507, mask_ave=0.498, ppl=1.27, wps=3249.3, ups=0.59, wpb=5522.1, bsz=359.3, num_updates=26600, lr=0.000193892, gnorm=0.258, train_wall=137, gb_free=9.1, wall=44499
2022-09-08 12:28:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:28:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:28:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:28:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:28:57 | INFO | valid | epoch 330 | valid on 'valid' subset | loss 5.045 | nll_loss 2.419 | mask_loss 9.56125 | p_2 0.04851 | mask_ave 0.616 | ppl 5.35 | bleu 56.74 | wps 1485.8 | wpb 933.5 | bsz 59.6 | num_updates 26649 | best_bleu 57.29
2022-09-08 12:28:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 330 @ 26649 updates
2022-09-08 12:28:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint330.pt
2022-09-08 12:28:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint330.pt
2022-09-08 12:29:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint330.pt (epoch 330 @ 26649 updates, score 56.74) (writing took 25.738940328359604 seconds)
2022-09-08 12:29:23 | INFO | fairseq_cli.train | end of epoch 330 (average epoch stats below)
2022-09-08 12:29:23 | INFO | train | epoch 330 | loss 3.385 | nll_loss 0.347 | mask_loss 8.58414 | p_2 0.03515 | mask_ave 0.497 | ppl 1.27 | wps 3293.5 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 26649 | lr 0.000193713 | gnorm 0.259 | train_wall 97 | gb_free 9.2 | wall 44596
2022-09-08 12:29:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:29:23 | INFO | fairseq.trainer | begin training epoch 331
2022-09-08 12:29:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:30:26 | INFO | train_inner | epoch 331:     51 / 81 loss=3.385, nll_loss=0.347, mask_loss=8.66989, p_2=0.03491, mask_ave=0.496, ppl=1.27, wps=3458.1, ups=0.62, wpb=5540.7, bsz=351, num_updates=26700, lr=0.000193528, gnorm=0.257, train_wall=121, gb_free=9.2, wall=44659
2022-09-08 12:31:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:31:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:31:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:31:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:31:14 | INFO | valid | epoch 331 | valid on 'valid' subset | loss 5.062 | nll_loss 2.434 | mask_loss 9.54401 | p_2 0.04824 | mask_ave 0.625 | ppl 5.4 | bleu 55.5 | wps 1538.1 | wpb 933.5 | bsz 59.6 | num_updates 26730 | best_bleu 57.29
2022-09-08 12:31:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 331 @ 26730 updates
2022-09-08 12:31:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint331.pt
2022-09-08 12:31:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint331.pt
2022-09-08 12:31:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint331.pt (epoch 331 @ 26730 updates, score 55.5) (writing took 22.977607551962137 seconds)
2022-09-08 12:31:37 | INFO | fairseq_cli.train | end of epoch 331 (average epoch stats below)
2022-09-08 12:31:37 | INFO | train | epoch 331 | loss 3.384 | nll_loss 0.346 | mask_loss 8.67662 | p_2 0.03513 | mask_ave 0.497 | ppl 1.27 | wps 3338.5 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 26730 | lr 0.00019342 | gnorm 0.25 | train_wall 98 | gb_free 9 | wall 44730
2022-09-08 12:31:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:31:37 | INFO | fairseq.trainer | begin training epoch 332
2022-09-08 12:31:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:33:02 | INFO | train_inner | epoch 332:     70 / 81 loss=3.384, nll_loss=0.346, mask_loss=8.57579, p_2=0.03506, mask_ave=0.498, ppl=1.27, wps=3565.4, ups=0.64, wpb=5564.7, bsz=363.4, num_updates=26800, lr=0.000193167, gnorm=0.249, train_wall=120, gb_free=9, wall=44815
2022-09-08 12:33:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:33:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:33:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:33:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:33:26 | INFO | valid | epoch 332 | valid on 'valid' subset | loss 5.053 | nll_loss 2.427 | mask_loss 9.53725 | p_2 0.04841 | mask_ave 0.619 | ppl 5.38 | bleu 56.09 | wps 1540.9 | wpb 933.5 | bsz 59.6 | num_updates 26811 | best_bleu 57.29
2022-09-08 12:33:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 332 @ 26811 updates
2022-09-08 12:33:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint332.pt
2022-09-08 12:33:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint332.pt
2022-09-08 12:33:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint332.pt (epoch 332 @ 26811 updates, score 56.09) (writing took 20.13025861606002 seconds)
2022-09-08 12:33:47 | INFO | fairseq_cli.train | end of epoch 332 (average epoch stats below)
2022-09-08 12:33:47 | INFO | train | epoch 332 | loss 3.384 | nll_loss 0.346 | mask_loss 8.5564 | p_2 0.03506 | mask_ave 0.499 | ppl 1.27 | wps 3449.5 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 26811 | lr 0.000193127 | gnorm 0.253 | train_wall 97 | gb_free 9 | wall 44859
2022-09-08 12:33:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:33:47 | INFO | fairseq.trainer | begin training epoch 333
2022-09-08 12:33:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:35:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:35:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:35:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:35:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:35:37 | INFO | valid | epoch 333 | valid on 'valid' subset | loss 5.058 | nll_loss 2.435 | mask_loss 9.58293 | p_2 0.04854 | mask_ave 0.617 | ppl 5.41 | bleu 55.46 | wps 1513.7 | wpb 933.5 | bsz 59.6 | num_updates 26892 | best_bleu 57.29
2022-09-08 12:35:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 333 @ 26892 updates
2022-09-08 12:35:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint333.pt
2022-09-08 12:35:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint333.pt
2022-09-08 12:35:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint333.pt (epoch 333 @ 26892 updates, score 55.46) (writing took 21.659402154386044 seconds)
2022-09-08 12:35:58 | INFO | fairseq_cli.train | end of epoch 333 (average epoch stats below)
2022-09-08 12:35:58 | INFO | train | epoch 333 | loss 3.385 | nll_loss 0.347 | mask_loss 8.65896 | p_2 0.03519 | mask_ave 0.495 | ppl 1.27 | wps 3392.9 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 26892 | lr 0.000192836 | gnorm 0.255 | train_wall 97 | gb_free 9.1 | wall 44991
2022-09-08 12:35:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:35:59 | INFO | fairseq.trainer | begin training epoch 334
2022-09-08 12:35:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:36:09 | INFO | train_inner | epoch 334:      8 / 81 loss=3.384, nll_loss=0.347, mask_loss=8.65049, p_2=0.03528, mask_ave=0.496, ppl=1.27, wps=2937.3, ups=0.54, wpb=5473.9, bsz=355.2, num_updates=26900, lr=0.000192807, gnorm=0.257, train_wall=119, gb_free=9.1, wall=45001
2022-09-08 12:37:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:37:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:37:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:37:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:37:48 | INFO | valid | epoch 334 | valid on 'valid' subset | loss 5.048 | nll_loss 2.425 | mask_loss 9.6645 | p_2 0.04862 | mask_ave 0.613 | ppl 5.37 | bleu 55.71 | wps 1510.4 | wpb 933.5 | bsz 59.6 | num_updates 26973 | best_bleu 57.29
2022-09-08 12:37:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 334 @ 26973 updates
2022-09-08 12:37:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint334.pt
2022-09-08 12:37:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint334.pt
2022-09-08 12:38:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint334.pt (epoch 334 @ 26973 updates, score 55.71) (writing took 21.824089013040066 seconds)
2022-09-08 12:38:10 | INFO | fairseq_cli.train | end of epoch 334 (average epoch stats below)
2022-09-08 12:38:10 | INFO | train | epoch 334 | loss 3.384 | nll_loss 0.346 | mask_loss 8.59691 | p_2 0.03524 | mask_ave 0.494 | ppl 1.27 | wps 3401.5 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 26973 | lr 0.000192546 | gnorm 0.252 | train_wall 97 | gb_free 9.2 | wall 45123
2022-09-08 12:38:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:38:10 | INFO | fairseq.trainer | begin training epoch 335
2022-09-08 12:38:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:38:43 | INFO | train_inner | epoch 335:     27 / 81 loss=3.384, nll_loss=0.346, mask_loss=8.6168, p_2=0.03511, mask_ave=0.493, ppl=1.27, wps=3588.6, ups=0.65, wpb=5546.1, bsz=358.5, num_updates=27000, lr=0.00019245, gnorm=0.25, train_wall=119, gb_free=9.1, wall=45156
2022-09-08 12:39:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:39:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:39:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:39:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:40:00 | INFO | valid | epoch 335 | valid on 'valid' subset | loss 5.062 | nll_loss 2.445 | mask_loss 9.56689 | p_2 0.04881 | mask_ave 0.608 | ppl 5.44 | bleu 56.26 | wps 1521.5 | wpb 933.5 | bsz 59.6 | num_updates 27054 | best_bleu 57.29
2022-09-08 12:40:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 335 @ 27054 updates
2022-09-08 12:40:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint335.pt
2022-09-08 12:40:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint335.pt
2022-09-08 12:40:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint335.pt (epoch 335 @ 27054 updates, score 56.26) (writing took 2.510686546564102 seconds)
2022-09-08 12:40:03 | INFO | fairseq_cli.train | end of epoch 335 (average epoch stats below)
2022-09-08 12:40:03 | INFO | train | epoch 335 | loss 3.384 | nll_loss 0.346 | mask_loss 8.72113 | p_2 0.03519 | mask_ave 0.496 | ppl 1.27 | wps 3955.8 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 27054 | lr 0.000192258 | gnorm 0.258 | train_wall 97 | gb_free 9.1 | wall 45236
2022-09-08 12:40:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:40:03 | INFO | fairseq.trainer | begin training epoch 336
2022-09-08 12:40:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:41:01 | INFO | train_inner | epoch 336:     46 / 81 loss=3.385, nll_loss=0.347, mask_loss=8.70079, p_2=0.03545, mask_ave=0.499, ppl=1.27, wps=4007.7, ups=0.73, wpb=5507.4, bsz=357.8, num_updates=27100, lr=0.000192095, gnorm=0.262, train_wall=122, gb_free=9, wall=45293
2022-09-08 12:41:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:41:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:41:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:41:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:41:55 | INFO | valid | epoch 336 | valid on 'valid' subset | loss 5.058 | nll_loss 2.435 | mask_loss 9.48647 | p_2 0.04861 | mask_ave 0.614 | ppl 5.41 | bleu 56.06 | wps 1514.8 | wpb 933.5 | bsz 59.6 | num_updates 27135 | best_bleu 57.29
2022-09-08 12:41:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 336 @ 27135 updates
2022-09-08 12:41:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint336.pt
2022-09-08 12:41:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint336.pt
2022-09-08 12:42:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint336.pt (epoch 336 @ 27135 updates, score 56.06) (writing took 21.163300026208162 seconds)
2022-09-08 12:42:16 | INFO | fairseq_cli.train | end of epoch 336 (average epoch stats below)
2022-09-08 12:42:16 | INFO | train | epoch 336 | loss 3.385 | nll_loss 0.347 | mask_loss 8.70987 | p_2 0.03511 | mask_ave 0.498 | ppl 1.27 | wps 3357.3 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 27135 | lr 0.000191971 | gnorm 0.253 | train_wall 99 | gb_free 9.1 | wall 45369
2022-09-08 12:42:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:42:16 | INFO | fairseq.trainer | begin training epoch 337
2022-09-08 12:42:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:43:37 | INFO | train_inner | epoch 337:     65 / 81 loss=3.384, nll_loss=0.347, mask_loss=8.65649, p_2=0.03489, mask_ave=0.497, ppl=1.27, wps=3551.2, ups=0.64, wpb=5552.6, bsz=360.6, num_updates=27200, lr=0.000191741, gnorm=0.249, train_wall=122, gb_free=9, wall=45450
2022-09-08 12:43:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:43:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:43:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:43:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:44:07 | INFO | valid | epoch 337 | valid on 'valid' subset | loss 5.048 | nll_loss 2.428 | mask_loss 9.31429 | p_2 0.04859 | mask_ave 0.614 | ppl 5.38 | bleu 55.29 | wps 1519.4 | wpb 933.5 | bsz 59.6 | num_updates 27216 | best_bleu 57.29
2022-09-08 12:44:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 337 @ 27216 updates
2022-09-08 12:44:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint337.pt
2022-09-08 12:44:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint337.pt
2022-09-08 12:44:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint337.pt (epoch 337 @ 27216 updates, score 55.29) (writing took 17.40904337912798 seconds)
2022-09-08 12:44:24 | INFO | fairseq_cli.train | end of epoch 337 (average epoch stats below)
2022-09-08 12:44:24 | INFO | train | epoch 337 | loss 3.384 | nll_loss 0.346 | mask_loss 8.59069 | p_2 0.0351 | mask_ave 0.498 | ppl 1.27 | wps 3490.3 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 27216 | lr 0.000191685 | gnorm 0.25 | train_wall 98 | gb_free 9.2 | wall 45497
2022-09-08 12:44:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:44:25 | INFO | fairseq.trainer | begin training epoch 338
2022-09-08 12:44:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:46:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:46:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:46:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:46:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:46:15 | INFO | valid | epoch 338 | valid on 'valid' subset | loss 5.039 | nll_loss 2.411 | mask_loss 9.39017 | p_2 0.04841 | mask_ave 0.619 | ppl 5.32 | bleu 56.37 | wps 1563.4 | wpb 933.5 | bsz 59.6 | num_updates 27297 | best_bleu 57.29
2022-09-08 12:46:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 338 @ 27297 updates
2022-09-08 12:46:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint338.pt
2022-09-08 12:46:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint338.pt
2022-09-08 12:46:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint338.pt (epoch 338 @ 27297 updates, score 56.37) (writing took 23.841918244957924 seconds)
2022-09-08 12:46:39 | INFO | fairseq_cli.train | end of epoch 338 (average epoch stats below)
2022-09-08 12:46:39 | INFO | train | epoch 338 | loss 3.384 | nll_loss 0.346 | mask_loss 8.54435 | p_2 0.03512 | mask_ave 0.498 | ppl 1.27 | wps 3327.1 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 27297 | lr 0.0001914 | gnorm 0.254 | train_wall 98 | gb_free 9.2 | wall 45632
2022-09-08 12:46:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:46:39 | INFO | fairseq.trainer | begin training epoch 339
2022-09-08 12:46:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:46:44 | INFO | train_inner | epoch 339:      3 / 81 loss=3.384, nll_loss=0.346, mask_loss=8.54655, p_2=0.03512, mask_ave=0.497, ppl=1.27, wps=2944.9, ups=0.54, wpb=5495.8, bsz=355.2, num_updates=27300, lr=0.00019139, gnorm=0.255, train_wall=120, gb_free=9.2, wall=45636
2022-09-08 12:48:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:48:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:48:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:48:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:48:29 | INFO | valid | epoch 339 | valid on 'valid' subset | loss 5.047 | nll_loss 2.428 | mask_loss 9.33747 | p_2 0.04836 | mask_ave 0.621 | ppl 5.38 | bleu 56.45 | wps 1446.5 | wpb 933.5 | bsz 59.6 | num_updates 27378 | best_bleu 57.29
2022-09-08 12:48:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 339 @ 27378 updates
2022-09-08 12:48:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint339.pt
2022-09-08 12:48:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint339.pt
2022-09-08 12:48:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint339.pt (epoch 339 @ 27378 updates, score 56.45) (writing took 18.91919908300042 seconds)
2022-09-08 12:48:49 | INFO | fairseq_cli.train | end of epoch 339 (average epoch stats below)
2022-09-08 12:48:49 | INFO | train | epoch 339 | loss 3.384 | nll_loss 0.347 | mask_loss 8.51001 | p_2 0.03502 | mask_ave 0.501 | ppl 1.27 | wps 3450.4 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 27378 | lr 0.000191117 | gnorm 0.261 | train_wall 97 | gb_free 9.1 | wall 45761
2022-09-08 12:48:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:48:49 | INFO | fairseq.trainer | begin training epoch 340
2022-09-08 12:48:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:49:17 | INFO | train_inner | epoch 340:     22 / 81 loss=3.384, nll_loss=0.346, mask_loss=8.48752, p_2=0.03518, mask_ave=0.501, ppl=1.27, wps=3602.3, ups=0.65, wpb=5509.4, bsz=359.7, num_updates=27400, lr=0.00019104, gnorm=0.255, train_wall=120, gb_free=9.1, wall=45789
2022-09-08 12:50:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:50:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:50:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:50:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:50:38 | INFO | valid | epoch 340 | valid on 'valid' subset | loss 5.052 | nll_loss 2.428 | mask_loss 9.30484 | p_2 0.04869 | mask_ave 0.612 | ppl 5.38 | bleu 55.76 | wps 1543.5 | wpb 933.5 | bsz 59.6 | num_updates 27459 | best_bleu 57.29
2022-09-08 12:50:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 340 @ 27459 updates
2022-09-08 12:50:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint340.pt
2022-09-08 12:50:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint340.pt
2022-09-08 12:50:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint340.pt (epoch 340 @ 27459 updates, score 55.76) (writing took 17.280020155012608 seconds)
2022-09-08 12:50:55 | INFO | fairseq_cli.train | end of epoch 340 (average epoch stats below)
2022-09-08 12:50:55 | INFO | train | epoch 340 | loss 3.383 | nll_loss 0.345 | mask_loss 8.47673 | p_2 0.0351 | mask_ave 0.498 | ppl 1.27 | wps 3529 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 27459 | lr 0.000190835 | gnorm 0.235 | train_wall 96 | gb_free 9.1 | wall 45888
2022-09-08 12:50:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:50:56 | INFO | fairseq.trainer | begin training epoch 341
2022-09-08 12:50:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:51:46 | INFO | train_inner | epoch 341:     41 / 81 loss=3.383, nll_loss=0.345, mask_loss=8.51362, p_2=0.03516, mask_ave=0.497, ppl=1.27, wps=3703.2, ups=0.67, wpb=5521.2, bsz=359.2, num_updates=27500, lr=0.000190693, gnorm=0.235, train_wall=119, gb_free=9.1, wall=45938
2022-09-08 12:52:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:52:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:52:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:52:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:52:45 | INFO | valid | epoch 341 | valid on 'valid' subset | loss 5.058 | nll_loss 2.432 | mask_loss 9.48555 | p_2 0.04898 | mask_ave 0.603 | ppl 5.4 | bleu 56.38 | wps 1540.7 | wpb 933.5 | bsz 59.6 | num_updates 27540 | best_bleu 57.29
2022-09-08 12:52:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 341 @ 27540 updates
2022-09-08 12:52:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint341.pt
2022-09-08 12:52:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint341.pt
2022-09-08 12:53:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint341.pt (epoch 341 @ 27540 updates, score 56.38) (writing took 17.62468344718218 seconds)
2022-09-08 12:53:03 | INFO | fairseq_cli.train | end of epoch 341 (average epoch stats below)
2022-09-08 12:53:03 | INFO | train | epoch 341 | loss 3.383 | nll_loss 0.346 | mask_loss 8.54251 | p_2 0.03519 | mask_ave 0.495 | ppl 1.27 | wps 3504.8 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 27540 | lr 0.000190554 | gnorm 0.242 | train_wall 97 | gb_free 9.1 | wall 46016
2022-09-08 12:53:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:53:03 | INFO | fairseq.trainer | begin training epoch 342
2022-09-08 12:53:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:54:17 | INFO | train_inner | epoch 342:     60 / 81 loss=3.384, nll_loss=0.346, mask_loss=8.45527, p_2=0.03527, mask_ave=0.495, ppl=1.27, wps=3678.3, ups=0.66, wpb=5566.1, bsz=362.2, num_updates=27600, lr=0.000190347, gnorm=0.244, train_wall=121, gb_free=9, wall=46090
2022-09-08 12:54:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:54:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:54:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:54:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:54:53 | INFO | valid | epoch 342 | valid on 'valid' subset | loss 5.067 | nll_loss 2.443 | mask_loss 9.35607 | p_2 0.04858 | mask_ave 0.616 | ppl 5.44 | bleu 56.16 | wps 1487 | wpb 933.5 | bsz 59.6 | num_updates 27621 | best_bleu 57.29
2022-09-08 12:54:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 342 @ 27621 updates
2022-09-08 12:54:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint342.pt
2022-09-08 12:54:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint342.pt
2022-09-08 12:54:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint342.pt (epoch 342 @ 27621 updates, score 56.16) (writing took 2.5001456066966057 seconds)
2022-09-08 12:54:56 | INFO | fairseq_cli.train | end of epoch 342 (average epoch stats below)
2022-09-08 12:54:56 | INFO | train | epoch 342 | loss 3.384 | nll_loss 0.346 | mask_loss 8.44862 | p_2 0.0352 | mask_ave 0.495 | ppl 1.27 | wps 3956.2 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 27621 | lr 0.000190274 | gnorm 0.242 | train_wall 97 | gb_free 9.2 | wall 46129
2022-09-08 12:54:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:54:56 | INFO | fairseq.trainer | begin training epoch 343
2022-09-08 12:54:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:56:33 | INFO | train_inner | epoch 343:     79 / 81 loss=3.384, nll_loss=0.346, mask_loss=8.45103, p_2=0.03504, mask_ave=0.495, ppl=1.27, wps=4057.5, ups=0.74, wpb=5513.4, bsz=353, num_updates=27700, lr=0.000190003, gnorm=0.252, train_wall=120, gb_free=9.1, wall=46226
2022-09-08 12:56:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:56:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:56:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:56:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:56:46 | INFO | valid | epoch 343 | valid on 'valid' subset | loss 5.046 | nll_loss 2.425 | mask_loss 9.44397 | p_2 0.0484 | mask_ave 0.621 | ppl 5.37 | bleu 56.18 | wps 1504.1 | wpb 933.5 | bsz 59.6 | num_updates 27702 | best_bleu 57.29
2022-09-08 12:56:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 343 @ 27702 updates
2022-09-08 12:56:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint343.pt
2022-09-08 12:56:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint343.pt
2022-09-08 12:57:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint343.pt (epoch 343 @ 27702 updates, score 56.18) (writing took 23.01585327088833 seconds)
2022-09-08 12:57:09 | INFO | fairseq_cli.train | end of epoch 343 (average epoch stats below)
2022-09-08 12:57:09 | INFO | train | epoch 343 | loss 3.384 | nll_loss 0.346 | mask_loss 8.40389 | p_2 0.03523 | mask_ave 0.495 | ppl 1.27 | wps 3360.7 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 27702 | lr 0.000189996 | gnorm 0.255 | train_wall 97 | gb_free 9.1 | wall 46262
2022-09-08 12:57:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:57:09 | INFO | fairseq.trainer | begin training epoch 344
2022-09-08 12:57:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:58:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 12:58:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 12:58:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 12:58:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 12:58:59 | INFO | valid | epoch 344 | valid on 'valid' subset | loss 5.044 | nll_loss 2.419 | mask_loss 9.27603 | p_2 0.04846 | mask_ave 0.619 | ppl 5.35 | bleu 56.15 | wps 1516.4 | wpb 933.5 | bsz 59.6 | num_updates 27783 | best_bleu 57.29
2022-09-08 12:58:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 344 @ 27783 updates
2022-09-08 12:58:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint344.pt
2022-09-08 12:59:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint344.pt
2022-09-08 12:59:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint344.pt (epoch 344 @ 27783 updates, score 56.15) (writing took 17.328883338719606 seconds)
2022-09-08 12:59:17 | INFO | fairseq_cli.train | end of epoch 344 (average epoch stats below)
2022-09-08 12:59:17 | INFO | train | epoch 344 | loss 3.384 | nll_loss 0.347 | mask_loss 8.54999 | p_2 0.03504 | mask_ave 0.5 | ppl 1.27 | wps 3509.5 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 27783 | lr 0.000189719 | gnorm 0.279 | train_wall 97 | gb_free 9 | wall 46390
2022-09-08 12:59:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 12:59:17 | INFO | fairseq.trainer | begin training epoch 345
2022-09-08 12:59:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 12:59:38 | INFO | train_inner | epoch 345:     17 / 81 loss=3.384, nll_loss=0.346, mask_loss=8.53972, p_2=0.03499, mask_ave=0.5, ppl=1.27, wps=2983, ups=0.54, wpb=5511.3, bsz=356.4, num_updates=27800, lr=0.000189661, gnorm=0.273, train_wall=119, gb_free=9.2, wall=46410
2022-09-08 13:00:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:00:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:00:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:00:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:01:07 | INFO | valid | epoch 345 | valid on 'valid' subset | loss 5.061 | nll_loss 2.44 | mask_loss 9.16435 | p_2 0.0486 | mask_ave 0.615 | ppl 5.43 | bleu 56.14 | wps 1553.4 | wpb 933.5 | bsz 59.6 | num_updates 27864 | best_bleu 57.29
2022-09-08 13:01:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 345 @ 27864 updates
2022-09-08 13:01:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint345.pt
2022-09-08 13:01:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint345.pt
2022-09-08 13:01:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint345.pt (epoch 345 @ 27864 updates, score 56.14) (writing took 18.143537759780884 seconds)
2022-09-08 13:01:25 | INFO | fairseq_cli.train | end of epoch 345 (average epoch stats below)
2022-09-08 13:01:25 | INFO | train | epoch 345 | loss 3.383 | nll_loss 0.345 | mask_loss 8.44845 | p_2 0.0351 | mask_ave 0.498 | ppl 1.27 | wps 3478.7 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 27864 | lr 0.000189443 | gnorm 0.244 | train_wall 98 | gb_free 9.1 | wall 46518
2022-09-08 13:01:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:01:26 | INFO | fairseq.trainer | begin training epoch 346
2022-09-08 13:01:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:02:10 | INFO | train_inner | epoch 346:     36 / 81 loss=3.383, nll_loss=0.346, mask_loss=8.4107, p_2=0.03525, mask_ave=0.496, ppl=1.27, wps=3635.4, ups=0.66, wpb=5545.3, bsz=362.1, num_updates=27900, lr=0.000189321, gnorm=0.247, train_wall=122, gb_free=9, wall=46563
2022-09-08 13:03:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:03:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:03:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:03:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:03:15 | INFO | valid | epoch 346 | valid on 'valid' subset | loss 5.051 | nll_loss 2.425 | mask_loss 9.53224 | p_2 0.04871 | mask_ave 0.612 | ppl 5.37 | bleu 56.43 | wps 1488.4 | wpb 933.5 | bsz 59.6 | num_updates 27945 | best_bleu 57.29
2022-09-08 13:03:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 346 @ 27945 updates
2022-09-08 13:03:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint346.pt
2022-09-08 13:03:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint346.pt
2022-09-08 13:03:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint346.pt (epoch 346 @ 27945 updates, score 56.43) (writing took 20.372042383998632 seconds)
2022-09-08 13:03:36 | INFO | fairseq_cli.train | end of epoch 346 (average epoch stats below)
2022-09-08 13:03:36 | INFO | train | epoch 346 | loss 3.383 | nll_loss 0.346 | mask_loss 8.54183 | p_2 0.03528 | mask_ave 0.493 | ppl 1.27 | wps 3425.2 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 27945 | lr 0.000189168 | gnorm 0.262 | train_wall 97 | gb_free 9.2 | wall 46649
2022-09-08 13:03:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:03:36 | INFO | fairseq.trainer | begin training epoch 347
2022-09-08 13:03:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:04:44 | INFO | train_inner | epoch 347:     55 / 81 loss=3.384, nll_loss=0.346, mask_loss=8.57741, p_2=0.03521, mask_ave=0.495, ppl=1.27, wps=3582.2, ups=0.65, wpb=5509.3, bsz=354.3, num_updates=28000, lr=0.000188982, gnorm=0.259, train_wall=120, gb_free=9.1, wall=46717
2022-09-08 13:05:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:05:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:05:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:05:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:05:26 | INFO | valid | epoch 347 | valid on 'valid' subset | loss 5.044 | nll_loss 2.414 | mask_loss 9.43258 | p_2 0.04861 | mask_ave 0.615 | ppl 5.33 | bleu 56.27 | wps 1515.7 | wpb 933.5 | bsz 59.6 | num_updates 28026 | best_bleu 57.29
2022-09-08 13:05:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 347 @ 28026 updates
2022-09-08 13:05:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint347.pt
2022-09-08 13:05:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint347.pt
2022-09-08 13:05:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint347.pt (epoch 347 @ 28026 updates, score 56.27) (writing took 2.5543130300939083 seconds)
2022-09-08 13:05:29 | INFO | fairseq_cli.train | end of epoch 347 (average epoch stats below)
2022-09-08 13:05:29 | INFO | train | epoch 347 | loss 3.383 | nll_loss 0.346 | mask_loss 8.49418 | p_2 0.03517 | mask_ave 0.497 | ppl 1.27 | wps 3955.7 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 28026 | lr 0.000188895 | gnorm 0.25 | train_wall 97 | gb_free 9.2 | wall 46762
2022-09-08 13:05:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:05:29 | INFO | fairseq.trainer | begin training epoch 348
2022-09-08 13:05:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:07:00 | INFO | train_inner | epoch 348:     74 / 81 loss=3.383, nll_loss=0.346, mask_loss=8.57696, p_2=0.03509, mask_ave=0.499, ppl=1.27, wps=4059.9, ups=0.74, wpb=5520.2, bsz=358.3, num_updates=28100, lr=0.000188646, gnorm=0.258, train_wall=120, gb_free=9, wall=46853
2022-09-08 13:07:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:07:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:07:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:07:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:07:19 | INFO | valid | epoch 348 | valid on 'valid' subset | loss 5.046 | nll_loss 2.423 | mask_loss 9.47527 | p_2 0.04878 | mask_ave 0.61 | ppl 5.36 | bleu 56.33 | wps 1511.2 | wpb 933.5 | bsz 59.6 | num_updates 28107 | best_bleu 57.29
2022-09-08 13:07:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 348 @ 28107 updates
2022-09-08 13:07:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint348.pt
2022-09-08 13:07:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint348.pt
2022-09-08 13:07:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint348.pt (epoch 348 @ 28107 updates, score 56.33) (writing took 20.130665600299835 seconds)
2022-09-08 13:07:40 | INFO | fairseq_cli.train | end of epoch 348 (average epoch stats below)
2022-09-08 13:07:40 | INFO | train | epoch 348 | loss 3.383 | nll_loss 0.346 | mask_loss 8.58764 | p_2 0.03511 | mask_ave 0.498 | ppl 1.27 | wps 3425.7 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 28107 | lr 0.000188622 | gnorm 0.263 | train_wall 97 | gb_free 9 | wall 46893
2022-09-08 13:07:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:07:40 | INFO | fairseq.trainer | begin training epoch 349
2022-09-08 13:07:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:09:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:09:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:09:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:09:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:09:29 | INFO | valid | epoch 349 | valid on 'valid' subset | loss 5.035 | nll_loss 2.405 | mask_loss 9.43839 | p_2 0.04827 | mask_ave 0.625 | ppl 5.3 | bleu 55.96 | wps 1495.9 | wpb 933.5 | bsz 59.6 | num_updates 28188 | best_bleu 57.29
2022-09-08 13:09:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 349 @ 28188 updates
2022-09-08 13:09:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint349.pt
2022-09-08 13:09:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint349.pt
2022-09-08 13:09:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint349.pt (epoch 349 @ 28188 updates, score 55.96) (writing took 2.5150380320847034 seconds)
2022-09-08 13:09:31 | INFO | fairseq_cli.train | end of epoch 349 (average epoch stats below)
2022-09-08 13:09:31 | INFO | train | epoch 349 | loss 3.384 | nll_loss 0.346 | mask_loss 8.54894 | p_2 0.03506 | mask_ave 0.5 | ppl 1.27 | wps 4006.5 | ups 0.73 | wpb 5523.2 | bsz 358 | num_updates 28188 | lr 0.000188351 | gnorm 0.274 | train_wall 96 | gb_free 9.2 | wall 47004
2022-09-08 13:09:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:09:31 | INFO | fairseq.trainer | begin training epoch 350
2022-09-08 13:09:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:09:47 | INFO | train_inner | epoch 350:     12 / 81 loss=3.383, nll_loss=0.346, mask_loss=8.54419, p_2=0.03515, mask_ave=0.5, ppl=1.27, wps=3306, ups=0.6, wpb=5511.6, bsz=357.9, num_updates=28200, lr=0.000188311, gnorm=0.272, train_wall=118, gb_free=9.1, wall=47019
2022-09-08 13:11:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:11:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:11:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:11:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:11:21 | INFO | valid | epoch 350 | valid on 'valid' subset | loss 5.056 | nll_loss 2.434 | mask_loss 9.39755 | p_2 0.04862 | mask_ave 0.615 | ppl 5.4 | bleu 56.67 | wps 1542.7 | wpb 933.5 | bsz 59.6 | num_updates 28269 | best_bleu 57.29
2022-09-08 13:11:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 350 @ 28269 updates
2022-09-08 13:11:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint350.pt
2022-09-08 13:11:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint350.pt
2022-09-08 13:11:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint350.pt (epoch 350 @ 28269 updates, score 56.67) (writing took 22.246302645653486 seconds)
2022-09-08 13:11:44 | INFO | fairseq_cli.train | end of epoch 350 (average epoch stats below)
2022-09-08 13:11:44 | INFO | train | epoch 350 | loss 3.383 | nll_loss 0.346 | mask_loss 8.54909 | p_2 0.03511 | mask_ave 0.498 | ppl 1.27 | wps 3378.8 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 28269 | lr 0.000188081 | gnorm 0.275 | train_wall 97 | gb_free 9.1 | wall 47137
2022-09-08 13:11:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:11:44 | INFO | fairseq.trainer | begin training epoch 351
2022-09-08 13:11:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:12:22 | INFO | train_inner | epoch 351:     31 / 81 loss=3.383, nll_loss=0.345, mask_loss=8.51516, p_2=0.03515, mask_ave=0.497, ppl=1.27, wps=3564.1, ups=0.65, wpb=5522.8, bsz=361.3, num_updates=28300, lr=0.000187978, gnorm=0.273, train_wall=120, gb_free=9, wall=47174
2022-09-08 13:13:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:13:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:13:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:13:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:13:33 | INFO | valid | epoch 351 | valid on 'valid' subset | loss 5.05 | nll_loss 2.433 | mask_loss 9.3592 | p_2 0.04874 | mask_ave 0.61 | ppl 5.4 | bleu 56.38 | wps 1513.7 | wpb 933.5 | bsz 59.6 | num_updates 28350 | best_bleu 57.29
2022-09-08 13:13:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 351 @ 28350 updates
2022-09-08 13:13:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint351.pt
2022-09-08 13:13:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint351.pt
2022-09-08 13:13:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint351.pt (epoch 351 @ 28350 updates, score 56.38) (writing took 20.470920886844397 seconds)
2022-09-08 13:13:54 | INFO | fairseq_cli.train | end of epoch 351 (average epoch stats below)
2022-09-08 13:13:54 | INFO | train | epoch 351 | loss 3.383 | nll_loss 0.345 | mask_loss 8.50285 | p_2 0.03518 | mask_ave 0.496 | ppl 1.27 | wps 3431.2 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 28350 | lr 0.000187812 | gnorm 0.252 | train_wall 97 | gb_free 9 | wall 47267
2022-09-08 13:13:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:13:54 | INFO | fairseq.trainer | begin training epoch 352
2022-09-08 13:13:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:14:57 | INFO | train_inner | epoch 352:     50 / 81 loss=3.382, nll_loss=0.345, mask_loss=8.46375, p_2=0.03524, mask_ave=0.495, ppl=1.27, wps=3560.9, ups=0.64, wpb=5542.1, bsz=359.9, num_updates=28400, lr=0.000187647, gnorm=0.252, train_wall=122, gb_free=9.1, wall=47330
2022-09-08 13:15:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:15:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:15:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:15:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:15:46 | INFO | valid | epoch 352 | valid on 'valid' subset | loss 5.038 | nll_loss 2.415 | mask_loss 9.46215 | p_2 0.04864 | mask_ave 0.614 | ppl 5.33 | bleu 56.68 | wps 1499.9 | wpb 933.5 | bsz 59.6 | num_updates 28431 | best_bleu 57.29
2022-09-08 13:15:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 352 @ 28431 updates
2022-09-08 13:15:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint352.pt
2022-09-08 13:15:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint352.pt
2022-09-08 13:16:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint352.pt (epoch 352 @ 28431 updates, score 56.68) (writing took 18.02469226717949 seconds)
2022-09-08 13:16:04 | INFO | fairseq_cli.train | end of epoch 352 (average epoch stats below)
2022-09-08 13:16:04 | INFO | train | epoch 352 | loss 3.383 | nll_loss 0.345 | mask_loss 8.51362 | p_2 0.0352 | mask_ave 0.495 | ppl 1.27 | wps 3431.7 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 28431 | lr 0.000187544 | gnorm 0.255 | train_wall 99 | gb_free 9.2 | wall 47397
2022-09-08 13:16:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:16:05 | INFO | fairseq.trainer | begin training epoch 353
2022-09-08 13:16:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:17:30 | INFO | train_inner | epoch 353:     69 / 81 loss=3.383, nll_loss=0.345, mask_loss=8.56837, p_2=0.03502, mask_ave=0.5, ppl=1.27, wps=3615.5, ups=0.65, wpb=5524.4, bsz=354.6, num_updates=28500, lr=0.000187317, gnorm=0.244, train_wall=122, gb_free=9.1, wall=47483
2022-09-08 13:17:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:17:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:17:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:17:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:17:56 | INFO | valid | epoch 353 | valid on 'valid' subset | loss 5.053 | nll_loss 2.431 | mask_loss 9.44882 | p_2 0.04887 | mask_ave 0.607 | ppl 5.39 | bleu 56.59 | wps 1522.6 | wpb 933.5 | bsz 59.6 | num_updates 28512 | best_bleu 57.29
2022-09-08 13:17:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 353 @ 28512 updates
2022-09-08 13:17:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint353.pt
2022-09-08 13:17:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint353.pt
2022-09-08 13:18:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint353.pt (epoch 353 @ 28512 updates, score 56.59) (writing took 20.17881653830409 seconds)
2022-09-08 13:18:16 | INFO | fairseq_cli.train | end of epoch 353 (average epoch stats below)
2022-09-08 13:18:16 | INFO | train | epoch 353 | loss 3.382 | nll_loss 0.345 | mask_loss 8.51903 | p_2 0.03505 | mask_ave 0.5 | ppl 1.27 | wps 3402.6 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 28512 | lr 0.000187278 | gnorm 0.246 | train_wall 98 | gb_free 9.1 | wall 47529
2022-09-08 13:18:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:18:16 | INFO | fairseq.trainer | begin training epoch 354
2022-09-08 13:18:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:19:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:19:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:19:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:19:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:20:06 | INFO | valid | epoch 354 | valid on 'valid' subset | loss 5.039 | nll_loss 2.411 | mask_loss 9.20953 | p_2 0.04888 | mask_ave 0.606 | ppl 5.32 | bleu 56.65 | wps 1523.8 | wpb 933.5 | bsz 59.6 | num_updates 28593 | best_bleu 57.29
2022-09-08 13:20:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 354 @ 28593 updates
2022-09-08 13:20:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint354.pt
2022-09-08 13:20:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint354.pt
2022-09-08 13:20:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint354.pt (epoch 354 @ 28593 updates, score 56.65) (writing took 2.466803353279829 seconds)
2022-09-08 13:20:09 | INFO | fairseq_cli.train | end of epoch 354 (average epoch stats below)
2022-09-08 13:20:09 | INFO | train | epoch 354 | loss 3.383 | nll_loss 0.346 | mask_loss 8.47466 | p_2 0.03526 | mask_ave 0.493 | ppl 1.27 | wps 3962.5 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 28593 | lr 0.000187012 | gnorm 0.268 | train_wall 98 | gb_free 9 | wall 47642
2022-09-08 13:20:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:20:09 | INFO | fairseq.trainer | begin training epoch 355
2022-09-08 13:20:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:20:19 | INFO | train_inner | epoch 355:      7 / 81 loss=3.383, nll_loss=0.346, mask_loss=8.48348, p_2=0.03514, mask_ave=0.494, ppl=1.27, wps=3269.9, ups=0.59, wpb=5509.4, bsz=356.8, num_updates=28600, lr=0.000186989, gnorm=0.267, train_wall=120, gb_free=9, wall=47651
2022-09-08 13:21:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:21:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:21:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:21:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:22:00 | INFO | valid | epoch 355 | valid on 'valid' subset | loss 5.049 | nll_loss 2.436 | mask_loss 9.28187 | p_2 0.0489 | mask_ave 0.606 | ppl 5.41 | bleu 55.99 | wps 1495.9 | wpb 933.5 | bsz 59.6 | num_updates 28674 | best_bleu 57.29
2022-09-08 13:22:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 355 @ 28674 updates
2022-09-08 13:22:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint355.pt
2022-09-08 13:22:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint355.pt
2022-09-08 13:22:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint355.pt (epoch 355 @ 28674 updates, score 55.99) (writing took 19.795386031270027 seconds)
2022-09-08 13:22:20 | INFO | fairseq_cli.train | end of epoch 355 (average epoch stats below)
2022-09-08 13:22:20 | INFO | train | epoch 355 | loss 3.383 | nll_loss 0.345 | mask_loss 8.49844 | p_2 0.03516 | mask_ave 0.496 | ppl 1.27 | wps 3411.1 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 28674 | lr 0.000186748 | gnorm 0.255 | train_wall 98 | gb_free 9.1 | wall 47773
2022-09-08 13:22:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:22:20 | INFO | fairseq.trainer | begin training epoch 356
2022-09-08 13:22:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:22:52 | INFO | train_inner | epoch 356:     26 / 81 loss=3.382, nll_loss=0.345, mask_loss=8.5044, p_2=0.03566, mask_ave=0.499, ppl=1.27, wps=3590.3, ups=0.65, wpb=5514.1, bsz=365.2, num_updates=28700, lr=0.000186663, gnorm=0.246, train_wall=121, gb_free=9, wall=47805
2022-09-08 13:24:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:24:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:24:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:24:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:24:11 | INFO | valid | epoch 356 | valid on 'valid' subset | loss 5.049 | nll_loss 2.422 | mask_loss 9.46888 | p_2 0.04865 | mask_ave 0.614 | ppl 5.36 | bleu 56.53 | wps 1507.4 | wpb 933.5 | bsz 59.6 | num_updates 28755 | best_bleu 57.29
2022-09-08 13:24:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 356 @ 28755 updates
2022-09-08 13:24:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint356.pt
2022-09-08 13:24:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint356.pt
2022-09-08 13:24:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint356.pt (epoch 356 @ 28755 updates, score 56.53) (writing took 24.006527166813612 seconds)
2022-09-08 13:24:35 | INFO | fairseq_cli.train | end of epoch 356 (average epoch stats below)
2022-09-08 13:24:35 | INFO | train | epoch 356 | loss 3.382 | nll_loss 0.345 | mask_loss 8.64133 | p_2 0.03512 | mask_ave 0.498 | ppl 1.27 | wps 3305.9 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 28755 | lr 0.000186485 | gnorm 0.249 | train_wall 98 | gb_free 9.1 | wall 47908
2022-09-08 13:24:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:24:36 | INFO | fairseq.trainer | begin training epoch 357
2022-09-08 13:24:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:25:32 | INFO | train_inner | epoch 357:     45 / 81 loss=3.383, nll_loss=0.346, mask_loss=8.62189, p_2=0.03462, mask_ave=0.495, ppl=1.27, wps=3488.9, ups=0.63, wpb=5562.5, bsz=351.2, num_updates=28800, lr=0.000186339, gnorm=0.27, train_wall=122, gb_free=9.1, wall=47964
2022-09-08 13:26:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:26:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:26:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:26:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:26:26 | INFO | valid | epoch 357 | valid on 'valid' subset | loss 5.041 | nll_loss 2.416 | mask_loss 9.41756 | p_2 0.04863 | mask_ave 0.614 | ppl 5.34 | bleu 56.41 | wps 1529.2 | wpb 933.5 | bsz 59.6 | num_updates 28836 | best_bleu 57.29
2022-09-08 13:26:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 357 @ 28836 updates
2022-09-08 13:26:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint357.pt
2022-09-08 13:26:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint357.pt
2022-09-08 13:26:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint357.pt (epoch 357 @ 28836 updates, score 56.41) (writing took 19.285227824002504 seconds)
2022-09-08 13:26:45 | INFO | fairseq_cli.train | end of epoch 357 (average epoch stats below)
2022-09-08 13:26:45 | INFO | train | epoch 357 | loss 3.383 | nll_loss 0.346 | mask_loss 8.51173 | p_2 0.03512 | mask_ave 0.498 | ppl 1.27 | wps 3440.5 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 28836 | lr 0.000186223 | gnorm 0.281 | train_wall 98 | gb_free 9.2 | wall 48038
2022-09-08 13:26:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:26:46 | INFO | fairseq.trainer | begin training epoch 358
2022-09-08 13:26:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:28:04 | INFO | train_inner | epoch 358:     64 / 81 loss=3.383, nll_loss=0.346, mask_loss=8.53471, p_2=0.03511, mask_ave=0.501, ppl=1.27, wps=3586.5, ups=0.65, wpb=5485.7, bsz=356.6, num_updates=28900, lr=0.000186016, gnorm=0.259, train_wall=121, gb_free=9, wall=48117
2022-09-08 13:28:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:28:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:28:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:28:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:28:36 | INFO | valid | epoch 358 | valid on 'valid' subset | loss 5.055 | nll_loss 2.428 | mask_loss 9.33141 | p_2 0.0488 | mask_ave 0.609 | ppl 5.38 | bleu 56.21 | wps 1566.9 | wpb 933.5 | bsz 59.6 | num_updates 28917 | best_bleu 57.29
2022-09-08 13:28:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 358 @ 28917 updates
2022-09-08 13:28:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint358.pt
2022-09-08 13:28:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint358.pt
2022-09-08 13:29:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint358.pt (epoch 358 @ 28917 updates, score 56.21) (writing took 26.2875744253397 seconds)
2022-09-08 13:29:02 | INFO | fairseq_cli.train | end of epoch 358 (average epoch stats below)
2022-09-08 13:29:02 | INFO | train | epoch 358 | loss 3.382 | nll_loss 0.345 | mask_loss 8.5017 | p_2 0.03509 | mask_ave 0.499 | ppl 1.27 | wps 3268.6 | ups 0.59 | wpb 5523.2 | bsz 358 | num_updates 28917 | lr 0.000185962 | gnorm 0.243 | train_wall 98 | gb_free 9 | wall 48175
2022-09-08 13:29:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:29:02 | INFO | fairseq.trainer | begin training epoch 359
2022-09-08 13:29:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:30:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:30:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:30:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:30:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:30:52 | INFO | valid | epoch 359 | valid on 'valid' subset | loss 5.053 | nll_loss 2.434 | mask_loss 9.16064 | p_2 0.04874 | mask_ave 0.61 | ppl 5.4 | bleu 56.01 | wps 1529.5 | wpb 933.5 | bsz 59.6 | num_updates 28998 | best_bleu 57.29
2022-09-08 13:30:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 359 @ 28998 updates
2022-09-08 13:30:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint359.pt
2022-09-08 13:30:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint359.pt
2022-09-08 13:30:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint359.pt (epoch 359 @ 28998 updates, score 56.01) (writing took 2.490181375294924 seconds)
2022-09-08 13:30:55 | INFO | fairseq_cli.train | end of epoch 359 (average epoch stats below)
2022-09-08 13:30:55 | INFO | train | epoch 359 | loss 3.382 | nll_loss 0.345 | mask_loss 8.4426 | p_2 0.03516 | mask_ave 0.497 | ppl 1.27 | wps 3978.8 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 28998 | lr 0.000185702 | gnorm 0.259 | train_wall 97 | gb_free 9 | wall 48288
2022-09-08 13:30:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:30:55 | INFO | fairseq.trainer | begin training epoch 360
2022-09-08 13:30:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:30:59 | INFO | train_inner | epoch 360:      2 / 81 loss=3.382, nll_loss=0.345, mask_loss=8.40724, p_2=0.03526, mask_ave=0.496, ppl=1.27, wps=3163.7, ups=0.57, wpb=5519.7, bsz=360.2, num_updates=29000, lr=0.000185695, gnorm=0.256, train_wall=120, gb_free=9, wall=48292
2022-09-08 13:32:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:32:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:32:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:32:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:32:46 | INFO | valid | epoch 360 | valid on 'valid' subset | loss 5.052 | nll_loss 2.427 | mask_loss 9.43209 | p_2 0.04876 | mask_ave 0.611 | ppl 5.38 | bleu 56.57 | wps 1494.3 | wpb 933.5 | bsz 59.6 | num_updates 29079 | best_bleu 57.29
2022-09-08 13:32:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 360 @ 29079 updates
2022-09-08 13:32:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint360.pt
2022-09-08 13:32:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint360.pt
2022-09-08 13:33:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint360.pt (epoch 360 @ 29079 updates, score 56.57) (writing took 19.880464635789394 seconds)
2022-09-08 13:33:06 | INFO | fairseq_cli.train | end of epoch 360 (average epoch stats below)
2022-09-08 13:33:06 | INFO | train | epoch 360 | loss 3.382 | nll_loss 0.345 | mask_loss 8.51608 | p_2 0.03524 | mask_ave 0.494 | ppl 1.27 | wps 3415.2 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 29079 | lr 0.000185443 | gnorm 0.274 | train_wall 98 | gb_free 9 | wall 48419
2022-09-08 13:33:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:33:06 | INFO | fairseq.trainer | begin training epoch 361
2022-09-08 13:33:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:33:32 | INFO | train_inner | epoch 361:     21 / 81 loss=3.382, nll_loss=0.345, mask_loss=8.49897, p_2=0.03533, mask_ave=0.493, ppl=1.27, wps=3631, ups=0.65, wpb=5546.9, bsz=361.4, num_updates=29100, lr=0.000185376, gnorm=0.267, train_wall=120, gb_free=9.1, wall=48445
2022-09-08 13:34:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:34:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:34:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:34:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:34:56 | INFO | valid | epoch 361 | valid on 'valid' subset | loss 5.057 | nll_loss 2.434 | mask_loss 9.33447 | p_2 0.04878 | mask_ave 0.61 | ppl 5.4 | bleu 56.22 | wps 1547.2 | wpb 933.5 | bsz 59.6 | num_updates 29160 | best_bleu 57.29
2022-09-08 13:34:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 361 @ 29160 updates
2022-09-08 13:34:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint361.pt
2022-09-08 13:34:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint361.pt
2022-09-08 13:35:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint361.pt (epoch 361 @ 29160 updates, score 56.22) (writing took 17.980057079344988 seconds)
2022-09-08 13:35:14 | INFO | fairseq_cli.train | end of epoch 361 (average epoch stats below)
2022-09-08 13:35:14 | INFO | train | epoch 361 | loss 3.382 | nll_loss 0.345 | mask_loss 8.46655 | p_2 0.03524 | mask_ave 0.494 | ppl 1.27 | wps 3492.2 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 29160 | lr 0.000185185 | gnorm 0.248 | train_wall 97 | gb_free 9.1 | wall 48547
2022-09-08 13:35:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:35:14 | INFO | fairseq.trainer | begin training epoch 362
2022-09-08 13:35:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:36:03 | INFO | train_inner | epoch 362:     40 / 81 loss=3.382, nll_loss=0.345, mask_loss=8.55894, p_2=0.03491, mask_ave=0.496, ppl=1.27, wps=3644, ups=0.66, wpb=5524.8, bsz=352.8, num_updates=29200, lr=0.000185058, gnorm=0.258, train_wall=121, gb_free=9.1, wall=48596
2022-09-08 13:36:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:36:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:36:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:36:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:37:04 | INFO | valid | epoch 362 | valid on 'valid' subset | loss 5.044 | nll_loss 2.414 | mask_loss 9.44985 | p_2 0.04904 | mask_ave 0.602 | ppl 5.33 | bleu 56.14 | wps 1519.2 | wpb 933.5 | bsz 59.6 | num_updates 29241 | best_bleu 57.29
2022-09-08 13:37:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 362 @ 29241 updates
2022-09-08 13:37:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint362.pt
2022-09-08 13:37:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint362.pt
2022-09-08 13:37:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint362.pt (epoch 362 @ 29241 updates, score 56.14) (writing took 2.532451018691063 seconds)
2022-09-08 13:37:07 | INFO | fairseq_cli.train | end of epoch 362 (average epoch stats below)
2022-09-08 13:37:07 | INFO | train | epoch 362 | loss 3.383 | nll_loss 0.346 | mask_loss 8.58322 | p_2 0.03524 | mask_ave 0.494 | ppl 1.27 | wps 3966.2 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 29241 | lr 0.000184929 | gnorm 0.269 | train_wall 97 | gb_free 9.2 | wall 48660
2022-09-08 13:37:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:37:07 | INFO | fairseq.trainer | begin training epoch 363
2022-09-08 13:37:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:38:19 | INFO | train_inner | epoch 363:     59 / 81 loss=3.382, nll_loss=0.345, mask_loss=8.61834, p_2=0.03551, mask_ave=0.493, ppl=1.27, wps=4062.1, ups=0.74, wpb=5513, bsz=359.5, num_updates=29300, lr=0.000184742, gnorm=0.256, train_wall=120, gb_free=9.2, wall=48732
2022-09-08 13:38:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:38:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:38:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:38:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:38:57 | INFO | valid | epoch 363 | valid on 'valid' subset | loss 5.065 | nll_loss 2.447 | mask_loss 9.55309 | p_2 0.04908 | mask_ave 0.601 | ppl 5.45 | bleu 56.15 | wps 1519.7 | wpb 933.5 | bsz 59.6 | num_updates 29322 | best_bleu 57.29
2022-09-08 13:38:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 363 @ 29322 updates
2022-09-08 13:38:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint363.pt
2022-09-08 13:38:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint363.pt
2022-09-08 13:39:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint363.pt (epoch 363 @ 29322 updates, score 56.15) (writing took 20.863645806908607 seconds)
2022-09-08 13:39:18 | INFO | fairseq_cli.train | end of epoch 363 (average epoch stats below)
2022-09-08 13:39:18 | INFO | train | epoch 363 | loss 3.382 | nll_loss 0.345 | mask_loss 8.67977 | p_2 0.03533 | mask_ave 0.492 | ppl 1.27 | wps 3412.6 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 29322 | lr 0.000184673 | gnorm 0.247 | train_wall 97 | gb_free 9.2 | wall 48791
2022-09-08 13:39:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:39:18 | INFO | fairseq.trainer | begin training epoch 364
2022-09-08 13:39:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:40:54 | INFO | train_inner | epoch 364:     78 / 81 loss=3.382, nll_loss=0.345, mask_loss=8.60595, p_2=0.03528, mask_ave=0.493, ppl=1.27, wps=3570.9, ups=0.64, wpb=5536.2, bsz=360.4, num_updates=29400, lr=0.000184428, gnorm=0.249, train_wall=121, gb_free=8.9, wall=48887
2022-09-08 13:40:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:40:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:40:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:40:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:41:08 | INFO | valid | epoch 364 | valid on 'valid' subset | loss 5.052 | nll_loss 2.424 | mask_loss 9.27549 | p_2 0.04903 | mask_ave 0.602 | ppl 5.37 | bleu 56.47 | wps 1548.7 | wpb 933.5 | bsz 59.6 | num_updates 29403 | best_bleu 57.29
2022-09-08 13:41:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 364 @ 29403 updates
2022-09-08 13:41:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint364.pt
2022-09-08 13:41:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint364.pt
2022-09-08 13:41:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint364.pt (epoch 364 @ 29403 updates, score 56.47) (writing took 21.84464931115508 seconds)
2022-09-08 13:41:30 | INFO | fairseq_cli.train | end of epoch 364 (average epoch stats below)
2022-09-08 13:41:30 | INFO | train | epoch 364 | loss 3.382 | nll_loss 0.345 | mask_loss 8.6002 | p_2 0.03524 | mask_ave 0.494 | ppl 1.27 | wps 3377.6 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 29403 | lr 0.000184418 | gnorm 0.251 | train_wall 98 | gb_free 9.2 | wall 48923
2022-09-08 13:41:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:41:30 | INFO | fairseq.trainer | begin training epoch 365
2022-09-08 13:41:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:43:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:43:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:43:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:43:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:43:21 | INFO | valid | epoch 365 | valid on 'valid' subset | loss 5.049 | nll_loss 2.427 | mask_loss 9.44974 | p_2 0.0487 | mask_ave 0.612 | ppl 5.38 | bleu 56.36 | wps 1501.7 | wpb 933.5 | bsz 59.6 | num_updates 29484 | best_bleu 57.29
2022-09-08 13:43:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 365 @ 29484 updates
2022-09-08 13:43:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint365.pt
2022-09-08 13:43:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint365.pt
2022-09-08 13:43:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint365.pt (epoch 365 @ 29484 updates, score 56.36) (writing took 18.989420674741268 seconds)
2022-09-08 13:43:40 | INFO | fairseq_cli.train | end of epoch 365 (average epoch stats below)
2022-09-08 13:43:40 | INFO | train | epoch 365 | loss 3.382 | nll_loss 0.345 | mask_loss 8.50528 | p_2 0.03519 | mask_ave 0.496 | ppl 1.27 | wps 3446.3 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 29484 | lr 0.000184165 | gnorm 0.258 | train_wall 98 | gb_free 9.1 | wall 49053
2022-09-08 13:43:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:43:40 | INFO | fairseq.trainer | begin training epoch 366
2022-09-08 13:43:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:44:00 | INFO | train_inner | epoch 366:     16 / 81 loss=3.382, nll_loss=0.345, mask_loss=8.53666, p_2=0.03518, mask_ave=0.496, ppl=1.27, wps=2953.9, ups=0.54, wpb=5495.7, bsz=355.4, num_updates=29500, lr=0.000184115, gnorm=0.257, train_wall=119, gb_free=9.1, wall=49073
2022-09-08 13:45:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:45:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:45:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:45:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:45:32 | INFO | valid | epoch 366 | valid on 'valid' subset | loss 5.05 | nll_loss 2.429 | mask_loss 9.54201 | p_2 0.04892 | mask_ave 0.606 | ppl 5.38 | bleu 56.42 | wps 1552.8 | wpb 933.5 | bsz 59.6 | num_updates 29565 | best_bleu 57.29
2022-09-08 13:45:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 366 @ 29565 updates
2022-09-08 13:45:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint366.pt
2022-09-08 13:45:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint366.pt
2022-09-08 13:45:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint366.pt (epoch 366 @ 29565 updates, score 56.42) (writing took 19.111681889742613 seconds)
2022-09-08 13:45:51 | INFO | fairseq_cli.train | end of epoch 366 (average epoch stats below)
2022-09-08 13:45:51 | INFO | train | epoch 366 | loss 3.383 | nll_loss 0.346 | mask_loss 8.66184 | p_2 0.03522 | mask_ave 0.495 | ppl 1.27 | wps 3416.2 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 29565 | lr 0.000183912 | gnorm 0.269 | train_wall 99 | gb_free 9.1 | wall 49184
2022-09-08 13:45:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:45:51 | INFO | fairseq.trainer | begin training epoch 367
2022-09-08 13:45:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:46:35 | INFO | train_inner | epoch 367:     35 / 81 loss=3.383, nll_loss=0.346, mask_loss=8.64197, p_2=0.035, mask_ave=0.49, ppl=1.27, wps=3583.4, ups=0.65, wpb=5550.9, bsz=355.5, num_updates=29600, lr=0.000183804, gnorm=0.271, train_wall=123, gb_free=9.1, wall=49228
2022-09-08 13:47:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:47:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:47:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:47:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:47:42 | INFO | valid | epoch 367 | valid on 'valid' subset | loss 5.052 | nll_loss 2.429 | mask_loss 9.25889 | p_2 0.04882 | mask_ave 0.608 | ppl 5.39 | bleu 56.96 | wps 1479.7 | wpb 933.5 | bsz 59.6 | num_updates 29646 | best_bleu 57.29
2022-09-08 13:47:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 367 @ 29646 updates
2022-09-08 13:47:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint367.pt
2022-09-08 13:47:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint367.pt
2022-09-08 13:47:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint367.pt (epoch 367 @ 29646 updates, score 56.96) (writing took 2.4985582157969475 seconds)
2022-09-08 13:47:45 | INFO | fairseq_cli.train | end of epoch 367 (average epoch stats below)
2022-09-08 13:47:45 | INFO | train | epoch 367 | loss 3.382 | nll_loss 0.346 | mask_loss 8.56094 | p_2 0.03532 | mask_ave 0.492 | ppl 1.27 | wps 3928.4 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 29646 | lr 0.000183661 | gnorm 0.277 | train_wall 98 | gb_free 9.1 | wall 49298
2022-09-08 13:47:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:47:45 | INFO | fairseq.trainer | begin training epoch 368
2022-09-08 13:47:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:48:51 | INFO | train_inner | epoch 368:     54 / 81 loss=3.382, nll_loss=0.345, mask_loss=8.51652, p_2=0.03542, mask_ave=0.492, ppl=1.27, wps=4063.7, ups=0.74, wpb=5526.7, bsz=361.9, num_updates=29700, lr=0.000183494, gnorm=0.27, train_wall=120, gb_free=9, wall=49364
2022-09-08 13:49:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:49:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:49:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:49:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:49:35 | INFO | valid | epoch 368 | valid on 'valid' subset | loss 5.039 | nll_loss 2.415 | mask_loss 9.30093 | p_2 0.04909 | mask_ave 0.6 | ppl 5.33 | bleu 56.29 | wps 1513.9 | wpb 933.5 | bsz 59.6 | num_updates 29727 | best_bleu 57.29
2022-09-08 13:49:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 368 @ 29727 updates
2022-09-08 13:49:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint368.pt
2022-09-08 13:49:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint368.pt
2022-09-08 13:49:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint368.pt (epoch 368 @ 29727 updates, score 56.29) (writing took 23.78825519233942 seconds)
2022-09-08 13:49:59 | INFO | fairseq_cli.train | end of epoch 368 (average epoch stats below)
2022-09-08 13:49:59 | INFO | train | epoch 368 | loss 3.382 | nll_loss 0.345 | mask_loss 8.52093 | p_2 0.03532 | mask_ave 0.492 | ppl 1.27 | wps 3336.6 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 29727 | lr 0.000183411 | gnorm 0.259 | train_wall 97 | gb_free 9.1 | wall 49432
2022-09-08 13:49:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:49:59 | INFO | fairseq.trainer | begin training epoch 369
2022-09-08 13:49:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:51:29 | INFO | train_inner | epoch 369:     73 / 81 loss=3.381, nll_loss=0.345, mask_loss=8.58929, p_2=0.03543, mask_ave=0.493, ppl=1.27, wps=3501.9, ups=0.63, wpb=5523.2, bsz=357.8, num_updates=29800, lr=0.000183186, gnorm=0.246, train_wall=121, gb_free=9, wall=49522
2022-09-08 13:51:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:51:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:51:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:51:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:51:49 | INFO | valid | epoch 369 | valid on 'valid' subset | loss 5.055 | nll_loss 2.439 | mask_loss 9.61094 | p_2 0.04898 | mask_ave 0.603 | ppl 5.42 | bleu 55.83 | wps 1482.8 | wpb 933.5 | bsz 59.6 | num_updates 29808 | best_bleu 57.29
2022-09-08 13:51:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 369 @ 29808 updates
2022-09-08 13:51:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint369.pt
2022-09-08 13:51:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint369.pt
2022-09-08 13:52:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint369.pt (epoch 369 @ 29808 updates, score 55.83) (writing took 21.726314321160316 seconds)
2022-09-08 13:52:11 | INFO | fairseq_cli.train | end of epoch 369 (average epoch stats below)
2022-09-08 13:52:11 | INFO | train | epoch 369 | loss 3.381 | nll_loss 0.344 | mask_loss 8.62518 | p_2 0.03532 | mask_ave 0.492 | ppl 1.27 | wps 3385.4 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 29808 | lr 0.000183161 | gnorm 0.242 | train_wall 97 | gb_free 9.2 | wall 49564
2022-09-08 13:52:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:52:11 | INFO | fairseq.trainer | begin training epoch 370
2022-09-08 13:52:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:53:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:53:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:53:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:53:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:54:02 | INFO | valid | epoch 370 | valid on 'valid' subset | loss 5.055 | nll_loss 2.428 | mask_loss 9.42732 | p_2 0.04917 | mask_ave 0.598 | ppl 5.38 | bleu 56.44 | wps 1542.6 | wpb 933.5 | bsz 59.6 | num_updates 29889 | best_bleu 57.29
2022-09-08 13:54:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 370 @ 29889 updates
2022-09-08 13:54:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint370.pt
2022-09-08 13:54:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint370.pt
2022-09-08 13:54:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint370.pt (epoch 370 @ 29889 updates, score 56.44) (writing took 19.02935368567705 seconds)
2022-09-08 13:54:21 | INFO | fairseq_cli.train | end of epoch 370 (average epoch stats below)
2022-09-08 13:54:21 | INFO | train | epoch 370 | loss 3.381 | nll_loss 0.344 | mask_loss 8.66868 | p_2 0.03534 | mask_ave 0.491 | ppl 1.27 | wps 3442.3 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 29889 | lr 0.000182913 | gnorm 0.239 | train_wall 98 | gb_free 9.2 | wall 49694
2022-09-08 13:54:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:54:21 | INFO | fairseq.trainer | begin training epoch 371
2022-09-08 13:54:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:54:36 | INFO | train_inner | epoch 371:     11 / 81 loss=3.381, nll_loss=0.344, mask_loss=8.65711, p_2=0.03541, mask_ave=0.491, ppl=1.27, wps=2942.6, ups=0.54, wpb=5498.1, bsz=356.7, num_updates=29900, lr=0.000182879, gnorm=0.24, train_wall=120, gb_free=9, wall=49709
2022-09-08 13:56:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:56:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:56:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:56:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:56:12 | INFO | valid | epoch 371 | valid on 'valid' subset | loss 5.041 | nll_loss 2.416 | mask_loss 9.39813 | p_2 0.04904 | mask_ave 0.601 | ppl 5.34 | bleu 56.32 | wps 1509.6 | wpb 933.5 | bsz 59.6 | num_updates 29970 | best_bleu 57.29
2022-09-08 13:56:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 371 @ 29970 updates
2022-09-08 13:56:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint371.pt
2022-09-08 13:56:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint371.pt
2022-09-08 13:56:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint371.pt (epoch 371 @ 29970 updates, score 56.32) (writing took 19.83906314522028 seconds)
2022-09-08 13:56:32 | INFO | fairseq_cli.train | end of epoch 371 (average epoch stats below)
2022-09-08 13:56:32 | INFO | train | epoch 371 | loss 3.382 | nll_loss 0.345 | mask_loss 8.59213 | p_2 0.0354 | mask_ave 0.489 | ppl 1.27 | wps 3419.3 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 29970 | lr 0.000182666 | gnorm 0.254 | train_wall 98 | gb_free 9.1 | wall 49825
2022-09-08 13:56:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:56:32 | INFO | fairseq.trainer | begin training epoch 372
2022-09-08 13:56:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:57:09 | INFO | train_inner | epoch 372:     30 / 81 loss=3.381, nll_loss=0.345, mask_loss=8.60828, p_2=0.03525, mask_ave=0.489, ppl=1.27, wps=3630.1, ups=0.65, wpb=5551, bsz=360.9, num_updates=30000, lr=0.000182574, gnorm=0.256, train_wall=120, gb_free=9, wall=49861
2022-09-08 13:58:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 13:58:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 13:58:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 13:58:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 13:58:21 | INFO | valid | epoch 372 | valid on 'valid' subset | loss 5.045 | nll_loss 2.424 | mask_loss 9.69643 | p_2 0.04885 | mask_ave 0.607 | ppl 5.37 | bleu 56.45 | wps 1521.1 | wpb 933.5 | bsz 59.6 | num_updates 30051 | best_bleu 57.29
2022-09-08 13:58:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 372 @ 30051 updates
2022-09-08 13:58:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint372.pt
2022-09-08 13:58:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint372.pt
2022-09-08 13:58:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint372.pt (epoch 372 @ 30051 updates, score 56.45) (writing took 20.979185823351145 seconds)
2022-09-08 13:58:42 | INFO | fairseq_cli.train | end of epoch 372 (average epoch stats below)
2022-09-08 13:58:42 | INFO | train | epoch 372 | loss 3.382 | nll_loss 0.345 | mask_loss 8.66948 | p_2 0.03529 | mask_ave 0.493 | ppl 1.27 | wps 3436.4 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 30051 | lr 0.000182419 | gnorm 0.257 | train_wall 96 | gb_free 9.1 | wall 49955
2022-09-08 13:58:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 13:58:42 | INFO | fairseq.trainer | begin training epoch 373
2022-09-08 13:58:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 13:59:42 | INFO | train_inner | epoch 373:     49 / 81 loss=3.382, nll_loss=0.345, mask_loss=8.71058, p_2=0.03525, mask_ave=0.496, ppl=1.27, wps=3597.8, ups=0.65, wpb=5503.4, bsz=351.9, num_updates=30100, lr=0.000182271, gnorm=0.253, train_wall=119, gb_free=9.1, wall=50014
2022-09-08 14:00:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:00:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:00:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:00:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:00:31 | INFO | valid | epoch 373 | valid on 'valid' subset | loss 5.063 | nll_loss 2.446 | mask_loss 9.53078 | p_2 0.04891 | mask_ave 0.606 | ppl 5.45 | bleu 55.77 | wps 1522.6 | wpb 933.5 | bsz 59.6 | num_updates 30132 | best_bleu 57.29
2022-09-08 14:00:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 373 @ 30132 updates
2022-09-08 14:00:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint373.pt
2022-09-08 14:00:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint373.pt
2022-09-08 14:00:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint373.pt (epoch 373 @ 30132 updates, score 55.77) (writing took 21.912537701427937 seconds)
2022-09-08 14:00:53 | INFO | fairseq_cli.train | end of epoch 373 (average epoch stats below)
2022-09-08 14:00:53 | INFO | train | epoch 373 | loss 3.381 | nll_loss 0.345 | mask_loss 8.70042 | p_2 0.03526 | mask_ave 0.494 | ppl 1.27 | wps 3409.8 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 30132 | lr 0.000182174 | gnorm 0.247 | train_wall 96 | gb_free 9.1 | wall 50086
2022-09-08 14:00:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:00:54 | INFO | fairseq.trainer | begin training epoch 374
2022-09-08 14:00:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:02:18 | INFO | train_inner | epoch 374:     68 / 81 loss=3.381, nll_loss=0.345, mask_loss=8.67262, p_2=0.0352, mask_ave=0.491, ppl=1.27, wps=3558.9, ups=0.64, wpb=5555.3, bsz=363.7, num_updates=30200, lr=0.000181969, gnorm=0.253, train_wall=121, gb_free=9, wall=50171
2022-09-08 14:02:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:02:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:02:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:02:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:02:44 | INFO | valid | epoch 374 | valid on 'valid' subset | loss 5.077 | nll_loss 2.462 | mask_loss 9.68349 | p_2 0.04888 | mask_ave 0.608 | ppl 5.51 | bleu 56.03 | wps 1561.4 | wpb 933.5 | bsz 59.6 | num_updates 30213 | best_bleu 57.29
2022-09-08 14:02:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 374 @ 30213 updates
2022-09-08 14:02:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint374.pt
2022-09-08 14:02:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint374.pt
2022-09-08 14:03:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint374.pt (epoch 374 @ 30213 updates, score 56.03) (writing took 19.116678427904844 seconds)
2022-09-08 14:03:03 | INFO | fairseq_cli.train | end of epoch 374 (average epoch stats below)
2022-09-08 14:03:03 | INFO | train | epoch 374 | loss 3.381 | nll_loss 0.344 | mask_loss 8.67233 | p_2 0.03529 | mask_ave 0.493 | ppl 1.27 | wps 3445.8 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 30213 | lr 0.000181929 | gnorm 0.251 | train_wall 98 | gb_free 9.1 | wall 50216
2022-09-08 14:03:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:03:03 | INFO | fairseq.trainer | begin training epoch 375
2022-09-08 14:03:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:04:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:04:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:04:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:04:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:04:52 | INFO | valid | epoch 375 | valid on 'valid' subset | loss 5.063 | nll_loss 2.448 | mask_loss 9.55691 | p_2 0.04893 | mask_ave 0.605 | ppl 5.46 | bleu 56.42 | wps 1562.9 | wpb 933.5 | bsz 59.6 | num_updates 30294 | best_bleu 57.29
2022-09-08 14:04:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 375 @ 30294 updates
2022-09-08 14:04:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint375.pt
2022-09-08 14:04:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint375.pt
2022-09-08 14:05:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint375.pt (epoch 375 @ 30294 updates, score 56.42) (writing took 19.07358928024769 seconds)
2022-09-08 14:05:11 | INFO | fairseq_cli.train | end of epoch 375 (average epoch stats below)
2022-09-08 14:05:11 | INFO | train | epoch 375 | loss 3.381 | nll_loss 0.345 | mask_loss 8.80091 | p_2 0.03529 | mask_ave 0.493 | ppl 1.27 | wps 3491.4 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 30294 | lr 0.000181686 | gnorm 0.255 | train_wall 97 | gb_free 9.6 | wall 50344
2022-09-08 14:05:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:05:12 | INFO | fairseq.trainer | begin training epoch 376
2022-09-08 14:05:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:05:19 | INFO | train_inner | epoch 376:      6 / 81 loss=3.381, nll_loss=0.345, mask_loss=8.78293, p_2=0.03548, mask_ave=0.494, ppl=1.27, wps=3012.6, ups=0.55, wpb=5477.3, bsz=355.7, num_updates=30300, lr=0.000181668, gnorm=0.252, train_wall=119, gb_free=9.1, wall=50352
2022-09-08 14:06:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:06:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:06:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:06:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:07:02 | INFO | valid | epoch 376 | valid on 'valid' subset | loss 5.043 | nll_loss 2.42 | mask_loss 9.40373 | p_2 0.04889 | mask_ave 0.606 | ppl 5.35 | bleu 56.96 | wps 1492 | wpb 933.5 | bsz 59.6 | num_updates 30375 | best_bleu 57.29
2022-09-08 14:07:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 376 @ 30375 updates
2022-09-08 14:07:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint376.pt
2022-09-08 14:07:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint376.pt
2022-09-08 14:07:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint376.pt (epoch 376 @ 30375 updates, score 56.96) (writing took 19.539884362369776 seconds)
2022-09-08 14:07:21 | INFO | fairseq_cli.train | end of epoch 376 (average epoch stats below)
2022-09-08 14:07:22 | INFO | train | epoch 376 | loss 3.381 | nll_loss 0.345 | mask_loss 8.71321 | p_2 0.03535 | mask_ave 0.491 | ppl 1.27 | wps 3437.2 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 30375 | lr 0.000181444 | gnorm 0.248 | train_wall 98 | gb_free 9.1 | wall 50474
2022-09-08 14:07:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:07:22 | INFO | fairseq.trainer | begin training epoch 377
2022-09-08 14:07:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:07:52 | INFO | train_inner | epoch 377:     25 / 81 loss=3.381, nll_loss=0.344, mask_loss=8.72064, p_2=0.03512, mask_ave=0.49, ppl=1.27, wps=3622.8, ups=0.65, wpb=5540.4, bsz=355.1, num_updates=30400, lr=0.000181369, gnorm=0.251, train_wall=120, gb_free=9.1, wall=50505
2022-09-08 14:09:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:09:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:09:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:09:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:09:11 | INFO | valid | epoch 377 | valid on 'valid' subset | loss 5.051 | nll_loss 2.432 | mask_loss 9.53665 | p_2 0.04879 | mask_ave 0.61 | ppl 5.4 | bleu 56.28 | wps 1488 | wpb 933.5 | bsz 59.6 | num_updates 30456 | best_bleu 57.29
2022-09-08 14:09:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 377 @ 30456 updates
2022-09-08 14:09:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint377.pt
2022-09-08 14:09:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint377.pt
2022-09-08 14:09:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint377.pt (epoch 377 @ 30456 updates, score 56.28) (writing took 18.952696979045868 seconds)
2022-09-08 14:09:31 | INFO | fairseq_cli.train | end of epoch 377 (average epoch stats below)
2022-09-08 14:09:31 | INFO | train | epoch 377 | loss 3.381 | nll_loss 0.344 | mask_loss 8.6705 | p_2 0.03535 | mask_ave 0.491 | ppl 1.27 | wps 3465.2 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 30456 | lr 0.000181202 | gnorm 0.254 | train_wall 97 | gb_free 9.1 | wall 50603
2022-09-08 14:09:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:09:31 | INFO | fairseq.trainer | begin training epoch 378
2022-09-08 14:09:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:10:25 | INFO | train_inner | epoch 378:     44 / 81 loss=3.38, nll_loss=0.344, mask_loss=8.67595, p_2=0.03546, mask_ave=0.491, ppl=1.27, wps=3632.5, ups=0.66, wpb=5541.9, bsz=361.8, num_updates=30500, lr=0.000181071, gnorm=0.255, train_wall=120, gb_free=9, wall=50658
2022-09-08 14:11:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:11:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:11:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:11:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:11:21 | INFO | valid | epoch 378 | valid on 'valid' subset | loss 5.047 | nll_loss 2.423 | mask_loss 9.51389 | p_2 0.04886 | mask_ave 0.608 | ppl 5.36 | bleu 57 | wps 1507 | wpb 933.5 | bsz 59.6 | num_updates 30537 | best_bleu 57.29
2022-09-08 14:11:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 378 @ 30537 updates
2022-09-08 14:11:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint378.pt
2022-09-08 14:11:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint378.pt
2022-09-08 14:11:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint378.pt (epoch 378 @ 30537 updates, score 57.0) (writing took 19.133439257740974 seconds)
2022-09-08 14:11:40 | INFO | fairseq_cli.train | end of epoch 378 (average epoch stats below)
2022-09-08 14:11:40 | INFO | train | epoch 378 | loss 3.381 | nll_loss 0.345 | mask_loss 8.72193 | p_2 0.03532 | mask_ave 0.492 | ppl 1.27 | wps 3461.9 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 30537 | lr 0.000180962 | gnorm 0.265 | train_wall 97 | gb_free 9.1 | wall 50733
2022-09-08 14:11:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:11:40 | INFO | fairseq.trainer | begin training epoch 379
2022-09-08 14:11:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:12:58 | INFO | train_inner | epoch 379:     63 / 81 loss=3.382, nll_loss=0.346, mask_loss=8.70143, p_2=0.03536, mask_ave=0.492, ppl=1.27, wps=3605.4, ups=0.65, wpb=5523.3, bsz=359.6, num_updates=30600, lr=0.000180775, gnorm=0.275, train_wall=121, gb_free=9.1, wall=50811
2022-09-08 14:13:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:13:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:13:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:13:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:13:31 | INFO | valid | epoch 379 | valid on 'valid' subset | loss 5.044 | nll_loss 2.418 | mask_loss 9.5857 | p_2 0.04897 | mask_ave 0.604 | ppl 5.35 | bleu 56.64 | wps 1509.4 | wpb 933.5 | bsz 59.6 | num_updates 30618 | best_bleu 57.29
2022-09-08 14:13:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 379 @ 30618 updates
2022-09-08 14:13:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint379.pt
2022-09-08 14:13:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint379.pt
2022-09-08 14:13:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint379.pt (epoch 379 @ 30618 updates, score 56.64) (writing took 22.652495350688696 seconds)
2022-09-08 14:13:54 | INFO | fairseq_cli.train | end of epoch 379 (average epoch stats below)
2022-09-08 14:13:54 | INFO | train | epoch 379 | loss 3.382 | nll_loss 0.345 | mask_loss 8.70066 | p_2 0.03533 | mask_ave 0.492 | ppl 1.27 | wps 3343 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 30618 | lr 0.000180722 | gnorm 0.273 | train_wall 98 | gb_free 9.2 | wall 50867
2022-09-08 14:13:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:13:54 | INFO | fairseq.trainer | begin training epoch 380
2022-09-08 14:13:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:15:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:15:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:15:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:15:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:15:46 | INFO | valid | epoch 380 | valid on 'valid' subset | loss 5.044 | nll_loss 2.423 | mask_loss 9.64121 | p_2 0.0489 | mask_ave 0.606 | ppl 5.36 | bleu 56.52 | wps 1455.9 | wpb 933.5 | bsz 59.6 | num_updates 30699 | best_bleu 57.29
2022-09-08 14:15:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 380 @ 30699 updates
2022-09-08 14:15:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint380.pt
2022-09-08 14:15:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint380.pt
2022-09-08 14:15:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint380.pt (epoch 380 @ 30699 updates, score 56.52) (writing took 2.4873600155115128 seconds)
2022-09-08 14:15:49 | INFO | fairseq_cli.train | end of epoch 380 (average epoch stats below)
2022-09-08 14:15:49 | INFO | train | epoch 380 | loss 3.381 | nll_loss 0.345 | mask_loss 8.72146 | p_2 0.03533 | mask_ave 0.492 | ppl 1.27 | wps 3894.9 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 30699 | lr 0.000180484 | gnorm 0.261 | train_wall 99 | gb_free 9.1 | wall 50981
2022-09-08 14:15:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:15:49 | INFO | fairseq.trainer | begin training epoch 381
2022-09-08 14:15:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:15:51 | INFO | train_inner | epoch 381:      1 / 81 loss=3.381, nll_loss=0.345, mask_loss=8.73122, p_2=0.0354, mask_ave=0.493, ppl=1.27, wps=3177.4, ups=0.58, wpb=5485.2, bsz=354.2, num_updates=30700, lr=0.000180481, gnorm=0.261, train_wall=121, gb_free=9.1, wall=50984
2022-09-08 14:17:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:17:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:17:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:17:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:17:39 | INFO | valid | epoch 381 | valid on 'valid' subset | loss 5.059 | nll_loss 2.439 | mask_loss 9.60478 | p_2 0.04891 | mask_ave 0.606 | ppl 5.42 | bleu 56.82 | wps 1568.1 | wpb 933.5 | bsz 59.6 | num_updates 30780 | best_bleu 57.29
2022-09-08 14:17:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 381 @ 30780 updates
2022-09-08 14:17:39 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint381.pt
2022-09-08 14:17:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint381.pt
2022-09-08 14:17:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint381.pt (epoch 381 @ 30780 updates, score 56.82) (writing took 19.207059998065233 seconds)
2022-09-08 14:17:58 | INFO | fairseq_cli.train | end of epoch 381 (average epoch stats below)
2022-09-08 14:17:58 | INFO | train | epoch 381 | loss 3.38 | nll_loss 0.343 | mask_loss 8.73555 | p_2 0.03536 | mask_ave 0.491 | ppl 1.27 | wps 3446.4 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 30780 | lr 0.000180246 | gnorm 0.235 | train_wall 98 | gb_free 9 | wall 51111
2022-09-08 14:17:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:17:59 | INFO | fairseq.trainer | begin training epoch 382
2022-09-08 14:17:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:18:23 | INFO | train_inner | epoch 382:     20 / 81 loss=3.38, nll_loss=0.343, mask_loss=8.7519, p_2=0.03522, mask_ave=0.491, ppl=1.27, wps=3633.6, ups=0.66, wpb=5532.2, bsz=355.4, num_updates=30800, lr=0.000180187, gnorm=0.238, train_wall=120, gb_free=9.2, wall=51136
2022-09-08 14:19:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:19:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:19:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:19:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:19:49 | INFO | valid | epoch 382 | valid on 'valid' subset | loss 5.05 | nll_loss 2.428 | mask_loss 9.47336 | p_2 0.04895 | mask_ave 0.604 | ppl 5.38 | bleu 56.81 | wps 1503.7 | wpb 933.5 | bsz 59.6 | num_updates 30861 | best_bleu 57.29
2022-09-08 14:19:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 382 @ 30861 updates
2022-09-08 14:19:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint382.pt
2022-09-08 14:19:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint382.pt
2022-09-08 14:20:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint382.pt (epoch 382 @ 30861 updates, score 56.81) (writing took 18.10936039686203 seconds)
2022-09-08 14:20:07 | INFO | fairseq_cli.train | end of epoch 382 (average epoch stats below)
2022-09-08 14:20:07 | INFO | train | epoch 382 | loss 3.38 | nll_loss 0.343 | mask_loss 8.71775 | p_2 0.03529 | mask_ave 0.493 | ppl 1.27 | wps 3466.6 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 30861 | lr 0.000180009 | gnorm 0.239 | train_wall 98 | gb_free 9.1 | wall 51240
2022-09-08 14:20:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:20:08 | INFO | fairseq.trainer | begin training epoch 383
2022-09-08 14:20:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:20:55 | INFO | train_inner | epoch 383:     39 / 81 loss=3.38, nll_loss=0.343, mask_loss=8.66414, p_2=0.03548, mask_ave=0.495, ppl=1.27, wps=3624.4, ups=0.66, wpb=5523.3, bsz=363.6, num_updates=30900, lr=0.000179896, gnorm=0.238, train_wall=121, gb_free=9, wall=51288
2022-09-08 14:21:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:21:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:21:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:21:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:21:57 | INFO | valid | epoch 383 | valid on 'valid' subset | loss 5.064 | nll_loss 2.448 | mask_loss 9.49452 | p_2 0.04868 | mask_ave 0.611 | ppl 5.46 | bleu 56.15 | wps 1579.9 | wpb 933.5 | bsz 59.6 | num_updates 30942 | best_bleu 57.29
2022-09-08 14:21:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 383 @ 30942 updates
2022-09-08 14:21:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint383.pt
2022-09-08 14:21:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint383.pt
2022-09-08 14:22:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint383.pt (epoch 383 @ 30942 updates, score 56.15) (writing took 21.582237254828215 seconds)
2022-09-08 14:22:18 | INFO | fairseq_cli.train | end of epoch 383 (average epoch stats below)
2022-09-08 14:22:18 | INFO | train | epoch 383 | loss 3.38 | nll_loss 0.344 | mask_loss 8.64702 | p_2 0.03528 | mask_ave 0.493 | ppl 1.27 | wps 3418.1 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 30942 | lr 0.000179774 | gnorm 0.251 | train_wall 97 | gb_free 9.2 | wall 51371
2022-09-08 14:22:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:22:18 | INFO | fairseq.trainer | begin training epoch 384
2022-09-08 14:22:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:23:31 | INFO | train_inner | epoch 384:     58 / 81 loss=3.381, nll_loss=0.345, mask_loss=8.63117, p_2=0.03515, mask_ave=0.491, ppl=1.27, wps=3570.5, ups=0.64, wpb=5536.6, bsz=357.8, num_updates=31000, lr=0.000179605, gnorm=0.251, train_wall=121, gb_free=9.1, wall=51443
2022-09-08 14:23:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:24:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:24:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:24:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:24:09 | INFO | valid | epoch 384 | valid on 'valid' subset | loss 5.065 | nll_loss 2.452 | mask_loss 9.41087 | p_2 0.04885 | mask_ave 0.608 | ppl 5.47 | bleu 56.52 | wps 1551.3 | wpb 933.5 | bsz 59.6 | num_updates 31023 | best_bleu 57.29
2022-09-08 14:24:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 384 @ 31023 updates
2022-09-08 14:24:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint384.pt
2022-09-08 14:24:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint384.pt
2022-09-08 14:24:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint384.pt (epoch 384 @ 31023 updates, score 56.52) (writing took 21.01790364086628 seconds)
2022-09-08 14:24:30 | INFO | fairseq_cli.train | end of epoch 384 (average epoch stats below)
2022-09-08 14:24:30 | INFO | train | epoch 384 | loss 3.381 | nll_loss 0.344 | mask_loss 8.63321 | p_2 0.03528 | mask_ave 0.493 | ppl 1.27 | wps 3390.5 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 31023 | lr 0.000179539 | gnorm 0.25 | train_wall 98 | gb_free 9.1 | wall 51503
2022-09-08 14:24:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:24:30 | INFO | fairseq.trainer | begin training epoch 385
2022-09-08 14:24:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:26:05 | INFO | train_inner | epoch 385:     77 / 81 loss=3.381, nll_loss=0.345, mask_loss=8.57229, p_2=0.03538, mask_ave=0.495, ppl=1.27, wps=3584.8, ups=0.65, wpb=5530, bsz=358.3, num_updates=31100, lr=0.000179316, gnorm=0.256, train_wall=120, gb_free=9.2, wall=51598
2022-09-08 14:26:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:26:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:26:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:26:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:26:21 | INFO | valid | epoch 385 | valid on 'valid' subset | loss 5.051 | nll_loss 2.435 | mask_loss 9.52134 | p_2 0.04889 | mask_ave 0.607 | ppl 5.41 | bleu 56.26 | wps 1514.4 | wpb 933.5 | bsz 59.6 | num_updates 31104 | best_bleu 57.29
2022-09-08 14:26:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 385 @ 31104 updates
2022-09-08 14:26:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint385.pt
2022-09-08 14:26:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint385.pt
2022-09-08 14:26:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint385.pt (epoch 385 @ 31104 updates, score 56.26) (writing took 20.67340949550271 seconds)
2022-09-08 14:26:42 | INFO | fairseq_cli.train | end of epoch 385 (average epoch stats below)
2022-09-08 14:26:42 | INFO | train | epoch 385 | loss 3.381 | nll_loss 0.345 | mask_loss 8.55776 | p_2 0.03525 | mask_ave 0.494 | ppl 1.27 | wps 3408.4 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 31104 | lr 0.000179305 | gnorm 0.253 | train_wall 97 | gb_free 9.1 | wall 51634
2022-09-08 14:26:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:26:42 | INFO | fairseq.trainer | begin training epoch 386
2022-09-08 14:26:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:28:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:28:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:28:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:28:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:28:31 | INFO | valid | epoch 386 | valid on 'valid' subset | loss 5.062 | nll_loss 2.448 | mask_loss 9.62605 | p_2 0.04863 | mask_ave 0.614 | ppl 5.46 | bleu 56.31 | wps 1533.7 | wpb 933.5 | bsz 59.6 | num_updates 31185 | best_bleu 57.29
2022-09-08 14:28:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 386 @ 31185 updates
2022-09-08 14:28:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint386.pt
2022-09-08 14:28:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint386.pt
2022-09-08 14:28:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint386.pt (epoch 386 @ 31185 updates, score 56.31) (writing took 22.129150606691837 seconds)
2022-09-08 14:28:54 | INFO | fairseq_cli.train | end of epoch 386 (average epoch stats below)
2022-09-08 14:28:54 | INFO | train | epoch 386 | loss 3.381 | nll_loss 0.344 | mask_loss 8.6794 | p_2 0.03527 | mask_ave 0.493 | ppl 1.27 | wps 3386.8 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 31185 | lr 0.000179072 | gnorm 0.248 | train_wall 97 | gb_free 9.2 | wall 51767
2022-09-08 14:28:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:28:54 | INFO | fairseq.trainer | begin training epoch 387
2022-09-08 14:28:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:29:13 | INFO | train_inner | epoch 387:     15 / 81 loss=3.38, nll_loss=0.344, mask_loss=8.70143, p_2=0.03512, mask_ave=0.493, ppl=1.27, wps=2926.1, ups=0.53, wpb=5507.4, bsz=357, num_updates=31200, lr=0.000179029, gnorm=0.241, train_wall=119, gb_free=9, wall=51786
2022-09-08 14:30:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:30:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:30:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:30:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:30:44 | INFO | valid | epoch 387 | valid on 'valid' subset | loss 5.063 | nll_loss 2.445 | mask_loss 9.45308 | p_2 0.04857 | mask_ave 0.616 | ppl 5.44 | bleu 56.62 | wps 1507.6 | wpb 933.5 | bsz 59.6 | num_updates 31266 | best_bleu 57.29
2022-09-08 14:30:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 387 @ 31266 updates
2022-09-08 14:30:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint387.pt
2022-09-08 14:30:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint387.pt
2022-09-08 14:31:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint387.pt (epoch 387 @ 31266 updates, score 56.62) (writing took 24.549075342714787 seconds)
2022-09-08 14:31:09 | INFO | fairseq_cli.train | end of epoch 387 (average epoch stats below)
2022-09-08 14:31:09 | INFO | train | epoch 387 | loss 3.38 | nll_loss 0.344 | mask_loss 8.70095 | p_2 0.03524 | mask_ave 0.494 | ppl 1.27 | wps 3309.5 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 31266 | lr 0.00017884 | gnorm 0.252 | train_wall 98 | gb_free 9.2 | wall 51902
2022-09-08 14:31:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:31:09 | INFO | fairseq.trainer | begin training epoch 388
2022-09-08 14:31:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:31:50 | INFO | train_inner | epoch 388:     34 / 81 loss=3.381, nll_loss=0.344, mask_loss=8.68494, p_2=0.03549, mask_ave=0.496, ppl=1.27, wps=3502.5, ups=0.64, wpb=5514.7, bsz=359.6, num_updates=31300, lr=0.000178743, gnorm=0.263, train_wall=120, gb_free=9.2, wall=51943
2022-09-08 14:32:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:32:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:32:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:32:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:32:59 | INFO | valid | epoch 388 | valid on 'valid' subset | loss 5.074 | nll_loss 2.462 | mask_loss 9.7277 | p_2 0.04882 | mask_ave 0.609 | ppl 5.51 | bleu 56.21 | wps 1476.8 | wpb 933.5 | bsz 59.6 | num_updates 31347 | best_bleu 57.29
2022-09-08 14:32:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 388 @ 31347 updates
2022-09-08 14:32:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint388.pt
2022-09-08 14:33:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint388.pt
2022-09-08 14:33:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint388.pt (epoch 388 @ 31347 updates, score 56.21) (writing took 17.676683239638805 seconds)
2022-09-08 14:33:17 | INFO | fairseq_cli.train | end of epoch 388 (average epoch stats below)
2022-09-08 14:33:17 | INFO | train | epoch 388 | loss 3.38 | nll_loss 0.344 | mask_loss 8.78018 | p_2 0.03523 | mask_ave 0.495 | ppl 1.27 | wps 3485.4 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 31347 | lr 0.000178608 | gnorm 0.258 | train_wall 97 | gb_free 9.1 | wall 52030
2022-09-08 14:33:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:33:17 | INFO | fairseq.trainer | begin training epoch 389
2022-09-08 14:33:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:34:24 | INFO | train_inner | epoch 389:     53 / 81 loss=3.38, nll_loss=0.344, mask_loss=8.81848, p_2=0.03498, mask_ave=0.494, ppl=1.27, wps=3613.6, ups=0.65, wpb=5545.8, bsz=355.7, num_updates=31400, lr=0.000178458, gnorm=0.246, train_wall=122, gb_free=9.1, wall=52097
2022-09-08 14:34:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:35:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:35:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:35:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:35:10 | INFO | valid | epoch 389 | valid on 'valid' subset | loss 5.065 | nll_loss 2.447 | mask_loss 9.53157 | p_2 0.04879 | mask_ave 0.609 | ppl 5.45 | bleu 56.69 | wps 1480.2 | wpb 933.5 | bsz 59.6 | num_updates 31428 | best_bleu 57.29
2022-09-08 14:35:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 389 @ 31428 updates
2022-09-08 14:35:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint389.pt
2022-09-08 14:35:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint389.pt
2022-09-08 14:35:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint389.pt (epoch 389 @ 31428 updates, score 56.69) (writing took 18.545908838510513 seconds)
2022-09-08 14:35:28 | INFO | fairseq_cli.train | end of epoch 389 (average epoch stats below)
2022-09-08 14:35:28 | INFO | train | epoch 389 | loss 3.38 | nll_loss 0.344 | mask_loss 8.75121 | p_2 0.03524 | mask_ave 0.494 | ppl 1.27 | wps 3412.2 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 31428 | lr 0.000178378 | gnorm 0.257 | train_wall 99 | gb_free 9.3 | wall 52161
2022-09-08 14:35:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:35:28 | INFO | fairseq.trainer | begin training epoch 390
2022-09-08 14:35:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:36:57 | INFO | train_inner | epoch 390:     72 / 81 loss=3.38, nll_loss=0.344, mask_loss=8.69436, p_2=0.03526, mask_ave=0.494, ppl=1.27, wps=3618.4, ups=0.65, wpb=5533.7, bsz=358.5, num_updates=31500, lr=0.000178174, gnorm=0.252, train_wall=121, gb_free=9.2, wall=52250
2022-09-08 14:37:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:37:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:37:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:37:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:37:18 | INFO | valid | epoch 390 | valid on 'valid' subset | loss 5.082 | nll_loss 2.468 | mask_loss 9.6179 | p_2 0.04888 | mask_ave 0.607 | ppl 5.53 | bleu 56.48 | wps 1587.8 | wpb 933.5 | bsz 59.6 | num_updates 31509 | best_bleu 57.29
2022-09-08 14:37:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 390 @ 31509 updates
2022-09-08 14:37:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint390.pt
2022-09-08 14:37:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint390.pt
2022-09-08 14:37:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint390.pt (epoch 390 @ 31509 updates, score 56.48) (writing took 20.143204398453236 seconds)
2022-09-08 14:37:39 | INFO | fairseq_cli.train | end of epoch 390 (average epoch stats below)
2022-09-08 14:37:39 | INFO | train | epoch 390 | loss 3.38 | nll_loss 0.343 | mask_loss 8.70879 | p_2 0.03525 | mask_ave 0.494 | ppl 1.27 | wps 3435.9 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 31509 | lr 0.000178149 | gnorm 0.233 | train_wall 98 | gb_free 9.1 | wall 52291
2022-09-08 14:37:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:37:39 | INFO | fairseq.trainer | begin training epoch 391
2022-09-08 14:37:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:39:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:39:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:39:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:39:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:39:28 | INFO | valid | epoch 391 | valid on 'valid' subset | loss 5.083 | nll_loss 2.467 | mask_loss 9.56458 | p_2 0.04892 | mask_ave 0.605 | ppl 5.53 | bleu 57.04 | wps 1553.2 | wpb 933.5 | bsz 59.6 | num_updates 31590 | best_bleu 57.29
2022-09-08 14:39:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 391 @ 31590 updates
2022-09-08 14:39:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint391.pt
2022-09-08 14:39:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint391.pt
2022-09-08 14:39:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint391.pt (epoch 391 @ 31590 updates, score 57.04) (writing took 18.825763545930386 seconds)
2022-09-08 14:39:47 | INFO | fairseq_cli.train | end of epoch 391 (average epoch stats below)
2022-09-08 14:39:47 | INFO | train | epoch 391 | loss 3.38 | nll_loss 0.343 | mask_loss 8.73207 | p_2 0.0353 | mask_ave 0.492 | ppl 1.27 | wps 3481.8 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 31590 | lr 0.00017792 | gnorm 0.237 | train_wall 97 | gb_free 9.1 | wall 52420
2022-09-08 14:39:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:39:47 | INFO | fairseq.trainer | begin training epoch 392
2022-09-08 14:39:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:40:00 | INFO | train_inner | epoch 392:     10 / 81 loss=3.38, nll_loss=0.343, mask_loss=8.7419, p_2=0.03536, mask_ave=0.493, ppl=1.27, wps=2996, ups=0.55, wpb=5492.5, bsz=357.4, num_updates=31600, lr=0.000177892, gnorm=0.239, train_wall=120, gb_free=9.1, wall=52433
2022-09-08 14:41:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:41:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:41:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:41:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:41:36 | INFO | valid | epoch 392 | valid on 'valid' subset | loss 5.071 | nll_loss 2.455 | mask_loss 9.73465 | p_2 0.04885 | mask_ave 0.607 | ppl 5.48 | bleu 56.45 | wps 1524.7 | wpb 933.5 | bsz 59.6 | num_updates 31671 | best_bleu 57.29
2022-09-08 14:41:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 392 @ 31671 updates
2022-09-08 14:41:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint392.pt
2022-09-08 14:41:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint392.pt
2022-09-08 14:42:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint392.pt (epoch 392 @ 31671 updates, score 56.45) (writing took 23.775068309158087 seconds)
2022-09-08 14:42:00 | INFO | fairseq_cli.train | end of epoch 392 (average epoch stats below)
2022-09-08 14:42:00 | INFO | train | epoch 392 | loss 3.381 | nll_loss 0.344 | mask_loss 8.8348 | p_2 0.03523 | mask_ave 0.495 | ppl 1.27 | wps 3358.3 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 31671 | lr 0.000177693 | gnorm 0.263 | train_wall 97 | gb_free 9.1 | wall 52553
2022-09-08 14:42:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:42:00 | INFO | fairseq.trainer | begin training epoch 393
2022-09-08 14:42:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:42:37 | INFO | train_inner | epoch 393:     29 / 81 loss=3.381, nll_loss=0.344, mask_loss=8.83627, p_2=0.03526, mask_ave=0.495, ppl=1.27, wps=3540.3, ups=0.64, wpb=5534.7, bsz=357.7, num_updates=31700, lr=0.000177611, gnorm=0.252, train_wall=120, gb_free=9, wall=52589
2022-09-08 14:43:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:43:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:43:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:43:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:43:51 | INFO | valid | epoch 393 | valid on 'valid' subset | loss 5.079 | nll_loss 2.465 | mask_loss 9.78294 | p_2 0.0488 | mask_ave 0.61 | ppl 5.52 | bleu 56.8 | wps 1516.6 | wpb 933.5 | bsz 59.6 | num_updates 31752 | best_bleu 57.29
2022-09-08 14:43:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 393 @ 31752 updates
2022-09-08 14:43:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint393.pt
2022-09-08 14:43:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint393.pt
2022-09-08 14:43:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint393.pt (epoch 393 @ 31752 updates, score 56.8) (writing took 2.5152138210833073 seconds)
2022-09-08 14:43:54 | INFO | fairseq_cli.train | end of epoch 393 (average epoch stats below)
2022-09-08 14:43:54 | INFO | train | epoch 393 | loss 3.38 | nll_loss 0.343 | mask_loss 8.79006 | p_2 0.03518 | mask_ave 0.496 | ppl 1.27 | wps 3943.8 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 31752 | lr 0.000177466 | gnorm 0.239 | train_wall 98 | gb_free 9.2 | wall 52667
2022-09-08 14:43:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:43:54 | INFO | fairseq.trainer | begin training epoch 394
2022-09-08 14:43:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:44:54 | INFO | train_inner | epoch 394:     48 / 81 loss=3.379, nll_loss=0.343, mask_loss=8.81344, p_2=0.0351, mask_ave=0.497, ppl=1.27, wps=4033.4, ups=0.73, wpb=5531.4, bsz=356.3, num_updates=31800, lr=0.000177332, gnorm=0.24, train_wall=121, gb_free=9.1, wall=52727
2022-09-08 14:45:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:45:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:45:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:45:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:45:43 | INFO | valid | epoch 394 | valid on 'valid' subset | loss 5.079 | nll_loss 2.469 | mask_loss 9.60682 | p_2 0.04868 | mask_ave 0.612 | ppl 5.54 | bleu 56.63 | wps 1534 | wpb 933.5 | bsz 59.6 | num_updates 31833 | best_bleu 57.29
2022-09-08 14:45:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 394 @ 31833 updates
2022-09-08 14:45:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint394.pt
2022-09-08 14:45:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint394.pt
2022-09-08 14:45:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint394.pt (epoch 394 @ 31833 updates, score 56.63) (writing took 2.50009785592556 seconds)
2022-09-08 14:45:46 | INFO | fairseq_cli.train | end of epoch 394 (average epoch stats below)
2022-09-08 14:45:46 | INFO | train | epoch 394 | loss 3.38 | nll_loss 0.344 | mask_loss 8.79238 | p_2 0.0352 | mask_ave 0.496 | ppl 1.27 | wps 3981.5 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 31833 | lr 0.00017724 | gnorm 0.249 | train_wall 97 | gb_free 9.1 | wall 52779
2022-09-08 14:45:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:45:46 | INFO | fairseq.trainer | begin training epoch 395
2022-09-08 14:45:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:47:07 | INFO | train_inner | epoch 395:     67 / 81 loss=3.38, nll_loss=0.344, mask_loss=8.75933, p_2=0.03504, mask_ave=0.495, ppl=1.27, wps=4147.6, ups=0.75, wpb=5531.2, bsz=360.2, num_updates=31900, lr=0.000177054, gnorm=0.247, train_wall=118, gb_free=9.1, wall=52860
2022-09-08 14:47:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:47:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:47:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:47:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:47:35 | INFO | valid | epoch 395 | valid on 'valid' subset | loss 5.082 | nll_loss 2.472 | mask_loss 9.63389 | p_2 0.04845 | mask_ave 0.62 | ppl 5.55 | bleu 56.16 | wps 1494.3 | wpb 933.5 | bsz 59.6 | num_updates 31914 | best_bleu 57.29
2022-09-08 14:47:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 395 @ 31914 updates
2022-09-08 14:47:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint395.pt
2022-09-08 14:47:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint395.pt
2022-09-08 14:47:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint395.pt (epoch 395 @ 31914 updates, score 56.16) (writing took 18.648228339850903 seconds)
2022-09-08 14:47:54 | INFO | fairseq_cli.train | end of epoch 395 (average epoch stats below)
2022-09-08 14:47:54 | INFO | train | epoch 395 | loss 3.38 | nll_loss 0.343 | mask_loss 8.7978 | p_2 0.03509 | mask_ave 0.499 | ppl 1.27 | wps 3508.8 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 31914 | lr 0.000177015 | gnorm 0.24 | train_wall 96 | gb_free 9.1 | wall 52906
2022-09-08 14:47:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:47:54 | INFO | fairseq.trainer | begin training epoch 396
2022-09-08 14:47:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:49:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:49:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:49:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:49:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:49:45 | INFO | valid | epoch 396 | valid on 'valid' subset | loss 5.07 | nll_loss 2.46 | mask_loss 9.76689 | p_2 0.0487 | mask_ave 0.612 | ppl 5.5 | bleu 56.36 | wps 1470.8 | wpb 933.5 | bsz 59.6 | num_updates 31995 | best_bleu 57.29
2022-09-08 14:49:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 396 @ 31995 updates
2022-09-08 14:49:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint396.pt
2022-09-08 14:49:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint396.pt
2022-09-08 14:50:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint396.pt (epoch 396 @ 31995 updates, score 56.36) (writing took 19.036551136523485 seconds)
2022-09-08 14:50:04 | INFO | fairseq_cli.train | end of epoch 396 (average epoch stats below)
2022-09-08 14:50:04 | INFO | train | epoch 396 | loss 3.38 | nll_loss 0.344 | mask_loss 8.7348 | p_2 0.03524 | mask_ave 0.494 | ppl 1.27 | wps 3435.3 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 31995 | lr 0.000176791 | gnorm 0.255 | train_wall 98 | gb_free 9 | wall 53037
2022-09-08 14:50:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:50:04 | INFO | fairseq.trainer | begin training epoch 397
2022-09-08 14:50:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:50:11 | INFO | train_inner | epoch 397:      5 / 81 loss=3.38, nll_loss=0.344, mask_loss=8.77265, p_2=0.03526, mask_ave=0.497, ppl=1.27, wps=2992.6, ups=0.55, wpb=5490.1, bsz=355.3, num_updates=32000, lr=0.000176777, gnorm=0.256, train_wall=120, gb_free=9.1, wall=53043
2022-09-08 14:51:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:51:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:51:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:51:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:51:55 | INFO | valid | epoch 397 | valid on 'valid' subset | loss 5.064 | nll_loss 2.448 | mask_loss 9.7304 | p_2 0.04853 | mask_ave 0.618 | ppl 5.46 | bleu 56.29 | wps 1538.3 | wpb 933.5 | bsz 59.6 | num_updates 32076 | best_bleu 57.29
2022-09-08 14:51:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 397 @ 32076 updates
2022-09-08 14:51:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint397.pt
2022-09-08 14:51:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint397.pt
2022-09-08 14:52:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint397.pt (epoch 397 @ 32076 updates, score 56.29) (writing took 19.011432696133852 seconds)
2022-09-08 14:52:14 | INFO | fairseq_cli.train | end of epoch 397 (average epoch stats below)
2022-09-08 14:52:14 | INFO | train | epoch 397 | loss 3.38 | nll_loss 0.344 | mask_loss 8.73357 | p_2 0.03521 | mask_ave 0.495 | ppl 1.27 | wps 3442.2 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 32076 | lr 0.000176567 | gnorm 0.251 | train_wall 98 | gb_free 9.2 | wall 53167
2022-09-08 14:52:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:52:14 | INFO | fairseq.trainer | begin training epoch 398
2022-09-08 14:52:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:52:45 | INFO | train_inner | epoch 398:     24 / 81 loss=3.38, nll_loss=0.343, mask_loss=8.723, p_2=0.03526, mask_ave=0.496, ppl=1.27, wps=3598.4, ups=0.65, wpb=5540.9, bsz=364.1, num_updates=32100, lr=0.000176501, gnorm=0.246, train_wall=122, gb_free=9.1, wall=53197
2022-09-08 14:53:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:53:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:53:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:53:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:54:05 | INFO | valid | epoch 398 | valid on 'valid' subset | loss 5.073 | nll_loss 2.462 | mask_loss 9.60512 | p_2 0.04845 | mask_ave 0.62 | ppl 5.51 | bleu 56.44 | wps 1546.3 | wpb 933.5 | bsz 59.6 | num_updates 32157 | best_bleu 57.29
2022-09-08 14:54:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 398 @ 32157 updates
2022-09-08 14:54:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint398.pt
2022-09-08 14:54:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint398.pt
2022-09-08 14:54:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint398.pt (epoch 398 @ 32157 updates, score 56.44) (writing took 18.638071842491627 seconds)
2022-09-08 14:54:23 | INFO | fairseq_cli.train | end of epoch 398 (average epoch stats below)
2022-09-08 14:54:23 | INFO | train | epoch 398 | loss 3.38 | nll_loss 0.344 | mask_loss 8.72889 | p_2 0.0351 | mask_ave 0.499 | ppl 1.27 | wps 3448.8 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 32157 | lr 0.000176345 | gnorm 0.257 | train_wall 98 | gb_free 9.1 | wall 53296
2022-09-08 14:54:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:54:24 | INFO | fairseq.trainer | begin training epoch 399
2022-09-08 14:54:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:55:16 | INFO | train_inner | epoch 399:     43 / 81 loss=3.38, nll_loss=0.344, mask_loss=8.6747, p_2=0.03518, mask_ave=0.5, ppl=1.27, wps=3629.9, ups=0.66, wpb=5512.9, bsz=354.9, num_updates=32200, lr=0.000176227, gnorm=0.257, train_wall=120, gb_free=9, wall=53349
2022-09-08 14:56:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:56:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:56:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:56:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:56:14 | INFO | valid | epoch 399 | valid on 'valid' subset | loss 5.079 | nll_loss 2.46 | mask_loss 9.49892 | p_2 0.04839 | mask_ave 0.621 | ppl 5.5 | bleu 56.77 | wps 1537.8 | wpb 933.5 | bsz 59.6 | num_updates 32238 | best_bleu 57.29
2022-09-08 14:56:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 399 @ 32238 updates
2022-09-08 14:56:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint399.pt
2022-09-08 14:56:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint399.pt
2022-09-08 14:56:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint399.pt (epoch 399 @ 32238 updates, score 56.77) (writing took 2.5691597871482372 seconds)
2022-09-08 14:56:16 | INFO | fairseq_cli.train | end of epoch 399 (average epoch stats below)
2022-09-08 14:56:16 | INFO | train | epoch 399 | loss 3.38 | nll_loss 0.344 | mask_loss 8.66092 | p_2 0.035 | mask_ave 0.502 | ppl 1.27 | wps 3964.2 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 32238 | lr 0.000176123 | gnorm 0.255 | train_wall 97 | gb_free 9.2 | wall 53409
2022-09-08 14:56:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:56:17 | INFO | fairseq.trainer | begin training epoch 400
2022-09-08 14:56:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:57:33 | INFO | train_inner | epoch 400:     62 / 81 loss=3.38, nll_loss=0.344, mask_loss=8.76936, p_2=0.03481, mask_ave=0.499, ppl=1.27, wps=4090, ups=0.73, wpb=5566.5, bsz=360.7, num_updates=32300, lr=0.000175954, gnorm=0.257, train_wall=121, gb_free=9, wall=53485
2022-09-08 14:57:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:57:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:57:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:57:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:58:06 | INFO | valid | epoch 400 | valid on 'valid' subset | loss 5.075 | nll_loss 2.459 | mask_loss 9.75525 | p_2 0.04863 | mask_ave 0.614 | ppl 5.5 | bleu 56.22 | wps 1520.9 | wpb 933.5 | bsz 59.6 | num_updates 32319 | best_bleu 57.29
2022-09-08 14:58:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 400 @ 32319 updates
2022-09-08 14:58:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint400.pt
2022-09-08 14:58:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint400.pt
2022-09-08 14:58:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint400.pt (epoch 400 @ 32319 updates, score 56.22) (writing took 2.5578461922705173 seconds)
2022-09-08 14:58:09 | INFO | fairseq_cli.train | end of epoch 400 (average epoch stats below)
2022-09-08 14:58:09 | INFO | train | epoch 400 | loss 3.38 | nll_loss 0.344 | mask_loss 8.83459 | p_2 0.03507 | mask_ave 0.499 | ppl 1.27 | wps 3986 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 32319 | lr 0.000175902 | gnorm 0.261 | train_wall 97 | gb_free 9.2 | wall 53521
2022-09-08 14:58:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 14:58:09 | INFO | fairseq.trainer | begin training epoch 401
2022-09-08 14:58:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 14:59:47 | INFO | train_inner | epoch 401:     81 / 81 loss=3.38, nll_loss=0.344, mask_loss=8.77837, p_2=0.03527, mask_ave=0.498, ppl=1.27, wps=4080.8, ups=0.75, wpb=5475.5, bsz=354.4, num_updates=32400, lr=0.000175682, gnorm=0.268, train_wall=119, gb_free=9.2, wall=53620
2022-09-08 14:59:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 14:59:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 14:59:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 14:59:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 14:59:58 | INFO | valid | epoch 401 | valid on 'valid' subset | loss 5.072 | nll_loss 2.455 | mask_loss 9.62653 | p_2 0.04873 | mask_ave 0.611 | ppl 5.48 | bleu 56.2 | wps 1518 | wpb 933.5 | bsz 59.6 | num_updates 32400 | best_bleu 57.29
2022-09-08 14:59:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 401 @ 32400 updates
2022-09-08 14:59:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint401.pt
2022-09-08 14:59:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint401.pt
2022-09-08 15:00:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint401.pt (epoch 401 @ 32400 updates, score 56.2) (writing took 22.67493885010481 seconds)
2022-09-08 15:00:21 | INFO | fairseq_cli.train | end of epoch 401 (average epoch stats below)
2022-09-08 15:00:21 | INFO | train | epoch 401 | loss 3.38 | nll_loss 0.344 | mask_loss 8.75363 | p_2 0.03519 | mask_ave 0.496 | ppl 1.27 | wps 3385.3 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 32400 | lr 0.000175682 | gnorm 0.264 | train_wall 97 | gb_free 9.2 | wall 53654
2022-09-08 15:00:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:00:21 | INFO | fairseq.trainer | begin training epoch 402
2022-09-08 15:00:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:02:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 15:02:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 15:02:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 15:02:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 15:02:12 | INFO | valid | epoch 402 | valid on 'valid' subset | loss 5.072 | nll_loss 2.455 | mask_loss 9.68274 | p_2 0.04898 | mask_ave 0.604 | ppl 5.48 | bleu 56.45 | wps 1549.6 | wpb 933.5 | bsz 59.6 | num_updates 32481 | best_bleu 57.29
2022-09-08 15:02:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 402 @ 32481 updates
2022-09-08 15:02:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint402.pt
2022-09-08 15:02:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint402.pt
2022-09-08 15:02:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint402.pt (epoch 402 @ 32481 updates, score 56.45) (writing took 23.22761444747448 seconds)
2022-09-08 15:02:36 | INFO | fairseq_cli.train | end of epoch 402 (average epoch stats below)
2022-09-08 15:02:36 | INFO | train | epoch 402 | loss 3.38 | nll_loss 0.344 | mask_loss 8.8086 | p_2 0.03527 | mask_ave 0.494 | ppl 1.27 | wps 3312.3 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 32481 | lr 0.000175463 | gnorm 0.24 | train_wall 99 | gb_free 9.2 | wall 53789
2022-09-08 15:02:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:02:36 | INFO | fairseq.trainer | begin training epoch 403
2022-09-08 15:02:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:03:00 | INFO | train_inner | epoch 403:     19 / 81 loss=3.38, nll_loss=0.343, mask_loss=8.80554, p_2=0.03547, mask_ave=0.493, ppl=1.27, wps=2850.4, ups=0.52, wpb=5506.4, bsz=358.2, num_updates=32500, lr=0.000175412, gnorm=0.238, train_wall=122, gb_free=9, wall=53813
2022-09-08 15:04:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 15:04:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 15:04:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 15:04:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 15:04:27 | INFO | valid | epoch 403 | valid on 'valid' subset | loss 5.049 | nll_loss 2.43 | mask_loss 9.79645 | p_2 0.04868 | mask_ave 0.613 | ppl 5.39 | bleu 56.28 | wps 1524.6 | wpb 933.5 | bsz 59.6 | num_updates 32562 | best_bleu 57.29
2022-09-08 15:04:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 403 @ 32562 updates
2022-09-08 15:04:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint403.pt
2022-09-08 15:04:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint403.pt
2022-09-08 15:04:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint403.pt (epoch 403 @ 32562 updates, score 56.28) (writing took 22.77495238184929 seconds)
2022-09-08 15:04:50 | INFO | fairseq_cli.train | end of epoch 403 (average epoch stats below)
2022-09-08 15:04:50 | INFO | train | epoch 403 | loss 3.38 | nll_loss 0.344 | mask_loss 8.80912 | p_2 0.03528 | mask_ave 0.493 | ppl 1.27 | wps 3330 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 32562 | lr 0.000175245 | gnorm 0.253 | train_wall 98 | gb_free 9.1 | wall 53923
2022-09-08 15:04:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:04:50 | INFO | fairseq.trainer | begin training epoch 404
2022-09-08 15:04:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:05:37 | INFO | train_inner | epoch 404:     38 / 81 loss=3.379, nll_loss=0.343, mask_loss=8.81503, p_2=0.03487, mask_ave=0.495, ppl=1.27, wps=3531.3, ups=0.64, wpb=5543.6, bsz=356.9, num_updates=32600, lr=0.000175142, gnorm=0.263, train_wall=121, gb_free=9.1, wall=53970
2022-09-08 15:06:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 15:06:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 15:06:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 15:06:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 15:06:40 | INFO | valid | epoch 404 | valid on 'valid' subset | loss 5.052 | nll_loss 2.428 | mask_loss 9.65342 | p_2 0.04897 | mask_ave 0.604 | ppl 5.38 | bleu 56.82 | wps 1528.4 | wpb 933.5 | bsz 59.6 | num_updates 32643 | best_bleu 57.29
2022-09-08 15:06:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 404 @ 32643 updates
2022-09-08 15:06:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint404.pt
2022-09-08 15:06:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint404.pt
2022-09-08 15:07:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint404.pt (epoch 404 @ 32643 updates, score 56.82) (writing took 28.508227981626987 seconds)
2022-09-08 15:07:09 | INFO | fairseq_cli.train | end of epoch 404 (average epoch stats below)
2022-09-08 15:07:09 | INFO | train | epoch 404 | loss 3.379 | nll_loss 0.343 | mask_loss 8.7536 | p_2 0.03515 | mask_ave 0.497 | ppl 1.27 | wps 3234.3 | ups 0.59 | wpb 5523.2 | bsz 358 | num_updates 32643 | lr 0.000175027 | gnorm 0.263 | train_wall 97 | gb_free 9.1 | wall 54061
2022-09-08 15:07:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:07:09 | INFO | fairseq.trainer | begin training epoch 405
2022-09-08 15:07:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:08:19 | INFO | train_inner | epoch 405:     57 / 81 loss=3.379, nll_loss=0.343, mask_loss=8.74746, p_2=0.03549, mask_ave=0.495, ppl=1.27, wps=3411.3, ups=0.62, wpb=5531.6, bsz=360.7, num_updates=32700, lr=0.000174874, gnorm=0.247, train_wall=121, gb_free=9, wall=54132
2022-09-08 15:08:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 15:08:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 15:08:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 15:08:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 15:08:59 | INFO | valid | epoch 405 | valid on 'valid' subset | loss 5.051 | nll_loss 2.428 | mask_loss 9.72273 | p_2 0.04863 | mask_ave 0.614 | ppl 5.38 | bleu 56.64 | wps 1504.1 | wpb 933.5 | bsz 59.6 | num_updates 32724 | best_bleu 57.29
2022-09-08 15:08:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 405 @ 32724 updates
2022-09-08 15:08:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint405.pt
2022-09-08 15:09:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint405.pt
2022-09-08 15:09:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint405.pt (epoch 405 @ 32724 updates, score 56.64) (writing took 2.510801561176777 seconds)
2022-09-08 15:09:02 | INFO | fairseq_cli.train | end of epoch 405 (average epoch stats below)
2022-09-08 15:09:02 | INFO | train | epoch 405 | loss 3.379 | nll_loss 0.343 | mask_loss 8.81815 | p_2 0.03528 | mask_ave 0.493 | ppl 1.27 | wps 3937.6 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 32724 | lr 0.00017481 | gnorm 0.24 | train_wall 98 | gb_free 9 | wall 54175
2022-09-08 15:09:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:09:02 | INFO | fairseq.trainer | begin training epoch 406
2022-09-08 15:09:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:10:36 | INFO | train_inner | epoch 406:     76 / 81 loss=3.38, nll_loss=0.344, mask_loss=8.84734, p_2=0.03519, mask_ave=0.494, ppl=1.27, wps=4031.1, ups=0.73, wpb=5534.8, bsz=359, num_updates=32800, lr=0.000174608, gnorm=0.264, train_wall=122, gb_free=9, wall=54269
2022-09-08 15:10:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 15:10:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 15:10:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 15:10:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 15:10:52 | INFO | valid | epoch 406 | valid on 'valid' subset | loss 5.057 | nll_loss 2.437 | mask_loss 9.4472 | p_2 0.04885 | mask_ave 0.607 | ppl 5.41 | bleu 56.73 | wps 1592.2 | wpb 933.5 | bsz 59.6 | num_updates 32805 | best_bleu 57.29
2022-09-08 15:10:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 406 @ 32805 updates
2022-09-08 15:10:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint406.pt
2022-09-08 15:10:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint406.pt
2022-09-08 15:10:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint406.pt (epoch 406 @ 32805 updates, score 56.73) (writing took 2.5330548472702503 seconds)
2022-09-08 15:10:55 | INFO | fairseq_cli.train | end of epoch 406 (average epoch stats below)
2022-09-08 15:10:55 | INFO | train | epoch 406 | loss 3.38 | nll_loss 0.344 | mask_loss 8.83153 | p_2 0.03521 | mask_ave 0.495 | ppl 1.27 | wps 3960.1 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 32805 | lr 0.000174594 | gnorm 0.273 | train_wall 98 | gb_free 9.1 | wall 54288
2022-09-08 15:10:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:10:55 | INFO | fairseq.trainer | begin training epoch 407
2022-09-08 15:10:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:12:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 15:12:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 15:12:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 15:12:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 15:12:46 | INFO | valid | epoch 407 | valid on 'valid' subset | loss 5.073 | nll_loss 2.456 | mask_loss 9.62063 | p_2 0.0488 | mask_ave 0.609 | ppl 5.49 | bleu 56.53 | wps 1555.6 | wpb 933.5 | bsz 59.6 | num_updates 32886 | best_bleu 57.29
2022-09-08 15:12:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 407 @ 32886 updates
2022-09-08 15:12:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint407.pt
2022-09-08 15:12:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint407.pt
2022-09-08 15:13:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint407.pt (epoch 407 @ 32886 updates, score 56.53) (writing took 19.942769076675177 seconds)
2022-09-08 15:13:06 | INFO | fairseq_cli.train | end of epoch 407 (average epoch stats below)
2022-09-08 15:13:06 | INFO | train | epoch 407 | loss 3.379 | nll_loss 0.343 | mask_loss 8.71292 | p_2 0.03517 | mask_ave 0.496 | ppl 1.27 | wps 3409.3 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 32886 | lr 0.000174379 | gnorm 0.236 | train_wall 99 | gb_free 9.3 | wall 54419
2022-09-08 15:13:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:13:07 | INFO | fairseq.trainer | begin training epoch 408
2022-09-08 15:13:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:13:24 | INFO | train_inner | epoch 408:     14 / 81 loss=3.379, nll_loss=0.343, mask_loss=8.71772, p_2=0.03504, mask_ave=0.496, ppl=1.27, wps=3285.5, ups=0.6, wpb=5511.1, bsz=354, num_updates=32900, lr=0.000174342, gnorm=0.244, train_wall=121, gb_free=9.2, wall=54437
2022-09-08 15:14:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 15:14:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 15:14:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 15:14:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 15:14:57 | INFO | valid | epoch 408 | valid on 'valid' subset | loss 5.054 | nll_loss 2.434 | mask_loss 9.45724 | p_2 0.04889 | mask_ave 0.606 | ppl 5.4 | bleu 56.8 | wps 1523.2 | wpb 933.5 | bsz 59.6 | num_updates 32967 | best_bleu 57.29
2022-09-08 15:14:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 408 @ 32967 updates
2022-09-08 15:14:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint408.pt
2022-09-08 15:14:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint408.pt
2022-09-08 15:15:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint408.pt (epoch 408 @ 32967 updates, score 56.8) (writing took 2.4977355897426605 seconds)
2022-09-08 15:15:00 | INFO | fairseq_cli.train | end of epoch 408 (average epoch stats below)
2022-09-08 15:15:00 | INFO | train | epoch 408 | loss 3.379 | nll_loss 0.343 | mask_loss 8.76335 | p_2 0.03525 | mask_ave 0.494 | ppl 1.27 | wps 3936.2 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 32967 | lr 0.000174165 | gnorm 0.253 | train_wall 98 | gb_free 9.3 | wall 54533
2022-09-08 15:15:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:15:00 | INFO | fairseq.trainer | begin training epoch 409
2022-09-08 15:15:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:15:42 | INFO | train_inner | epoch 409:     33 / 81 loss=3.379, nll_loss=0.343, mask_loss=8.75247, p_2=0.0355, mask_ave=0.493, ppl=1.27, wps=3996.4, ups=0.73, wpb=5504.3, bsz=360.8, num_updates=33000, lr=0.000174078, gnorm=0.247, train_wall=122, gb_free=9.1, wall=54575
2022-09-08 15:16:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 15:16:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 15:16:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 15:16:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 15:16:53 | INFO | valid | epoch 409 | valid on 'valid' subset | loss 5.062 | nll_loss 2.442 | mask_loss 9.56922 | p_2 0.04887 | mask_ave 0.607 | ppl 5.43 | bleu 56.66 | wps 1412.4 | wpb 933.5 | bsz 59.6 | num_updates 33048 | best_bleu 57.29
2022-09-08 15:16:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 409 @ 33048 updates
2022-09-08 15:16:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint409.pt
2022-09-08 15:16:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint409.pt
2022-09-08 15:17:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint409.pt (epoch 409 @ 33048 updates, score 56.66) (writing took 21.84540507569909 seconds)
2022-09-08 15:17:15 | INFO | fairseq_cli.train | end of epoch 409 (average epoch stats below)
2022-09-08 15:17:15 | INFO | train | epoch 409 | loss 3.379 | nll_loss 0.344 | mask_loss 8.69125 | p_2 0.03532 | mask_ave 0.492 | ppl 1.27 | wps 3320.4 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 33048 | lr 0.000173951 | gnorm 0.249 | train_wall 99 | gb_free 9.2 | wall 54668
2022-09-08 15:17:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:17:15 | INFO | fairseq.trainer | begin training epoch 410
2022-09-08 15:17:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:18:19 | INFO | train_inner | epoch 410:     52 / 81 loss=3.379, nll_loss=0.344, mask_loss=8.68556, p_2=0.03526, mask_ave=0.493, ppl=1.27, wps=3534.7, ups=0.64, wpb=5551.3, bsz=362.2, num_updates=33100, lr=0.000173814, gnorm=0.246, train_wall=122, gb_free=9.1, wall=54732
2022-09-08 15:18:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 15:18:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 15:18:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 15:18:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 15:19:05 | INFO | valid | epoch 410 | valid on 'valid' subset | loss 5.054 | nll_loss 2.439 | mask_loss 9.49292 | p_2 0.04866 | mask_ave 0.613 | ppl 5.42 | bleu 57.11 | wps 1508.5 | wpb 933.5 | bsz 59.6 | num_updates 33129 | best_bleu 57.29
2022-09-08 15:19:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 410 @ 33129 updates
2022-09-08 15:19:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint410.pt
2022-09-08 15:19:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint410.pt
2022-09-08 15:19:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint410.pt (epoch 410 @ 33129 updates, score 57.11) (writing took 2.4720142669975758 seconds)
2022-09-08 15:19:07 | INFO | fairseq_cli.train | end of epoch 410 (average epoch stats below)
2022-09-08 15:19:07 | INFO | train | epoch 410 | loss 3.379 | nll_loss 0.343 | mask_loss 8.73402 | p_2 0.03526 | mask_ave 0.494 | ppl 1.27 | wps 3976 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 33129 | lr 0.000173738 | gnorm 0.245 | train_wall 97 | gb_free 9.1 | wall 54780
2022-09-08 15:19:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:19:07 | INFO | fairseq.trainer | begin training epoch 411
2022-09-08 15:19:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:20:35 | INFO | train_inner | epoch 411:     71 / 81 loss=3.379, nll_loss=0.344, mask_loss=8.75428, p_2=0.03521, mask_ave=0.495, ppl=1.27, wps=4060.4, ups=0.74, wpb=5524.2, bsz=354.3, num_updates=33200, lr=0.000173553, gnorm=0.261, train_wall=120, gb_free=9.1, wall=54868
2022-09-08 15:20:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 15:20:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 15:20:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 15:20:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 15:20:58 | INFO | valid | epoch 411 | valid on 'valid' subset | loss 5.065 | nll_loss 2.448 | mask_loss 9.54629 | p_2 0.04873 | mask_ave 0.611 | ppl 5.46 | bleu 56.94 | wps 1499.1 | wpb 933.5 | bsz 59.6 | num_updates 33210 | best_bleu 57.29
2022-09-08 15:20:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 411 @ 33210 updates
2022-09-08 15:20:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint411.pt
2022-09-08 15:21:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint411.pt
2022-09-08 15:21:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint411.pt (epoch 411 @ 33210 updates, score 56.94) (writing took 20.254945170134306 seconds)
2022-09-08 15:21:18 | INFO | fairseq_cli.train | end of epoch 411 (average epoch stats below)
2022-09-08 15:21:18 | INFO | train | epoch 411 | loss 3.379 | nll_loss 0.344 | mask_loss 8.73919 | p_2 0.03528 | mask_ave 0.493 | ppl 1.27 | wps 3412.2 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 33210 | lr 0.000173526 | gnorm 0.262 | train_wall 98 | gb_free 9.1 | wall 54911
2022-09-08 15:21:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:21:19 | INFO | fairseq.trainer | begin training epoch 412
2022-09-08 15:21:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:22:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 15:22:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 15:22:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 15:22:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 15:23:09 | INFO | valid | epoch 412 | valid on 'valid' subset | loss 5.062 | nll_loss 2.444 | mask_loss 9.49865 | p_2 0.04893 | mask_ave 0.605 | ppl 5.44 | bleu 56.46 | wps 1547.2 | wpb 933.5 | bsz 59.6 | num_updates 33291 | best_bleu 57.29
2022-09-08 15:23:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 412 @ 33291 updates
2022-09-08 15:23:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint412.pt
2022-09-08 15:23:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint412.pt
2022-09-08 15:23:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint412.pt (epoch 412 @ 33291 updates, score 56.46) (writing took 20.022695913910866 seconds)
2022-09-08 15:23:29 | INFO | fairseq_cli.train | end of epoch 412 (average epoch stats below)
2022-09-08 15:23:29 | INFO | train | epoch 412 | loss 3.379 | nll_loss 0.343 | mask_loss 8.68517 | p_2 0.03518 | mask_ave 0.496 | ppl 1.27 | wps 3426.3 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 33291 | lr 0.000173315 | gnorm 0.256 | train_wall 98 | gb_free 9 | wall 55042
2022-09-08 15:23:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:23:29 | INFO | fairseq.trainer | begin training epoch 413
2022-09-08 15:23:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:23:41 | INFO | train_inner | epoch 413:      9 / 81 loss=3.379, nll_loss=0.343, mask_loss=8.69429, p_2=0.03513, mask_ave=0.495, ppl=1.27, wps=2969.1, ups=0.54, wpb=5516, bsz=357.7, num_updates=33300, lr=0.000173292, gnorm=0.25, train_wall=120, gb_free=9.1, wall=55054
2022-09-08 15:25:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 15:25:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 15:25:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 15:25:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 15:25:18 | INFO | valid | epoch 413 | valid on 'valid' subset | loss 5.054 | nll_loss 2.441 | mask_loss 9.56776 | p_2 0.0487 | mask_ave 0.612 | ppl 5.43 | bleu 56.83 | wps 1507.4 | wpb 933.5 | bsz 59.6 | num_updates 33372 | best_bleu 57.29
2022-09-08 15:25:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 413 @ 33372 updates
2022-09-08 15:25:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint413.pt
2022-09-08 15:25:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint413.pt
2022-09-08 15:25:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint413.pt (epoch 413 @ 33372 updates, score 56.83) (writing took 22.898630008101463 seconds)
2022-09-08 15:25:41 | INFO | fairseq_cli.train | end of epoch 413 (average epoch stats below)
2022-09-08 15:25:41 | INFO | train | epoch 413 | loss 3.379 | nll_loss 0.343 | mask_loss 8.70513 | p_2 0.0353 | mask_ave 0.492 | ppl 1.27 | wps 3375.9 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 33372 | lr 0.000173105 | gnorm 0.255 | train_wall 96 | gb_free 9.2 | wall 55174
2022-09-08 15:25:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:25:42 | INFO | fairseq.trainer | begin training epoch 414
2022-09-08 15:25:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:26:17 | INFO | train_inner | epoch 414:     28 / 81 loss=3.379, nll_loss=0.343, mask_loss=8.72381, p_2=0.03533, mask_ave=0.495, ppl=1.27, wps=3533, ups=0.64, wpb=5508.4, bsz=356.5, num_updates=33400, lr=0.000173032, gnorm=0.255, train_wall=120, gb_free=9, wall=55210
2022-09-08 15:27:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 15:27:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 15:27:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 15:27:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 15:27:33 | INFO | valid | epoch 414 | valid on 'valid' subset | loss 5.068 | nll_loss 2.456 | mask_loss 9.77339 | p_2 0.04851 | mask_ave 0.618 | ppl 5.49 | bleu 55.81 | wps 1468.2 | wpb 933.5 | bsz 59.6 | num_updates 33453 | best_bleu 57.29
2022-09-08 15:27:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 414 @ 33453 updates
2022-09-08 15:27:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint414.pt
2022-09-08 15:27:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint414.pt
2022-09-08 15:27:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint414.pt (epoch 414 @ 33453 updates, score 55.81) (writing took 2.572494313120842 seconds)
2022-09-08 15:27:36 | INFO | fairseq_cli.train | end of epoch 414 (average epoch stats below)
2022-09-08 15:27:36 | INFO | train | epoch 414 | loss 3.379 | nll_loss 0.344 | mask_loss 8.76472 | p_2 0.03519 | mask_ave 0.496 | ppl 1.27 | wps 3903 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 33453 | lr 0.000172895 | gnorm 0.265 | train_wall 99 | gb_free 9 | wall 55289
2022-09-08 15:27:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:27:36 | INFO | fairseq.trainer | begin training epoch 415
2022-09-08 15:27:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:28:35 | INFO | train_inner | epoch 415:     47 / 81 loss=3.379, nll_loss=0.343, mask_loss=8.81567, p_2=0.03512, mask_ave=0.497, ppl=1.27, wps=3980.8, ups=0.72, wpb=5521.9, bsz=357.7, num_updates=33500, lr=0.000172774, gnorm=0.257, train_wall=123, gb_free=9.1, wall=55348
2022-09-08 15:29:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 15:29:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 15:29:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 15:29:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 15:29:27 | INFO | valid | epoch 415 | valid on 'valid' subset | loss 5.081 | nll_loss 2.468 | mask_loss 9.5896 | p_2 0.0485 | mask_ave 0.618 | ppl 5.53 | bleu 56.35 | wps 1446.2 | wpb 933.5 | bsz 59.6 | num_updates 33534 | best_bleu 57.29
2022-09-08 15:29:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 415 @ 33534 updates
2022-09-08 15:29:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint415.pt
2022-09-08 15:29:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint415.pt
2022-09-08 15:29:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint415.pt (epoch 415 @ 33534 updates, score 56.35) (writing took 19.13073880970478 seconds)
2022-09-08 15:29:47 | INFO | fairseq_cli.train | end of epoch 415 (average epoch stats below)
2022-09-08 15:29:47 | INFO | train | epoch 415 | loss 3.378 | nll_loss 0.342 | mask_loss 8.79295 | p_2 0.03516 | mask_ave 0.497 | ppl 1.27 | wps 3424.5 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 33534 | lr 0.000172686 | gnorm 0.231 | train_wall 98 | gb_free 9.2 | wall 55420
2022-09-08 15:29:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:29:47 | INFO | fairseq.trainer | begin training epoch 416
2022-09-08 15:29:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:31:08 | INFO | train_inner | epoch 416:     66 / 81 loss=3.378, nll_loss=0.343, mask_loss=8.70196, p_2=0.03533, mask_ave=0.494, ppl=1.27, wps=3624.2, ups=0.65, wpb=5537.5, bsz=361.8, num_updates=33600, lr=0.000172516, gnorm=0.236, train_wall=120, gb_free=9, wall=55501
2022-09-08 15:31:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 15:31:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 15:31:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 15:31:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 15:31:37 | INFO | valid | epoch 416 | valid on 'valid' subset | loss 5.085 | nll_loss 2.468 | mask_loss 9.65757 | p_2 0.04869 | mask_ave 0.613 | ppl 5.53 | bleu 56.54 | wps 1508.2 | wpb 933.5 | bsz 59.6 | num_updates 33615 | best_bleu 57.29
2022-09-08 15:31:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 416 @ 33615 updates
2022-09-08 15:31:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint416.pt
2022-09-08 15:31:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint416.pt
2022-09-08 15:31:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint416.pt (epoch 416 @ 33615 updates, score 56.54) (writing took 21.42525987327099 seconds)
2022-09-08 15:31:59 | INFO | fairseq_cli.train | end of epoch 416 (average epoch stats below)
2022-09-08 15:31:59 | INFO | train | epoch 416 | loss 3.378 | nll_loss 0.343 | mask_loss 8.75314 | p_2 0.03523 | mask_ave 0.495 | ppl 1.27 | wps 3392.3 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 33615 | lr 0.000172478 | gnorm 0.244 | train_wall 97 | gb_free 9.2 | wall 55552
2022-09-08 15:31:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:31:59 | INFO | fairseq.trainer | begin training epoch 417
2022-09-08 15:31:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:33:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 15:33:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 15:33:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 15:33:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 15:33:49 | INFO | valid | epoch 417 | valid on 'valid' subset | loss 5.083 | nll_loss 2.475 | mask_loss 9.64671 | p_2 0.04888 | mask_ave 0.606 | ppl 5.56 | bleu 55.99 | wps 1500.2 | wpb 933.5 | bsz 59.6 | num_updates 33696 | best_bleu 57.29
2022-09-08 15:33:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 417 @ 33696 updates
2022-09-08 15:33:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint417.pt
2022-09-08 15:33:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint417.pt
2022-09-08 15:34:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint417.pt (epoch 417 @ 33696 updates, score 55.99) (writing took 24.04670460522175 seconds)
2022-09-08 15:34:13 | INFO | fairseq_cli.train | end of epoch 417 (average epoch stats below)
2022-09-08 15:34:13 | INFO | train | epoch 417 | loss 3.378 | nll_loss 0.343 | mask_loss 8.76755 | p_2 0.03531 | mask_ave 0.492 | ppl 1.27 | wps 3320.9 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 33696 | lr 0.00017227 | gnorm 0.241 | train_wall 97 | gb_free 9.1 | wall 55686
2022-09-08 15:34:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:34:14 | INFO | fairseq.trainer | begin training epoch 418
2022-09-08 15:34:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:34:19 | INFO | train_inner | epoch 418:      4 / 81 loss=3.378, nll_loss=0.343, mask_loss=8.79368, p_2=0.03517, mask_ave=0.493, ppl=1.27, wps=2885.8, ups=0.52, wpb=5516, bsz=355.8, num_updates=33700, lr=0.00017226, gnorm=0.246, train_wall=120, gb_free=9.1, wall=55692
2022-09-08 15:35:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 15:35:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 15:35:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 15:35:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 15:36:04 | INFO | valid | epoch 418 | valid on 'valid' subset | loss 5.073 | nll_loss 2.456 | mask_loss 9.46786 | p_2 0.04859 | mask_ave 0.615 | ppl 5.49 | bleu 56.53 | wps 1502 | wpb 933.5 | bsz 59.6 | num_updates 33777 | best_bleu 57.29
2022-09-08 15:36:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 418 @ 33777 updates
2022-09-08 15:36:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint418.pt
2022-09-08 15:36:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint418.pt
2022-09-08 15:36:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint418.pt (epoch 418 @ 33777 updates, score 56.53) (writing took 31.158983774483204 seconds)
2022-09-08 15:36:36 | INFO | fairseq_cli.train | end of epoch 418 (average epoch stats below)
2022-09-08 15:36:36 | INFO | train | epoch 418 | loss 3.378 | nll_loss 0.343 | mask_loss 8.79127 | p_2 0.03522 | mask_ave 0.495 | ppl 1.27 | wps 3146.7 | ups 0.57 | wpb 5523.2 | bsz 358 | num_updates 33777 | lr 0.000172064 | gnorm 0.253 | train_wall 98 | gb_free 9.2 | wall 55828
2022-09-08 15:36:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:36:36 | INFO | fairseq.trainer | begin training epoch 419
2022-09-08 15:36:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:37:04 | INFO | train_inner | epoch 419:     23 / 81 loss=3.379, nll_loss=0.343, mask_loss=8.78289, p_2=0.03531, mask_ave=0.497, ppl=1.27, wps=3341.9, ups=0.61, wpb=5496.6, bsz=351.4, num_updates=33800, lr=0.000172005, gnorm=0.249, train_wall=120, gb_free=9, wall=55857
2022-09-08 15:38:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 15:38:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 15:38:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 15:38:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 15:38:26 | INFO | valid | epoch 419 | valid on 'valid' subset | loss 5.078 | nll_loss 2.463 | mask_loss 9.5524 | p_2 0.04857 | mask_ave 0.616 | ppl 5.51 | bleu 56.72 | wps 1561.8 | wpb 933.5 | bsz 59.6 | num_updates 33858 | best_bleu 57.29
2022-09-08 15:38:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 419 @ 33858 updates
2022-09-08 15:38:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint419.pt
2022-09-08 15:38:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint419.pt
2022-09-08 15:38:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint419.pt (epoch 419 @ 33858 updates, score 56.72) (writing took 33.186360608786345 seconds)
2022-09-08 15:39:00 | INFO | fairseq_cli.train | end of epoch 419 (average epoch stats below)
2022-09-08 15:39:00 | INFO | train | epoch 419 | loss 3.378 | nll_loss 0.343 | mask_loss 8.74985 | p_2 0.03512 | mask_ave 0.498 | ppl 1.27 | wps 3107.6 | ups 0.56 | wpb 5523.2 | bsz 358 | num_updates 33858 | lr 0.000171858 | gnorm 0.244 | train_wall 98 | gb_free 9 | wall 55972
2022-09-08 15:39:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:39:00 | INFO | fairseq.trainer | begin training epoch 420
2022-09-08 15:39:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:39:51 | INFO | train_inner | epoch 420:     42 / 81 loss=3.378, nll_loss=0.342, mask_loss=8.70608, p_2=0.03528, mask_ave=0.497, ppl=1.27, wps=3314, ups=0.6, wpb=5540.5, bsz=367.2, num_updates=33900, lr=0.000171751, gnorm=0.241, train_wall=121, gb_free=9.2, wall=56024
2022-09-08 15:40:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 15:40:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 15:40:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 15:40:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 15:40:50 | INFO | valid | epoch 420 | valid on 'valid' subset | loss 5.078 | nll_loss 2.468 | mask_loss 9.51804 | p_2 0.04818 | mask_ave 0.627 | ppl 5.53 | bleu 56.35 | wps 1451.8 | wpb 933.5 | bsz 59.6 | num_updates 33939 | best_bleu 57.29
2022-09-08 15:40:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 420 @ 33939 updates
2022-09-08 15:40:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint420.pt
2022-09-08 15:40:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint420.pt
2022-09-08 15:41:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint420.pt (epoch 420 @ 33939 updates, score 56.35) (writing took 39.37829635664821 seconds)
2022-09-08 15:41:29 | INFO | fairseq_cli.train | end of epoch 420 (average epoch stats below)
2022-09-08 15:41:29 | INFO | train | epoch 420 | loss 3.378 | nll_loss 0.342 | mask_loss 8.70515 | p_2 0.03512 | mask_ave 0.498 | ppl 1.27 | wps 2987.8 | ups 0.54 | wpb 5523.2 | bsz 358 | num_updates 33939 | lr 0.000171653 | gnorm 0.241 | train_wall 97 | gb_free 9.2 | wall 56122
2022-09-08 15:41:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:41:29 | INFO | fairseq.trainer | begin training epoch 421
2022-09-08 15:41:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:42:45 | INFO | train_inner | epoch 421:     61 / 81 loss=3.378, nll_loss=0.343, mask_loss=8.75654, p_2=0.03473, mask_ave=0.497, ppl=1.27, wps=3197.1, ups=0.58, wpb=5558.7, bsz=355, num_updates=34000, lr=0.000171499, gnorm=0.245, train_wall=121, gb_free=9.1, wall=56198
2022-09-08 15:43:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 15:43:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 15:43:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 15:43:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 15:43:20 | INFO | valid | epoch 421 | valid on 'valid' subset | loss 5.068 | nll_loss 2.455 | mask_loss 9.53675 | p_2 0.04842 | mask_ave 0.621 | ppl 5.48 | bleu 56.91 | wps 1534.4 | wpb 933.5 | bsz 59.6 | num_updates 34020 | best_bleu 57.29
2022-09-08 15:43:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 421 @ 34020 updates
2022-09-08 15:43:20 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint421.pt
2022-09-08 15:43:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint421.pt
2022-09-08 15:43:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint421.pt (epoch 421 @ 34020 updates, score 56.91) (writing took 35.298633717000484 seconds)
2022-09-08 15:43:56 | INFO | fairseq_cli.train | end of epoch 421 (average epoch stats below)
2022-09-08 15:43:56 | INFO | train | epoch 421 | loss 3.378 | nll_loss 0.342 | mask_loss 8.73607 | p_2 0.03514 | mask_ave 0.498 | ppl 1.27 | wps 3059 | ups 0.55 | wpb 5523.2 | bsz 358 | num_updates 34020 | lr 0.000171448 | gnorm 0.252 | train_wall 98 | gb_free 9.2 | wall 56268
2022-09-08 15:43:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:43:56 | INFO | fairseq.trainer | begin training epoch 422
2022-09-08 15:43:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:45:34 | INFO | train_inner | epoch 422:     80 / 81 loss=3.379, nll_loss=0.344, mask_loss=8.7751, p_2=0.03526, mask_ave=0.5, ppl=1.27, wps=3261.5, ups=0.59, wpb=5518.2, bsz=358.9, num_updates=34100, lr=0.000171247, gnorm=0.267, train_wall=121, gb_free=9.1, wall=56367
2022-09-08 15:45:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 15:45:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 15:45:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 15:45:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 15:45:46 | INFO | valid | epoch 422 | valid on 'valid' subset | loss 5.064 | nll_loss 2.445 | mask_loss 9.65624 | p_2 0.04817 | mask_ave 0.627 | ppl 5.45 | bleu 56.4 | wps 1546.4 | wpb 933.5 | bsz 59.6 | num_updates 34101 | best_bleu 57.29
2022-09-08 15:45:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 422 @ 34101 updates
2022-09-08 15:45:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint422.pt
2022-09-08 15:45:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint422.pt
2022-09-08 15:46:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint422.pt (epoch 422 @ 34101 updates, score 56.4) (writing took 40.61080930382013 seconds)
2022-09-08 15:46:27 | INFO | fairseq_cli.train | end of epoch 422 (average epoch stats below)
2022-09-08 15:46:27 | INFO | train | epoch 422 | loss 3.379 | nll_loss 0.344 | mask_loss 8.79236 | p_2 0.03511 | mask_ave 0.499 | ppl 1.27 | wps 2960.4 | ups 0.54 | wpb 5523.2 | bsz 358 | num_updates 34101 | lr 0.000171244 | gnorm 0.269 | train_wall 98 | gb_free 9.1 | wall 56420
2022-09-08 15:46:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:46:27 | INFO | fairseq.trainer | begin training epoch 423
2022-09-08 15:46:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:48:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 15:48:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 15:48:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 15:48:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 15:48:16 | INFO | valid | epoch 423 | valid on 'valid' subset | loss 5.075 | nll_loss 2.463 | mask_loss 9.56112 | p_2 0.04836 | mask_ave 0.622 | ppl 5.51 | bleu 56.13 | wps 1595.4 | wpb 933.5 | bsz 59.6 | num_updates 34182 | best_bleu 57.29
2022-09-08 15:48:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 423 @ 34182 updates
2022-09-08 15:48:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint423.pt
2022-09-08 15:48:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint423.pt
2022-09-08 15:48:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint423.pt (epoch 423 @ 34182 updates, score 56.13) (writing took 40.60228617861867 seconds)
2022-09-08 15:48:57 | INFO | fairseq_cli.train | end of epoch 423 (average epoch stats below)
2022-09-08 15:48:57 | INFO | train | epoch 423 | loss 3.378 | nll_loss 0.343 | mask_loss 8.7147 | p_2 0.03512 | mask_ave 0.498 | ppl 1.27 | wps 2976.5 | ups 0.54 | wpb 5523.2 | bsz 358 | num_updates 34182 | lr 0.000171041 | gnorm 0.271 | train_wall 97 | gb_free 9.1 | wall 56570
2022-09-08 15:48:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:48:57 | INFO | fairseq.trainer | begin training epoch 424
2022-09-08 15:48:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:49:19 | INFO | train_inner | epoch 424:     18 / 81 loss=3.378, nll_loss=0.343, mask_loss=8.74478, p_2=0.03502, mask_ave=0.497, ppl=1.27, wps=2446.7, ups=0.44, wpb=5508, bsz=355.3, num_updates=34200, lr=0.000170996, gnorm=0.27, train_wall=119, gb_free=9.2, wall=56592
2022-09-08 15:50:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 15:50:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 15:50:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 15:50:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 15:50:47 | INFO | valid | epoch 424 | valid on 'valid' subset | loss 5.076 | nll_loss 2.462 | mask_loss 9.65241 | p_2 0.04811 | mask_ave 0.629 | ppl 5.51 | bleu 56.33 | wps 1560.3 | wpb 933.5 | bsz 59.6 | num_updates 34263 | best_bleu 57.29
2022-09-08 15:50:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 424 @ 34263 updates
2022-09-08 15:50:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint424.pt
2022-09-08 15:50:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint424.pt
2022-09-08 15:51:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint424.pt (epoch 424 @ 34263 updates, score 56.33) (writing took 16.29167752712965 seconds)
2022-09-08 15:51:03 | INFO | fairseq_cli.train | end of epoch 424 (average epoch stats below)
2022-09-08 15:51:03 | INFO | train | epoch 424 | loss 3.378 | nll_loss 0.342 | mask_loss 8.75526 | p_2 0.03514 | mask_ave 0.498 | ppl 1.27 | wps 3538.4 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 34263 | lr 0.000170839 | gnorm 0.24 | train_wall 97 | gb_free 9.2 | wall 56696
2022-09-08 15:51:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:51:04 | INFO | fairseq.trainer | begin training epoch 425
2022-09-08 15:51:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:51:49 | INFO | train_inner | epoch 425:     37 / 81 loss=3.378, nll_loss=0.342, mask_loss=8.77484, p_2=0.03493, mask_ave=0.499, ppl=1.27, wps=3704, ups=0.67, wpb=5534.5, bsz=359.1, num_updates=34300, lr=0.000170747, gnorm=0.245, train_wall=120, gb_free=9, wall=56741
2022-09-08 15:52:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 15:52:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 15:52:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 15:52:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 15:52:53 | INFO | valid | epoch 425 | valid on 'valid' subset | loss 5.073 | nll_loss 2.462 | mask_loss 9.67952 | p_2 0.04809 | mask_ave 0.63 | ppl 5.51 | bleu 56.54 | wps 1493.1 | wpb 933.5 | bsz 59.6 | num_updates 34344 | best_bleu 57.29
2022-09-08 15:52:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 425 @ 34344 updates
2022-09-08 15:52:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint425.pt
2022-09-08 15:52:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint425.pt
2022-09-08 15:53:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint425.pt (epoch 425 @ 34344 updates, score 56.54) (writing took 21.945581674575806 seconds)
2022-09-08 15:53:16 | INFO | fairseq_cli.train | end of epoch 425 (average epoch stats below)
2022-09-08 15:53:16 | INFO | train | epoch 425 | loss 3.378 | nll_loss 0.342 | mask_loss 8.85313 | p_2 0.03499 | mask_ave 0.502 | ppl 1.27 | wps 3386.5 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 34344 | lr 0.000170638 | gnorm 0.247 | train_wall 97 | gb_free 9 | wall 56828
2022-09-08 15:53:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:53:16 | INFO | fairseq.trainer | begin training epoch 426
2022-09-08 15:53:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:54:24 | INFO | train_inner | epoch 426:     56 / 81 loss=3.378, nll_loss=0.342, mask_loss=8.79359, p_2=0.03519, mask_ave=0.503, ppl=1.27, wps=3553.6, ups=0.64, wpb=5529.6, bsz=360.3, num_updates=34400, lr=0.000170499, gnorm=0.252, train_wall=120, gb_free=9.2, wall=56897
2022-09-08 15:54:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 15:54:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 15:54:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 15:54:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 15:55:05 | INFO | valid | epoch 426 | valid on 'valid' subset | loss 5.065 | nll_loss 2.446 | mask_loss 9.61826 | p_2 0.04828 | mask_ave 0.623 | ppl 5.45 | bleu 56.04 | wps 1554.9 | wpb 933.5 | bsz 59.6 | num_updates 34425 | best_bleu 57.29
2022-09-08 15:55:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 426 @ 34425 updates
2022-09-08 15:55:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint426.pt
2022-09-08 15:55:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint426.pt
2022-09-08 15:55:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint426.pt (epoch 426 @ 34425 updates, score 56.04) (writing took 15.719244044274092 seconds)
2022-09-08 15:55:21 | INFO | fairseq_cli.train | end of epoch 426 (average epoch stats below)
2022-09-08 15:55:21 | INFO | train | epoch 426 | loss 3.378 | nll_loss 0.343 | mask_loss 8.75131 | p_2 0.03501 | mask_ave 0.501 | ppl 1.27 | wps 3563.9 | ups 0.65 | wpb 5523.2 | bsz 358 | num_updates 34425 | lr 0.000170437 | gnorm 0.257 | train_wall 97 | gb_free 9.1 | wall 56954
2022-09-08 15:55:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:55:21 | INFO | fairseq.trainer | begin training epoch 427
2022-09-08 15:55:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:56:55 | INFO | train_inner | epoch 427:     75 / 81 loss=3.378, nll_loss=0.343, mask_loss=8.70513, p_2=0.0351, mask_ave=0.501, ppl=1.27, wps=3667.9, ups=0.66, wpb=5520.4, bsz=357, num_updates=34500, lr=0.000170251, gnorm=0.251, train_wall=122, gb_free=9.1, wall=57048
2022-09-08 15:57:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 15:57:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 15:57:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 15:57:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 15:57:13 | INFO | valid | epoch 427 | valid on 'valid' subset | loss 5.078 | nll_loss 2.466 | mask_loss 9.59012 | p_2 0.04815 | mask_ave 0.627 | ppl 5.53 | bleu 56.08 | wps 1567 | wpb 933.5 | bsz 59.6 | num_updates 34506 | best_bleu 57.29
2022-09-08 15:57:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 427 @ 34506 updates
2022-09-08 15:57:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint427.pt
2022-09-08 15:57:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint427.pt
2022-09-08 15:57:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint427.pt (epoch 427 @ 34506 updates, score 56.08) (writing took 16.765086207538843 seconds)
2022-09-08 15:57:29 | INFO | fairseq_cli.train | end of epoch 427 (average epoch stats below)
2022-09-08 15:57:29 | INFO | train | epoch 427 | loss 3.378 | nll_loss 0.343 | mask_loss 8.68999 | p_2 0.03502 | mask_ave 0.501 | ppl 1.27 | wps 3484.1 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 34506 | lr 0.000170237 | gnorm 0.257 | train_wall 99 | gb_free 9.1 | wall 57082
2022-09-08 15:57:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:57:30 | INFO | fairseq.trainer | begin training epoch 428
2022-09-08 15:57:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:59:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 15:59:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 15:59:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 15:59:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 15:59:18 | INFO | valid | epoch 428 | valid on 'valid' subset | loss 5.063 | nll_loss 2.441 | mask_loss 9.74269 | p_2 0.04829 | mask_ave 0.623 | ppl 5.43 | bleu 56.24 | wps 1576 | wpb 933.5 | bsz 59.6 | num_updates 34587 | best_bleu 57.29
2022-09-08 15:59:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 428 @ 34587 updates
2022-09-08 15:59:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint428.pt
2022-09-08 15:59:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint428.pt
2022-09-08 15:59:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint428.pt (epoch 428 @ 34587 updates, score 56.24) (writing took 17.114834919571877 seconds)
2022-09-08 15:59:36 | INFO | fairseq_cli.train | end of epoch 428 (average epoch stats below)
2022-09-08 15:59:36 | INFO | train | epoch 428 | loss 3.378 | nll_loss 0.342 | mask_loss 8.71254 | p_2 0.03516 | mask_ave 0.497 | ppl 1.27 | wps 3542.3 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 34587 | lr 0.000170037 | gnorm 0.236 | train_wall 97 | gb_free 9.1 | wall 57209
2022-09-08 15:59:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 15:59:36 | INFO | fairseq.trainer | begin training epoch 429
2022-09-08 15:59:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 15:59:52 | INFO | train_inner | epoch 429:     13 / 81 loss=3.378, nll_loss=0.342, mask_loss=8.72978, p_2=0.03518, mask_ave=0.498, ppl=1.27, wps=3096.9, ups=0.56, wpb=5501.8, bsz=357.9, num_updates=34600, lr=0.000170005, gnorm=0.243, train_wall=119, gb_free=9.1, wall=57225
2022-09-08 16:01:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:01:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:01:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:01:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:01:27 | INFO | valid | epoch 429 | valid on 'valid' subset | loss 5.057 | nll_loss 2.44 | mask_loss 9.56306 | p_2 0.0484 | mask_ave 0.621 | ppl 5.43 | bleu 56.44 | wps 1568.9 | wpb 933.5 | bsz 59.6 | num_updates 34668 | best_bleu 57.29
2022-09-08 16:01:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 429 @ 34668 updates
2022-09-08 16:01:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint429.pt
2022-09-08 16:01:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint429.pt
2022-09-08 16:01:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint429.pt (epoch 429 @ 34668 updates, score 56.44) (writing took 19.280177634209394 seconds)
2022-09-08 16:01:47 | INFO | fairseq_cli.train | end of epoch 429 (average epoch stats below)
2022-09-08 16:01:47 | INFO | train | epoch 429 | loss 3.378 | nll_loss 0.343 | mask_loss 8.74262 | p_2 0.03514 | mask_ave 0.498 | ppl 1.27 | wps 3417.1 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 34668 | lr 0.000169838 | gnorm 0.251 | train_wall 99 | gb_free 9.1 | wall 57340
2022-09-08 16:01:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 16:01:47 | INFO | fairseq.trainer | begin training epoch 430
2022-09-08 16:01:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 16:02:26 | INFO | train_inner | epoch 430:     32 / 81 loss=3.378, nll_loss=0.343, mask_loss=8.70023, p_2=0.03526, mask_ave=0.497, ppl=1.27, wps=3603, ups=0.65, wpb=5528.8, bsz=359.4, num_updates=34700, lr=0.00016976, gnorm=0.257, train_wall=122, gb_free=9.1, wall=57379
2022-09-08 16:03:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:03:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:03:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:03:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:03:36 | INFO | valid | epoch 430 | valid on 'valid' subset | loss 5.077 | nll_loss 2.462 | mask_loss 9.54768 | p_2 0.0483 | mask_ave 0.623 | ppl 5.51 | bleu 56.43 | wps 1548.3 | wpb 933.5 | bsz 59.6 | num_updates 34749 | best_bleu 57.29
2022-09-08 16:03:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 430 @ 34749 updates
2022-09-08 16:03:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint430.pt
2022-09-08 16:03:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint430.pt
2022-09-08 16:03:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint430.pt (epoch 430 @ 34749 updates, score 56.43) (writing took 21.978558585047722 seconds)
2022-09-08 16:03:58 | INFO | fairseq_cli.train | end of epoch 430 (average epoch stats below)
2022-09-08 16:03:58 | INFO | train | epoch 430 | loss 3.379 | nll_loss 0.344 | mask_loss 8.73708 | p_2 0.03513 | mask_ave 0.498 | ppl 1.27 | wps 3407.5 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 34749 | lr 0.00016964 | gnorm 0.264 | train_wall 97 | gb_free 9.2 | wall 57471
2022-09-08 16:03:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 16:03:58 | INFO | fairseq.trainer | begin training epoch 431
2022-09-08 16:03:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 16:05:01 | INFO | train_inner | epoch 431:     51 / 81 loss=3.379, nll_loss=0.343, mask_loss=8.74901, p_2=0.03511, mask_ave=0.502, ppl=1.27, wps=3549.4, ups=0.65, wpb=5501, bsz=354.7, num_updates=34800, lr=0.000169516, gnorm=0.264, train_wall=120, gb_free=8.9, wall=57534
2022-09-08 16:05:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:05:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:05:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:05:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:05:49 | INFO | valid | epoch 431 | valid on 'valid' subset | loss 5.076 | nll_loss 2.465 | mask_loss 9.56733 | p_2 0.04844 | mask_ave 0.619 | ppl 5.52 | bleu 56.57 | wps 1525.4 | wpb 933.5 | bsz 59.6 | num_updates 34830 | best_bleu 57.29
2022-09-08 16:05:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 431 @ 34830 updates
2022-09-08 16:05:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint431.pt
2022-09-08 16:05:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint431.pt
2022-09-08 16:05:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint431.pt (epoch 431 @ 34830 updates, score 56.57) (writing took 2.520457949489355 seconds)
2022-09-08 16:05:51 | INFO | fairseq_cli.train | end of epoch 431 (average epoch stats below)
2022-09-08 16:05:51 | INFO | train | epoch 431 | loss 3.378 | nll_loss 0.343 | mask_loss 8.71698 | p_2 0.03508 | mask_ave 0.499 | ppl 1.27 | wps 3943.6 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 34830 | lr 0.000169443 | gnorm 0.264 | train_wall 98 | gb_free 9.1 | wall 57584
2022-09-08 16:05:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 16:05:52 | INFO | fairseq.trainer | begin training epoch 432
2022-09-08 16:05:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 16:07:18 | INFO | train_inner | epoch 432:     70 / 81 loss=3.378, nll_loss=0.343, mask_loss=8.6628, p_2=0.03507, mask_ave=0.494, ppl=1.27, wps=4050, ups=0.73, wpb=5551.6, bsz=359.7, num_updates=34900, lr=0.000169273, gnorm=0.251, train_wall=122, gb_free=9, wall=57671
2022-09-08 16:07:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:07:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:07:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:07:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:07:42 | INFO | valid | epoch 432 | valid on 'valid' subset | loss 5.065 | nll_loss 2.454 | mask_loss 9.63687 | p_2 0.04846 | mask_ave 0.619 | ppl 5.48 | bleu 57 | wps 1530.1 | wpb 933.5 | bsz 59.6 | num_updates 34911 | best_bleu 57.29
2022-09-08 16:07:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 432 @ 34911 updates
2022-09-08 16:07:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint432.pt
2022-09-08 16:07:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint432.pt
2022-09-08 16:07:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint432.pt (epoch 432 @ 34911 updates, score 57.0) (writing took 15.379175901412964 seconds)
2022-09-08 16:07:58 | INFO | fairseq_cli.train | end of epoch 432 (average epoch stats below)
2022-09-08 16:07:58 | INFO | train | epoch 432 | loss 3.378 | nll_loss 0.342 | mask_loss 8.65672 | p_2 0.03521 | mask_ave 0.495 | ppl 1.27 | wps 3535.4 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 34911 | lr 0.000169246 | gnorm 0.245 | train_wall 98 | gb_free 9.2 | wall 57711
2022-09-08 16:07:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 16:07:58 | INFO | fairseq.trainer | begin training epoch 433
2022-09-08 16:07:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 16:09:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:09:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:09:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:09:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:09:48 | INFO | valid | epoch 433 | valid on 'valid' subset | loss 5.069 | nll_loss 2.456 | mask_loss 9.87659 | p_2 0.04879 | mask_ave 0.609 | ppl 5.49 | bleu 56.84 | wps 1485.6 | wpb 933.5 | bsz 59.6 | num_updates 34992 | best_bleu 57.29
2022-09-08 16:09:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 433 @ 34992 updates
2022-09-08 16:09:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint433.pt
2022-09-08 16:09:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint433.pt
2022-09-08 16:10:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint433.pt (epoch 433 @ 34992 updates, score 56.84) (writing took 18.95562554895878 seconds)
2022-09-08 16:10:07 | INFO | fairseq_cli.train | end of epoch 433 (average epoch stats below)
2022-09-08 16:10:07 | INFO | train | epoch 433 | loss 3.378 | nll_loss 0.342 | mask_loss 8.78951 | p_2 0.03514 | mask_ave 0.497 | ppl 1.27 | wps 3457.7 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 34992 | lr 0.00016905 | gnorm 0.246 | train_wall 97 | gb_free 9.2 | wall 57840
2022-09-08 16:10:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 16:10:08 | INFO | fairseq.trainer | begin training epoch 434
2022-09-08 16:10:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 16:10:18 | INFO | train_inner | epoch 434:      8 / 81 loss=3.377, nll_loss=0.342, mask_loss=8.80448, p_2=0.03517, mask_ave=0.497, ppl=1.27, wps=3059.2, ups=0.56, wpb=5501.3, bsz=357.8, num_updates=35000, lr=0.000169031, gnorm=0.241, train_wall=120, gb_free=9.1, wall=57851
2022-09-08 16:11:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:11:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:11:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:11:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:11:59 | INFO | valid | epoch 434 | valid on 'valid' subset | loss 5.055 | nll_loss 2.437 | mask_loss 9.76887 | p_2 0.04858 | mask_ave 0.615 | ppl 5.42 | bleu 56.57 | wps 1492.9 | wpb 933.5 | bsz 59.6 | num_updates 35073 | best_bleu 57.29
2022-09-08 16:11:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 434 @ 35073 updates
2022-09-08 16:11:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint434.pt
2022-09-08 16:12:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint434.pt
2022-09-08 16:12:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint434.pt (epoch 434 @ 35073 updates, score 56.57) (writing took 19.650145150721073 seconds)
2022-09-08 16:12:19 | INFO | fairseq_cli.train | end of epoch 434 (average epoch stats below)
2022-09-08 16:12:19 | INFO | train | epoch 434 | loss 3.378 | nll_loss 0.342 | mask_loss 8.84541 | p_2 0.03514 | mask_ave 0.497 | ppl 1.27 | wps 3410.6 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 35073 | lr 0.000168855 | gnorm 0.253 | train_wall 99 | gb_free 9.2 | wall 57971
2022-09-08 16:12:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 16:12:19 | INFO | fairseq.trainer | begin training epoch 435
2022-09-08 16:12:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 16:12:52 | INFO | train_inner | epoch 435:     27 / 81 loss=3.378, nll_loss=0.342, mask_loss=8.85815, p_2=0.03489, mask_ave=0.497, ppl=1.27, wps=3599.8, ups=0.65, wpb=5541.6, bsz=357.1, num_updates=35100, lr=0.00016879, gnorm=0.247, train_wall=121, gb_free=9, wall=58005
2022-09-08 16:13:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:13:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:13:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:13:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:14:08 | INFO | valid | epoch 435 | valid on 'valid' subset | loss 5.074 | nll_loss 2.456 | mask_loss 9.79171 | p_2 0.04865 | mask_ave 0.613 | ppl 5.49 | bleu 56.91 | wps 1480.1 | wpb 933.5 | bsz 59.6 | num_updates 35154 | best_bleu 57.29
2022-09-08 16:14:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 435 @ 35154 updates
2022-09-08 16:14:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint435.pt
2022-09-08 16:14:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint435.pt
2022-09-08 16:14:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint435.pt (epoch 435 @ 35154 updates, score 56.91) (writing took 19.133013155311346 seconds)
2022-09-08 16:14:28 | INFO | fairseq_cli.train | end of epoch 435 (average epoch stats below)
2022-09-08 16:14:28 | INFO | train | epoch 435 | loss 3.377 | nll_loss 0.342 | mask_loss 8.86288 | p_2 0.03517 | mask_ave 0.497 | ppl 1.27 | wps 3467.1 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 35154 | lr 0.00016866 | gnorm 0.233 | train_wall 97 | gb_free 9.1 | wall 58100
2022-09-08 16:14:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 16:14:28 | INFO | fairseq.trainer | begin training epoch 436
2022-09-08 16:14:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 16:15:26 | INFO | train_inner | epoch 436:     46 / 81 loss=3.377, nll_loss=0.342, mask_loss=8.87858, p_2=0.03524, mask_ave=0.497, ppl=1.27, wps=3586.2, ups=0.65, wpb=5532.4, bsz=357.4, num_updates=35200, lr=0.00016855, gnorm=0.24, train_wall=122, gb_free=9.1, wall=58159
2022-09-08 16:16:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:16:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:16:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:16:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:16:20 | INFO | valid | epoch 436 | valid on 'valid' subset | loss 5.059 | nll_loss 2.441 | mask_loss 9.81008 | p_2 0.04854 | mask_ave 0.615 | ppl 5.43 | bleu 56.39 | wps 1553.6 | wpb 933.5 | bsz 59.6 | num_updates 35235 | best_bleu 57.29
2022-09-08 16:16:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 436 @ 35235 updates
2022-09-08 16:16:20 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint436.pt
2022-09-08 16:16:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint436.pt
2022-09-08 16:16:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint436.pt (epoch 436 @ 35235 updates, score 56.39) (writing took 19.578366614878178 seconds)
2022-09-08 16:16:40 | INFO | fairseq_cli.train | end of epoch 436 (average epoch stats below)
2022-09-08 16:16:40 | INFO | train | epoch 436 | loss 3.378 | nll_loss 0.343 | mask_loss 8.93181 | p_2 0.03509 | mask_ave 0.499 | ppl 1.27 | wps 3377.2 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 35235 | lr 0.000168466 | gnorm 0.253 | train_wall 100 | gb_free 9 | wall 58233
2022-09-08 16:16:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 16:16:40 | INFO | fairseq.trainer | begin training epoch 437
2022-09-08 16:16:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 16:18:01 | INFO | train_inner | epoch 437:     65 / 81 loss=3.378, nll_loss=0.343, mask_loss=8.91418, p_2=0.03502, mask_ave=0.5, ppl=1.27, wps=3585.3, ups=0.65, wpb=5540.4, bsz=359.8, num_updates=35300, lr=0.000168311, gnorm=0.256, train_wall=122, gb_free=8.9, wall=58313
2022-09-08 16:18:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:18:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:18:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:18:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:18:30 | INFO | valid | epoch 437 | valid on 'valid' subset | loss 5.045 | nll_loss 2.421 | mask_loss 9.74318 | p_2 0.04864 | mask_ave 0.613 | ppl 5.36 | bleu 56.96 | wps 1548.8 | wpb 933.5 | bsz 59.6 | num_updates 35316 | best_bleu 57.29
2022-09-08 16:18:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 437 @ 35316 updates
2022-09-08 16:18:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint437.pt
2022-09-08 16:18:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint437.pt
2022-09-08 16:18:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint437.pt (epoch 437 @ 35316 updates, score 56.96) (writing took 19.970805503427982 seconds)
2022-09-08 16:18:51 | INFO | fairseq_cli.train | end of epoch 437 (average epoch stats below)
2022-09-08 16:18:51 | INFO | train | epoch 437 | loss 3.378 | nll_loss 0.343 | mask_loss 8.90189 | p_2 0.03504 | mask_ave 0.5 | ppl 1.27 | wps 3426.4 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 35316 | lr 0.000168273 | gnorm 0.246 | train_wall 98 | gb_free 9.1 | wall 58364
2022-09-08 16:18:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 16:18:51 | INFO | fairseq.trainer | begin training epoch 438
2022-09-08 16:18:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 16:20:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:20:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:20:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:20:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:20:41 | INFO | valid | epoch 438 | valid on 'valid' subset | loss 5.068 | nll_loss 2.455 | mask_loss 9.73864 | p_2 0.04864 | mask_ave 0.613 | ppl 5.48 | bleu 57.24 | wps 1505.9 | wpb 933.5 | bsz 59.6 | num_updates 35397 | best_bleu 57.29
2022-09-08 16:20:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 438 @ 35397 updates
2022-09-08 16:20:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint438.pt
2022-09-08 16:20:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint438.pt
2022-09-08 16:20:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint438.pt (epoch 438 @ 35397 updates, score 57.24) (writing took 18.540518809109926 seconds)
2022-09-08 16:21:00 | INFO | fairseq_cli.train | end of epoch 438 (average epoch stats below)
2022-09-08 16:21:00 | INFO | train | epoch 438 | loss 3.377 | nll_loss 0.341 | mask_loss 8.87886 | p_2 0.03521 | mask_ave 0.496 | ppl 1.27 | wps 3469.4 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 35397 | lr 0.00016808 | gnorm 0.24 | train_wall 98 | gb_free 9.1 | wall 58492
2022-09-08 16:21:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 16:21:00 | INFO | fairseq.trainer | begin training epoch 439
2022-09-08 16:21:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 16:21:05 | INFO | train_inner | epoch 439:      3 / 81 loss=3.377, nll_loss=0.341, mask_loss=8.87243, p_2=0.0353, mask_ave=0.496, ppl=1.27, wps=2985.4, ups=0.54, wpb=5492.7, bsz=358.6, num_updates=35400, lr=0.000168073, gnorm=0.239, train_wall=120, gb_free=9, wall=58497
2022-09-08 16:22:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:22:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:22:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:22:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:22:49 | INFO | valid | epoch 439 | valid on 'valid' subset | loss 5.078 | nll_loss 2.467 | mask_loss 9.76164 | p_2 0.04853 | mask_ave 0.617 | ppl 5.53 | bleu 56.61 | wps 1604.2 | wpb 933.5 | bsz 59.6 | num_updates 35478 | best_bleu 57.29
2022-09-08 16:22:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 439 @ 35478 updates
2022-09-08 16:22:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint439.pt
2022-09-08 16:22:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint439.pt
2022-09-08 16:23:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint439.pt (epoch 439 @ 35478 updates, score 56.61) (writing took 19.392078764736652 seconds)
2022-09-08 16:23:08 | INFO | fairseq_cli.train | end of epoch 439 (average epoch stats below)
2022-09-08 16:23:08 | INFO | train | epoch 439 | loss 3.377 | nll_loss 0.342 | mask_loss 8.89614 | p_2 0.03519 | mask_ave 0.496 | ppl 1.27 | wps 3483.4 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 35478 | lr 0.000167888 | gnorm 0.266 | train_wall 97 | gb_free 9 | wall 58621
2022-09-08 16:23:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 16:23:08 | INFO | fairseq.trainer | begin training epoch 440
2022-09-08 16:23:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 16:23:35 | INFO | train_inner | epoch 440:     22 / 81 loss=3.378, nll_loss=0.342, mask_loss=8.88457, p_2=0.03518, mask_ave=0.496, ppl=1.27, wps=3676.1, ups=0.66, wpb=5534.9, bsz=357, num_updates=35500, lr=0.000167836, gnorm=0.264, train_wall=119, gb_free=9, wall=58648
2022-09-08 16:24:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:24:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:24:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:24:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:24:58 | INFO | valid | epoch 440 | valid on 'valid' subset | loss 5.073 | nll_loss 2.464 | mask_loss 9.67915 | p_2 0.04872 | mask_ave 0.611 | ppl 5.52 | bleu 56.95 | wps 1578.9 | wpb 933.5 | bsz 59.6 | num_updates 35559 | best_bleu 57.29
2022-09-08 16:24:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 440 @ 35559 updates
2022-09-08 16:24:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint440.pt
2022-09-08 16:24:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint440.pt
2022-09-08 16:25:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint440.pt (epoch 440 @ 35559 updates, score 56.95) (writing took 20.239558793604374 seconds)
2022-09-08 16:25:18 | INFO | fairseq_cli.train | end of epoch 440 (average epoch stats below)
2022-09-08 16:25:18 | INFO | train | epoch 440 | loss 3.377 | nll_loss 0.342 | mask_loss 8.8969 | p_2 0.03518 | mask_ave 0.497 | ppl 1.27 | wps 3444.1 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 35559 | lr 0.000167697 | gnorm 0.268 | train_wall 97 | gb_free 9.1 | wall 58751
2022-09-08 16:25:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 16:25:18 | INFO | fairseq.trainer | begin training epoch 441
2022-09-08 16:25:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 16:26:09 | INFO | train_inner | epoch 441:     41 / 81 loss=3.377, nll_loss=0.342, mask_loss=8.92893, p_2=0.03514, mask_ave=0.496, ppl=1.27, wps=3590, ups=0.65, wpb=5534.6, bsz=359.4, num_updates=35600, lr=0.0001676, gnorm=0.258, train_wall=121, gb_free=9.1, wall=58802
2022-09-08 16:26:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:26:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:26:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:26:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:27:09 | INFO | valid | epoch 441 | valid on 'valid' subset | loss 5.077 | nll_loss 2.463 | mask_loss 9.76953 | p_2 0.0488 | mask_ave 0.608 | ppl 5.52 | bleu 57.02 | wps 1463.6 | wpb 933.5 | bsz 59.6 | num_updates 35640 | best_bleu 57.29
2022-09-08 16:27:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 441 @ 35640 updates
2022-09-08 16:27:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint441.pt
2022-09-08 16:27:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint441.pt
2022-09-08 16:27:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint441.pt (epoch 441 @ 35640 updates, score 57.02) (writing took 20.631314497441053 seconds)
2022-09-08 16:27:30 | INFO | fairseq_cli.train | end of epoch 441 (average epoch stats below)
2022-09-08 16:27:30 | INFO | train | epoch 441 | loss 3.377 | nll_loss 0.341 | mask_loss 8.94673 | p_2 0.03522 | mask_ave 0.495 | ppl 1.27 | wps 3392.1 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 35640 | lr 0.000167506 | gnorm 0.234 | train_wall 98 | gb_free 9.1 | wall 58883
2022-09-08 16:27:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 16:27:30 | INFO | fairseq.trainer | begin training epoch 442
2022-09-08 16:27:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 16:28:43 | INFO | train_inner | epoch 442:     60 / 81 loss=3.377, nll_loss=0.341, mask_loss=8.92427, p_2=0.03533, mask_ave=0.497, ppl=1.27, wps=3570.4, ups=0.65, wpb=5487.8, bsz=353, num_updates=35700, lr=0.000167365, gnorm=0.241, train_wall=120, gb_free=9, wall=58956
2022-09-08 16:29:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:29:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:29:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:29:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:29:19 | INFO | valid | epoch 442 | valid on 'valid' subset | loss 5.087 | nll_loss 2.475 | mask_loss 9.81973 | p_2 0.04829 | mask_ave 0.624 | ppl 5.56 | bleu 56.89 | wps 1524.2 | wpb 933.5 | bsz 59.6 | num_updates 35721 | best_bleu 57.29
2022-09-08 16:29:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 442 @ 35721 updates
2022-09-08 16:29:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint442.pt
2022-09-08 16:29:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint442.pt
2022-09-08 16:29:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint442.pt (epoch 442 @ 35721 updates, score 56.89) (writing took 19.99362627789378 seconds)
2022-09-08 16:29:40 | INFO | fairseq_cli.train | end of epoch 442 (average epoch stats below)
2022-09-08 16:29:40 | INFO | train | epoch 442 | loss 3.377 | nll_loss 0.342 | mask_loss 8.91326 | p_2 0.03524 | mask_ave 0.495 | ppl 1.27 | wps 3450 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 35721 | lr 0.000167316 | gnorm 0.245 | train_wall 97 | gb_free 9.2 | wall 59012
2022-09-08 16:29:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 16:29:40 | INFO | fairseq.trainer | begin training epoch 443
2022-09-08 16:29:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 16:31:16 | INFO | train_inner | epoch 443:     79 / 81 loss=3.377, nll_loss=0.342, mask_loss=8.92344, p_2=0.03509, mask_ave=0.498, ppl=1.27, wps=3624, ups=0.65, wpb=5553.6, bsz=363.2, num_updates=35800, lr=0.000167132, gnorm=0.24, train_wall=120, gb_free=9.1, wall=59109
2022-09-08 16:31:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:31:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:31:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:31:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:31:29 | INFO | valid | epoch 443 | valid on 'valid' subset | loss 5.082 | nll_loss 2.468 | mask_loss 9.84252 | p_2 0.0484 | mask_ave 0.62 | ppl 5.53 | bleu 57.1 | wps 1544.1 | wpb 933.5 | bsz 59.6 | num_updates 35802 | best_bleu 57.29
2022-09-08 16:31:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 443 @ 35802 updates
2022-09-08 16:31:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint443.pt
2022-09-08 16:31:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint443.pt
2022-09-08 16:31:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint443.pt (epoch 443 @ 35802 updates, score 57.1) (writing took 2.3779073506593704 seconds)
2022-09-08 16:31:32 | INFO | fairseq_cli.train | end of epoch 443 (average epoch stats below)
2022-09-08 16:31:32 | INFO | train | epoch 443 | loss 3.377 | nll_loss 0.342 | mask_loss 8.92056 | p_2 0.03507 | mask_ave 0.5 | ppl 1.27 | wps 3991.3 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 35802 | lr 0.000167127 | gnorm 0.238 | train_wall 97 | gb_free 9.1 | wall 59125
2022-09-08 16:31:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 16:31:32 | INFO | fairseq.trainer | begin training epoch 444
2022-09-08 16:31:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 16:33:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:33:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:33:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:33:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:33:20 | INFO | valid | epoch 444 | valid on 'valid' subset | loss 5.089 | nll_loss 2.481 | mask_loss 9.83887 | p_2 0.04818 | mask_ave 0.627 | ppl 5.58 | bleu 56.55 | wps 1521.4 | wpb 933.5 | bsz 59.6 | num_updates 35883 | best_bleu 57.29
2022-09-08 16:33:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 444 @ 35883 updates
2022-09-08 16:33:20 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint444.pt
2022-09-08 16:33:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint444.pt
2022-09-08 16:33:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint444.pt (epoch 444 @ 35883 updates, score 56.55) (writing took 2.4240871854126453 seconds)
2022-09-08 16:33:23 | INFO | fairseq_cli.train | end of epoch 444 (average epoch stats below)
2022-09-08 16:33:23 | INFO | train | epoch 444 | loss 3.377 | nll_loss 0.342 | mask_loss 8.94786 | p_2 0.0351 | mask_ave 0.499 | ppl 1.27 | wps 4019.3 | ups 0.73 | wpb 5523.2 | bsz 358 | num_updates 35883 | lr 0.000166938 | gnorm 0.253 | train_wall 96 | gb_free 9.1 | wall 59236
2022-09-08 16:33:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 16:33:23 | INFO | fairseq.trainer | begin training epoch 445
2022-09-08 16:33:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 16:33:45 | INFO | train_inner | epoch 445:     17 / 81 loss=3.377, nll_loss=0.342, mask_loss=8.94381, p_2=0.03517, mask_ave=0.499, ppl=1.27, wps=3690.8, ups=0.67, wpb=5492.7, bsz=357, num_updates=35900, lr=0.000166899, gnorm=0.259, train_wall=119, gb_free=9.1, wall=59258
2022-09-08 16:35:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:35:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:35:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:35:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:35:13 | INFO | valid | epoch 445 | valid on 'valid' subset | loss 5.082 | nll_loss 2.468 | mask_loss 9.67599 | p_2 0.04857 | mask_ave 0.615 | ppl 5.53 | bleu 56.23 | wps 1512.2 | wpb 933.5 | bsz 59.6 | num_updates 35964 | best_bleu 57.29
2022-09-08 16:35:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 445 @ 35964 updates
2022-09-08 16:35:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint445.pt
2022-09-08 16:35:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint445.pt
2022-09-08 16:35:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint445.pt (epoch 445 @ 35964 updates, score 56.23) (writing took 2.5589177571237087 seconds)
2022-09-08 16:35:16 | INFO | fairseq_cli.train | end of epoch 445 (average epoch stats below)
2022-09-08 16:35:16 | INFO | train | epoch 445 | loss 3.378 | nll_loss 0.343 | mask_loss 8.85211 | p_2 0.03524 | mask_ave 0.495 | ppl 1.27 | wps 3957.3 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 35964 | lr 0.00016675 | gnorm 0.265 | train_wall 98 | gb_free 9.1 | wall 59349
2022-09-08 16:35:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 16:35:16 | INFO | fairseq.trainer | begin training epoch 446
2022-09-08 16:35:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 16:36:01 | INFO | train_inner | epoch 446:     36 / 81 loss=3.377, nll_loss=0.342, mask_loss=8.87269, p_2=0.03526, mask_ave=0.495, ppl=1.27, wps=4070.5, ups=0.74, wpb=5535.3, bsz=361.7, num_updates=36000, lr=0.000166667, gnorm=0.244, train_wall=121, gb_free=9.1, wall=59394
2022-09-08 16:36:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:36:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:36:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:36:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:37:06 | INFO | valid | epoch 446 | valid on 'valid' subset | loss 5.078 | nll_loss 2.469 | mask_loss 9.78147 | p_2 0.04854 | mask_ave 0.617 | ppl 5.54 | bleu 56.48 | wps 1520.2 | wpb 933.5 | bsz 59.6 | num_updates 36045 | best_bleu 57.29
2022-09-08 16:37:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 446 @ 36045 updates
2022-09-08 16:37:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint446.pt
2022-09-08 16:37:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint446.pt
2022-09-08 16:37:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint446.pt (epoch 446 @ 36045 updates, score 56.48) (writing took 21.438809778541327 seconds)
2022-09-08 16:37:28 | INFO | fairseq_cli.train | end of epoch 446 (average epoch stats below)
2022-09-08 16:37:28 | INFO | train | epoch 446 | loss 3.377 | nll_loss 0.341 | mask_loss 8.91623 | p_2 0.03521 | mask_ave 0.496 | ppl 1.27 | wps 3386.8 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 36045 | lr 0.000166563 | gnorm 0.234 | train_wall 98 | gb_free 9 | wall 59481
2022-09-08 16:37:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 16:37:28 | INFO | fairseq.trainer | begin training epoch 447
2022-09-08 16:37:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 16:38:36 | INFO | train_inner | epoch 447:     55 / 81 loss=3.377, nll_loss=0.342, mask_loss=8.88391, p_2=0.03506, mask_ave=0.494, ppl=1.27, wps=3572, ups=0.65, wpb=5532.1, bsz=353.5, num_updates=36100, lr=0.000166436, gnorm=0.245, train_wall=120, gb_free=9.1, wall=59549
2022-09-08 16:39:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:39:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:39:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:39:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:39:18 | INFO | valid | epoch 447 | valid on 'valid' subset | loss 5.088 | nll_loss 2.477 | mask_loss 9.81834 | p_2 0.0485 | mask_ave 0.618 | ppl 5.57 | bleu 56.66 | wps 1516.4 | wpb 933.5 | bsz 59.6 | num_updates 36126 | best_bleu 57.29
2022-09-08 16:39:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 447 @ 36126 updates
2022-09-08 16:39:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint447.pt
2022-09-08 16:39:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint447.pt
2022-09-08 16:39:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint447.pt (epoch 447 @ 36126 updates, score 56.66) (writing took 22.472787376493216 seconds)
2022-09-08 16:39:41 | INFO | fairseq_cli.train | end of epoch 447 (average epoch stats below)
2022-09-08 16:39:41 | INFO | train | epoch 447 | loss 3.377 | nll_loss 0.342 | mask_loss 8.8368 | p_2 0.03529 | mask_ave 0.493 | ppl 1.27 | wps 3374.6 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 36126 | lr 0.000166376 | gnorm 0.243 | train_wall 97 | gb_free 9.1 | wall 59614
2022-09-08 16:39:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 16:39:41 | INFO | fairseq.trainer | begin training epoch 448
2022-09-08 16:39:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 16:41:12 | INFO | train_inner | epoch 448:     74 / 81 loss=3.377, nll_loss=0.342, mask_loss=8.87288, p_2=0.03535, mask_ave=0.494, ppl=1.27, wps=3534.6, ups=0.64, wpb=5523, bsz=358.3, num_updates=36200, lr=0.000166206, gnorm=0.254, train_wall=121, gb_free=9.2, wall=59705
2022-09-08 16:41:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:41:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:41:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:41:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:41:31 | INFO | valid | epoch 448 | valid on 'valid' subset | loss 5.078 | nll_loss 2.468 | mask_loss 9.7129 | p_2 0.04872 | mask_ave 0.611 | ppl 5.53 | bleu 56.03 | wps 1578.9 | wpb 933.5 | bsz 59.6 | num_updates 36207 | best_bleu 57.29
2022-09-08 16:41:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 448 @ 36207 updates
2022-09-08 16:41:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint448.pt
2022-09-08 16:41:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint448.pt
2022-09-08 16:41:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint448.pt (epoch 448 @ 36207 updates, score 56.03) (writing took 22.67864941805601 seconds)
2022-09-08 16:41:54 | INFO | fairseq_cli.train | end of epoch 448 (average epoch stats below)
2022-09-08 16:41:54 | INFO | train | epoch 448 | loss 3.377 | nll_loss 0.342 | mask_loss 8.88392 | p_2 0.03523 | mask_ave 0.495 | ppl 1.27 | wps 3354.8 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 36207 | lr 0.00016619 | gnorm 0.261 | train_wall 98 | gb_free 9.1 | wall 59747
2022-09-08 16:41:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 16:41:54 | INFO | fairseq.trainer | begin training epoch 449
2022-09-08 16:41:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 16:43:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:43:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:43:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:43:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:43:45 | INFO | valid | epoch 449 | valid on 'valid' subset | loss 5.072 | nll_loss 2.461 | mask_loss 9.73388 | p_2 0.04853 | mask_ave 0.617 | ppl 5.51 | bleu 56.62 | wps 1515.6 | wpb 933.5 | bsz 59.6 | num_updates 36288 | best_bleu 57.29
2022-09-08 16:43:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 449 @ 36288 updates
2022-09-08 16:43:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint449.pt
2022-09-08 16:43:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint449.pt
2022-09-08 16:44:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint449.pt (epoch 449 @ 36288 updates, score 56.62) (writing took 22.051937390118837 seconds)
2022-09-08 16:44:07 | INFO | fairseq_cli.train | end of epoch 449 (average epoch stats below)
2022-09-08 16:44:07 | INFO | train | epoch 449 | loss 3.377 | nll_loss 0.342 | mask_loss 8.76954 | p_2 0.03515 | mask_ave 0.498 | ppl 1.27 | wps 3371 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 36288 | lr 0.000166004 | gnorm 0.264 | train_wall 98 | gb_free 9.1 | wall 59880
2022-09-08 16:44:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 16:44:07 | INFO | fairseq.trainer | begin training epoch 450
2022-09-08 16:44:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 16:44:22 | INFO | train_inner | epoch 450:     12 / 81 loss=3.377, nll_loss=0.342, mask_loss=8.78035, p_2=0.03508, mask_ave=0.497, ppl=1.27, wps=2902.1, ups=0.53, wpb=5511.5, bsz=356.8, num_updates=36300, lr=0.000165977, gnorm=0.261, train_wall=120, gb_free=9.1, wall=59895
2022-09-08 16:45:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:45:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:45:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:45:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:45:57 | INFO | valid | epoch 450 | valid on 'valid' subset | loss 5.081 | nll_loss 2.475 | mask_loss 9.76046 | p_2 0.04894 | mask_ave 0.605 | ppl 5.56 | bleu 56.24 | wps 1517.5 | wpb 933.5 | bsz 59.6 | num_updates 36369 | best_bleu 57.29
2022-09-08 16:45:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 450 @ 36369 updates
2022-09-08 16:45:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint450.pt
2022-09-08 16:45:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint450.pt
2022-09-08 16:46:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint450.pt (epoch 450 @ 36369 updates, score 56.24) (writing took 19.502131301909685 seconds)
2022-09-08 16:46:16 | INFO | fairseq_cli.train | end of epoch 450 (average epoch stats below)
2022-09-08 16:46:16 | INFO | train | epoch 450 | loss 3.377 | nll_loss 0.342 | mask_loss 8.85873 | p_2 0.0353 | mask_ave 0.493 | ppl 1.27 | wps 3450.9 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 36369 | lr 0.000165819 | gnorm 0.273 | train_wall 97 | gb_free 9.1 | wall 60009
2022-09-08 16:46:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 16:46:17 | INFO | fairseq.trainer | begin training epoch 451
2022-09-08 16:46:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 16:46:55 | INFO | train_inner | epoch 451:     31 / 81 loss=3.377, nll_loss=0.342, mask_loss=8.84315, p_2=0.03544, mask_ave=0.493, ppl=1.27, wps=3609.4, ups=0.66, wpb=5505.8, bsz=356.7, num_updates=36400, lr=0.000165748, gnorm=0.268, train_wall=120, gb_free=9.1, wall=60047
2022-09-08 16:47:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:47:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:47:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:47:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:48:07 | INFO | valid | epoch 451 | valid on 'valid' subset | loss 5.075 | nll_loss 2.462 | mask_loss 9.67269 | p_2 0.04865 | mask_ave 0.613 | ppl 5.51 | bleu 56.75 | wps 1516.8 | wpb 933.5 | bsz 59.6 | num_updates 36450 | best_bleu 57.29
2022-09-08 16:48:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 451 @ 36450 updates
2022-09-08 16:48:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint451.pt
2022-09-08 16:48:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint451.pt
2022-09-08 16:48:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint451.pt (epoch 451 @ 36450 updates, score 56.75) (writing took 21.264292985200882 seconds)
2022-09-08 16:48:28 | INFO | fairseq_cli.train | end of epoch 451 (average epoch stats below)
2022-09-08 16:48:28 | INFO | train | epoch 451 | loss 3.378 | nll_loss 0.343 | mask_loss 8.76874 | p_2 0.03532 | mask_ave 0.492 | ppl 1.27 | wps 3400.8 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 36450 | lr 0.000165635 | gnorm 0.261 | train_wall 98 | gb_free 9.1 | wall 60141
2022-09-08 16:48:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 16:48:28 | INFO | fairseq.trainer | begin training epoch 452
2022-09-08 16:48:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 16:49:29 | INFO | train_inner | epoch 452:     50 / 81 loss=3.377, nll_loss=0.342, mask_loss=8.76575, p_2=0.03529, mask_ave=0.49, ppl=1.27, wps=3608, ups=0.65, wpb=5571.3, bsz=364.2, num_updates=36500, lr=0.000165521, gnorm=0.271, train_wall=120, gb_free=9.1, wall=60202
2022-09-08 16:50:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:50:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:50:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:50:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:50:17 | INFO | valid | epoch 452 | valid on 'valid' subset | loss 5.069 | nll_loss 2.449 | mask_loss 9.71477 | p_2 0.04862 | mask_ave 0.614 | ppl 5.46 | bleu 57.42 | wps 1508.2 | wpb 933.5 | bsz 59.6 | num_updates 36531 | best_bleu 57.42
2022-09-08 16:50:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 452 @ 36531 updates
2022-09-08 16:50:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint452.pt
2022-09-08 16:50:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint452.pt
2022-09-08 16:51:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint452.pt (epoch 452 @ 36531 updates, score 57.42) (writing took 64.76499731466174 seconds)
2022-09-08 16:51:22 | INFO | fairseq_cli.train | end of epoch 452 (average epoch stats below)
2022-09-08 16:51:22 | INFO | train | epoch 452 | loss 3.377 | nll_loss 0.342 | mask_loss 8.83068 | p_2 0.03533 | mask_ave 0.492 | ppl 1.27 | wps 2566 | ups 0.46 | wpb 5523.2 | bsz 358 | num_updates 36531 | lr 0.000165451 | gnorm 0.27 | train_wall 96 | gb_free 9.1 | wall 60315
2022-09-08 16:51:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 16:51:23 | INFO | fairseq.trainer | begin training epoch 453
2022-09-08 16:51:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 16:52:48 | INFO | train_inner | epoch 453:     69 / 81 loss=3.377, nll_loss=0.343, mask_loss=8.87675, p_2=0.03508, mask_ave=0.496, ppl=1.27, wps=2789.6, ups=0.5, wpb=5548.1, bsz=358.6, num_updates=36600, lr=0.000165295, gnorm=0.268, train_wall=121, gb_free=9, wall=60401
2022-09-08 16:53:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:53:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:53:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:53:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:53:13 | INFO | valid | epoch 453 | valid on 'valid' subset | loss 5.07 | nll_loss 2.452 | mask_loss 9.82079 | p_2 0.04873 | mask_ave 0.611 | ppl 5.47 | bleu 56.33 | wps 1523.1 | wpb 933.5 | bsz 59.6 | num_updates 36612 | best_bleu 57.42
2022-09-08 16:53:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 453 @ 36612 updates
2022-09-08 16:53:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint453.pt
2022-09-08 16:53:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint453.pt
2022-09-08 16:53:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint453.pt (epoch 453 @ 36612 updates, score 56.33) (writing took 30.837706107646227 seconds)
2022-09-08 16:53:44 | INFO | fairseq_cli.train | end of epoch 453 (average epoch stats below)
2022-09-08 16:53:44 | INFO | train | epoch 453 | loss 3.377 | nll_loss 0.342 | mask_loss 8.91529 | p_2 0.03515 | mask_ave 0.498 | ppl 1.27 | wps 3163.1 | ups 0.57 | wpb 5523.2 | bsz 358 | num_updates 36612 | lr 0.000165268 | gnorm 0.267 | train_wall 98 | gb_free 9.1 | wall 60457
2022-09-08 16:53:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 16:53:44 | INFO | fairseq.trainer | begin training epoch 454
2022-09-08 16:53:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 16:55:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:55:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:55:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:55:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:55:34 | INFO | valid | epoch 454 | valid on 'valid' subset | loss 5.072 | nll_loss 2.458 | mask_loss 9.54489 | p_2 0.04867 | mask_ave 0.613 | ppl 5.49 | bleu 56.38 | wps 1541.5 | wpb 933.5 | bsz 59.6 | num_updates 36693 | best_bleu 57.42
2022-09-08 16:55:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 454 @ 36693 updates
2022-09-08 16:55:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint454.pt
2022-09-08 16:55:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint454.pt
2022-09-08 16:55:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint454.pt (epoch 454 @ 36693 updates, score 56.38) (writing took 14.993775822222233 seconds)
2022-09-08 16:55:49 | INFO | fairseq_cli.train | end of epoch 454 (average epoch stats below)
2022-09-08 16:55:49 | INFO | train | epoch 454 | loss 3.377 | nll_loss 0.342 | mask_loss 8.78284 | p_2 0.0353 | mask_ave 0.493 | ppl 1.27 | wps 3562.9 | ups 0.65 | wpb 5523.2 | bsz 358 | num_updates 36693 | lr 0.000165085 | gnorm 0.253 | train_wall 98 | gb_free 9.1 | wall 60582
2022-09-08 16:55:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 16:55:49 | INFO | fairseq.trainer | begin training epoch 455
2022-09-08 16:55:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 16:55:59 | INFO | train_inner | epoch 455:      7 / 81 loss=3.376, nll_loss=0.341, mask_loss=8.80848, p_2=0.03543, mask_ave=0.495, ppl=1.27, wps=2870.8, ups=0.52, wpb=5473.8, bsz=356.2, num_updates=36700, lr=0.00016507, gnorm=0.255, train_wall=119, gb_free=9.1, wall=60591
2022-09-08 16:57:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:57:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:57:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:57:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:57:40 | INFO | valid | epoch 455 | valid on 'valid' subset | loss 5.078 | nll_loss 2.467 | mask_loss 9.81402 | p_2 0.04854 | mask_ave 0.616 | ppl 5.53 | bleu 56.57 | wps 1548 | wpb 933.5 | bsz 59.6 | num_updates 36774 | best_bleu 57.42
2022-09-08 16:57:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 455 @ 36774 updates
2022-09-08 16:57:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint455.pt
2022-09-08 16:57:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint455.pt
2022-09-08 16:57:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint455.pt (epoch 455 @ 36774 updates, score 56.57) (writing took 14.78574600070715 seconds)
2022-09-08 16:57:55 | INFO | fairseq_cli.train | end of epoch 455 (average epoch stats below)
2022-09-08 16:57:55 | INFO | train | epoch 455 | loss 3.376 | nll_loss 0.341 | mask_loss 8.8323 | p_2 0.03521 | mask_ave 0.496 | ppl 1.27 | wps 3560.4 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 36774 | lr 0.000164903 | gnorm 0.233 | train_wall 98 | gb_free 9.1 | wall 60708
2022-09-08 16:57:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 16:57:55 | INFO | fairseq.trainer | begin training epoch 456
2022-09-08 16:57:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 16:58:27 | INFO | train_inner | epoch 456:     26 / 81 loss=3.376, nll_loss=0.341, mask_loss=8.81712, p_2=0.03527, mask_ave=0.495, ppl=1.27, wps=3742.7, ups=0.67, wpb=5545, bsz=360.9, num_updates=36800, lr=0.000164845, gnorm=0.231, train_wall=120, gb_free=9.1, wall=60740
2022-09-08 16:59:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 16:59:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 16:59:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 16:59:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 16:59:45 | INFO | valid | epoch 456 | valid on 'valid' subset | loss 5.064 | nll_loss 2.445 | mask_loss 9.6052 | p_2 0.04826 | mask_ave 0.624 | ppl 5.45 | bleu 57.08 | wps 1508.3 | wpb 933.5 | bsz 59.6 | num_updates 36855 | best_bleu 57.42
2022-09-08 16:59:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 456 @ 36855 updates
2022-09-08 16:59:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint456.pt
2022-09-08 16:59:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint456.pt
2022-09-08 17:00:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint456.pt (epoch 456 @ 36855 updates, score 57.08) (writing took 15.682675495743752 seconds)
2022-09-08 17:00:01 | INFO | fairseq_cli.train | end of epoch 456 (average epoch stats below)
2022-09-08 17:00:01 | INFO | train | epoch 456 | loss 3.376 | nll_loss 0.341 | mask_loss 8.80634 | p_2 0.03512 | mask_ave 0.498 | ppl 1.27 | wps 3563.9 | ups 0.65 | wpb 5523.2 | bsz 358 | num_updates 36855 | lr 0.000164722 | gnorm 0.242 | train_wall 97 | gb_free 9.1 | wall 60833
2022-09-08 17:00:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:00:01 | INFO | fairseq.trainer | begin training epoch 457
2022-09-08 17:00:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:00:56 | INFO | train_inner | epoch 457:     45 / 81 loss=3.376, nll_loss=0.341, mask_loss=8.81175, p_2=0.03502, mask_ave=0.501, ppl=1.27, wps=3694, ups=0.67, wpb=5508.2, bsz=352.9, num_updates=36900, lr=0.000164622, gnorm=0.237, train_wall=120, gb_free=9.1, wall=60889
2022-09-08 17:01:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:01:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:01:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:01:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:01:51 | INFO | valid | epoch 457 | valid on 'valid' subset | loss 5.077 | nll_loss 2.461 | mask_loss 9.72413 | p_2 0.04862 | mask_ave 0.614 | ppl 5.51 | bleu 57.21 | wps 1494.8 | wpb 933.5 | bsz 59.6 | num_updates 36936 | best_bleu 57.42
2022-09-08 17:01:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 457 @ 36936 updates
2022-09-08 17:01:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint457.pt
2022-09-08 17:01:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint457.pt
2022-09-08 17:02:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint457.pt (epoch 457 @ 36936 updates, score 57.21) (writing took 18.178863637149334 seconds)
2022-09-08 17:02:09 | INFO | fairseq_cli.train | end of epoch 457 (average epoch stats below)
2022-09-08 17:02:10 | INFO | train | epoch 457 | loss 3.376 | nll_loss 0.341 | mask_loss 8.76632 | p_2 0.03512 | mask_ave 0.498 | ppl 1.27 | wps 3468.9 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 36936 | lr 0.000164541 | gnorm 0.232 | train_wall 98 | gb_free 9.1 | wall 60962
2022-09-08 17:02:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:02:10 | INFO | fairseq.trainer | begin training epoch 458
2022-09-08 17:02:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:03:30 | INFO | train_inner | epoch 458:     64 / 81 loss=3.377, nll_loss=0.342, mask_loss=8.86193, p_2=0.03494, mask_ave=0.497, ppl=1.27, wps=3596.5, ups=0.65, wpb=5525.5, bsz=356.7, num_updates=37000, lr=0.000164399, gnorm=0.267, train_wall=122, gb_free=9.1, wall=61042
2022-09-08 17:03:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:03:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:03:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:03:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:04:01 | INFO | valid | epoch 458 | valid on 'valid' subset | loss 5.097 | nll_loss 2.495 | mask_loss 9.7054 | p_2 0.04863 | mask_ave 0.614 | ppl 5.64 | bleu 56.17 | wps 1522.9 | wpb 933.5 | bsz 59.6 | num_updates 37017 | best_bleu 57.42
2022-09-08 17:04:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 458 @ 37017 updates
2022-09-08 17:04:01 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint458.pt
2022-09-08 17:04:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint458.pt
2022-09-08 17:04:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint458.pt (epoch 458 @ 37017 updates, score 56.17) (writing took 15.521122358739376 seconds)
2022-09-08 17:04:16 | INFO | fairseq_cli.train | end of epoch 458 (average epoch stats below)
2022-09-08 17:04:16 | INFO | train | epoch 458 | loss 3.377 | nll_loss 0.342 | mask_loss 8.88627 | p_2 0.03512 | mask_ave 0.498 | ppl 1.27 | wps 3524.4 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 37017 | lr 0.000164361 | gnorm 0.275 | train_wall 98 | gb_free 9.1 | wall 61089
2022-09-08 17:04:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:04:17 | INFO | fairseq.trainer | begin training epoch 459
2022-09-08 17:04:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:05:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:05:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:05:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:05:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:06:07 | INFO | valid | epoch 459 | valid on 'valid' subset | loss 5.076 | nll_loss 2.462 | mask_loss 9.72058 | p_2 0.04826 | mask_ave 0.625 | ppl 5.51 | bleu 57.26 | wps 1503.4 | wpb 933.5 | bsz 59.6 | num_updates 37098 | best_bleu 57.42
2022-09-08 17:06:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 459 @ 37098 updates
2022-09-08 17:06:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint459.pt
2022-09-08 17:06:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint459.pt
2022-09-08 17:06:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint459.pt (epoch 459 @ 37098 updates, score 57.26) (writing took 18.36598575487733 seconds)
2022-09-08 17:06:25 | INFO | fairseq_cli.train | end of epoch 459 (average epoch stats below)
2022-09-08 17:06:25 | INFO | train | epoch 459 | loss 3.376 | nll_loss 0.342 | mask_loss 8.81006 | p_2 0.03518 | mask_ave 0.496 | ppl 1.27 | wps 3476.5 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 37098 | lr 0.000164182 | gnorm 0.238 | train_wall 97 | gb_free 9.2 | wall 61218
2022-09-08 17:06:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:06:25 | INFO | fairseq.trainer | begin training epoch 460
2022-09-08 17:06:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:06:29 | INFO | train_inner | epoch 460:      2 / 81 loss=3.376, nll_loss=0.342, mask_loss=8.80379, p_2=0.03535, mask_ave=0.497, ppl=1.27, wps=3069.5, ups=0.56, wpb=5500.1, bsz=358.7, num_updates=37100, lr=0.000164177, gnorm=0.24, train_wall=119, gb_free=9.1, wall=61222
2022-09-08 17:08:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:08:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:08:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:08:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:08:15 | INFO | valid | epoch 460 | valid on 'valid' subset | loss 5.076 | nll_loss 2.469 | mask_loss 9.82977 | p_2 0.04836 | mask_ave 0.621 | ppl 5.53 | bleu 56.67 | wps 1501.6 | wpb 933.5 | bsz 59.6 | num_updates 37179 | best_bleu 57.42
2022-09-08 17:08:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 460 @ 37179 updates
2022-09-08 17:08:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint460.pt
2022-09-08 17:08:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint460.pt
2022-09-08 17:08:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint460.pt (epoch 460 @ 37179 updates, score 56.67) (writing took 21.57609013840556 seconds)
2022-09-08 17:08:37 | INFO | fairseq_cli.train | end of epoch 460 (average epoch stats below)
2022-09-08 17:08:37 | INFO | train | epoch 460 | loss 3.377 | nll_loss 0.342 | mask_loss 8.85539 | p_2 0.03519 | mask_ave 0.497 | ppl 1.27 | wps 3404.8 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 37179 | lr 0.000164003 | gnorm 0.255 | train_wall 97 | gb_free 9.1 | wall 61349
2022-09-08 17:08:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:08:37 | INFO | fairseq.trainer | begin training epoch 461
2022-09-08 17:08:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:09:03 | INFO | train_inner | epoch 461:     21 / 81 loss=3.376, nll_loss=0.342, mask_loss=8.88271, p_2=0.03503, mask_ave=0.497, ppl=1.27, wps=3591.8, ups=0.65, wpb=5538.6, bsz=357.1, num_updates=37200, lr=0.000163956, gnorm=0.251, train_wall=119, gb_free=9, wall=61376
2022-09-08 17:10:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:10:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:10:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:10:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:10:26 | INFO | valid | epoch 461 | valid on 'valid' subset | loss 5.081 | nll_loss 2.474 | mask_loss 9.76415 | p_2 0.04866 | mask_ave 0.613 | ppl 5.56 | bleu 56.28 | wps 1520.4 | wpb 933.5 | bsz 59.6 | num_updates 37260 | best_bleu 57.42
2022-09-08 17:10:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 461 @ 37260 updates
2022-09-08 17:10:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint461.pt
2022-09-08 17:10:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint461.pt
2022-09-08 17:10:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint461.pt (epoch 461 @ 37260 updates, score 56.28) (writing took 22.681737311184406 seconds)
2022-09-08 17:10:49 | INFO | fairseq_cli.train | end of epoch 461 (average epoch stats below)
2022-09-08 17:10:49 | INFO | train | epoch 461 | loss 3.376 | nll_loss 0.341 | mask_loss 8.94733 | p_2 0.03508 | mask_ave 0.5 | ppl 1.27 | wps 3374.4 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 37260 | lr 0.000163824 | gnorm 0.226 | train_wall 97 | gb_free 9 | wall 61482
2022-09-08 17:10:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:10:49 | INFO | fairseq.trainer | begin training epoch 462
2022-09-08 17:10:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:11:39 | INFO | train_inner | epoch 462:     40 / 81 loss=3.376, nll_loss=0.341, mask_loss=8.90994, p_2=0.03532, mask_ave=0.5, ppl=1.27, wps=3542.2, ups=0.64, wpb=5510.9, bsz=359.9, num_updates=37300, lr=0.000163737, gnorm=0.23, train_wall=120, gb_free=9, wall=61531
2022-09-08 17:12:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:12:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:12:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:12:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:12:39 | INFO | valid | epoch 462 | valid on 'valid' subset | loss 5.079 | nll_loss 2.469 | mask_loss 9.9435 | p_2 0.0485 | mask_ave 0.618 | ppl 5.54 | bleu 56.5 | wps 1472.5 | wpb 933.5 | bsz 59.6 | num_updates 37341 | best_bleu 57.42
2022-09-08 17:12:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 462 @ 37341 updates
2022-09-08 17:12:39 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint462.pt
2022-09-08 17:12:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint462.pt
2022-09-08 17:13:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint462.pt (epoch 462 @ 37341 updates, score 56.5) (writing took 20.736767321825027 seconds)
2022-09-08 17:13:00 | INFO | fairseq_cli.train | end of epoch 462 (average epoch stats below)
2022-09-08 17:13:00 | INFO | train | epoch 462 | loss 3.376 | nll_loss 0.341 | mask_loss 8.96901 | p_2 0.03513 | mask_ave 0.498 | ppl 1.27 | wps 3415.7 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 37341 | lr 0.000163647 | gnorm 0.246 | train_wall 97 | gb_free 9.1 | wall 61613
2022-09-08 17:13:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:13:00 | INFO | fairseq.trainer | begin training epoch 463
2022-09-08 17:13:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:14:15 | INFO | train_inner | epoch 463:     59 / 81 loss=3.377, nll_loss=0.342, mask_loss=8.99731, p_2=0.03514, mask_ave=0.499, ppl=1.27, wps=3548.3, ups=0.64, wpb=5538.4, bsz=358.9, num_updates=37400, lr=0.000163517, gnorm=0.263, train_wall=122, gb_free=9.1, wall=61687
2022-09-08 17:14:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:14:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:14:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:14:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:14:54 | INFO | valid | epoch 463 | valid on 'valid' subset | loss 5.078 | nll_loss 2.466 | mask_loss 9.94048 | p_2 0.04852 | mask_ave 0.617 | ppl 5.52 | bleu 57.01 | wps 1473.3 | wpb 933.5 | bsz 59.6 | num_updates 37422 | best_bleu 57.42
2022-09-08 17:14:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 463 @ 37422 updates
2022-09-08 17:14:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint463.pt
2022-09-08 17:14:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint463.pt
2022-09-08 17:15:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint463.pt (epoch 463 @ 37422 updates, score 57.01) (writing took 22.856024377048016 seconds)
2022-09-08 17:15:17 | INFO | fairseq_cli.train | end of epoch 463 (average epoch stats below)
2022-09-08 17:15:17 | INFO | train | epoch 463 | loss 3.377 | nll_loss 0.343 | mask_loss 8.9911 | p_2 0.0351 | mask_ave 0.499 | ppl 1.27 | wps 3274.7 | ups 0.59 | wpb 5523.2 | bsz 358 | num_updates 37422 | lr 0.000163469 | gnorm 0.268 | train_wall 100 | gb_free 9.1 | wall 61750
2022-09-08 17:15:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:15:17 | INFO | fairseq.trainer | begin training epoch 464
2022-09-08 17:15:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:16:54 | INFO | train_inner | epoch 464:     78 / 81 loss=3.377, nll_loss=0.342, mask_loss=8.96793, p_2=0.03497, mask_ave=0.498, ppl=1.27, wps=3477.4, ups=0.63, wpb=5532.6, bsz=357, num_updates=37500, lr=0.000163299, gnorm=0.258, train_wall=123, gb_free=9.1, wall=61847
2022-09-08 17:16:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:16:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:16:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:16:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:17:08 | INFO | valid | epoch 464 | valid on 'valid' subset | loss 5.086 | nll_loss 2.475 | mask_loss 9.82775 | p_2 0.04853 | mask_ave 0.617 | ppl 5.56 | bleu 56.1 | wps 1550.2 | wpb 933.5 | bsz 59.6 | num_updates 37503 | best_bleu 57.42
2022-09-08 17:17:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 464 @ 37503 updates
2022-09-08 17:17:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint464.pt
2022-09-08 17:17:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint464.pt
2022-09-08 17:17:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint464.pt (epoch 464 @ 37503 updates, score 56.1) (writing took 27.627846617251635 seconds)
2022-09-08 17:17:36 | INFO | fairseq_cli.train | end of epoch 464 (average epoch stats below)
2022-09-08 17:17:36 | INFO | train | epoch 464 | loss 3.376 | nll_loss 0.341 | mask_loss 8.92103 | p_2 0.03512 | mask_ave 0.498 | ppl 1.27 | wps 3221.7 | ups 0.58 | wpb 5523.2 | bsz 358 | num_updates 37503 | lr 0.000163293 | gnorm 0.257 | train_wall 98 | gb_free 9.2 | wall 61888
2022-09-08 17:17:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:17:36 | INFO | fairseq.trainer | begin training epoch 465
2022-09-08 17:17:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:19:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:19:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:19:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:19:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:19:25 | INFO | valid | epoch 465 | valid on 'valid' subset | loss 5.082 | nll_loss 2.475 | mask_loss 10.0838 | p_2 0.04826 | mask_ave 0.626 | ppl 5.56 | bleu 56.76 | wps 1564.1 | wpb 933.5 | bsz 59.6 | num_updates 37584 | best_bleu 57.42
2022-09-08 17:19:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 465 @ 37584 updates
2022-09-08 17:19:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint465.pt
2022-09-08 17:19:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint465.pt
2022-09-08 17:19:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint465.pt (epoch 465 @ 37584 updates, score 56.76) (writing took 2.4009567722678185 seconds)
2022-09-08 17:19:28 | INFO | fairseq_cli.train | end of epoch 465 (average epoch stats below)
2022-09-08 17:19:28 | INFO | train | epoch 465 | loss 3.376 | nll_loss 0.341 | mask_loss 8.9295 | p_2 0.03503 | mask_ave 0.501 | ppl 1.27 | wps 3985.8 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 37584 | lr 0.000163117 | gnorm 0.243 | train_wall 97 | gb_free 9.1 | wall 62001
2022-09-08 17:19:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:19:28 | INFO | fairseq.trainer | begin training epoch 466
2022-09-08 17:19:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:19:48 | INFO | train_inner | epoch 466:     16 / 81 loss=3.376, nll_loss=0.341, mask_loss=8.94886, p_2=0.03508, mask_ave=0.502, ppl=1.27, wps=3144.1, ups=0.57, wpb=5492.7, bsz=355.5, num_updates=37600, lr=0.000163082, gnorm=0.245, train_wall=119, gb_free=9.1, wall=62021
2022-09-08 17:21:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:21:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:21:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:21:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:21:18 | INFO | valid | epoch 466 | valid on 'valid' subset | loss 5.081 | nll_loss 2.471 | mask_loss 9.8824 | p_2 0.04851 | mask_ave 0.618 | ppl 5.54 | bleu 56.86 | wps 1572.5 | wpb 933.5 | bsz 59.6 | num_updates 37665 | best_bleu 57.42
2022-09-08 17:21:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 466 @ 37665 updates
2022-09-08 17:21:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint466.pt
2022-09-08 17:21:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint466.pt
2022-09-08 17:21:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint466.pt (epoch 466 @ 37665 updates, score 56.86) (writing took 18.86569758504629 seconds)
2022-09-08 17:21:37 | INFO | fairseq_cli.train | end of epoch 466 (average epoch stats below)
2022-09-08 17:21:37 | INFO | train | epoch 466 | loss 3.376 | nll_loss 0.341 | mask_loss 9.02578 | p_2 0.03506 | mask_ave 0.5 | ppl 1.27 | wps 3471.2 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 37665 | lr 0.000162941 | gnorm 0.252 | train_wall 97 | gb_free 9 | wall 62130
2022-09-08 17:21:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:21:37 | INFO | fairseq.trainer | begin training epoch 467
2022-09-08 17:21:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:22:19 | INFO | train_inner | epoch 467:     35 / 81 loss=3.376, nll_loss=0.341, mask_loss=9.01284, p_2=0.0352, mask_ave=0.499, ppl=1.27, wps=3649.8, ups=0.66, wpb=5506.2, bsz=357, num_updates=37700, lr=0.000162866, gnorm=0.258, train_wall=119, gb_free=9.2, wall=62172
2022-09-08 17:23:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:23:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:23:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:23:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:23:26 | INFO | valid | epoch 467 | valid on 'valid' subset | loss 5.072 | nll_loss 2.461 | mask_loss 9.81816 | p_2 0.04864 | mask_ave 0.614 | ppl 5.51 | bleu 56.88 | wps 1581.4 | wpb 933.5 | bsz 59.6 | num_updates 37746 | best_bleu 57.42
2022-09-08 17:23:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 467 @ 37746 updates
2022-09-08 17:23:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint467.pt
2022-09-08 17:23:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint467.pt
2022-09-08 17:23:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint467.pt (epoch 467 @ 37746 updates, score 56.88) (writing took 19.544605314731598 seconds)
2022-09-08 17:23:46 | INFO | fairseq_cli.train | end of epoch 467 (average epoch stats below)
2022-09-08 17:23:46 | INFO | train | epoch 467 | loss 3.377 | nll_loss 0.342 | mask_loss 8.99649 | p_2 0.03514 | mask_ave 0.498 | ppl 1.27 | wps 3456.1 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 37746 | lr 0.000162766 | gnorm 0.27 | train_wall 97 | gb_free 9.2 | wall 62259
2022-09-08 17:23:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:23:46 | INFO | fairseq.trainer | begin training epoch 468
2022-09-08 17:23:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:24:53 | INFO | train_inner | epoch 468:     54 / 81 loss=3.377, nll_loss=0.342, mask_loss=8.9899, p_2=0.03491, mask_ave=0.494, ppl=1.27, wps=3620, ups=0.65, wpb=5559.3, bsz=357.8, num_updates=37800, lr=0.00016265, gnorm=0.259, train_wall=121, gb_free=9, wall=62326
2022-09-08 17:25:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:25:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:25:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:25:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:25:37 | INFO | valid | epoch 468 | valid on 'valid' subset | loss 5.067 | nll_loss 2.455 | mask_loss 9.75409 | p_2 0.04871 | mask_ave 0.612 | ppl 5.48 | bleu 56.58 | wps 1513.2 | wpb 933.5 | bsz 59.6 | num_updates 37827 | best_bleu 57.42
2022-09-08 17:25:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 468 @ 37827 updates
2022-09-08 17:25:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint468.pt
2022-09-08 17:25:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint468.pt
2022-09-08 17:25:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint468.pt (epoch 468 @ 37827 updates, score 56.58) (writing took 18.24807919934392 seconds)
2022-09-08 17:25:56 | INFO | fairseq_cli.train | end of epoch 468 (average epoch stats below)
2022-09-08 17:25:56 | INFO | train | epoch 468 | loss 3.376 | nll_loss 0.341 | mask_loss 8.95345 | p_2 0.03525 | mask_ave 0.495 | ppl 1.27 | wps 3451.6 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 37827 | lr 0.000162592 | gnorm 0.242 | train_wall 98 | gb_free 9.2 | wall 62389
2022-09-08 17:25:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:25:56 | INFO | fairseq.trainer | begin training epoch 469
2022-09-08 17:25:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:27:27 | INFO | train_inner | epoch 469:     73 / 81 loss=3.376, nll_loss=0.341, mask_loss=8.92883, p_2=0.03535, mask_ave=0.497, ppl=1.27, wps=3597.3, ups=0.65, wpb=5533.4, bsz=361.8, num_updates=37900, lr=0.000162435, gnorm=0.245, train_wall=123, gb_free=9.1, wall=62480
2022-09-08 17:27:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:27:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:27:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:27:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:27:47 | INFO | valid | epoch 469 | valid on 'valid' subset | loss 5.063 | nll_loss 2.445 | mask_loss 9.78932 | p_2 0.04858 | mask_ave 0.616 | ppl 5.44 | bleu 56.83 | wps 1560.1 | wpb 933.5 | bsz 59.6 | num_updates 37908 | best_bleu 57.42
2022-09-08 17:27:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 469 @ 37908 updates
2022-09-08 17:27:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint469.pt
2022-09-08 17:27:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint469.pt
2022-09-08 17:27:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint469.pt (epoch 469 @ 37908 updates, score 56.83) (writing took 2.3842613883316517 seconds)
2022-09-08 17:27:49 | INFO | fairseq_cli.train | end of epoch 469 (average epoch stats below)
2022-09-08 17:27:49 | INFO | train | epoch 469 | loss 3.376 | nll_loss 0.341 | mask_loss 8.95304 | p_2 0.0352 | mask_ave 0.496 | ppl 1.27 | wps 3940.1 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 37908 | lr 0.000162418 | gnorm 0.251 | train_wall 99 | gb_free 9.1 | wall 62502
2022-09-08 17:27:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:27:49 | INFO | fairseq.trainer | begin training epoch 470
2022-09-08 17:27:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:29:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:29:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:29:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:29:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:29:38 | INFO | valid | epoch 470 | valid on 'valid' subset | loss 5.077 | nll_loss 2.464 | mask_loss 9.77552 | p_2 0.04893 | mask_ave 0.605 | ppl 5.52 | bleu 56.33 | wps 1575.5 | wpb 933.5 | bsz 59.6 | num_updates 37989 | best_bleu 57.42
2022-09-08 17:29:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 470 @ 37989 updates
2022-09-08 17:29:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint470.pt
2022-09-08 17:29:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint470.pt
2022-09-08 17:29:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint470.pt (epoch 470 @ 37989 updates, score 56.33) (writing took 20.002758741378784 seconds)
2022-09-08 17:29:58 | INFO | fairseq_cli.train | end of epoch 470 (average epoch stats below)
2022-09-08 17:29:58 | INFO | train | epoch 470 | loss 3.376 | nll_loss 0.341 | mask_loss 8.88046 | p_2 0.03515 | mask_ave 0.497 | ppl 1.27 | wps 3464.5 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 37989 | lr 0.000162245 | gnorm 0.246 | train_wall 97 | gb_free 9.2 | wall 62631
2022-09-08 17:29:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:29:59 | INFO | fairseq.trainer | begin training epoch 471
2022-09-08 17:29:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:30:12 | INFO | train_inner | epoch 471:     11 / 81 loss=3.376, nll_loss=0.341, mask_loss=8.89588, p_2=0.03515, mask_ave=0.497, ppl=1.27, wps=3326.4, ups=0.6, wpb=5499.4, bsz=355.4, num_updates=38000, lr=0.000162221, gnorm=0.252, train_wall=118, gb_free=9.1, wall=62645
2022-09-08 17:31:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:31:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:31:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:31:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:31:47 | INFO | valid | epoch 471 | valid on 'valid' subset | loss 5.071 | nll_loss 2.458 | mask_loss 9.75243 | p_2 0.04868 | mask_ave 0.612 | ppl 5.5 | bleu 56.79 | wps 1587.4 | wpb 933.5 | bsz 59.6 | num_updates 38070 | best_bleu 57.42
2022-09-08 17:31:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 471 @ 38070 updates
2022-09-08 17:31:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint471.pt
2022-09-08 17:31:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint471.pt
2022-09-08 17:32:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint471.pt (epoch 471 @ 38070 updates, score 56.79) (writing took 21.508085504174232 seconds)
2022-09-08 17:32:08 | INFO | fairseq_cli.train | end of epoch 471 (average epoch stats below)
2022-09-08 17:32:08 | INFO | train | epoch 471 | loss 3.376 | nll_loss 0.341 | mask_loss 8.89831 | p_2 0.03524 | mask_ave 0.495 | ppl 1.27 | wps 3441.5 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 38070 | lr 0.000162072 | gnorm 0.252 | train_wall 96 | gb_free 9.2 | wall 62761
2022-09-08 17:32:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:32:09 | INFO | fairseq.trainer | begin training epoch 472
2022-09-08 17:32:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:32:46 | INFO | train_inner | epoch 472:     30 / 81 loss=3.376, nll_loss=0.341, mask_loss=8.89006, p_2=0.03519, mask_ave=0.494, ppl=1.27, wps=3603.3, ups=0.65, wpb=5545.8, bsz=361, num_updates=38100, lr=0.000162008, gnorm=0.25, train_wall=120, gb_free=9, wall=62799
2022-09-08 17:33:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:33:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:33:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:33:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:33:59 | INFO | valid | epoch 472 | valid on 'valid' subset | loss 5.06 | nll_loss 2.44 | mask_loss 9.7688 | p_2 0.04833 | mask_ave 0.622 | ppl 5.43 | bleu 57.14 | wps 1559.4 | wpb 933.5 | bsz 59.6 | num_updates 38151 | best_bleu 57.42
2022-09-08 17:33:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 472 @ 38151 updates
2022-09-08 17:33:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint472.pt
2022-09-08 17:34:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint472.pt
2022-09-08 17:34:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint472.pt (epoch 472 @ 38151 updates, score 57.14) (writing took 2.4179283007979393 seconds)
2022-09-08 17:34:01 | INFO | fairseq_cli.train | end of epoch 472 (average epoch stats below)
2022-09-08 17:34:01 | INFO | train | epoch 472 | loss 3.376 | nll_loss 0.342 | mask_loss 8.89502 | p_2 0.03516 | mask_ave 0.497 | ppl 1.27 | wps 3962.9 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 38151 | lr 0.0001619 | gnorm 0.261 | train_wall 98 | gb_free 9.2 | wall 62874
2022-09-08 17:34:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:34:02 | INFO | fairseq.trainer | begin training epoch 473
2022-09-08 17:34:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:35:02 | INFO | train_inner | epoch 473:     49 / 81 loss=3.376, nll_loss=0.342, mask_loss=8.93755, p_2=0.03501, mask_ave=0.497, ppl=1.27, wps=4076.5, ups=0.74, wpb=5537.9, bsz=358.9, num_updates=38200, lr=0.000161796, gnorm=0.25, train_wall=121, gb_free=9, wall=62935
2022-09-08 17:35:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:35:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:35:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:35:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:35:51 | INFO | valid | epoch 473 | valid on 'valid' subset | loss 5.092 | nll_loss 2.483 | mask_loss 9.72649 | p_2 0.04861 | mask_ave 0.614 | ppl 5.59 | bleu 56.27 | wps 1519.6 | wpb 933.5 | bsz 59.6 | num_updates 38232 | best_bleu 57.42
2022-09-08 17:35:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 473 @ 38232 updates
2022-09-08 17:35:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint473.pt
2022-09-08 17:35:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint473.pt
2022-09-08 17:36:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint473.pt (epoch 473 @ 38232 updates, score 56.27) (writing took 18.716645684093237 seconds)
2022-09-08 17:36:10 | INFO | fairseq_cli.train | end of epoch 473 (average epoch stats below)
2022-09-08 17:36:10 | INFO | train | epoch 473 | loss 3.375 | nll_loss 0.341 | mask_loss 8.94037 | p_2 0.03513 | mask_ave 0.498 | ppl 1.27 | wps 3474.9 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 38232 | lr 0.000161728 | gnorm 0.243 | train_wall 97 | gb_free 9.3 | wall 63003
2022-09-08 17:36:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:36:10 | INFO | fairseq.trainer | begin training epoch 474
2022-09-08 17:36:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:37:33 | INFO | train_inner | epoch 474:     68 / 81 loss=3.375, nll_loss=0.341, mask_loss=8.87777, p_2=0.03534, mask_ave=0.5, ppl=1.27, wps=3645.4, ups=0.66, wpb=5520.4, bsz=360.7, num_updates=38300, lr=0.000161585, gnorm=0.233, train_wall=120, gb_free=9, wall=63086
2022-09-08 17:37:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:37:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:37:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:37:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:37:59 | INFO | valid | epoch 474 | valid on 'valid' subset | loss 5.094 | nll_loss 2.486 | mask_loss 9.71253 | p_2 0.04819 | mask_ave 0.627 | ppl 5.6 | bleu 56.47 | wps 1513.2 | wpb 933.5 | bsz 59.6 | num_updates 38313 | best_bleu 57.42
2022-09-08 17:37:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 474 @ 38313 updates
2022-09-08 17:37:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint474.pt
2022-09-08 17:38:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint474.pt
2022-09-08 17:38:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint474.pt (epoch 474 @ 38313 updates, score 56.47) (writing took 2.4516593664884567 seconds)
2022-09-08 17:38:02 | INFO | fairseq_cli.train | end of epoch 474 (average epoch stats below)
2022-09-08 17:38:02 | INFO | train | epoch 474 | loss 3.376 | nll_loss 0.341 | mask_loss 8.88818 | p_2 0.03508 | mask_ave 0.5 | ppl 1.27 | wps 4006.5 | ups 0.73 | wpb 5523.2 | bsz 358 | num_updates 38313 | lr 0.000161557 | gnorm 0.247 | train_wall 96 | gb_free 9.1 | wall 63115
2022-09-08 17:38:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:38:02 | INFO | fairseq.trainer | begin training epoch 475
2022-09-08 17:38:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:39:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:39:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:39:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:39:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:39:50 | INFO | valid | epoch 475 | valid on 'valid' subset | loss 5.078 | nll_loss 2.47 | mask_loss 9.77496 | p_2 0.04862 | mask_ave 0.614 | ppl 5.54 | bleu 56.75 | wps 1559.6 | wpb 933.5 | bsz 59.6 | num_updates 38394 | best_bleu 57.42
2022-09-08 17:39:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 475 @ 38394 updates
2022-09-08 17:39:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint475.pt
2022-09-08 17:39:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint475.pt
2022-09-08 17:40:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint475.pt (epoch 475 @ 38394 updates, score 56.75) (writing took 18.32262435182929 seconds)
2022-09-08 17:40:09 | INFO | fairseq_cli.train | end of epoch 475 (average epoch stats below)
2022-09-08 17:40:09 | INFO | train | epoch 475 | loss 3.376 | nll_loss 0.341 | mask_loss 8.9866 | p_2 0.03511 | mask_ave 0.499 | ppl 1.27 | wps 3519.7 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 38394 | lr 0.000161387 | gnorm 0.247 | train_wall 96 | gb_free 9.1 | wall 63242
2022-09-08 17:40:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:40:09 | INFO | fairseq.trainer | begin training epoch 476
2022-09-08 17:40:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:40:17 | INFO | train_inner | epoch 476:      6 / 81 loss=3.376, nll_loss=0.341, mask_loss=8.96835, p_2=0.03508, mask_ave=0.499, ppl=1.27, wps=3352, ups=0.61, wpb=5497.7, bsz=355, num_updates=38400, lr=0.000161374, gnorm=0.26, train_wall=118, gb_free=9, wall=63250
2022-09-08 17:41:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:41:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:41:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:41:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:42:00 | INFO | valid | epoch 476 | valid on 'valid' subset | loss 5.071 | nll_loss 2.459 | mask_loss 9.82537 | p_2 0.04871 | mask_ave 0.612 | ppl 5.5 | bleu 57.04 | wps 1520.4 | wpb 933.5 | bsz 59.6 | num_updates 38475 | best_bleu 57.42
2022-09-08 17:42:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 476 @ 38475 updates
2022-09-08 17:42:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint476.pt
2022-09-08 17:42:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint476.pt
2022-09-08 17:42:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint476.pt (epoch 476 @ 38475 updates, score 57.04) (writing took 2.5754497684538364 seconds)
2022-09-08 17:42:02 | INFO | fairseq_cli.train | end of epoch 476 (average epoch stats below)
2022-09-08 17:42:02 | INFO | train | epoch 476 | loss 3.375 | nll_loss 0.341 | mask_loss 8.96512 | p_2 0.03515 | mask_ave 0.497 | ppl 1.27 | wps 3941.1 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 38475 | lr 0.000161217 | gnorm 0.245 | train_wall 98 | gb_free 9.1 | wall 63355
2022-09-08 17:42:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:42:03 | INFO | fairseq.trainer | begin training epoch 477
2022-09-08 17:42:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:42:33 | INFO | train_inner | epoch 477:     25 / 81 loss=3.376, nll_loss=0.341, mask_loss=9.00453, p_2=0.03506, mask_ave=0.498, ppl=1.27, wps=4059.7, ups=0.74, wpb=5510.1, bsz=351.4, num_updates=38500, lr=0.000161165, gnorm=0.249, train_wall=120, gb_free=9.2, wall=63386
2022-09-08 17:43:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:43:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:43:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:43:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:43:53 | INFO | valid | epoch 477 | valid on 'valid' subset | loss 5.083 | nll_loss 2.472 | mask_loss 9.96623 | p_2 0.04852 | mask_ave 0.618 | ppl 5.55 | bleu 56.51 | wps 1577.6 | wpb 933.5 | bsz 59.6 | num_updates 38556 | best_bleu 57.42
2022-09-08 17:43:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 477 @ 38556 updates
2022-09-08 17:43:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint477.pt
2022-09-08 17:43:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint477.pt
2022-09-08 17:44:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint477.pt (epoch 477 @ 38556 updates, score 56.51) (writing took 20.476510979235172 seconds)
2022-09-08 17:44:13 | INFO | fairseq_cli.train | end of epoch 477 (average epoch stats below)
2022-09-08 17:44:13 | INFO | train | epoch 477 | loss 3.376 | nll_loss 0.342 | mask_loss 9.05339 | p_2 0.03521 | mask_ave 0.496 | ppl 1.27 | wps 3417.8 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 38556 | lr 0.000161048 | gnorm 0.258 | train_wall 98 | gb_free 9.1 | wall 63486
2022-09-08 17:44:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:44:14 | INFO | fairseq.trainer | begin training epoch 478
2022-09-08 17:44:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:45:07 | INFO | train_inner | epoch 478:     44 / 81 loss=3.376, nll_loss=0.341, mask_loss=8.99185, p_2=0.03519, mask_ave=0.496, ppl=1.27, wps=3596.6, ups=0.65, wpb=5548.6, bsz=366.3, num_updates=38600, lr=0.000160956, gnorm=0.262, train_wall=121, gb_free=9.1, wall=63540
2022-09-08 17:45:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:45:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:45:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:45:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:46:03 | INFO | valid | epoch 478 | valid on 'valid' subset | loss 5.084 | nll_loss 2.469 | mask_loss 9.93876 | p_2 0.04838 | mask_ave 0.621 | ppl 5.54 | bleu 56.31 | wps 1521.5 | wpb 933.5 | bsz 59.6 | num_updates 38637 | best_bleu 57.42
2022-09-08 17:46:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 478 @ 38637 updates
2022-09-08 17:46:03 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint478.pt
2022-09-08 17:46:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint478.pt
2022-09-08 17:46:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint478.pt (epoch 478 @ 38637 updates, score 56.31) (writing took 17.597787961363792 seconds)
2022-09-08 17:46:20 | INFO | fairseq_cli.train | end of epoch 478 (average epoch stats below)
2022-09-08 17:46:20 | INFO | train | epoch 478 | loss 3.376 | nll_loss 0.342 | mask_loss 8.96433 | p_2 0.03512 | mask_ave 0.498 | ppl 1.27 | wps 3519.2 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 38637 | lr 0.000160879 | gnorm 0.267 | train_wall 97 | gb_free 9.3 | wall 63613
2022-09-08 17:46:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:46:21 | INFO | fairseq.trainer | begin training epoch 479
2022-09-08 17:46:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:47:38 | INFO | train_inner | epoch 479:     63 / 81 loss=3.376, nll_loss=0.341, mask_loss=8.96594, p_2=0.03518, mask_ave=0.499, ppl=1.27, wps=3668.4, ups=0.66, wpb=5525, bsz=357.8, num_updates=38700, lr=0.000160748, gnorm=0.254, train_wall=120, gb_free=9, wall=63691
2022-09-08 17:48:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:48:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:48:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:48:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:48:11 | INFO | valid | epoch 479 | valid on 'valid' subset | loss 5.093 | nll_loss 2.489 | mask_loss 9.9731 | p_2 0.04825 | mask_ave 0.625 | ppl 5.62 | bleu 56.6 | wps 1537.5 | wpb 933.5 | bsz 59.6 | num_updates 38718 | best_bleu 57.42
2022-09-08 17:48:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 479 @ 38718 updates
2022-09-08 17:48:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint479.pt
2022-09-08 17:48:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint479.pt
2022-09-08 17:48:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint479.pt (epoch 479 @ 38718 updates, score 56.6) (writing took 18.223886251449585 seconds)
2022-09-08 17:48:29 | INFO | fairseq_cli.train | end of epoch 479 (average epoch stats below)
2022-09-08 17:48:29 | INFO | train | epoch 479 | loss 3.375 | nll_loss 0.341 | mask_loss 8.97741 | p_2 0.03509 | mask_ave 0.499 | ppl 1.27 | wps 3479 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 38718 | lr 0.00016071 | gnorm 0.244 | train_wall 98 | gb_free 9.1 | wall 63742
2022-09-08 17:48:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:48:29 | INFO | fairseq.trainer | begin training epoch 480
2022-09-08 17:48:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:50:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:50:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:50:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:50:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:50:19 | INFO | valid | epoch 480 | valid on 'valid' subset | loss 5.092 | nll_loss 2.489 | mask_loss 9.59625 | p_2 0.04836 | mask_ave 0.622 | ppl 5.61 | bleu 56.39 | wps 1531.6 | wpb 933.5 | bsz 59.6 | num_updates 38799 | best_bleu 57.42
2022-09-08 17:50:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 480 @ 38799 updates
2022-09-08 17:50:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint480.pt
2022-09-08 17:50:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint480.pt
2022-09-08 17:50:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint480.pt (epoch 480 @ 38799 updates, score 56.39) (writing took 20.65979692339897 seconds)
2022-09-08 17:50:40 | INFO | fairseq_cli.train | end of epoch 480 (average epoch stats below)
2022-09-08 17:50:40 | INFO | train | epoch 480 | loss 3.376 | nll_loss 0.341 | mask_loss 8.94135 | p_2 0.03508 | mask_ave 0.5 | ppl 1.27 | wps 3420.5 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 38799 | lr 0.000160542 | gnorm 0.254 | train_wall 97 | gb_free 9.2 | wall 63873
2022-09-08 17:50:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:50:40 | INFO | fairseq.trainer | begin training epoch 481
2022-09-08 17:50:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:50:42 | INFO | train_inner | epoch 481:      1 / 81 loss=3.376, nll_loss=0.341, mask_loss=8.9606, p_2=0.03508, mask_ave=0.5, ppl=1.27, wps=2980, ups=0.54, wpb=5500.9, bsz=356.1, num_updates=38800, lr=0.00016054, gnorm=0.252, train_wall=120, gb_free=8.9, wall=63875
2022-09-08 17:52:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:52:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:52:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:52:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:52:31 | INFO | valid | epoch 481 | valid on 'valid' subset | loss 5.074 | nll_loss 2.462 | mask_loss 9.81578 | p_2 0.04848 | mask_ave 0.619 | ppl 5.51 | bleu 56.39 | wps 1540.3 | wpb 933.5 | bsz 59.6 | num_updates 38880 | best_bleu 57.42
2022-09-08 17:52:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 481 @ 38880 updates
2022-09-08 17:52:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint481.pt
2022-09-08 17:52:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint481.pt
2022-09-08 17:52:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint481.pt (epoch 481 @ 38880 updates, score 56.39) (writing took 25.068715654313564 seconds)
2022-09-08 17:52:57 | INFO | fairseq_cli.train | end of epoch 481 (average epoch stats below)
2022-09-08 17:52:57 | INFO | train | epoch 481 | loss 3.376 | nll_loss 0.342 | mask_loss 8.85731 | p_2 0.03512 | mask_ave 0.499 | ppl 1.27 | wps 3272.4 | ups 0.59 | wpb 5523.2 | bsz 358 | num_updates 38880 | lr 0.000160375 | gnorm 0.284 | train_wall 99 | gb_free 9.2 | wall 64009
2022-09-08 17:52:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:52:57 | INFO | fairseq.trainer | begin training epoch 482
2022-09-08 17:52:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:53:22 | INFO | train_inner | epoch 482:     20 / 81 loss=3.376, nll_loss=0.341, mask_loss=8.90257, p_2=0.03498, mask_ave=0.498, ppl=1.27, wps=3470.1, ups=0.63, wpb=5536.5, bsz=356.5, num_updates=38900, lr=0.000160334, gnorm=0.276, train_wall=121, gb_free=9.2, wall=64035
2022-09-08 17:54:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:54:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:54:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:54:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:54:46 | INFO | valid | epoch 482 | valid on 'valid' subset | loss 5.079 | nll_loss 2.468 | mask_loss 9.75466 | p_2 0.04861 | mask_ave 0.614 | ppl 5.53 | bleu 56.76 | wps 1538 | wpb 933.5 | bsz 59.6 | num_updates 38961 | best_bleu 57.42
2022-09-08 17:54:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 482 @ 38961 updates
2022-09-08 17:54:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint482.pt
2022-09-08 17:54:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint482.pt
2022-09-08 17:55:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint482.pt (epoch 482 @ 38961 updates, score 56.76) (writing took 18.268394954502583 seconds)
2022-09-08 17:55:04 | INFO | fairseq_cli.train | end of epoch 482 (average epoch stats below)
2022-09-08 17:55:04 | INFO | train | epoch 482 | loss 3.376 | nll_loss 0.341 | mask_loss 8.92895 | p_2 0.03517 | mask_ave 0.497 | ppl 1.27 | wps 3506.5 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 38961 | lr 0.000160208 | gnorm 0.252 | train_wall 96 | gb_free 9 | wall 64137
2022-09-08 17:55:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:55:04 | INFO | fairseq.trainer | begin training epoch 483
2022-09-08 17:55:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:55:53 | INFO | train_inner | epoch 483:     39 / 81 loss=3.376, nll_loss=0.341, mask_loss=8.90993, p_2=0.03519, mask_ave=0.496, ppl=1.27, wps=3668.8, ups=0.66, wpb=5532.9, bsz=360.6, num_updates=39000, lr=0.000160128, gnorm=0.26, train_wall=120, gb_free=9, wall=64186
2022-09-08 17:56:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:56:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:56:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:56:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:56:56 | INFO | valid | epoch 483 | valid on 'valid' subset | loss 5.089 | nll_loss 2.483 | mask_loss 9.75709 | p_2 0.04836 | mask_ave 0.622 | ppl 5.59 | bleu 56.44 | wps 1529.5 | wpb 933.5 | bsz 59.6 | num_updates 39042 | best_bleu 57.42
2022-09-08 17:56:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 483 @ 39042 updates
2022-09-08 17:56:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint483.pt
2022-09-08 17:56:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint483.pt
2022-09-08 17:57:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint483.pt (epoch 483 @ 39042 updates, score 56.44) (writing took 24.945794578641653 seconds)
2022-09-08 17:57:21 | INFO | fairseq_cli.train | end of epoch 483 (average epoch stats below)
2022-09-08 17:57:21 | INFO | train | epoch 483 | loss 3.376 | nll_loss 0.341 | mask_loss 8.97302 | p_2 0.03511 | mask_ave 0.499 | ppl 1.27 | wps 3275.1 | ups 0.59 | wpb 5523.2 | bsz 358 | num_updates 39042 | lr 0.000160042 | gnorm 0.256 | train_wall 99 | gb_free 9.2 | wall 64274
2022-09-08 17:57:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:57:21 | INFO | fairseq.trainer | begin training epoch 484
2022-09-08 17:57:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 17:58:32 | INFO | train_inner | epoch 484:     58 / 81 loss=3.376, nll_loss=0.342, mask_loss=8.94738, p_2=0.03532, mask_ave=0.502, ppl=1.27, wps=3468.8, ups=0.63, wpb=5522.2, bsz=360.3, num_updates=39100, lr=0.000159923, gnorm=0.254, train_wall=121, gb_free=9.1, wall=64345
2022-09-08 17:58:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 17:59:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 17:59:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 17:59:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 17:59:10 | INFO | valid | epoch 484 | valid on 'valid' subset | loss 5.077 | nll_loss 2.467 | mask_loss 10.0653 | p_2 0.04863 | mask_ave 0.615 | ppl 5.53 | bleu 56.42 | wps 1584.7 | wpb 933.5 | bsz 59.6 | num_updates 39123 | best_bleu 57.42
2022-09-08 17:59:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 484 @ 39123 updates
2022-09-08 17:59:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint484.pt
2022-09-08 17:59:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint484.pt
2022-09-08 17:59:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint484.pt (epoch 484 @ 39123 updates, score 56.42) (writing took 21.329528957605362 seconds)
2022-09-08 17:59:31 | INFO | fairseq_cli.train | end of epoch 484 (average epoch stats below)
2022-09-08 17:59:31 | INFO | train | epoch 484 | loss 3.376 | nll_loss 0.341 | mask_loss 9.00327 | p_2 0.03505 | mask_ave 0.501 | ppl 1.27 | wps 3424.8 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 39123 | lr 0.000159876 | gnorm 0.257 | train_wall 97 | gb_free 9.1 | wall 64404
2022-09-08 17:59:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 17:59:32 | INFO | fairseq.trainer | begin training epoch 485
2022-09-08 17:59:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:01:07 | INFO | train_inner | epoch 485:     77 / 81 loss=3.375, nll_loss=0.341, mask_loss=9.11885, p_2=0.03484, mask_ave=0.502, ppl=1.27, wps=3571.6, ups=0.65, wpb=5524.6, bsz=354.2, num_updates=39200, lr=0.000159719, gnorm=0.247, train_wall=121, gb_free=9.1, wall=64500
2022-09-08 18:01:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:01:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:01:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:01:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:01:22 | INFO | valid | epoch 485 | valid on 'valid' subset | loss 5.096 | nll_loss 2.49 | mask_loss 9.8557 | p_2 0.04825 | mask_ave 0.625 | ppl 5.62 | bleu 56.13 | wps 1562.9 | wpb 933.5 | bsz 59.6 | num_updates 39204 | best_bleu 57.42
2022-09-08 18:01:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 485 @ 39204 updates
2022-09-08 18:01:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint485.pt
2022-09-08 18:01:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint485.pt
2022-09-08 18:01:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint485.pt (epoch 485 @ 39204 updates, score 56.13) (writing took 19.945522274821997 seconds)
2022-09-08 18:01:42 | INFO | fairseq_cli.train | end of epoch 485 (average epoch stats below)
2022-09-08 18:01:42 | INFO | train | epoch 485 | loss 3.375 | nll_loss 0.341 | mask_loss 9.0639 | p_2 0.035 | mask_ave 0.502 | ppl 1.27 | wps 3425.4 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 39204 | lr 0.000159711 | gnorm 0.248 | train_wall 98 | gb_free 9.1 | wall 64535
2022-09-08 18:01:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:01:42 | INFO | fairseq.trainer | begin training epoch 486
2022-09-08 18:01:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:03:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:03:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:03:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:03:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:03:32 | INFO | valid | epoch 486 | valid on 'valid' subset | loss 5.095 | nll_loss 2.486 | mask_loss 9.75163 | p_2 0.04843 | mask_ave 0.62 | ppl 5.6 | bleu 56.61 | wps 1540.4 | wpb 933.5 | bsz 59.6 | num_updates 39285 | best_bleu 57.42
2022-09-08 18:03:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 486 @ 39285 updates
2022-09-08 18:03:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint486.pt
2022-09-08 18:03:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint486.pt
2022-09-08 18:03:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint486.pt (epoch 486 @ 39285 updates, score 56.61) (writing took 2.861717112362385 seconds)
2022-09-08 18:03:35 | INFO | fairseq_cli.train | end of epoch 486 (average epoch stats below)
2022-09-08 18:03:35 | INFO | train | epoch 486 | loss 3.376 | nll_loss 0.341 | mask_loss 8.95836 | p_2 0.03498 | mask_ave 0.503 | ppl 1.27 | wps 3960.1 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 39285 | lr 0.000159546 | gnorm 0.265 | train_wall 97 | gb_free 9.1 | wall 64648
2022-09-08 18:03:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:03:35 | INFO | fairseq.trainer | begin training epoch 487
2022-09-08 18:03:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:03:54 | INFO | train_inner | epoch 487:     15 / 81 loss=3.375, nll_loss=0.341, mask_loss=8.93156, p_2=0.03501, mask_ave=0.502, ppl=1.27, wps=3295.5, ups=0.6, wpb=5515.4, bsz=360.4, num_updates=39300, lr=0.000159516, gnorm=0.265, train_wall=119, gb_free=9.1, wall=64667
2022-09-08 18:05:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:05:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:05:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:05:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:05:25 | INFO | valid | epoch 487 | valid on 'valid' subset | loss 5.076 | nll_loss 2.463 | mask_loss 9.89923 | p_2 0.04854 | mask_ave 0.617 | ppl 5.51 | bleu 57.01 | wps 1539.4 | wpb 933.5 | bsz 59.6 | num_updates 39366 | best_bleu 57.42
2022-09-08 18:05:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 487 @ 39366 updates
2022-09-08 18:05:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint487.pt
2022-09-08 18:05:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint487.pt
2022-09-08 18:05:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint487.pt (epoch 487 @ 39366 updates, score 57.01) (writing took 20.197817843407393 seconds)
2022-09-08 18:05:46 | INFO | fairseq_cli.train | end of epoch 487 (average epoch stats below)
2022-09-08 18:05:46 | INFO | train | epoch 487 | loss 3.375 | nll_loss 0.341 | mask_loss 9.01508 | p_2 0.0351 | mask_ave 0.499 | ppl 1.27 | wps 3423.5 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 39366 | lr 0.000159382 | gnorm 0.244 | train_wall 98 | gb_free 9.1 | wall 64779
2022-09-08 18:05:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:05:46 | INFO | fairseq.trainer | begin training epoch 488
2022-09-08 18:05:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:06:28 | INFO | train_inner | epoch 488:     34 / 81 loss=3.375, nll_loss=0.341, mask_loss=9.07675, p_2=0.03498, mask_ave=0.498, ppl=1.27, wps=3580, ups=0.65, wpb=5511.3, bsz=354, num_updates=39400, lr=0.000159313, gnorm=0.238, train_wall=121, gb_free=9.1, wall=64821
2022-09-08 18:07:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:07:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:07:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:07:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:07:37 | INFO | valid | epoch 488 | valid on 'valid' subset | loss 5.091 | nll_loss 2.484 | mask_loss 9.71623 | p_2 0.04841 | mask_ave 0.621 | ppl 5.59 | bleu 56.21 | wps 1515.3 | wpb 933.5 | bsz 59.6 | num_updates 39447 | best_bleu 57.42
2022-09-08 18:07:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 488 @ 39447 updates
2022-09-08 18:07:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint488.pt
2022-09-08 18:07:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint488.pt
2022-09-08 18:07:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint488.pt (epoch 488 @ 39447 updates, score 56.21) (writing took 19.632981337606907 seconds)
2022-09-08 18:07:57 | INFO | fairseq_cli.train | end of epoch 488 (average epoch stats below)
2022-09-08 18:07:57 | INFO | train | epoch 488 | loss 3.375 | nll_loss 0.34 | mask_loss 8.98009 | p_2 0.03514 | mask_ave 0.498 | ppl 1.27 | wps 3415.4 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 39447 | lr 0.000159218 | gnorm 0.239 | train_wall 98 | gb_free 9.1 | wall 64910
2022-09-08 18:07:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:07:57 | INFO | fairseq.trainer | begin training epoch 489
2022-09-08 18:07:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:09:02 | INFO | train_inner | epoch 489:     53 / 81 loss=3.375, nll_loss=0.34, mask_loss=8.78602, p_2=0.03545, mask_ave=0.499, ppl=1.27, wps=3595.4, ups=0.65, wpb=5549.1, bsz=367, num_updates=39500, lr=0.000159111, gnorm=0.248, train_wall=121, gb_free=9.1, wall=64975
2022-09-08 18:09:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:09:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:09:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:09:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:09:47 | INFO | valid | epoch 489 | valid on 'valid' subset | loss 5.078 | nll_loss 2.469 | mask_loss 9.57231 | p_2 0.04872 | mask_ave 0.611 | ppl 5.54 | bleu 56.92 | wps 1517.7 | wpb 933.5 | bsz 59.6 | num_updates 39528 | best_bleu 57.42
2022-09-08 18:09:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 489 @ 39528 updates
2022-09-08 18:09:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint489.pt
2022-09-08 18:09:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint489.pt
2022-09-08 18:10:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint489.pt (epoch 489 @ 39528 updates, score 56.92) (writing took 23.294053588062525 seconds)
2022-09-08 18:10:11 | INFO | fairseq_cli.train | end of epoch 489 (average epoch stats below)
2022-09-08 18:10:11 | INFO | train | epoch 489 | loss 3.376 | nll_loss 0.341 | mask_loss 8.81275 | p_2 0.03509 | mask_ave 0.499 | ppl 1.27 | wps 3335.8 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 39528 | lr 0.000159055 | gnorm 0.259 | train_wall 98 | gb_free 9.4 | wall 65044
2022-09-08 18:10:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:10:11 | INFO | fairseq.trainer | begin training epoch 490
2022-09-08 18:10:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:11:39 | INFO | train_inner | epoch 490:     72 / 81 loss=3.375, nll_loss=0.341, mask_loss=8.85488, p_2=0.03497, mask_ave=0.497, ppl=1.27, wps=3523.8, ups=0.64, wpb=5526.8, bsz=353.8, num_updates=39600, lr=0.00015891, gnorm=0.252, train_wall=121, gb_free=9.1, wall=65132
2022-09-08 18:11:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:11:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:11:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:11:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:12:00 | INFO | valid | epoch 490 | valid on 'valid' subset | loss 5.08 | nll_loss 2.47 | mask_loss 9.63538 | p_2 0.04857 | mask_ave 0.615 | ppl 5.54 | bleu 56.45 | wps 1574.2 | wpb 933.5 | bsz 59.6 | num_updates 39609 | best_bleu 57.42
2022-09-08 18:12:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 490 @ 39609 updates
2022-09-08 18:12:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint490.pt
2022-09-08 18:12:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint490.pt
2022-09-08 18:12:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint490.pt (epoch 490 @ 39609 updates, score 56.45) (writing took 23.370338212698698 seconds)
2022-09-08 18:12:24 | INFO | fairseq_cli.train | end of epoch 490 (average epoch stats below)
2022-09-08 18:12:24 | INFO | train | epoch 490 | loss 3.375 | nll_loss 0.341 | mask_loss 8.80607 | p_2 0.0352 | mask_ave 0.496 | ppl 1.27 | wps 3361.4 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 39609 | lr 0.000158892 | gnorm 0.245 | train_wall 97 | gb_free 9.2 | wall 65177
2022-09-08 18:12:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:12:24 | INFO | fairseq.trainer | begin training epoch 491
2022-09-08 18:12:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:14:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:14:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:14:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:14:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:14:15 | INFO | valid | epoch 491 | valid on 'valid' subset | loss 5.068 | nll_loss 2.459 | mask_loss 9.66524 | p_2 0.04885 | mask_ave 0.607 | ppl 5.5 | bleu 57.24 | wps 1469.3 | wpb 933.5 | bsz 59.6 | num_updates 39690 | best_bleu 57.42
2022-09-08 18:14:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 491 @ 39690 updates
2022-09-08 18:14:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint491.pt
2022-09-08 18:14:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint491.pt
2022-09-08 18:14:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint491.pt (epoch 491 @ 39690 updates, score 57.24) (writing took 2.5793303698301315 seconds)
2022-09-08 18:14:18 | INFO | fairseq_cli.train | end of epoch 491 (average epoch stats below)
2022-09-08 18:14:18 | INFO | train | epoch 491 | loss 3.375 | nll_loss 0.34 | mask_loss 8.84338 | p_2 0.03527 | mask_ave 0.494 | ppl 1.27 | wps 3938.3 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 39690 | lr 0.00015873 | gnorm 0.236 | train_wall 98 | gb_free 9 | wall 65290
2022-09-08 18:14:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:14:18 | INFO | fairseq.trainer | begin training epoch 492
2022-09-08 18:14:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:14:31 | INFO | train_inner | epoch 492:     10 / 81 loss=3.375, nll_loss=0.34, mask_loss=8.84532, p_2=0.03521, mask_ave=0.493, ppl=1.27, wps=3205.5, ups=0.58, wpb=5508.6, bsz=355.8, num_updates=39700, lr=0.00015871, gnorm=0.237, train_wall=120, gb_free=9.1, wall=65304
2022-09-08 18:15:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:15:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:15:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:15:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:16:07 | INFO | valid | epoch 492 | valid on 'valid' subset | loss 5.086 | nll_loss 2.474 | mask_loss 9.86723 | p_2 0.04845 | mask_ave 0.62 | ppl 5.55 | bleu 56.8 | wps 1562.8 | wpb 933.5 | bsz 59.6 | num_updates 39771 | best_bleu 57.42
2022-09-08 18:16:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 492 @ 39771 updates
2022-09-08 18:16:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint492.pt
2022-09-08 18:16:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint492.pt
2022-09-08 18:16:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint492.pt (epoch 492 @ 39771 updates, score 56.8) (writing took 17.753733422607183 seconds)
2022-09-08 18:16:25 | INFO | fairseq_cli.train | end of epoch 492 (average epoch stats below)
2022-09-08 18:16:25 | INFO | train | epoch 492 | loss 3.375 | nll_loss 0.34 | mask_loss 8.92777 | p_2 0.03517 | mask_ave 0.497 | ppl 1.27 | wps 3504.8 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 39771 | lr 0.000158568 | gnorm 0.233 | train_wall 97 | gb_free 9.2 | wall 65418
2022-09-08 18:16:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:16:25 | INFO | fairseq.trainer | begin training epoch 493
2022-09-08 18:16:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:17:02 | INFO | train_inner | epoch 493:     29 / 81 loss=3.374, nll_loss=0.34, mask_loss=8.93053, p_2=0.03525, mask_ave=0.498, ppl=1.27, wps=3661, ups=0.66, wpb=5524.6, bsz=361, num_updates=39800, lr=0.000158511, gnorm=0.231, train_wall=120, gb_free=9.1, wall=65455
2022-09-08 18:18:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:18:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:18:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:18:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:18:17 | INFO | valid | epoch 493 | valid on 'valid' subset | loss 5.072 | nll_loss 2.463 | mask_loss 9.76715 | p_2 0.04849 | mask_ave 0.618 | ppl 5.51 | bleu 57.24 | wps 1552.4 | wpb 933.5 | bsz 59.6 | num_updates 39852 | best_bleu 57.42
2022-09-08 18:18:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 493 @ 39852 updates
2022-09-08 18:18:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint493.pt
2022-09-08 18:18:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint493.pt
2022-09-08 18:18:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint493.pt (epoch 493 @ 39852 updates, score 57.24) (writing took 20.317656729370356 seconds)
2022-09-08 18:18:37 | INFO | fairseq_cli.train | end of epoch 493 (average epoch stats below)
2022-09-08 18:18:37 | INFO | train | epoch 493 | loss 3.375 | nll_loss 0.341 | mask_loss 8.93344 | p_2 0.03513 | mask_ave 0.498 | ppl 1.27 | wps 3386.1 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 39852 | lr 0.000158407 | gnorm 0.24 | train_wall 99 | gb_free 9.1 | wall 65550
2022-09-08 18:18:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:18:37 | INFO | fairseq.trainer | begin training epoch 494
2022-09-08 18:18:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:19:37 | INFO | train_inner | epoch 494:     48 / 81 loss=3.376, nll_loss=0.342, mask_loss=8.97604, p_2=0.03481, mask_ave=0.5, ppl=1.27, wps=3553.2, ups=0.64, wpb=5513.6, bsz=349, num_updates=39900, lr=0.000158312, gnorm=0.259, train_wall=122, gb_free=9, wall=65610
2022-09-08 18:20:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:20:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:20:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:20:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:20:28 | INFO | valid | epoch 494 | valid on 'valid' subset | loss 5.083 | nll_loss 2.468 | mask_loss 9.83053 | p_2 0.0486 | mask_ave 0.614 | ppl 5.53 | bleu 56.87 | wps 1575.1 | wpb 933.5 | bsz 59.6 | num_updates 39933 | best_bleu 57.42
2022-09-08 18:20:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 494 @ 39933 updates
2022-09-08 18:20:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint494.pt
2022-09-08 18:20:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint494.pt
2022-09-08 18:20:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint494.pt (epoch 494 @ 39933 updates, score 56.87) (writing took 2.559709742665291 seconds)
2022-09-08 18:20:31 | INFO | fairseq_cli.train | end of epoch 494 (average epoch stats below)
2022-09-08 18:20:31 | INFO | train | epoch 494 | loss 3.375 | nll_loss 0.341 | mask_loss 8.92972 | p_2 0.03507 | mask_ave 0.5 | ppl 1.27 | wps 3944.6 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 39933 | lr 0.000158246 | gnorm 0.27 | train_wall 98 | gb_free 9.2 | wall 65664
2022-09-08 18:20:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:20:31 | INFO | fairseq.trainer | begin training epoch 495
2022-09-08 18:20:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:21:54 | INFO | train_inner | epoch 495:     67 / 81 loss=3.375, nll_loss=0.341, mask_loss=8.86947, p_2=0.03536, mask_ave=0.497, ppl=1.27, wps=4054.8, ups=0.73, wpb=5543.5, bsz=363.6, num_updates=40000, lr=0.000158114, gnorm=0.24, train_wall=121, gb_free=8.9, wall=65747
2022-09-08 18:22:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:22:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:22:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:22:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:22:22 | INFO | valid | epoch 495 | valid on 'valid' subset | loss 5.076 | nll_loss 2.465 | mask_loss 9.75296 | p_2 0.04856 | mask_ave 0.616 | ppl 5.52 | bleu 57.06 | wps 1504.8 | wpb 933.5 | bsz 59.6 | num_updates 40014 | best_bleu 57.42
2022-09-08 18:22:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 495 @ 40014 updates
2022-09-08 18:22:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint495.pt
2022-09-08 18:22:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint495.pt
2022-09-08 18:22:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint495.pt (epoch 495 @ 40014 updates, score 57.06) (writing took 21.671791456639767 seconds)
2022-09-08 18:22:44 | INFO | fairseq_cli.train | end of epoch 495 (average epoch stats below)
2022-09-08 18:22:44 | INFO | train | epoch 495 | loss 3.375 | nll_loss 0.34 | mask_loss 8.89491 | p_2 0.0352 | mask_ave 0.496 | ppl 1.27 | wps 3361.2 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 40014 | lr 0.000158086 | gnorm 0.225 | train_wall 98 | gb_free 9.1 | wall 65797
2022-09-08 18:22:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:22:44 | INFO | fairseq.trainer | begin training epoch 496
2022-09-08 18:22:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:24:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:24:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:24:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:24:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:24:35 | INFO | valid | epoch 496 | valid on 'valid' subset | loss 5.081 | nll_loss 2.474 | mask_loss 9.93219 | p_2 0.04828 | mask_ave 0.624 | ppl 5.56 | bleu 56.64 | wps 1556.7 | wpb 933.5 | bsz 59.6 | num_updates 40095 | best_bleu 57.42
2022-09-08 18:24:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 496 @ 40095 updates
2022-09-08 18:24:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint496.pt
2022-09-08 18:24:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint496.pt
2022-09-08 18:24:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint496.pt (epoch 496 @ 40095 updates, score 56.64) (writing took 21.573860876262188 seconds)
2022-09-08 18:24:57 | INFO | fairseq_cli.train | end of epoch 496 (average epoch stats below)
2022-09-08 18:24:57 | INFO | train | epoch 496 | loss 3.375 | nll_loss 0.34 | mask_loss 8.90433 | p_2 0.035 | mask_ave 0.502 | ppl 1.27 | wps 3371.9 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 40095 | lr 0.000157926 | gnorm 0.245 | train_wall 99 | gb_free 9.1 | wall 65929
2022-09-08 18:24:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:24:57 | INFO | fairseq.trainer | begin training epoch 497
2022-09-08 18:24:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:25:04 | INFO | train_inner | epoch 497:      5 / 81 loss=3.375, nll_loss=0.34, mask_loss=8.89996, p_2=0.03517, mask_ave=0.502, ppl=1.27, wps=2895.7, ups=0.53, wpb=5493.2, bsz=359, num_updates=40100, lr=0.000157917, gnorm=0.242, train_wall=121, gb_free=9, wall=65936
2022-09-08 18:26:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:26:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:26:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:26:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:26:47 | INFO | valid | epoch 497 | valid on 'valid' subset | loss 5.075 | nll_loss 2.464 | mask_loss 9.87516 | p_2 0.04855 | mask_ave 0.616 | ppl 5.52 | bleu 56.79 | wps 1484.7 | wpb 933.5 | bsz 59.6 | num_updates 40176 | best_bleu 57.42
2022-09-08 18:26:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 497 @ 40176 updates
2022-09-08 18:26:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint497.pt
2022-09-08 18:26:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint497.pt
2022-09-08 18:27:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint497.pt (epoch 497 @ 40176 updates, score 56.79) (writing took 20.90132985264063 seconds)
2022-09-08 18:27:08 | INFO | fairseq_cli.train | end of epoch 497 (average epoch stats below)
2022-09-08 18:27:09 | INFO | train | epoch 497 | loss 3.375 | nll_loss 0.341 | mask_loss 8.9423 | p_2 0.03508 | mask_ave 0.5 | ppl 1.27 | wps 3389.7 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 40176 | lr 0.000157767 | gnorm 0.256 | train_wall 98 | gb_free 9.1 | wall 66061
2022-09-08 18:27:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:27:09 | INFO | fairseq.trainer | begin training epoch 498
2022-09-08 18:27:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:27:39 | INFO | train_inner | epoch 498:     24 / 81 loss=3.375, nll_loss=0.341, mask_loss=8.94454, p_2=0.03496, mask_ave=0.5, ppl=1.27, wps=3569.1, ups=0.64, wpb=5546, bsz=358.2, num_updates=40200, lr=0.00015772, gnorm=0.258, train_wall=121, gb_free=9, wall=66092
2022-09-08 18:28:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:28:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:28:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:28:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:28:59 | INFO | valid | epoch 498 | valid on 'valid' subset | loss 5.109 | nll_loss 2.502 | mask_loss 9.8847 | p_2 0.04857 | mask_ave 0.615 | ppl 5.67 | bleu 56.65 | wps 1555.4 | wpb 933.5 | bsz 59.6 | num_updates 40257 | best_bleu 57.42
2022-09-08 18:28:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 498 @ 40257 updates
2022-09-08 18:28:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint498.pt
2022-09-08 18:29:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint498.pt
2022-09-08 18:29:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint498.pt (epoch 498 @ 40257 updates, score 56.65) (writing took 20.86558958888054 seconds)
2022-09-08 18:29:20 | INFO | fairseq_cli.train | end of epoch 498 (average epoch stats below)
2022-09-08 18:29:20 | INFO | train | epoch 498 | loss 3.374 | nll_loss 0.34 | mask_loss 8.91389 | p_2 0.03504 | mask_ave 0.501 | ppl 1.27 | wps 3391.1 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 40257 | lr 0.000157608 | gnorm 0.238 | train_wall 98 | gb_free 9.2 | wall 66193
2022-09-08 18:29:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:29:21 | INFO | fairseq.trainer | begin training epoch 499
2022-09-08 18:29:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:30:14 | INFO | train_inner | epoch 499:     43 / 81 loss=3.374, nll_loss=0.34, mask_loss=8.92647, p_2=0.03498, mask_ave=0.501, ppl=1.27, wps=3554.5, ups=0.64, wpb=5516.9, bsz=356.6, num_updates=40300, lr=0.000157524, gnorm=0.235, train_wall=122, gb_free=8.9, wall=66247
2022-09-08 18:31:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:31:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:31:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:31:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:31:12 | INFO | valid | epoch 499 | valid on 'valid' subset | loss 5.09 | nll_loss 2.488 | mask_loss 9.73166 | p_2 0.04838 | mask_ave 0.621 | ppl 5.61 | bleu 56.57 | wps 1581.6 | wpb 933.5 | bsz 59.6 | num_updates 40338 | best_bleu 57.42
2022-09-08 18:31:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 499 @ 40338 updates
2022-09-08 18:31:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint499.pt
2022-09-08 18:31:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint499.pt
2022-09-08 18:31:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint499.pt (epoch 499 @ 40338 updates, score 56.57) (writing took 26.27525056898594 seconds)
2022-09-08 18:31:38 | INFO | fairseq_cli.train | end of epoch 499 (average epoch stats below)
2022-09-08 18:31:38 | INFO | train | epoch 499 | loss 3.374 | nll_loss 0.34 | mask_loss 8.87248 | p_2 0.03504 | mask_ave 0.501 | ppl 1.27 | wps 3252.2 | ups 0.59 | wpb 5523.2 | bsz 358 | num_updates 40338 | lr 0.00015745 | gnorm 0.252 | train_wall 99 | gb_free 9.2 | wall 66331
2022-09-08 18:31:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:31:38 | INFO | fairseq.trainer | begin training epoch 500
2022-09-08 18:31:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:32:56 | INFO | train_inner | epoch 500:     62 / 81 loss=3.375, nll_loss=0.341, mask_loss=8.87774, p_2=0.03498, mask_ave=0.499, ppl=1.27, wps=3450.7, ups=0.62, wpb=5566.9, bsz=361.2, num_updates=40400, lr=0.000157329, gnorm=0.245, train_wall=122, gb_free=9.1, wall=66408
2022-09-08 18:33:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:33:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:33:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:33:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:33:29 | INFO | valid | epoch 500 | valid on 'valid' subset | loss 5.086 | nll_loss 2.478 | mask_loss 9.85295 | p_2 0.04849 | mask_ave 0.618 | ppl 5.57 | bleu 56.24 | wps 1543.3 | wpb 933.5 | bsz 59.6 | num_updates 40419 | best_bleu 57.42
2022-09-08 18:33:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 500 @ 40419 updates
2022-09-08 18:33:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint500.pt
2022-09-08 18:33:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint500.pt
2022-09-08 18:34:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint500.pt (epoch 500 @ 40419 updates, score 56.24) (writing took 32.151666305959225 seconds)
2022-09-08 18:34:01 | INFO | fairseq_cli.train | end of epoch 500 (average epoch stats below)
2022-09-08 18:34:01 | INFO | train | epoch 500 | loss 3.375 | nll_loss 0.341 | mask_loss 8.94422 | p_2 0.03507 | mask_ave 0.5 | ppl 1.27 | wps 3118.1 | ups 0.56 | wpb 5523.2 | bsz 358 | num_updates 40419 | lr 0.000157292 | gnorm 0.233 | train_wall 98 | gb_free 9.2 | wall 66474
2022-09-08 18:34:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:34:02 | INFO | fairseq.trainer | begin training epoch 501
2022-09-08 18:34:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:35:40 | INFO | train_inner | epoch 501:     81 / 81 loss=3.374, nll_loss=0.34, mask_loss=8.90798, p_2=0.03526, mask_ave=0.5, ppl=1.27, wps=3323.2, ups=0.61, wpb=5464.3, bsz=354, num_updates=40500, lr=0.000157135, gnorm=0.226, train_wall=119, gb_free=9.2, wall=66573
2022-09-08 18:35:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:35:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:35:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:35:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:35:51 | INFO | valid | epoch 501 | valid on 'valid' subset | loss 5.078 | nll_loss 2.47 | mask_loss 9.74994 | p_2 0.04874 | mask_ave 0.61 | ppl 5.54 | bleu 56.5 | wps 1541.5 | wpb 933.5 | bsz 59.6 | num_updates 40500 | best_bleu 57.42
2022-09-08 18:35:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 501 @ 40500 updates
2022-09-08 18:35:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint501.pt
2022-09-08 18:35:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint501.pt
2022-09-08 18:35:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint501.pt (epoch 501 @ 40500 updates, score 56.5) (writing took 2.4007840380072594 seconds)
2022-09-08 18:35:54 | INFO | fairseq_cli.train | end of epoch 501 (average epoch stats below)
2022-09-08 18:35:54 | INFO | train | epoch 501 | loss 3.374 | nll_loss 0.34 | mask_loss 8.88376 | p_2 0.03513 | mask_ave 0.498 | ppl 1.27 | wps 3991.9 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 40500 | lr 0.000157135 | gnorm 0.227 | train_wall 97 | gb_free 9.2 | wall 66586
2022-09-08 18:35:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:35:54 | INFO | fairseq.trainer | begin training epoch 502
2022-09-08 18:35:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:37:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:37:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:37:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:37:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:37:44 | INFO | valid | epoch 502 | valid on 'valid' subset | loss 5.099 | nll_loss 2.498 | mask_loss 9.64253 | p_2 0.0485 | mask_ave 0.618 | ppl 5.65 | bleu 55.58 | wps 1559 | wpb 933.5 | bsz 59.6 | num_updates 40581 | best_bleu 57.42
2022-09-08 18:37:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 502 @ 40581 updates
2022-09-08 18:37:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint502.pt
2022-09-08 18:37:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint502.pt
2022-09-08 18:38:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint502.pt (epoch 502 @ 40581 updates, score 55.58) (writing took 19.757165148854256 seconds)
2022-09-08 18:38:04 | INFO | fairseq_cli.train | end of epoch 502 (average epoch stats below)
2022-09-08 18:38:04 | INFO | train | epoch 502 | loss 3.375 | nll_loss 0.341 | mask_loss 8.804 | p_2 0.03515 | mask_ave 0.498 | ppl 1.27 | wps 3438.6 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 40581 | lr 0.000156978 | gnorm 0.261 | train_wall 98 | gb_free 9.1 | wall 66717
2022-09-08 18:38:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:38:04 | INFO | fairseq.trainer | begin training epoch 503
2022-09-08 18:38:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:38:27 | INFO | train_inner | epoch 503:     19 / 81 loss=3.375, nll_loss=0.341, mask_loss=8.83864, p_2=0.03489, mask_ave=0.497, ppl=1.27, wps=3316, ups=0.6, wpb=5545.4, bsz=357.5, num_updates=40600, lr=0.000156941, gnorm=0.254, train_wall=120, gb_free=9.1, wall=66740
2022-09-08 18:39:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:39:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:39:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:39:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:39:53 | INFO | valid | epoch 503 | valid on 'valid' subset | loss 5.078 | nll_loss 2.473 | mask_loss 9.75653 | p_2 0.04845 | mask_ave 0.619 | ppl 5.55 | bleu 57.17 | wps 1566.9 | wpb 933.5 | bsz 59.6 | num_updates 40662 | best_bleu 57.42
2022-09-08 18:39:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 503 @ 40662 updates
2022-09-08 18:39:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint503.pt
2022-09-08 18:39:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint503.pt
2022-09-08 18:39:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint503.pt (epoch 503 @ 40662 updates, score 57.17) (writing took 2.476813852787018 seconds)
2022-09-08 18:39:56 | INFO | fairseq_cli.train | end of epoch 503 (average epoch stats below)
2022-09-08 18:39:56 | INFO | train | epoch 503 | loss 3.374 | nll_loss 0.34 | mask_loss 8.88213 | p_2 0.03509 | mask_ave 0.499 | ppl 1.27 | wps 3992.8 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 40662 | lr 0.000156822 | gnorm 0.234 | train_wall 97 | gb_free 9.1 | wall 66829
2022-09-08 18:39:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:39:56 | INFO | fairseq.trainer | begin training epoch 504
2022-09-08 18:39:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:40:43 | INFO | train_inner | epoch 504:     38 / 81 loss=3.374, nll_loss=0.34, mask_loss=8.91841, p_2=0.03543, mask_ave=0.501, ppl=1.27, wps=4060.9, ups=0.74, wpb=5514, bsz=360.2, num_updates=40700, lr=0.000156748, gnorm=0.241, train_wall=121, gb_free=9, wall=66876
2022-09-08 18:41:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:41:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:41:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:41:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:41:46 | INFO | valid | epoch 504 | valid on 'valid' subset | loss 5.077 | nll_loss 2.47 | mask_loss 9.94859 | p_2 0.04841 | mask_ave 0.621 | ppl 5.54 | bleu 56.62 | wps 1544.9 | wpb 933.5 | bsz 59.6 | num_updates 40743 | best_bleu 57.42
2022-09-08 18:41:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 504 @ 40743 updates
2022-09-08 18:41:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint504.pt
2022-09-08 18:41:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint504.pt
2022-09-08 18:42:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint504.pt (epoch 504 @ 40743 updates, score 56.62) (writing took 20.23299005255103 seconds)
2022-09-08 18:42:06 | INFO | fairseq_cli.train | end of epoch 504 (average epoch stats below)
2022-09-08 18:42:06 | INFO | train | epoch 504 | loss 3.375 | nll_loss 0.341 | mask_loss 9.03269 | p_2 0.03516 | mask_ave 0.497 | ppl 1.27 | wps 3430.9 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 40743 | lr 0.000156666 | gnorm 0.25 | train_wall 98 | gb_free 9.2 | wall 66959
2022-09-08 18:42:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:42:06 | INFO | fairseq.trainer | begin training epoch 505
2022-09-08 18:42:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:43:17 | INFO | train_inner | epoch 505:     57 / 81 loss=3.375, nll_loss=0.342, mask_loss=9.11097, p_2=0.03489, mask_ave=0.497, ppl=1.27, wps=3597.3, ups=0.65, wpb=5532.9, bsz=357, num_updates=40800, lr=0.000156556, gnorm=0.277, train_wall=121, gb_free=9.1, wall=67030
2022-09-08 18:43:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:43:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:43:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:43:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:43:56 | INFO | valid | epoch 505 | valid on 'valid' subset | loss 5.088 | nll_loss 2.48 | mask_loss 9.95544 | p_2 0.04837 | mask_ave 0.622 | ppl 5.58 | bleu 56.28 | wps 1547 | wpb 933.5 | bsz 59.6 | num_updates 40824 | best_bleu 57.42
2022-09-08 18:43:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 505 @ 40824 updates
2022-09-08 18:43:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint505.pt
2022-09-08 18:43:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint505.pt
2022-09-08 18:44:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint505.pt (epoch 505 @ 40824 updates, score 56.28) (writing took 18.725705333054066 seconds)
2022-09-08 18:44:15 | INFO | fairseq_cli.train | end of epoch 505 (average epoch stats below)
2022-09-08 18:44:15 | INFO | train | epoch 505 | loss 3.376 | nll_loss 0.342 | mask_loss 9.12495 | p_2 0.03516 | mask_ave 0.497 | ppl 1.27 | wps 3470.4 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 40824 | lr 0.00015651 | gnorm 0.288 | train_wall 97 | gb_free 9.1 | wall 67088
2022-09-08 18:44:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:44:15 | INFO | fairseq.trainer | begin training epoch 506
2022-09-08 18:44:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:45:48 | INFO | train_inner | epoch 506:     76 / 81 loss=3.374, nll_loss=0.34, mask_loss=8.99602, p_2=0.03529, mask_ave=0.499, ppl=1.27, wps=3645.8, ups=0.66, wpb=5528.1, bsz=361, num_updates=40900, lr=0.000156365, gnorm=0.24, train_wall=120, gb_free=9.1, wall=67181
2022-09-08 18:45:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:45:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:45:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:45:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:46:05 | INFO | valid | epoch 506 | valid on 'valid' subset | loss 5.071 | nll_loss 2.462 | mask_loss 9.81429 | p_2 0.04825 | mask_ave 0.626 | ppl 5.51 | bleu 56.18 | wps 1518.2 | wpb 933.5 | bsz 59.6 | num_updates 40905 | best_bleu 57.42
2022-09-08 18:46:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 506 @ 40905 updates
2022-09-08 18:46:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint506.pt
2022-09-08 18:46:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint506.pt
2022-09-08 18:46:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint506.pt (epoch 506 @ 40905 updates, score 56.18) (writing took 21.063869666308165 seconds)
2022-09-08 18:46:26 | INFO | fairseq_cli.train | end of epoch 506 (average epoch stats below)
2022-09-08 18:46:26 | INFO | train | epoch 506 | loss 3.374 | nll_loss 0.34 | mask_loss 8.99167 | p_2 0.03504 | mask_ave 0.501 | ppl 1.27 | wps 3414 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 40905 | lr 0.000156355 | gnorm 0.233 | train_wall 97 | gb_free 9.2 | wall 67219
2022-09-08 18:46:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:46:26 | INFO | fairseq.trainer | begin training epoch 507
2022-09-08 18:46:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:48:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:48:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:48:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:48:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:48:17 | INFO | valid | epoch 507 | valid on 'valid' subset | loss 5.074 | nll_loss 2.463 | mask_loss 9.92955 | p_2 0.04865 | mask_ave 0.613 | ppl 5.52 | bleu 56.16 | wps 1509.8 | wpb 933.5 | bsz 59.6 | num_updates 40986 | best_bleu 57.42
2022-09-08 18:48:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 507 @ 40986 updates
2022-09-08 18:48:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint507.pt
2022-09-08 18:48:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint507.pt
2022-09-08 18:48:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint507.pt (epoch 507 @ 40986 updates, score 56.16) (writing took 2.411211684346199 seconds)
2022-09-08 18:48:19 | INFO | fairseq_cli.train | end of epoch 507 (average epoch stats below)
2022-09-08 18:48:19 | INFO | train | epoch 507 | loss 3.375 | nll_loss 0.341 | mask_loss 8.9639 | p_2 0.03511 | mask_ave 0.499 | ppl 1.27 | wps 3954.4 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 40986 | lr 0.0001562 | gnorm 0.262 | train_wall 98 | gb_free 9.1 | wall 67332
2022-09-08 18:48:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:48:19 | INFO | fairseq.trainer | begin training epoch 508
2022-09-08 18:48:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:48:37 | INFO | train_inner | epoch 508:     14 / 81 loss=3.375, nll_loss=0.34, mask_loss=8.99021, p_2=0.0351, mask_ave=0.499, ppl=1.27, wps=3271.9, ups=0.59, wpb=5507.4, bsz=354.7, num_updates=41000, lr=0.000156174, gnorm=0.259, train_wall=119, gb_free=9, wall=67350
2022-09-08 18:49:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:49:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:49:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:49:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:50:08 | INFO | valid | epoch 508 | valid on 'valid' subset | loss 5.081 | nll_loss 2.474 | mask_loss 9.86016 | p_2 0.04862 | mask_ave 0.615 | ppl 5.56 | bleu 56.45 | wps 1564.5 | wpb 933.5 | bsz 59.6 | num_updates 41067 | best_bleu 57.42
2022-09-08 18:50:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 508 @ 41067 updates
2022-09-08 18:50:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint508.pt
2022-09-08 18:50:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint508.pt
2022-09-08 18:50:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint508.pt (epoch 508 @ 41067 updates, score 56.45) (writing took 2.4072513692080975 seconds)
2022-09-08 18:50:11 | INFO | fairseq_cli.train | end of epoch 508 (average epoch stats below)
2022-09-08 18:50:11 | INFO | train | epoch 508 | loss 3.375 | nll_loss 0.341 | mask_loss 8.96174 | p_2 0.0351 | mask_ave 0.499 | ppl 1.27 | wps 4001.7 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 41067 | lr 0.000156046 | gnorm 0.245 | train_wall 97 | gb_free 9.2 | wall 67444
2022-09-08 18:50:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:50:11 | INFO | fairseq.trainer | begin training epoch 509
2022-09-08 18:50:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:50:52 | INFO | train_inner | epoch 509:     33 / 81 loss=3.375, nll_loss=0.341, mask_loss=8.94885, p_2=0.03502, mask_ave=0.501, ppl=1.27, wps=4069.6, ups=0.74, wpb=5498.7, bsz=353.8, num_updates=41100, lr=0.000155984, gnorm=0.257, train_wall=120, gb_free=9.2, wall=67485
2022-09-08 18:52:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:52:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:52:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:52:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:52:19 | INFO | valid | epoch 509 | valid on 'valid' subset | loss 5.094 | nll_loss 2.487 | mask_loss 9.67432 | p_2 0.04874 | mask_ave 0.611 | ppl 5.61 | bleu 56.97 | wps 1291.9 | wpb 933.5 | bsz 59.6 | num_updates 41148 | best_bleu 57.42
2022-09-08 18:52:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 509 @ 41148 updates
2022-09-08 18:52:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint509.pt
2022-09-08 18:52:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint509.pt
2022-09-08 18:52:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint509.pt (epoch 509 @ 41148 updates, score 56.97) (writing took 38.56558906659484 seconds)
2022-09-08 18:52:57 | INFO | fairseq_cli.train | end of epoch 509 (average epoch stats below)
2022-09-08 18:52:57 | INFO | train | epoch 509 | loss 3.375 | nll_loss 0.341 | mask_loss 9.00028 | p_2 0.03511 | mask_ave 0.499 | ppl 1.27 | wps 2688.1 | ups 0.49 | wpb 5523.2 | bsz 358 | num_updates 41148 | lr 0.000155893 | gnorm 0.249 | train_wall 113 | gb_free 9 | wall 67610
2022-09-08 18:52:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:52:58 | INFO | fairseq.trainer | begin training epoch 510
2022-09-08 18:52:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:54:01 | INFO | train_inner | epoch 510:     52 / 81 loss=3.374, nll_loss=0.339, mask_loss=8.94073, p_2=0.03523, mask_ave=0.497, ppl=1.27, wps=2944.5, ups=0.53, wpb=5573, bsz=368.6, num_updates=41200, lr=0.000155794, gnorm=0.228, train_wall=136, gb_free=9, wall=67674
2022-09-08 18:54:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:54:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:54:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:54:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:54:46 | INFO | valid | epoch 510 | valid on 'valid' subset | loss 5.084 | nll_loss 2.483 | mask_loss 9.9082 | p_2 0.0487 | mask_ave 0.611 | ppl 5.59 | bleu 55.96 | wps 1560.6 | wpb 933.5 | bsz 59.6 | num_updates 41229 | best_bleu 57.42
2022-09-08 18:54:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 510 @ 41229 updates
2022-09-08 18:54:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint510.pt
2022-09-08 18:54:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint510.pt
2022-09-08 18:54:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint510.pt (epoch 510 @ 41229 updates, score 55.96) (writing took 2.49482199922204 seconds)
2022-09-08 18:54:49 | INFO | fairseq_cli.train | end of epoch 510 (average epoch stats below)
2022-09-08 18:54:49 | INFO | train | epoch 510 | loss 3.374 | nll_loss 0.34 | mask_loss 8.94501 | p_2 0.03503 | mask_ave 0.501 | ppl 1.27 | wps 4011 | ups 0.73 | wpb 5523.2 | bsz 358 | num_updates 41229 | lr 0.000155739 | gnorm 0.244 | train_wall 96 | gb_free 9.1 | wall 67722
2022-09-08 18:54:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:54:49 | INFO | fairseq.trainer | begin training epoch 511
2022-09-08 18:54:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:56:16 | INFO | train_inner | epoch 511:     71 / 81 loss=3.374, nll_loss=0.34, mask_loss=9.05168, p_2=0.03493, mask_ave=0.502, ppl=1.27, wps=4115.7, ups=0.74, wpb=5529.5, bsz=357, num_updates=41300, lr=0.000155606, gnorm=0.243, train_wall=119, gb_free=9.1, wall=67808
2022-09-08 18:56:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:56:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:56:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:56:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:56:38 | INFO | valid | epoch 511 | valid on 'valid' subset | loss 5.082 | nll_loss 2.474 | mask_loss 9.97318 | p_2 0.04823 | mask_ave 0.627 | ppl 5.56 | bleu 56.04 | wps 1564.1 | wpb 933.5 | bsz 59.6 | num_updates 41310 | best_bleu 57.42
2022-09-08 18:56:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 511 @ 41310 updates
2022-09-08 18:56:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint511.pt
2022-09-08 18:56:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint511.pt
2022-09-08 18:56:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint511.pt (epoch 511 @ 41310 updates, score 56.04) (writing took 17.94133608415723 seconds)
2022-09-08 18:56:56 | INFO | fairseq_cli.train | end of epoch 511 (average epoch stats below)
2022-09-08 18:56:56 | INFO | train | epoch 511 | loss 3.374 | nll_loss 0.34 | mask_loss 9.06304 | p_2 0.03501 | mask_ave 0.502 | ppl 1.27 | wps 3528.9 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 41310 | lr 0.000155587 | gnorm 0.235 | train_wall 96 | gb_free 9.1 | wall 67849
2022-09-08 18:56:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:56:56 | INFO | fairseq.trainer | begin training epoch 512
2022-09-08 18:56:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:58:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 18:58:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 18:58:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 18:58:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 18:58:45 | INFO | valid | epoch 512 | valid on 'valid' subset | loss 5.079 | nll_loss 2.464 | mask_loss 9.76753 | p_2 0.04848 | mask_ave 0.618 | ppl 5.52 | bleu 56.65 | wps 1534.5 | wpb 933.5 | bsz 59.6 | num_updates 41391 | best_bleu 57.42
2022-09-08 18:58:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 512 @ 41391 updates
2022-09-08 18:58:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint512.pt
2022-09-08 18:58:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint512.pt
2022-09-08 18:58:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint512.pt (epoch 512 @ 41391 updates, score 56.65) (writing took 2.5605306178331375 seconds)
2022-09-08 18:58:48 | INFO | fairseq_cli.train | end of epoch 512 (average epoch stats below)
2022-09-08 18:58:48 | INFO | train | epoch 512 | loss 3.374 | nll_loss 0.34 | mask_loss 9.04482 | p_2 0.03488 | mask_ave 0.506 | ppl 1.27 | wps 3980.8 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 41391 | lr 0.000155434 | gnorm 0.246 | train_wall 97 | gb_free 9.2 | wall 67961
2022-09-08 18:58:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 18:58:48 | INFO | fairseq.trainer | begin training epoch 513
2022-09-08 18:58:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 18:59:00 | INFO | train_inner | epoch 513:      9 / 81 loss=3.374, nll_loss=0.34, mask_loss=9.04449, p_2=0.03488, mask_ave=0.505, ppl=1.27, wps=3329.5, ups=0.61, wpb=5483.6, bsz=354, num_updates=41400, lr=0.000155417, gnorm=0.245, train_wall=119, gb_free=9.1, wall=67973
2022-09-08 19:00:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:00:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:00:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:00:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:00:38 | INFO | valid | epoch 513 | valid on 'valid' subset | loss 5.085 | nll_loss 2.477 | mask_loss 9.82717 | p_2 0.04838 | mask_ave 0.621 | ppl 5.57 | bleu 57.52 | wps 1513.5 | wpb 933.5 | bsz 59.6 | num_updates 41472 | best_bleu 57.52
2022-09-08 19:00:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 513 @ 41472 updates
2022-09-08 19:00:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint513.pt
2022-09-08 19:00:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint513.pt
2022-09-08 19:01:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint513.pt (epoch 513 @ 41472 updates, score 57.52) (writing took 46.18172512203455 seconds)
2022-09-08 19:01:25 | INFO | fairseq_cli.train | end of epoch 513 (average epoch stats below)
2022-09-08 19:01:25 | INFO | train | epoch 513 | loss 3.374 | nll_loss 0.34 | mask_loss 8.98755 | p_2 0.03506 | mask_ave 0.5 | ppl 1.27 | wps 2859.7 | ups 0.52 | wpb 5523.2 | bsz 358 | num_updates 41472 | lr 0.000155282 | gnorm 0.252 | train_wall 97 | gb_free 9.1 | wall 68118
2022-09-08 19:01:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:01:25 | INFO | fairseq.trainer | begin training epoch 514
2022-09-08 19:01:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:01:59 | INFO | train_inner | epoch 514:     28 / 81 loss=3.374, nll_loss=0.34, mask_loss=8.99361, p_2=0.03524, mask_ave=0.502, ppl=1.27, wps=3089, ups=0.56, wpb=5512, bsz=356.7, num_updates=41500, lr=0.00015523, gnorm=0.266, train_wall=119, gb_free=9, wall=68152
2022-09-08 19:03:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:03:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:03:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:03:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:03:16 | INFO | valid | epoch 514 | valid on 'valid' subset | loss 5.091 | nll_loss 2.485 | mask_loss 9.91315 | p_2 0.04806 | mask_ave 0.631 | ppl 5.6 | bleu 56.94 | wps 1473.5 | wpb 933.5 | bsz 59.6 | num_updates 41553 | best_bleu 57.52
2022-09-08 19:03:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 514 @ 41553 updates
2022-09-08 19:03:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint514.pt
2022-09-08 19:03:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint514.pt
2022-09-08 19:03:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint514.pt (epoch 514 @ 41553 updates, score 56.94) (writing took 2.5901673324406147 seconds)
2022-09-08 19:03:18 | INFO | fairseq_cli.train | end of epoch 514 (average epoch stats below)
2022-09-08 19:03:18 | INFO | train | epoch 514 | loss 3.375 | nll_loss 0.341 | mask_loss 8.97965 | p_2 0.03494 | mask_ave 0.504 | ppl 1.27 | wps 3929.9 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 41553 | lr 0.000155131 | gnorm 0.278 | train_wall 98 | gb_free 9.2 | wall 68231
2022-09-08 19:03:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:03:19 | INFO | fairseq.trainer | begin training epoch 515
2022-09-08 19:03:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:04:17 | INFO | train_inner | epoch 515:     47 / 81 loss=3.374, nll_loss=0.34, mask_loss=8.96693, p_2=0.0347, mask_ave=0.503, ppl=1.27, wps=4026.8, ups=0.72, wpb=5566.5, bsz=360.8, num_updates=41600, lr=0.000155043, gnorm=0.255, train_wall=122, gb_free=9, wall=68290
2022-09-08 19:04:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:04:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:04:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:04:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:05:09 | INFO | valid | epoch 515 | valid on 'valid' subset | loss 5.077 | nll_loss 2.467 | mask_loss 9.9553 | p_2 0.0484 | mask_ave 0.62 | ppl 5.53 | bleu 57.06 | wps 1488.4 | wpb 933.5 | bsz 59.6 | num_updates 41634 | best_bleu 57.52
2022-09-08 19:05:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 515 @ 41634 updates
2022-09-08 19:05:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint515.pt
2022-09-08 19:05:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint515.pt
2022-09-08 19:05:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint515.pt (epoch 515 @ 41634 updates, score 57.06) (writing took 20.43601318821311 seconds)
2022-09-08 19:05:29 | INFO | fairseq_cli.train | end of epoch 515 (average epoch stats below)
2022-09-08 19:05:29 | INFO | train | epoch 515 | loss 3.374 | nll_loss 0.34 | mask_loss 8.95505 | p_2 0.03496 | mask_ave 0.503 | ppl 1.27 | wps 3419.5 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 41634 | lr 0.00015498 | gnorm 0.233 | train_wall 97 | gb_free 9.2 | wall 68362
2022-09-08 19:05:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:05:30 | INFO | fairseq.trainer | begin training epoch 516
2022-09-08 19:05:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:06:49 | INFO | train_inner | epoch 516:     66 / 81 loss=3.374, nll_loss=0.34, mask_loss=8.9405, p_2=0.03503, mask_ave=0.501, ppl=1.27, wps=3611.7, ups=0.66, wpb=5504.1, bsz=353.4, num_updates=41700, lr=0.000154857, gnorm=0.231, train_wall=119, gb_free=9, wall=68442
2022-09-08 19:07:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:07:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:07:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:07:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:07:18 | INFO | valid | epoch 516 | valid on 'valid' subset | loss 5.076 | nll_loss 2.466 | mask_loss 9.85418 | p_2 0.04864 | mask_ave 0.614 | ppl 5.53 | bleu 57.38 | wps 1538.7 | wpb 933.5 | bsz 59.6 | num_updates 41715 | best_bleu 57.52
2022-09-08 19:07:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 516 @ 41715 updates
2022-09-08 19:07:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint516.pt
2022-09-08 19:07:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint516.pt
2022-09-08 19:07:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint516.pt (epoch 516 @ 41715 updates, score 57.38) (writing took 2.586243100464344 seconds)
2022-09-08 19:07:20 | INFO | fairseq_cli.train | end of epoch 516 (average epoch stats below)
2022-09-08 19:07:20 | INFO | train | epoch 516 | loss 3.374 | nll_loss 0.34 | mask_loss 8.91432 | p_2 0.03506 | mask_ave 0.5 | ppl 1.27 | wps 4025.3 | ups 0.73 | wpb 5523.2 | bsz 358 | num_updates 41715 | lr 0.00015483 | gnorm 0.242 | train_wall 96 | gb_free 9.2 | wall 68473
2022-09-08 19:07:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:07:21 | INFO | fairseq.trainer | begin training epoch 517
2022-09-08 19:07:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:09:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:09:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:09:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:09:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:09:11 | INFO | valid | epoch 517 | valid on 'valid' subset | loss 5.091 | nll_loss 2.488 | mask_loss 9.81082 | p_2 0.0483 | mask_ave 0.623 | ppl 5.61 | bleu 56.59 | wps 1480.8 | wpb 933.5 | bsz 59.6 | num_updates 41796 | best_bleu 57.52
2022-09-08 19:09:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 517 @ 41796 updates
2022-09-08 19:09:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint517.pt
2022-09-08 19:09:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint517.pt
2022-09-08 19:09:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint517.pt (epoch 517 @ 41796 updates, score 56.59) (writing took 21.785423174500465 seconds)
2022-09-08 19:09:33 | INFO | fairseq_cli.train | end of epoch 517 (average epoch stats below)
2022-09-08 19:09:33 | INFO | train | epoch 517 | loss 3.374 | nll_loss 0.341 | mask_loss 8.9552 | p_2 0.03508 | mask_ave 0.5 | ppl 1.27 | wps 3368.9 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 41796 | lr 0.000154679 | gnorm 0.27 | train_wall 98 | gb_free 9.1 | wall 68606
2022-09-08 19:09:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:09:33 | INFO | fairseq.trainer | begin training epoch 518
2022-09-08 19:09:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:09:39 | INFO | train_inner | epoch 518:      4 / 81 loss=3.374, nll_loss=0.341, mask_loss=8.93459, p_2=0.03523, mask_ave=0.5, ppl=1.27, wps=3240.3, ups=0.59, wpb=5504.3, bsz=359, num_updates=41800, lr=0.000154672, gnorm=0.268, train_wall=120, gb_free=9, wall=68612
2022-09-08 19:11:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:11:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:11:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:11:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:11:23 | INFO | valid | epoch 518 | valid on 'valid' subset | loss 5.098 | nll_loss 2.495 | mask_loss 9.71512 | p_2 0.04835 | mask_ave 0.622 | ppl 5.64 | bleu 56.48 | wps 1523.5 | wpb 933.5 | bsz 59.6 | num_updates 41877 | best_bleu 57.52
2022-09-08 19:11:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 518 @ 41877 updates
2022-09-08 19:11:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint518.pt
2022-09-08 19:11:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint518.pt
2022-09-08 19:11:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint518.pt (epoch 518 @ 41877 updates, score 56.48) (writing took 2.490590400993824 seconds)
2022-09-08 19:11:26 | INFO | fairseq_cli.train | end of epoch 518 (average epoch stats below)
2022-09-08 19:11:26 | INFO | train | epoch 518 | loss 3.374 | nll_loss 0.34 | mask_loss 8.96121 | p_2 0.03503 | mask_ave 0.501 | ppl 1.27 | wps 3977.3 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 41877 | lr 0.00015453 | gnorm 0.267 | train_wall 97 | gb_free 9 | wall 68719
2022-09-08 19:11:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:11:26 | INFO | fairseq.trainer | begin training epoch 519
2022-09-08 19:11:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:11:55 | INFO | train_inner | epoch 519:     23 / 81 loss=3.374, nll_loss=0.34, mask_loss=8.97665, p_2=0.03507, mask_ave=0.502, ppl=1.27, wps=4086.6, ups=0.74, wpb=5530.2, bsz=359, num_updates=41900, lr=0.000154487, gnorm=0.265, train_wall=120, gb_free=9.1, wall=68747
2022-09-08 19:13:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:13:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:13:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:13:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:13:16 | INFO | valid | epoch 519 | valid on 'valid' subset | loss 5.084 | nll_loss 2.481 | mask_loss 9.73603 | p_2 0.04853 | mask_ave 0.617 | ppl 5.58 | bleu 56.95 | wps 1542.8 | wpb 933.5 | bsz 59.6 | num_updates 41958 | best_bleu 57.52
2022-09-08 19:13:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 519 @ 41958 updates
2022-09-08 19:13:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint519.pt
2022-09-08 19:13:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint519.pt
2022-09-08 19:13:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint519.pt (epoch 519 @ 41958 updates, score 56.95) (writing took 19.887568581849337 seconds)
2022-09-08 19:13:37 | INFO | fairseq_cli.train | end of epoch 519 (average epoch stats below)
2022-09-08 19:13:37 | INFO | train | epoch 519 | loss 3.374 | nll_loss 0.34 | mask_loss 8.95991 | p_2 0.03503 | mask_ave 0.501 | ppl 1.27 | wps 3421.5 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 41958 | lr 0.000154381 | gnorm 0.25 | train_wall 98 | gb_free 9.2 | wall 68849
2022-09-08 19:13:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:13:37 | INFO | fairseq.trainer | begin training epoch 520
2022-09-08 19:13:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:14:28 | INFO | train_inner | epoch 520:     42 / 81 loss=3.374, nll_loss=0.34, mask_loss=8.93809, p_2=0.03507, mask_ave=0.5, ppl=1.27, wps=3588.8, ups=0.65, wpb=5514.7, bsz=357.8, num_updates=42000, lr=0.000154303, gnorm=0.242, train_wall=121, gb_free=9.1, wall=68901
2022-09-08 19:15:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:15:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:15:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:15:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:15:27 | INFO | valid | epoch 520 | valid on 'valid' subset | loss 5.096 | nll_loss 2.493 | mask_loss 9.90021 | p_2 0.0484 | mask_ave 0.621 | ppl 5.63 | bleu 56.85 | wps 1494.9 | wpb 933.5 | bsz 59.6 | num_updates 42039 | best_bleu 57.52
2022-09-08 19:15:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 520 @ 42039 updates
2022-09-08 19:15:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint520.pt
2022-09-08 19:15:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint520.pt
2022-09-08 19:15:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint520.pt (epoch 520 @ 42039 updates, score 56.85) (writing took 21.797205172479153 seconds)
2022-09-08 19:15:49 | INFO | fairseq_cli.train | end of epoch 520 (average epoch stats below)
2022-09-08 19:15:49 | INFO | train | epoch 520 | loss 3.374 | nll_loss 0.34 | mask_loss 9.00559 | p_2 0.03504 | mask_ave 0.501 | ppl 1.27 | wps 3372.8 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 42039 | lr 0.000154232 | gnorm 0.238 | train_wall 98 | gb_free 9 | wall 68982
2022-09-08 19:15:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:15:49 | INFO | fairseq.trainer | begin training epoch 521
2022-09-08 19:15:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:17:03 | INFO | train_inner | epoch 521:     61 / 81 loss=3.374, nll_loss=0.34, mask_loss=9.03103, p_2=0.03479, mask_ave=0.498, ppl=1.27, wps=3600.6, ups=0.65, wpb=5578.1, bsz=361.1, num_updates=42100, lr=0.00015412, gnorm=0.258, train_wall=120, gb_free=9.2, wall=69056
2022-09-08 19:17:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:17:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:17:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:17:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:17:38 | INFO | valid | epoch 521 | valid on 'valid' subset | loss 5.08 | nll_loss 2.471 | mask_loss 10.0141 | p_2 0.04847 | mask_ave 0.619 | ppl 5.54 | bleu 57.38 | wps 1484.9 | wpb 933.5 | bsz 59.6 | num_updates 42120 | best_bleu 57.52
2022-09-08 19:17:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 521 @ 42120 updates
2022-09-08 19:17:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint521.pt
2022-09-08 19:17:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint521.pt
2022-09-08 19:17:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint521.pt (epoch 521 @ 42120 updates, score 57.38) (writing took 20.474212519824505 seconds)
2022-09-08 19:17:59 | INFO | fairseq_cli.train | end of epoch 521 (average epoch stats below)
2022-09-08 19:17:59 | INFO | train | epoch 521 | loss 3.375 | nll_loss 0.341 | mask_loss 9.03266 | p_2 0.0351 | mask_ave 0.499 | ppl 1.27 | wps 3448.9 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 42120 | lr 0.000154083 | gnorm 0.272 | train_wall 96 | gb_free 9.2 | wall 69112
2022-09-08 19:17:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:17:59 | INFO | fairseq.trainer | begin training epoch 522
2022-09-08 19:17:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:19:37 | INFO | train_inner | epoch 522:     80 / 81 loss=3.374, nll_loss=0.34, mask_loss=9.12044, p_2=0.03531, mask_ave=0.5, ppl=1.27, wps=3562.9, ups=0.65, wpb=5498.1, bsz=356.6, num_updates=42200, lr=0.000153937, gnorm=0.251, train_wall=120, gb_free=9, wall=69210
2022-09-08 19:19:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:19:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:19:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:19:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:19:49 | INFO | valid | epoch 522 | valid on 'valid' subset | loss 5.094 | nll_loss 2.49 | mask_loss 10.056 | p_2 0.0485 | mask_ave 0.618 | ppl 5.62 | bleu 57.25 | wps 1529.8 | wpb 933.5 | bsz 59.6 | num_updates 42201 | best_bleu 57.52
2022-09-08 19:19:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 522 @ 42201 updates
2022-09-08 19:19:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint522.pt
2022-09-08 19:19:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint522.pt
2022-09-08 19:20:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint522.pt (epoch 522 @ 42201 updates, score 57.25) (writing took 20.865307666361332 seconds)
2022-09-08 19:20:10 | INFO | fairseq_cli.train | end of epoch 522 (average epoch stats below)
2022-09-08 19:20:10 | INFO | train | epoch 522 | loss 3.374 | nll_loss 0.34 | mask_loss 9.11868 | p_2 0.03518 | mask_ave 0.497 | ppl 1.27 | wps 3406.8 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 42201 | lr 0.000153935 | gnorm 0.244 | train_wall 97 | gb_free 9.1 | wall 69243
2022-09-08 19:20:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:20:10 | INFO | fairseq.trainer | begin training epoch 523
2022-09-08 19:20:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:21:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:21:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:21:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:21:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:22:01 | INFO | valid | epoch 523 | valid on 'valid' subset | loss 5.091 | nll_loss 2.485 | mask_loss 10.1895 | p_2 0.04863 | mask_ave 0.614 | ppl 5.6 | bleu 57.16 | wps 1533.1 | wpb 933.5 | bsz 59.6 | num_updates 42282 | best_bleu 57.52
2022-09-08 19:22:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 523 @ 42282 updates
2022-09-08 19:22:01 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint523.pt
2022-09-08 19:22:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint523.pt
2022-09-08 19:22:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint523.pt (epoch 523 @ 42282 updates, score 57.16) (writing took 20.34804129973054 seconds)
2022-09-08 19:22:21 | INFO | fairseq_cli.train | end of epoch 523 (average epoch stats below)
2022-09-08 19:22:21 | INFO | train | epoch 523 | loss 3.373 | nll_loss 0.339 | mask_loss 9.10445 | p_2 0.03518 | mask_ave 0.497 | ppl 1.27 | wps 3408.6 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 42282 | lr 0.000153788 | gnorm 0.228 | train_wall 98 | gb_free 9.1 | wall 69374
2022-09-08 19:22:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:22:22 | INFO | fairseq.trainer | begin training epoch 524
2022-09-08 19:22:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:22:44 | INFO | train_inner | epoch 524:     18 / 81 loss=3.373, nll_loss=0.339, mask_loss=9.13492, p_2=0.03518, mask_ave=0.498, ppl=1.26, wps=2941.9, ups=0.54, wpb=5495.1, bsz=354.7, num_updates=42300, lr=0.000153755, gnorm=0.229, train_wall=120, gb_free=9.1, wall=69397
2022-09-08 19:24:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:24:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:24:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:24:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:24:11 | INFO | valid | epoch 524 | valid on 'valid' subset | loss 5.089 | nll_loss 2.483 | mask_loss 9.80598 | p_2 0.04852 | mask_ave 0.618 | ppl 5.59 | bleu 57.11 | wps 1546.1 | wpb 933.5 | bsz 59.6 | num_updates 42363 | best_bleu 57.52
2022-09-08 19:24:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 524 @ 42363 updates
2022-09-08 19:24:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint524.pt
2022-09-08 19:24:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint524.pt
2022-09-08 19:24:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint524.pt (epoch 524 @ 42363 updates, score 57.11) (writing took 18.725457210093737 seconds)
2022-09-08 19:24:30 | INFO | fairseq_cli.train | end of epoch 524 (average epoch stats below)
2022-09-08 19:24:30 | INFO | train | epoch 524 | loss 3.373 | nll_loss 0.339 | mask_loss 9.08851 | p_2 0.03509 | mask_ave 0.5 | ppl 1.27 | wps 3481.8 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 42363 | lr 0.000153641 | gnorm 0.241 | train_wall 97 | gb_free 8.9 | wall 69503
2022-09-08 19:24:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:24:30 | INFO | fairseq.trainer | begin training epoch 525
2022-09-08 19:24:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:25:14 | INFO | train_inner | epoch 525:     37 / 81 loss=3.374, nll_loss=0.34, mask_loss=9.04764, p_2=0.03523, mask_ave=0.501, ppl=1.27, wps=3685.7, ups=0.67, wpb=5523.4, bsz=360.6, num_updates=42400, lr=0.000153574, gnorm=0.261, train_wall=118, gb_free=8.9, wall=69547
2022-09-08 19:26:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:26:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:26:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:26:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:26:17 | INFO | valid | epoch 525 | valid on 'valid' subset | loss 5.087 | nll_loss 2.479 | mask_loss 10.04 | p_2 0.04836 | mask_ave 0.622 | ppl 5.58 | bleu 56.99 | wps 1560.6 | wpb 933.5 | bsz 59.6 | num_updates 42444 | best_bleu 57.52
2022-09-08 19:26:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 525 @ 42444 updates
2022-09-08 19:26:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint525.pt
2022-09-08 19:26:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint525.pt
2022-09-08 19:26:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint525.pt (epoch 525 @ 42444 updates, score 56.99) (writing took 17.303030654788017 seconds)
2022-09-08 19:26:35 | INFO | fairseq_cli.train | end of epoch 525 (average epoch stats below)
2022-09-08 19:26:35 | INFO | train | epoch 525 | loss 3.374 | nll_loss 0.34 | mask_loss 9.08654 | p_2 0.03495 | mask_ave 0.504 | ppl 1.27 | wps 3581.3 | ups 0.65 | wpb 5523.2 | bsz 358 | num_updates 42444 | lr 0.000153494 | gnorm 0.256 | train_wall 95 | gb_free 9.1 | wall 69628
2022-09-08 19:26:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:26:35 | INFO | fairseq.trainer | begin training epoch 526
2022-09-08 19:26:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:27:43 | INFO | train_inner | epoch 526:     56 / 81 loss=3.374, nll_loss=0.34, mask_loss=9.14984, p_2=0.03494, mask_ave=0.501, ppl=1.27, wps=3704.4, ups=0.67, wpb=5531.8, bsz=360, num_updates=42500, lr=0.000153393, gnorm=0.249, train_wall=119, gb_free=9, wall=69696
2022-09-08 19:28:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:28:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:28:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:28:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:28:24 | INFO | valid | epoch 526 | valid on 'valid' subset | loss 5.093 | nll_loss 2.484 | mask_loss 10.1078 | p_2 0.04851 | mask_ave 0.618 | ppl 5.59 | bleu 56.83 | wps 1593.1 | wpb 933.5 | bsz 59.6 | num_updates 42525 | best_bleu 57.52
2022-09-08 19:28:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 526 @ 42525 updates
2022-09-08 19:28:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint526.pt
2022-09-08 19:28:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint526.pt
2022-09-08 19:28:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint526.pt (epoch 526 @ 42525 updates, score 56.83) (writing took 21.376206003129482 seconds)
2022-09-08 19:28:46 | INFO | fairseq_cli.train | end of epoch 526 (average epoch stats below)
2022-09-08 19:28:46 | INFO | train | epoch 526 | loss 3.374 | nll_loss 0.34 | mask_loss 9.16639 | p_2 0.03512 | mask_ave 0.499 | ppl 1.27 | wps 3421.2 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 42525 | lr 0.000153348 | gnorm 0.256 | train_wall 97 | gb_free 9 | wall 69759
2022-09-08 19:28:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:28:46 | INFO | fairseq.trainer | begin training epoch 527
2022-09-08 19:28:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:30:18 | INFO | train_inner | epoch 527:     75 / 81 loss=3.373, nll_loss=0.34, mask_loss=9.14042, p_2=0.035, mask_ave=0.5, ppl=1.27, wps=3593.5, ups=0.65, wpb=5539.6, bsz=357.4, num_updates=42600, lr=0.000153213, gnorm=0.238, train_wall=120, gb_free=9.1, wall=69850
2022-09-08 19:30:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:30:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:30:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:30:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:30:35 | INFO | valid | epoch 527 | valid on 'valid' subset | loss 5.099 | nll_loss 2.494 | mask_loss 9.8988 | p_2 0.04814 | mask_ave 0.629 | ppl 5.63 | bleu 56.35 | wps 1586.5 | wpb 933.5 | bsz 59.6 | num_updates 42606 | best_bleu 57.52
2022-09-08 19:30:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 527 @ 42606 updates
2022-09-08 19:30:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint527.pt
2022-09-08 19:30:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint527.pt
2022-09-08 19:30:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint527.pt (epoch 527 @ 42606 updates, score 56.35) (writing took 20.510150332003832 seconds)
2022-09-08 19:30:56 | INFO | fairseq_cli.train | end of epoch 527 (average epoch stats below)
2022-09-08 19:30:56 | INFO | train | epoch 527 | loss 3.373 | nll_loss 0.339 | mask_loss 9.14319 | p_2 0.03503 | mask_ave 0.501 | ppl 1.27 | wps 3434.6 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 42606 | lr 0.000153202 | gnorm 0.239 | train_wall 97 | gb_free 9.1 | wall 69889
2022-09-08 19:30:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:30:56 | INFO | fairseq.trainer | begin training epoch 528
2022-09-08 19:30:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:32:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:32:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:32:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:32:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:32:45 | INFO | valid | epoch 528 | valid on 'valid' subset | loss 5.079 | nll_loss 2.474 | mask_loss 10.0722 | p_2 0.04844 | mask_ave 0.619 | ppl 5.56 | bleu 56.59 | wps 1502.3 | wpb 933.5 | bsz 59.6 | num_updates 42687 | best_bleu 57.52
2022-09-08 19:32:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 528 @ 42687 updates
2022-09-08 19:32:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint528.pt
2022-09-08 19:32:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint528.pt
2022-09-08 19:33:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint528.pt (epoch 528 @ 42687 updates, score 56.59) (writing took 18.903339128941298 seconds)
2022-09-08 19:33:05 | INFO | fairseq_cli.train | end of epoch 528 (average epoch stats below)
2022-09-08 19:33:05 | INFO | train | epoch 528 | loss 3.374 | nll_loss 0.34 | mask_loss 9.09158 | p_2 0.03499 | mask_ave 0.502 | ppl 1.27 | wps 3479.5 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 42687 | lr 0.000153057 | gnorm 0.255 | train_wall 97 | gb_free 9.1 | wall 70017
2022-09-08 19:33:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:33:05 | INFO | fairseq.trainer | begin training epoch 529
2022-09-08 19:33:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:33:21 | INFO | train_inner | epoch 529:     13 / 81 loss=3.374, nll_loss=0.34, mask_loss=9.11618, p_2=0.03506, mask_ave=0.503, ppl=1.27, wps=2988.6, ups=0.55, wpb=5483.1, bsz=354.1, num_updates=42700, lr=0.000153033, gnorm=0.252, train_wall=119, gb_free=9, wall=70034
2022-09-08 19:34:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:34:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:34:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:34:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:34:53 | INFO | valid | epoch 529 | valid on 'valid' subset | loss 5.083 | nll_loss 2.472 | mask_loss 9.73601 | p_2 0.04858 | mask_ave 0.615 | ppl 5.55 | bleu 56.74 | wps 1511.8 | wpb 933.5 | bsz 59.6 | num_updates 42768 | best_bleu 57.52
2022-09-08 19:34:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 529 @ 42768 updates
2022-09-08 19:34:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint529.pt
2022-09-08 19:34:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint529.pt
2022-09-08 19:35:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint529.pt (epoch 529 @ 42768 updates, score 56.74) (writing took 18.94619721546769 seconds)
2022-09-08 19:35:13 | INFO | fairseq_cli.train | end of epoch 529 (average epoch stats below)
2022-09-08 19:35:13 | INFO | train | epoch 529 | loss 3.373 | nll_loss 0.34 | mask_loss 9.07767 | p_2 0.03508 | mask_ave 0.5 | ppl 1.27 | wps 3494.4 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 42768 | lr 0.000152912 | gnorm 0.231 | train_wall 96 | gb_free 9.2 | wall 70145
2022-09-08 19:35:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:35:13 | INFO | fairseq.trainer | begin training epoch 530
2022-09-08 19:35:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:35:53 | INFO | train_inner | epoch 530:     32 / 81 loss=3.373, nll_loss=0.339, mask_loss=9.00507, p_2=0.03495, mask_ave=0.499, ppl=1.27, wps=3665.5, ups=0.66, wpb=5566.1, bsz=363, num_updates=42800, lr=0.000152854, gnorm=0.233, train_wall=120, gb_free=9.1, wall=70186
2022-09-08 19:36:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:36:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:36:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:36:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:37:04 | INFO | valid | epoch 530 | valid on 'valid' subset | loss 5.089 | nll_loss 2.48 | mask_loss 9.88778 | p_2 0.04881 | mask_ave 0.609 | ppl 5.58 | bleu 56.73 | wps 1515 | wpb 933.5 | bsz 59.6 | num_updates 42849 | best_bleu 57.52
2022-09-08 19:37:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 530 @ 42849 updates
2022-09-08 19:37:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint530.pt
2022-09-08 19:37:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint530.pt
2022-09-08 19:37:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint530.pt (epoch 530 @ 42849 updates, score 56.73) (writing took 22.017063967883587 seconds)
2022-09-08 19:37:26 | INFO | fairseq_cli.train | end of epoch 530 (average epoch stats below)
2022-09-08 19:37:26 | INFO | train | epoch 530 | loss 3.373 | nll_loss 0.34 | mask_loss 8.94082 | p_2 0.03504 | mask_ave 0.501 | ppl 1.27 | wps 3343.9 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 42849 | lr 0.000152767 | gnorm 0.247 | train_wall 99 | gb_free 9.2 | wall 70279
2022-09-08 19:37:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:37:27 | INFO | fairseq.trainer | begin training epoch 531
2022-09-08 19:37:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:38:28 | INFO | train_inner | epoch 531:     51 / 81 loss=3.374, nll_loss=0.34, mask_loss=8.96545, p_2=0.03507, mask_ave=0.499, ppl=1.27, wps=3548.8, ups=0.64, wpb=5506, bsz=353.4, num_updates=42900, lr=0.000152676, gnorm=0.262, train_wall=120, gb_free=9.1, wall=70341
2022-09-08 19:39:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:39:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:39:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:39:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:39:15 | INFO | valid | epoch 531 | valid on 'valid' subset | loss 5.1 | nll_loss 2.495 | mask_loss 9.86698 | p_2 0.04837 | mask_ave 0.622 | ppl 5.64 | bleu 56.35 | wps 1583.7 | wpb 933.5 | bsz 59.6 | num_updates 42930 | best_bleu 57.52
2022-09-08 19:39:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 531 @ 42930 updates
2022-09-08 19:39:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint531.pt
2022-09-08 19:39:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint531.pt
2022-09-08 19:39:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint531.pt (epoch 531 @ 42930 updates, score 56.35) (writing took 19.498862106353045 seconds)
2022-09-08 19:39:35 | INFO | fairseq_cli.train | end of epoch 531 (average epoch stats below)
2022-09-08 19:39:35 | INFO | train | epoch 531 | loss 3.373 | nll_loss 0.34 | mask_loss 8.95818 | p_2 0.03511 | mask_ave 0.499 | ppl 1.27 | wps 3484.9 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 42930 | lr 0.000152623 | gnorm 0.254 | train_wall 96 | gb_free 9.1 | wall 70408
2022-09-08 19:39:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:39:35 | INFO | fairseq.trainer | begin training epoch 532
2022-09-08 19:39:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:41:01 | INFO | train_inner | epoch 532:     70 / 81 loss=3.373, nll_loss=0.34, mask_loss=8.95553, p_2=0.03515, mask_ave=0.5, ppl=1.27, wps=3629.3, ups=0.66, wpb=5538.6, bsz=362.9, num_updates=43000, lr=0.000152499, gnorm=0.251, train_wall=121, gb_free=8.9, wall=70494
2022-09-08 19:41:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:41:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:41:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:41:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:41:25 | INFO | valid | epoch 532 | valid on 'valid' subset | loss 5.086 | nll_loss 2.475 | mask_loss 9.82613 | p_2 0.04858 | mask_ave 0.616 | ppl 5.56 | bleu 57 | wps 1478.7 | wpb 933.5 | bsz 59.6 | num_updates 43011 | best_bleu 57.52
2022-09-08 19:41:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 532 @ 43011 updates
2022-09-08 19:41:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint532.pt
2022-09-08 19:41:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint532.pt
2022-09-08 19:41:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint532.pt (epoch 532 @ 43011 updates, score 57.0) (writing took 20.761501491069794 seconds)
2022-09-08 19:41:46 | INFO | fairseq_cli.train | end of epoch 532 (average epoch stats below)
2022-09-08 19:41:46 | INFO | train | epoch 532 | loss 3.374 | nll_loss 0.34 | mask_loss 8.99214 | p_2 0.03508 | mask_ave 0.5 | ppl 1.27 | wps 3412.5 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 43011 | lr 0.000152479 | gnorm 0.258 | train_wall 97 | gb_free 9.2 | wall 70539
2022-09-08 19:41:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:41:46 | INFO | fairseq.trainer | begin training epoch 533
2022-09-08 19:41:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:43:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:43:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:43:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:43:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:43:35 | INFO | valid | epoch 533 | valid on 'valid' subset | loss 5.102 | nll_loss 2.498 | mask_loss 9.90968 | p_2 0.04848 | mask_ave 0.619 | ppl 5.65 | bleu 56.7 | wps 1554.7 | wpb 933.5 | bsz 59.6 | num_updates 43092 | best_bleu 57.52
2022-09-08 19:43:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 533 @ 43092 updates
2022-09-08 19:43:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint533.pt
2022-09-08 19:43:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint533.pt
2022-09-08 19:44:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint533.pt (epoch 533 @ 43092 updates, score 56.7) (writing took 26.176315527409315 seconds)
2022-09-08 19:44:02 | INFO | fairseq_cli.train | end of epoch 533 (average epoch stats below)
2022-09-08 19:44:02 | INFO | train | epoch 533 | loss 3.373 | nll_loss 0.34 | mask_loss 8.99924 | p_2 0.03499 | mask_ave 0.502 | ppl 1.27 | wps 3296.1 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 43092 | lr 0.000152336 | gnorm 0.254 | train_wall 97 | gb_free 9.1 | wall 70674
2022-09-08 19:44:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:44:02 | INFO | fairseq.trainer | begin training epoch 534
2022-09-08 19:44:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:44:12 | INFO | train_inner | epoch 534:      8 / 81 loss=3.374, nll_loss=0.341, mask_loss=9.02127, p_2=0.03486, mask_ave=0.502, ppl=1.27, wps=2876.3, ups=0.52, wpb=5507.1, bsz=353.8, num_updates=43100, lr=0.000152322, gnorm=0.257, train_wall=118, gb_free=9.2, wall=70685
2022-09-08 19:45:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:45:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:45:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:45:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:45:51 | INFO | valid | epoch 534 | valid on 'valid' subset | loss 5.101 | nll_loss 2.498 | mask_loss 9.88454 | p_2 0.04876 | mask_ave 0.611 | ppl 5.65 | bleu 56.32 | wps 1507.5 | wpb 933.5 | bsz 59.6 | num_updates 43173 | best_bleu 57.52
2022-09-08 19:45:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 534 @ 43173 updates
2022-09-08 19:45:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint534.pt
2022-09-08 19:45:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint534.pt
2022-09-08 19:46:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint534.pt (epoch 534 @ 43173 updates, score 56.32) (writing took 22.88191280514002 seconds)
2022-09-08 19:46:14 | INFO | fairseq_cli.train | end of epoch 534 (average epoch stats below)
2022-09-08 19:46:14 | INFO | train | epoch 534 | loss 3.373 | nll_loss 0.339 | mask_loss 9.01493 | p_2 0.03505 | mask_ave 0.501 | ppl 1.27 | wps 3370.2 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 43173 | lr 0.000152193 | gnorm 0.237 | train_wall 97 | gb_free 9.1 | wall 70807
2022-09-08 19:46:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:46:14 | INFO | fairseq.trainer | begin training epoch 535
2022-09-08 19:46:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:46:48 | INFO | train_inner | epoch 535:     27 / 81 loss=3.373, nll_loss=0.339, mask_loss=9.00432, p_2=0.03517, mask_ave=0.5, ppl=1.26, wps=3542.6, ups=0.64, wpb=5516.8, bsz=360.2, num_updates=43200, lr=0.000152145, gnorm=0.237, train_wall=120, gb_free=9, wall=70841
2022-09-08 19:47:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:47:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:47:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:47:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:48:04 | INFO | valid | epoch 535 | valid on 'valid' subset | loss 5.094 | nll_loss 2.488 | mask_loss 9.71874 | p_2 0.04874 | mask_ave 0.611 | ppl 5.61 | bleu 56.46 | wps 1581.4 | wpb 933.5 | bsz 59.6 | num_updates 43254 | best_bleu 57.52
2022-09-08 19:48:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 535 @ 43254 updates
2022-09-08 19:48:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint535.pt
2022-09-08 19:48:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint535.pt
2022-09-08 19:48:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint535.pt (epoch 535 @ 43254 updates, score 56.46) (writing took 23.300505191087723 seconds)
2022-09-08 19:48:28 | INFO | fairseq_cli.train | end of epoch 535 (average epoch stats below)
2022-09-08 19:48:28 | INFO | train | epoch 535 | loss 3.373 | nll_loss 0.34 | mask_loss 8.94857 | p_2 0.03506 | mask_ave 0.5 | ppl 1.27 | wps 3359 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 43254 | lr 0.00015205 | gnorm 0.236 | train_wall 97 | gb_free 9.1 | wall 70940
2022-09-08 19:48:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:48:28 | INFO | fairseq.trainer | begin training epoch 536
2022-09-08 19:48:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:49:24 | INFO | train_inner | epoch 536:     46 / 81 loss=3.374, nll_loss=0.34, mask_loss=9.00419, p_2=0.0348, mask_ave=0.502, ppl=1.27, wps=3549.9, ups=0.64, wpb=5536.5, bsz=351.8, num_updates=43300, lr=0.000151969, gnorm=0.235, train_wall=120, gb_free=9.2, wall=70997
2022-09-08 19:50:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:50:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:50:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:50:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:50:17 | INFO | valid | epoch 536 | valid on 'valid' subset | loss 5.079 | nll_loss 2.471 | mask_loss 9.94484 | p_2 0.04884 | mask_ave 0.609 | ppl 5.54 | bleu 56.5 | wps 1523.6 | wpb 933.5 | bsz 59.6 | num_updates 43335 | best_bleu 57.52
2022-09-08 19:50:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 536 @ 43335 updates
2022-09-08 19:50:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint536.pt
2022-09-08 19:50:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint536.pt
2022-09-08 19:50:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint536.pt (epoch 536 @ 43335 updates, score 56.5) (writing took 19.431556463241577 seconds)
2022-09-08 19:50:37 | INFO | fairseq_cli.train | end of epoch 536 (average epoch stats below)
2022-09-08 19:50:37 | INFO | train | epoch 536 | loss 3.373 | nll_loss 0.34 | mask_loss 8.98242 | p_2 0.03504 | mask_ave 0.501 | ppl 1.27 | wps 3457.8 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 43335 | lr 0.000151908 | gnorm 0.244 | train_wall 97 | gb_free 9.1 | wall 71070
2022-09-08 19:50:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:50:37 | INFO | fairseq.trainer | begin training epoch 537
2022-09-08 19:50:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:51:56 | INFO | train_inner | epoch 537:     65 / 81 loss=3.373, nll_loss=0.34, mask_loss=8.96124, p_2=0.03538, mask_ave=0.496, ppl=1.27, wps=3636.8, ups=0.66, wpb=5536.8, bsz=367.1, num_updates=43400, lr=0.000151794, gnorm=0.253, train_wall=120, gb_free=9.1, wall=71149
2022-09-08 19:52:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:52:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:52:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:52:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:52:26 | INFO | valid | epoch 537 | valid on 'valid' subset | loss 5.073 | nll_loss 2.463 | mask_loss 9.79077 | p_2 0.04886 | mask_ave 0.607 | ppl 5.51 | bleu 57.19 | wps 1558.5 | wpb 933.5 | bsz 59.6 | num_updates 43416 | best_bleu 57.52
2022-09-08 19:52:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 537 @ 43416 updates
2022-09-08 19:52:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint537.pt
2022-09-08 19:52:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint537.pt
2022-09-08 19:52:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint537.pt (epoch 537 @ 43416 updates, score 57.19) (writing took 21.16072902455926 seconds)
2022-09-08 19:52:47 | INFO | fairseq_cli.train | end of epoch 537 (average epoch stats below)
2022-09-08 19:52:47 | INFO | train | epoch 537 | loss 3.374 | nll_loss 0.34 | mask_loss 9.05163 | p_2 0.03522 | mask_ave 0.496 | ppl 1.27 | wps 3435.4 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 43416 | lr 0.000151766 | gnorm 0.252 | train_wall 97 | gb_free 9.4 | wall 71200
2022-09-08 19:52:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:52:47 | INFO | fairseq.trainer | begin training epoch 538
2022-09-08 19:52:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:54:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:54:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:54:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:54:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:54:38 | INFO | valid | epoch 538 | valid on 'valid' subset | loss 5.071 | nll_loss 2.465 | mask_loss 9.89828 | p_2 0.04863 | mask_ave 0.614 | ppl 5.52 | bleu 56.95 | wps 1494.5 | wpb 933.5 | bsz 59.6 | num_updates 43497 | best_bleu 57.52
2022-09-08 19:54:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 538 @ 43497 updates
2022-09-08 19:54:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint538.pt
2022-09-08 19:54:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint538.pt
2022-09-08 19:54:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint538.pt (epoch 538 @ 43497 updates, score 56.95) (writing took 2.6728047095239162 seconds)
2022-09-08 19:54:41 | INFO | fairseq_cli.train | end of epoch 538 (average epoch stats below)
2022-09-08 19:54:41 | INFO | train | epoch 538 | loss 3.373 | nll_loss 0.339 | mask_loss 8.95953 | p_2 0.03514 | mask_ave 0.498 | ppl 1.27 | wps 3929.4 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 43497 | lr 0.000151625 | gnorm 0.24 | train_wall 98 | gb_free 9.2 | wall 71314
2022-09-08 19:54:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:54:41 | INFO | fairseq.trainer | begin training epoch 539
2022-09-08 19:54:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:54:46 | INFO | train_inner | epoch 539:      3 / 81 loss=3.373, nll_loss=0.34, mask_loss=8.99411, p_2=0.0351, mask_ave=0.498, ppl=1.27, wps=3237.8, ups=0.59, wpb=5495.2, bsz=353.1, num_updates=43500, lr=0.00015162, gnorm=0.237, train_wall=120, gb_free=9.1, wall=71319
2022-09-08 19:56:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:56:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:56:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:56:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:56:32 | INFO | valid | epoch 539 | valid on 'valid' subset | loss 5.096 | nll_loss 2.489 | mask_loss 9.72589 | p_2 0.04858 | mask_ave 0.615 | ppl 5.62 | bleu 56.23 | wps 1510.6 | wpb 933.5 | bsz 59.6 | num_updates 43578 | best_bleu 57.52
2022-09-08 19:56:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 539 @ 43578 updates
2022-09-08 19:56:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint539.pt
2022-09-08 19:56:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint539.pt
2022-09-08 19:56:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint539.pt (epoch 539 @ 43578 updates, score 56.23) (writing took 2.6267587058246136 seconds)
2022-09-08 19:56:34 | INFO | fairseq_cli.train | end of epoch 539 (average epoch stats below)
2022-09-08 19:56:34 | INFO | train | epoch 539 | loss 3.373 | nll_loss 0.34 | mask_loss 8.90757 | p_2 0.03499 | mask_ave 0.502 | ppl 1.27 | wps 3947.7 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 43578 | lr 0.000151484 | gnorm 0.258 | train_wall 98 | gb_free 9.1 | wall 71427
2022-09-08 19:56:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:56:35 | INFO | fairseq.trainer | begin training epoch 540
2022-09-08 19:56:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:57:02 | INFO | train_inner | epoch 540:     22 / 81 loss=3.373, nll_loss=0.339, mask_loss=8.92447, p_2=0.03524, mask_ave=0.502, ppl=1.27, wps=4053.6, ups=0.74, wpb=5506.9, bsz=358.4, num_updates=43600, lr=0.000151446, gnorm=0.26, train_wall=120, gb_free=9.1, wall=71455
2022-09-08 19:58:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 19:58:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 19:58:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 19:58:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 19:58:25 | INFO | valid | epoch 540 | valid on 'valid' subset | loss 5.083 | nll_loss 2.474 | mask_loss 9.88423 | p_2 0.04806 | mask_ave 0.631 | ppl 5.56 | bleu 56.59 | wps 1462.8 | wpb 933.5 | bsz 59.6 | num_updates 43659 | best_bleu 57.52
2022-09-08 19:58:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 540 @ 43659 updates
2022-09-08 19:58:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint540.pt
2022-09-08 19:58:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint540.pt
2022-09-08 19:58:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint540.pt (epoch 540 @ 43659 updates, score 56.59) (writing took 2.5357522144913673 seconds)
2022-09-08 19:58:28 | INFO | fairseq_cli.train | end of epoch 540 (average epoch stats below)
2022-09-08 19:58:28 | INFO | train | epoch 540 | loss 3.373 | nll_loss 0.34 | mask_loss 8.97748 | p_2 0.03516 | mask_ave 0.497 | ppl 1.27 | wps 3942.8 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 43659 | lr 0.000151343 | gnorm 0.248 | train_wall 98 | gb_free 9.1 | wall 71541
2022-09-08 19:58:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 19:58:28 | INFO | fairseq.trainer | begin training epoch 541
2022-09-08 19:58:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 19:59:19 | INFO | train_inner | epoch 541:     41 / 81 loss=3.374, nll_loss=0.34, mask_loss=8.97535, p_2=0.03479, mask_ave=0.499, ppl=1.27, wps=4030.6, ups=0.73, wpb=5553.4, bsz=357.1, num_updates=43700, lr=0.000151272, gnorm=0.262, train_wall=122, gb_free=9.1, wall=71592
2022-09-08 20:00:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:00:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:00:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:00:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:00:19 | INFO | valid | epoch 541 | valid on 'valid' subset | loss 5.076 | nll_loss 2.47 | mask_loss 9.71881 | p_2 0.04822 | mask_ave 0.626 | ppl 5.54 | bleu 56.56 | wps 1522.4 | wpb 933.5 | bsz 59.6 | num_updates 43740 | best_bleu 57.52
2022-09-08 20:00:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 541 @ 43740 updates
2022-09-08 20:00:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint541.pt
2022-09-08 20:00:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint541.pt
2022-09-08 20:00:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint541.pt (epoch 541 @ 43740 updates, score 56.56) (writing took 19.634399440139532 seconds)
2022-09-08 20:00:39 | INFO | fairseq_cli.train | end of epoch 541 (average epoch stats below)
2022-09-08 20:00:39 | INFO | train | epoch 541 | loss 3.374 | nll_loss 0.34 | mask_loss 8.97187 | p_2 0.035 | mask_ave 0.502 | ppl 1.27 | wps 3402.3 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 43740 | lr 0.000151203 | gnorm 0.265 | train_wall 99 | gb_free 9.1 | wall 71672
2022-09-08 20:00:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:00:39 | INFO | fairseq.trainer | begin training epoch 542
2022-09-08 20:00:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:01:54 | INFO | train_inner | epoch 542:     60 / 81 loss=3.373, nll_loss=0.34, mask_loss=8.94546, p_2=0.0352, mask_ave=0.503, ppl=1.27, wps=3588.7, ups=0.65, wpb=5532.6, bsz=361.4, num_updates=43800, lr=0.000151099, gnorm=0.28, train_wall=121, gb_free=9, wall=71747
2022-09-08 20:02:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:02:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:02:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:02:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:02:30 | INFO | valid | epoch 542 | valid on 'valid' subset | loss 5.079 | nll_loss 2.47 | mask_loss 9.76626 | p_2 0.04834 | mask_ave 0.622 | ppl 5.54 | bleu 56.69 | wps 1561.2 | wpb 933.5 | bsz 59.6 | num_updates 43821 | best_bleu 57.52
2022-09-08 20:02:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 542 @ 43821 updates
2022-09-08 20:02:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint542.pt
2022-09-08 20:02:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint542.pt
2022-09-08 20:02:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint542.pt (epoch 542 @ 43821 updates, score 56.69) (writing took 19.450736489146948 seconds)
2022-09-08 20:02:49 | INFO | fairseq_cli.train | end of epoch 542 (average epoch stats below)
2022-09-08 20:02:49 | INFO | train | epoch 542 | loss 3.374 | nll_loss 0.34 | mask_loss 8.93055 | p_2 0.03498 | mask_ave 0.503 | ppl 1.27 | wps 3446 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 43821 | lr 0.000151063 | gnorm 0.323 | train_wall 98 | gb_free 9.1 | wall 71802
2022-09-08 20:02:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:02:49 | INFO | fairseq.trainer | begin training epoch 543
2022-09-08 20:02:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:04:25 | INFO | train_inner | epoch 543:     79 / 81 loss=3.374, nll_loss=0.341, mask_loss=8.92, p_2=0.03487, mask_ave=0.503, ppl=1.27, wps=3657.1, ups=0.66, wpb=5524.1, bsz=357, num_updates=43900, lr=0.000150927, gnorm=0.294, train_wall=119, gb_free=9.1, wall=71898
2022-09-08 20:04:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:04:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:04:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:04:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:04:38 | INFO | valid | epoch 543 | valid on 'valid' subset | loss 5.075 | nll_loss 2.464 | mask_loss 9.93817 | p_2 0.04817 | mask_ave 0.628 | ppl 5.52 | bleu 56.79 | wps 1501.9 | wpb 933.5 | bsz 59.6 | num_updates 43902 | best_bleu 57.52
2022-09-08 20:04:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 543 @ 43902 updates
2022-09-08 20:04:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint543.pt
2022-09-08 20:04:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint543.pt
2022-09-08 20:04:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint543.pt (epoch 543 @ 43902 updates, score 56.79) (writing took 2.6220377571880817 seconds)
2022-09-08 20:04:41 | INFO | fairseq_cli.train | end of epoch 543 (average epoch stats below)
2022-09-08 20:04:41 | INFO | train | epoch 543 | loss 3.374 | nll_loss 0.34 | mask_loss 8.92336 | p_2 0.03494 | mask_ave 0.504 | ppl 1.27 | wps 4015.6 | ups 0.73 | wpb 5523.2 | bsz 358 | num_updates 43902 | lr 0.000150924 | gnorm 0.272 | train_wall 96 | gb_free 9.1 | wall 71913
2022-09-08 20:04:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:04:41 | INFO | fairseq.trainer | begin training epoch 544
2022-09-08 20:04:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:06:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:06:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:06:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:06:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:06:30 | INFO | valid | epoch 544 | valid on 'valid' subset | loss 5.08 | nll_loss 2.47 | mask_loss 9.85682 | p_2 0.04815 | mask_ave 0.628 | ppl 5.54 | bleu 56.84 | wps 1531.5 | wpb 933.5 | bsz 59.6 | num_updates 43983 | best_bleu 57.52
2022-09-08 20:06:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 544 @ 43983 updates
2022-09-08 20:06:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint544.pt
2022-09-08 20:06:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint544.pt
2022-09-08 20:06:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint544.pt (epoch 544 @ 43983 updates, score 56.84) (writing took 21.52979128435254 seconds)
2022-09-08 20:06:52 | INFO | fairseq_cli.train | end of epoch 544 (average epoch stats below)
2022-09-08 20:06:52 | INFO | train | epoch 544 | loss 3.373 | nll_loss 0.339 | mask_loss 9.03084 | p_2 0.03487 | mask_ave 0.506 | ppl 1.27 | wps 3412 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 43983 | lr 0.000150785 | gnorm 0.267 | train_wall 97 | gb_free 9.1 | wall 72045
2022-09-08 20:06:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:06:52 | INFO | fairseq.trainer | begin training epoch 545
2022-09-08 20:06:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:07:12 | INFO | train_inner | epoch 545:     17 / 81 loss=3.373, nll_loss=0.339, mask_loss=9.02569, p_2=0.03502, mask_ave=0.508, ppl=1.27, wps=3269.4, ups=0.6, wpb=5481.8, bsz=353.3, num_updates=44000, lr=0.000150756, gnorm=0.265, train_wall=118, gb_free=9.1, wall=72065
2022-09-08 20:08:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:08:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:08:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:08:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:08:44 | INFO | valid | epoch 545 | valid on 'valid' subset | loss 5.071 | nll_loss 2.461 | mask_loss 10.044 | p_2 0.04849 | mask_ave 0.618 | ppl 5.51 | bleu 56.68 | wps 1491.8 | wpb 933.5 | bsz 59.6 | num_updates 44064 | best_bleu 57.52
2022-09-08 20:08:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 545 @ 44064 updates
2022-09-08 20:08:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint545.pt
2022-09-08 20:08:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint545.pt
2022-09-08 20:09:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint545.pt (epoch 545 @ 44064 updates, score 56.68) (writing took 23.690219584852457 seconds)
2022-09-08 20:09:07 | INFO | fairseq_cli.train | end of epoch 545 (average epoch stats below)
2022-09-08 20:09:07 | INFO | train | epoch 545 | loss 3.373 | nll_loss 0.339 | mask_loss 9.09827 | p_2 0.03495 | mask_ave 0.504 | ppl 1.27 | wps 3296.2 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 44064 | lr 0.000150646 | gnorm 0.24 | train_wall 99 | gb_free 9.2 | wall 72180
2022-09-08 20:09:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:09:08 | INFO | fairseq.trainer | begin training epoch 546
2022-09-08 20:09:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:09:52 | INFO | train_inner | epoch 546:     36 / 81 loss=3.373, nll_loss=0.339, mask_loss=9.10743, p_2=0.03491, mask_ave=0.499, ppl=1.26, wps=3483.4, ups=0.63, wpb=5546.7, bsz=359.9, num_updates=44100, lr=0.000150585, gnorm=0.238, train_wall=122, gb_free=9, wall=72224
2022-09-08 20:10:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:10:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:10:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:10:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:10:58 | INFO | valid | epoch 546 | valid on 'valid' subset | loss 5.093 | nll_loss 2.486 | mask_loss 10.0278 | p_2 0.04849 | mask_ave 0.619 | ppl 5.6 | bleu 56.73 | wps 1513.1 | wpb 933.5 | bsz 59.6 | num_updates 44145 | best_bleu 57.52
2022-09-08 20:10:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 546 @ 44145 updates
2022-09-08 20:10:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint546.pt
2022-09-08 20:10:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint546.pt
2022-09-08 20:11:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint546.pt (epoch 546 @ 44145 updates, score 56.73) (writing took 19.26137451082468 seconds)
2022-09-08 20:11:17 | INFO | fairseq_cli.train | end of epoch 546 (average epoch stats below)
2022-09-08 20:11:17 | INFO | train | epoch 546 | loss 3.373 | nll_loss 0.339 | mask_loss 9.02619 | p_2 0.03504 | mask_ave 0.501 | ppl 1.26 | wps 3446.7 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 44145 | lr 0.000150508 | gnorm 0.24 | train_wall 97 | gb_free 9.1 | wall 72310
2022-09-08 20:11:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:11:17 | INFO | fairseq.trainer | begin training epoch 547
2022-09-08 20:11:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:12:25 | INFO | train_inner | epoch 547:     55 / 81 loss=3.373, nll_loss=0.339, mask_loss=9.04363, p_2=0.03495, mask_ave=0.504, ppl=1.27, wps=3606.5, ups=0.65, wpb=5525, bsz=361.8, num_updates=44200, lr=0.000150414, gnorm=0.247, train_wall=121, gb_free=9, wall=72378
2022-09-08 20:12:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:12:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:12:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:12:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:13:07 | INFO | valid | epoch 547 | valid on 'valid' subset | loss 5.082 | nll_loss 2.475 | mask_loss 10.0006 | p_2 0.04849 | mask_ave 0.618 | ppl 5.56 | bleu 56.48 | wps 1536.2 | wpb 933.5 | bsz 59.6 | num_updates 44226 | best_bleu 57.52
2022-09-08 20:13:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 547 @ 44226 updates
2022-09-08 20:13:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint547.pt
2022-09-08 20:13:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint547.pt
2022-09-08 20:13:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint547.pt (epoch 547 @ 44226 updates, score 56.48) (writing took 18.66211175173521 seconds)
2022-09-08 20:13:26 | INFO | fairseq_cli.train | end of epoch 547 (average epoch stats below)
2022-09-08 20:13:26 | INFO | train | epoch 547 | loss 3.373 | nll_loss 0.34 | mask_loss 9.07047 | p_2 0.03496 | mask_ave 0.503 | ppl 1.27 | wps 3478 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 44226 | lr 0.00015037 | gnorm 0.253 | train_wall 97 | gb_free 9.1 | wall 72439
2022-09-08 20:13:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:13:26 | INFO | fairseq.trainer | begin training epoch 548
2022-09-08 20:13:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:14:56 | INFO | train_inner | epoch 548:     74 / 81 loss=3.373, nll_loss=0.34, mask_loss=9.01838, p_2=0.0351, mask_ave=0.502, ppl=1.27, wps=3654.8, ups=0.66, wpb=5528.8, bsz=358, num_updates=44300, lr=0.000150244, gnorm=0.238, train_wall=120, gb_free=9, wall=72529
2022-09-08 20:15:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:15:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:15:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:15:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:15:16 | INFO | valid | epoch 548 | valid on 'valid' subset | loss 5.101 | nll_loss 2.5 | mask_loss 9.90506 | p_2 0.04845 | mask_ave 0.619 | ppl 5.66 | bleu 56.7 | wps 1465.3 | wpb 933.5 | bsz 59.6 | num_updates 44307 | best_bleu 57.52
2022-09-08 20:15:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 548 @ 44307 updates
2022-09-08 20:15:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint548.pt
2022-09-08 20:15:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint548.pt
2022-09-08 20:15:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint548.pt (epoch 548 @ 44307 updates, score 56.7) (writing took 17.42992988228798 seconds)
2022-09-08 20:15:34 | INFO | fairseq_cli.train | end of epoch 548 (average epoch stats below)
2022-09-08 20:15:34 | INFO | train | epoch 548 | loss 3.373 | nll_loss 0.339 | mask_loss 9.03749 | p_2 0.03498 | mask_ave 0.502 | ppl 1.27 | wps 3504.6 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 44307 | lr 0.000150232 | gnorm 0.232 | train_wall 97 | gb_free 9.2 | wall 72566
2022-09-08 20:15:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:15:34 | INFO | fairseq.trainer | begin training epoch 549
2022-09-08 20:15:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:17:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:17:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:17:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:17:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:17:23 | INFO | valid | epoch 549 | valid on 'valid' subset | loss 5.096 | nll_loss 2.494 | mask_loss 9.87676 | p_2 0.04856 | mask_ave 0.616 | ppl 5.63 | bleu 56.96 | wps 1519 | wpb 933.5 | bsz 59.6 | num_updates 44388 | best_bleu 57.52
2022-09-08 20:17:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 549 @ 44388 updates
2022-09-08 20:17:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint549.pt
2022-09-08 20:17:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint549.pt
2022-09-08 20:17:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint549.pt (epoch 549 @ 44388 updates, score 56.96) (writing took 2.5227503404021263 seconds)
2022-09-08 20:17:26 | INFO | fairseq_cli.train | end of epoch 549 (average epoch stats below)
2022-09-08 20:17:26 | INFO | train | epoch 549 | loss 3.373 | nll_loss 0.34 | mask_loss 9.06735 | p_2 0.035 | mask_ave 0.502 | ppl 1.27 | wps 3971.9 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 44388 | lr 0.000150095 | gnorm 0.258 | train_wall 97 | gb_free 9.1 | wall 72679
2022-09-08 20:17:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:17:26 | INFO | fairseq.trainer | begin training epoch 550
2022-09-08 20:17:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:17:42 | INFO | train_inner | epoch 550:     12 / 81 loss=3.373, nll_loss=0.339, mask_loss=9.0744, p_2=0.03483, mask_ave=0.501, ppl=1.27, wps=3327.3, ups=0.6, wpb=5507.8, bsz=354.6, num_updates=44400, lr=0.000150075, gnorm=0.251, train_wall=119, gb_free=9.1, wall=72694
2022-09-08 20:19:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:19:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:19:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:19:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:19:16 | INFO | valid | epoch 550 | valid on 'valid' subset | loss 5.089 | nll_loss 2.484 | mask_loss 9.85295 | p_2 0.04853 | mask_ave 0.616 | ppl 5.59 | bleu 56.42 | wps 1485.5 | wpb 933.5 | bsz 59.6 | num_updates 44469 | best_bleu 57.52
2022-09-08 20:19:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 550 @ 44469 updates
2022-09-08 20:19:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint550.pt
2022-09-08 20:19:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint550.pt
2022-09-08 20:19:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint550.pt (epoch 550 @ 44469 updates, score 56.42) (writing took 19.08710380271077 seconds)
2022-09-08 20:19:35 | INFO | fairseq_cli.train | end of epoch 550 (average epoch stats below)
2022-09-08 20:19:35 | INFO | train | epoch 550 | loss 3.373 | nll_loss 0.339 | mask_loss 9.00815 | p_2 0.03505 | mask_ave 0.501 | ppl 1.27 | wps 3477.3 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 44469 | lr 0.000149959 | gnorm 0.237 | train_wall 96 | gb_free 9.1 | wall 72808
2022-09-08 20:19:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:19:35 | INFO | fairseq.trainer | begin training epoch 551
2022-09-08 20:19:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:20:13 | INFO | train_inner | epoch 551:     31 / 81 loss=3.373, nll_loss=0.339, mask_loss=8.99356, p_2=0.03495, mask_ave=0.5, ppl=1.27, wps=3663.3, ups=0.66, wpb=5542.9, bsz=356.9, num_updates=44500, lr=0.000149906, gnorm=0.246, train_wall=119, gb_free=9.1, wall=72846
2022-09-08 20:21:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:21:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:21:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:21:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:21:25 | INFO | valid | epoch 551 | valid on 'valid' subset | loss 5.086 | nll_loss 2.48 | mask_loss 9.90858 | p_2 0.04844 | mask_ave 0.62 | ppl 5.58 | bleu 56.52 | wps 1512.3 | wpb 933.5 | bsz 59.6 | num_updates 44550 | best_bleu 57.52
2022-09-08 20:21:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 551 @ 44550 updates
2022-09-08 20:21:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint551.pt
2022-09-08 20:21:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint551.pt
2022-09-08 20:21:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint551.pt (epoch 551 @ 44550 updates, score 56.52) (writing took 16.452680334448814 seconds)
2022-09-08 20:21:42 | INFO | fairseq_cli.train | end of epoch 551 (average epoch stats below)
2022-09-08 20:21:42 | INFO | train | epoch 551 | loss 3.373 | nll_loss 0.339 | mask_loss 8.95432 | p_2 0.03506 | mask_ave 0.5 | ppl 1.27 | wps 3516.8 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 44550 | lr 0.000149822 | gnorm 0.26 | train_wall 98 | gb_free 9.3 | wall 72935
2022-09-08 20:21:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:21:42 | INFO | fairseq.trainer | begin training epoch 552
2022-09-08 20:21:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:22:44 | INFO | train_inner | epoch 552:     50 / 81 loss=3.373, nll_loss=0.34, mask_loss=8.98366, p_2=0.03499, mask_ave=0.501, ppl=1.27, wps=3655.3, ups=0.66, wpb=5514.7, bsz=358.1, num_updates=44600, lr=0.000149738, gnorm=0.268, train_wall=121, gb_free=9, wall=72997
2022-09-08 20:23:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:23:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:23:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:23:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:23:33 | INFO | valid | epoch 552 | valid on 'valid' subset | loss 5.093 | nll_loss 2.484 | mask_loss 9.90083 | p_2 0.04842 | mask_ave 0.62 | ppl 5.6 | bleu 56.2 | wps 1495.1 | wpb 933.5 | bsz 59.6 | num_updates 44631 | best_bleu 57.52
2022-09-08 20:23:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 552 @ 44631 updates
2022-09-08 20:23:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint552.pt
2022-09-08 20:23:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint552.pt
2022-09-08 20:23:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint552.pt (epoch 552 @ 44631 updates, score 56.2) (writing took 20.570390112698078 seconds)
2022-09-08 20:23:53 | INFO | fairseq_cli.train | end of epoch 552 (average epoch stats below)
2022-09-08 20:23:53 | INFO | train | epoch 552 | loss 3.374 | nll_loss 0.34 | mask_loss 8.94361 | p_2 0.03505 | mask_ave 0.5 | ppl 1.27 | wps 3404.7 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 44631 | lr 0.000149686 | gnorm 0.28 | train_wall 98 | gb_free 9 | wall 73066
2022-09-08 20:23:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:23:54 | INFO | fairseq.trainer | begin training epoch 553
2022-09-08 20:23:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:25:18 | INFO | train_inner | epoch 553:     69 / 81 loss=3.372, nll_loss=0.339, mask_loss=8.98636, p_2=0.03515, mask_ave=0.498, ppl=1.26, wps=3587.7, ups=0.65, wpb=5536.1, bsz=359.4, num_updates=44700, lr=0.000149571, gnorm=0.246, train_wall=120, gb_free=9, wall=73151
2022-09-08 20:25:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:25:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:25:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:25:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:25:43 | INFO | valid | epoch 553 | valid on 'valid' subset | loss 5.082 | nll_loss 2.472 | mask_loss 9.90457 | p_2 0.04876 | mask_ave 0.61 | ppl 5.55 | bleu 56.79 | wps 1486.1 | wpb 933.5 | bsz 59.6 | num_updates 44712 | best_bleu 57.52
2022-09-08 20:25:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 553 @ 44712 updates
2022-09-08 20:25:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint553.pt
2022-09-08 20:25:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint553.pt
2022-09-08 20:26:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint553.pt (epoch 553 @ 44712 updates, score 56.79) (writing took 17.19400479272008 seconds)
2022-09-08 20:26:00 | INFO | fairseq_cli.train | end of epoch 553 (average epoch stats below)
2022-09-08 20:26:00 | INFO | train | epoch 553 | loss 3.372 | nll_loss 0.339 | mask_loss 9.03085 | p_2 0.03514 | mask_ave 0.498 | ppl 1.26 | wps 3525.4 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 44712 | lr 0.000149551 | gnorm 0.256 | train_wall 96 | gb_free 8.9 | wall 73193
2022-09-08 20:26:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:26:01 | INFO | fairseq.trainer | begin training epoch 554
2022-09-08 20:26:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:27:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:27:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:27:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:27:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:27:49 | INFO | valid | epoch 554 | valid on 'valid' subset | loss 5.08 | nll_loss 2.469 | mask_loss 10.0679 | p_2 0.04839 | mask_ave 0.621 | ppl 5.54 | bleu 56.63 | wps 1572.5 | wpb 933.5 | bsz 59.6 | num_updates 44793 | best_bleu 57.52
2022-09-08 20:27:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 554 @ 44793 updates
2022-09-08 20:27:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint554.pt
2022-09-08 20:27:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint554.pt
2022-09-08 20:28:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint554.pt (epoch 554 @ 44793 updates, score 56.63) (writing took 20.1797113455832 seconds)
2022-09-08 20:28:09 | INFO | fairseq_cli.train | end of epoch 554 (average epoch stats below)
2022-09-08 20:28:09 | INFO | train | epoch 554 | loss 3.373 | nll_loss 0.34 | mask_loss 9.03894 | p_2 0.03513 | mask_ave 0.498 | ppl 1.27 | wps 3466.6 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 44793 | lr 0.000149415 | gnorm 0.257 | train_wall 96 | gb_free 9.2 | wall 73322
2022-09-08 20:28:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:28:10 | INFO | fairseq.trainer | begin training epoch 555
2022-09-08 20:28:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:28:19 | INFO | train_inner | epoch 555:      7 / 81 loss=3.373, nll_loss=0.34, mask_loss=9.04642, p_2=0.03516, mask_ave=0.498, ppl=1.27, wps=3051.5, ups=0.55, wpb=5507.6, bsz=356.6, num_updates=44800, lr=0.000149404, gnorm=0.274, train_wall=118, gb_free=9.2, wall=73331
2022-09-08 20:29:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:29:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:29:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:29:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:30:01 | INFO | valid | epoch 555 | valid on 'valid' subset | loss 5.102 | nll_loss 2.498 | mask_loss 9.98106 | p_2 0.04826 | mask_ave 0.625 | ppl 5.65 | bleu 56.06 | wps 1512.1 | wpb 933.5 | bsz 59.6 | num_updates 44874 | best_bleu 57.52
2022-09-08 20:30:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 555 @ 44874 updates
2022-09-08 20:30:01 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint555.pt
2022-09-08 20:30:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint555.pt
2022-09-08 20:30:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint555.pt (epoch 555 @ 44874 updates, score 56.06) (writing took 2.5000024251639843 seconds)
2022-09-08 20:30:04 | INFO | fairseq_cli.train | end of epoch 555 (average epoch stats below)
2022-09-08 20:30:04 | INFO | train | epoch 555 | loss 3.373 | nll_loss 0.339 | mask_loss 9.14583 | p_2 0.03498 | mask_ave 0.503 | ppl 1.27 | wps 3920 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 44874 | lr 0.00014928 | gnorm 0.256 | train_wall 99 | gb_free 9.1 | wall 73436
2022-09-08 20:30:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:30:04 | INFO | fairseq.trainer | begin training epoch 556
2022-09-08 20:30:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:30:36 | INFO | train_inner | epoch 556:     26 / 81 loss=3.372, nll_loss=0.339, mask_loss=9.08671, p_2=0.0353, mask_ave=0.502, ppl=1.26, wps=4026.2, ups=0.73, wpb=5530, bsz=363.8, num_updates=44900, lr=0.000149237, gnorm=0.249, train_wall=122, gb_free=9.1, wall=73469
2022-09-08 20:31:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:31:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:31:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:31:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:31:55 | INFO | valid | epoch 556 | valid on 'valid' subset | loss 5.09 | nll_loss 2.48 | mask_loss 10.044 | p_2 0.0485 | mask_ave 0.618 | ppl 5.58 | bleu 56.46 | wps 1450.6 | wpb 933.5 | bsz 59.6 | num_updates 44955 | best_bleu 57.52
2022-09-08 20:31:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 556 @ 44955 updates
2022-09-08 20:31:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint556.pt
2022-09-08 20:31:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint556.pt
2022-09-08 20:32:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint556.pt (epoch 556 @ 44955 updates, score 56.46) (writing took 18.966406784951687 seconds)
2022-09-08 20:32:14 | INFO | fairseq_cli.train | end of epoch 556 (average epoch stats below)
2022-09-08 20:32:14 | INFO | train | epoch 556 | loss 3.373 | nll_loss 0.339 | mask_loss 9.11797 | p_2 0.03507 | mask_ave 0.5 | ppl 1.27 | wps 3427.5 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 44955 | lr 0.000149146 | gnorm 0.256 | train_wall 98 | gb_free 9.1 | wall 73567
2022-09-08 20:32:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:32:14 | INFO | fairseq.trainer | begin training epoch 557
2022-09-08 20:32:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:33:10 | INFO | train_inner | epoch 557:     45 / 81 loss=3.373, nll_loss=0.34, mask_loss=9.15034, p_2=0.03515, mask_ave=0.501, ppl=1.27, wps=3581.4, ups=0.65, wpb=5507.9, bsz=355.9, num_updates=45000, lr=0.000149071, gnorm=0.265, train_wall=121, gb_free=9.1, wall=73623
2022-09-08 20:33:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:33:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:33:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:33:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:34:04 | INFO | valid | epoch 557 | valid on 'valid' subset | loss 5.082 | nll_loss 2.475 | mask_loss 9.90885 | p_2 0.04853 | mask_ave 0.617 | ppl 5.56 | bleu 56.52 | wps 1585.1 | wpb 933.5 | bsz 59.6 | num_updates 45036 | best_bleu 57.52
2022-09-08 20:34:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 557 @ 45036 updates
2022-09-08 20:34:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint557.pt
2022-09-08 20:34:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint557.pt
2022-09-08 20:34:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint557.pt (epoch 557 @ 45036 updates, score 56.52) (writing took 2.5710356272757053 seconds)
2022-09-08 20:34:07 | INFO | fairseq_cli.train | end of epoch 557 (average epoch stats below)
2022-09-08 20:34:07 | INFO | train | epoch 557 | loss 3.373 | nll_loss 0.339 | mask_loss 9.1758 | p_2 0.03509 | mask_ave 0.499 | ppl 1.26 | wps 3972 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 45036 | lr 0.000149012 | gnorm 0.252 | train_wall 98 | gb_free 9.1 | wall 73680
2022-09-08 20:34:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:34:07 | INFO | fairseq.trainer | begin training epoch 558
2022-09-08 20:34:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:35:26 | INFO | train_inner | epoch 558:     64 / 81 loss=3.372, nll_loss=0.339, mask_loss=9.13956, p_2=0.03472, mask_ave=0.499, ppl=1.26, wps=4088.2, ups=0.74, wpb=5549.6, bsz=355.4, num_updates=45100, lr=0.000148906, gnorm=0.238, train_wall=121, gb_free=9, wall=73758
2022-09-08 20:35:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:35:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:35:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:35:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:35:56 | INFO | valid | epoch 558 | valid on 'valid' subset | loss 5.077 | nll_loss 2.47 | mask_loss 9.80956 | p_2 0.04842 | mask_ave 0.619 | ppl 5.54 | bleu 56.8 | wps 1571.7 | wpb 933.5 | bsz 59.6 | num_updates 45117 | best_bleu 57.52
2022-09-08 20:35:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 558 @ 45117 updates
2022-09-08 20:35:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint558.pt
2022-09-08 20:35:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint558.pt
2022-09-08 20:36:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint558.pt (epoch 558 @ 45117 updates, score 56.8) (writing took 18.679995626211166 seconds)
2022-09-08 20:36:15 | INFO | fairseq_cli.train | end of epoch 558 (average epoch stats below)
2022-09-08 20:36:15 | INFO | train | epoch 558 | loss 3.372 | nll_loss 0.339 | mask_loss 9.0522 | p_2 0.03504 | mask_ave 0.501 | ppl 1.26 | wps 3491.1 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 45117 | lr 0.000148878 | gnorm 0.244 | train_wall 97 | gb_free 9.1 | wall 73808
2022-09-08 20:36:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:36:15 | INFO | fairseq.trainer | begin training epoch 559
2022-09-08 20:36:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:37:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:37:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:37:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:37:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:38:04 | INFO | valid | epoch 559 | valid on 'valid' subset | loss 5.097 | nll_loss 2.495 | mask_loss 9.9372 | p_2 0.04843 | mask_ave 0.62 | ppl 5.64 | bleu 55.97 | wps 1539.4 | wpb 933.5 | bsz 59.6 | num_updates 45198 | best_bleu 57.52
2022-09-08 20:38:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 559 @ 45198 updates
2022-09-08 20:38:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint559.pt
2022-09-08 20:38:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint559.pt
2022-09-08 20:38:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint559.pt (epoch 559 @ 45198 updates, score 55.97) (writing took 2.505814764648676 seconds)
2022-09-08 20:38:07 | INFO | fairseq_cli.train | end of epoch 559 (average epoch stats below)
2022-09-08 20:38:07 | INFO | train | epoch 559 | loss 3.373 | nll_loss 0.34 | mask_loss 8.99137 | p_2 0.03507 | mask_ave 0.5 | ppl 1.27 | wps 3992 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 45198 | lr 0.000148744 | gnorm 0.264 | train_wall 97 | gb_free 9.1 | wall 73920
2022-09-08 20:38:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:38:07 | INFO | fairseq.trainer | begin training epoch 560
2022-09-08 20:38:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:38:10 | INFO | train_inner | epoch 560:      2 / 81 loss=3.373, nll_loss=0.34, mask_loss=8.98774, p_2=0.03513, mask_ave=0.5, ppl=1.27, wps=3338.1, ups=0.61, wpb=5501.7, bsz=357.8, num_updates=45200, lr=0.000148741, gnorm=0.267, train_wall=119, gb_free=9, wall=73923
2022-09-08 20:39:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:39:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:39:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:39:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:39:56 | INFO | valid | epoch 560 | valid on 'valid' subset | loss 5.106 | nll_loss 2.505 | mask_loss 10.0321 | p_2 0.04853 | mask_ave 0.617 | ppl 5.68 | bleu 56.96 | wps 1488.7 | wpb 933.5 | bsz 59.6 | num_updates 45279 | best_bleu 57.52
2022-09-08 20:39:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 560 @ 45279 updates
2022-09-08 20:39:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint560.pt
2022-09-08 20:39:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint560.pt
2022-09-08 20:39:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint560.pt (epoch 560 @ 45279 updates, score 56.96) (writing took 2.525077775120735 seconds)
2022-09-08 20:39:59 | INFO | fairseq_cli.train | end of epoch 560 (average epoch stats below)
2022-09-08 20:39:59 | INFO | train | epoch 560 | loss 3.373 | nll_loss 0.339 | mask_loss 9.04996 | p_2 0.03514 | mask_ave 0.498 | ppl 1.27 | wps 4001 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 45279 | lr 0.000148611 | gnorm 0.262 | train_wall 96 | gb_free 9.2 | wall 74032
2022-09-08 20:39:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:39:59 | INFO | fairseq.trainer | begin training epoch 561
2022-09-08 20:39:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:40:25 | INFO | train_inner | epoch 561:     21 / 81 loss=3.372, nll_loss=0.339, mask_loss=9.07268, p_2=0.03505, mask_ave=0.498, ppl=1.26, wps=4121.4, ups=0.74, wpb=5538, bsz=357.4, num_updates=45300, lr=0.000148577, gnorm=0.255, train_wall=119, gb_free=9.2, wall=74058
2022-09-08 20:41:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:41:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:41:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:41:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:41:48 | INFO | valid | epoch 561 | valid on 'valid' subset | loss 5.102 | nll_loss 2.502 | mask_loss 10.1809 | p_2 0.0487 | mask_ave 0.612 | ppl 5.66 | bleu 56.04 | wps 1524.2 | wpb 933.5 | bsz 59.6 | num_updates 45360 | best_bleu 57.52
2022-09-08 20:41:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 561 @ 45360 updates
2022-09-08 20:41:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint561.pt
2022-09-08 20:41:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint561.pt
2022-09-08 20:42:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint561.pt (epoch 561 @ 45360 updates, score 56.04) (writing took 21.074659034609795 seconds)
2022-09-08 20:42:09 | INFO | fairseq_cli.train | end of epoch 561 (average epoch stats below)
2022-09-08 20:42:10 | INFO | train | epoch 561 | loss 3.372 | nll_loss 0.339 | mask_loss 9.09076 | p_2 0.0351 | mask_ave 0.499 | ppl 1.26 | wps 3422.2 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 45360 | lr 0.000148478 | gnorm 0.24 | train_wall 97 | gb_free 9 | wall 74162
2022-09-08 20:42:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:42:10 | INFO | fairseq.trainer | begin training epoch 562
2022-09-08 20:42:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:42:59 | INFO | train_inner | epoch 562:     40 / 81 loss=3.372, nll_loss=0.339, mask_loss=9.04978, p_2=0.03554, mask_ave=0.5, ppl=1.26, wps=3566.3, ups=0.65, wpb=5488.4, bsz=361, num_updates=45400, lr=0.000148413, gnorm=0.247, train_wall=120, gb_free=9.1, wall=74211
2022-09-08 20:43:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:43:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:43:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:43:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:44:00 | INFO | valid | epoch 562 | valid on 'valid' subset | loss 5.103 | nll_loss 2.501 | mask_loss 10.1452 | p_2 0.04821 | mask_ave 0.627 | ppl 5.66 | bleu 56.48 | wps 1570.2 | wpb 933.5 | bsz 59.6 | num_updates 45441 | best_bleu 57.52
2022-09-08 20:44:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 562 @ 45441 updates
2022-09-08 20:44:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint562.pt
2022-09-08 20:44:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint562.pt
2022-09-08 20:44:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint562.pt (epoch 562 @ 45441 updates, score 56.48) (writing took 16.106622606515884 seconds)
2022-09-08 20:44:16 | INFO | fairseq_cli.train | end of epoch 562 (average epoch stats below)
2022-09-08 20:44:16 | INFO | train | epoch 562 | loss 3.373 | nll_loss 0.34 | mask_loss 9.146 | p_2 0.03514 | mask_ave 0.498 | ppl 1.27 | wps 3540.6 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 45441 | lr 0.000148346 | gnorm 0.256 | train_wall 98 | gb_free 9.1 | wall 74289
2022-09-08 20:44:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:44:16 | INFO | fairseq.trainer | begin training epoch 563
2022-09-08 20:44:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:45:29 | INFO | train_inner | epoch 563:     59 / 81 loss=3.373, nll_loss=0.34, mask_loss=9.226, p_2=0.03501, mask_ave=0.498, ppl=1.27, wps=3705.5, ups=0.66, wpb=5574.2, bsz=363.2, num_updates=45500, lr=0.00014825, gnorm=0.25, train_wall=122, gb_free=9, wall=74362
2022-09-08 20:45:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:45:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:45:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:45:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:46:06 | INFO | valid | epoch 563 | valid on 'valid' subset | loss 5.086 | nll_loss 2.476 | mask_loss 10.166 | p_2 0.04876 | mask_ave 0.611 | ppl 5.56 | bleu 57.13 | wps 1533.4 | wpb 933.5 | bsz 59.6 | num_updates 45522 | best_bleu 57.52
2022-09-08 20:46:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 563 @ 45522 updates
2022-09-08 20:46:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint563.pt
2022-09-08 20:46:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint563.pt
2022-09-08 20:46:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint563.pt (epoch 563 @ 45522 updates, score 57.13) (writing took 17.210182540118694 seconds)
2022-09-08 20:46:23 | INFO | fairseq_cli.train | end of epoch 563 (average epoch stats below)
2022-09-08 20:46:23 | INFO | train | epoch 563 | loss 3.373 | nll_loss 0.34 | mask_loss 9.25031 | p_2 0.03514 | mask_ave 0.498 | ppl 1.27 | wps 3508.8 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 45522 | lr 0.000148214 | gnorm 0.247 | train_wall 97 | gb_free 9 | wall 74416
2022-09-08 20:46:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:46:24 | INFO | fairseq.trainer | begin training epoch 564
2022-09-08 20:46:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:47:59 | INFO | train_inner | epoch 564:     78 / 81 loss=3.372, nll_loss=0.339, mask_loss=9.25948, p_2=0.03501, mask_ave=0.495, ppl=1.27, wps=3693.6, ups=0.67, wpb=5521.2, bsz=354.4, num_updates=45600, lr=0.000148087, gnorm=0.238, train_wall=119, gb_free=9, wall=74511
2022-09-08 20:48:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:48:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:48:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:48:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:48:13 | INFO | valid | epoch 564 | valid on 'valid' subset | loss 5.095 | nll_loss 2.492 | mask_loss 10.3012 | p_2 0.04873 | mask_ave 0.612 | ppl 5.63 | bleu 57.09 | wps 1489 | wpb 933.5 | bsz 59.6 | num_updates 45603 | best_bleu 57.52
2022-09-08 20:48:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 564 @ 45603 updates
2022-09-08 20:48:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint564.pt
2022-09-08 20:48:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint564.pt
2022-09-08 20:48:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint564.pt (epoch 564 @ 45603 updates, score 57.09) (writing took 23.262531112879515 seconds)
2022-09-08 20:48:36 | INFO | fairseq_cli.train | end of epoch 564 (average epoch stats below)
2022-09-08 20:48:36 | INFO | train | epoch 564 | loss 3.372 | nll_loss 0.339 | mask_loss 9.23932 | p_2 0.03521 | mask_ave 0.496 | ppl 1.26 | wps 3366.1 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 45603 | lr 0.000148082 | gnorm 0.24 | train_wall 96 | gb_free 9.1 | wall 74549
2022-09-08 20:48:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:48:37 | INFO | fairseq.trainer | begin training epoch 565
2022-09-08 20:48:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:50:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:50:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:50:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:50:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:50:26 | INFO | valid | epoch 565 | valid on 'valid' subset | loss 5.113 | nll_loss 2.514 | mask_loss 10.1108 | p_2 0.04849 | mask_ave 0.619 | ppl 5.71 | bleu 56.35 | wps 1519.9 | wpb 933.5 | bsz 59.6 | num_updates 45684 | best_bleu 57.52
2022-09-08 20:50:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 565 @ 45684 updates
2022-09-08 20:50:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint565.pt
2022-09-08 20:50:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint565.pt
2022-09-08 20:50:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint565.pt (epoch 565 @ 45684 updates, score 56.35) (writing took 20.305115275084972 seconds)
2022-09-08 20:50:47 | INFO | fairseq_cli.train | end of epoch 565 (average epoch stats below)
2022-09-08 20:50:47 | INFO | train | epoch 565 | loss 3.372 | nll_loss 0.339 | mask_loss 9.23532 | p_2 0.03512 | mask_ave 0.499 | ppl 1.26 | wps 3423.6 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 45684 | lr 0.000147951 | gnorm 0.248 | train_wall 97 | gb_free 9.2 | wall 74680
2022-09-08 20:50:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:50:47 | INFO | fairseq.trainer | begin training epoch 566
2022-09-08 20:50:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:51:07 | INFO | train_inner | epoch 566:     16 / 81 loss=3.372, nll_loss=0.339, mask_loss=9.24155, p_2=0.03509, mask_ave=0.499, ppl=1.26, wps=2922.5, ups=0.53, wpb=5503.7, bsz=357.8, num_updates=45700, lr=0.000147925, gnorm=0.255, train_wall=119, gb_free=8.9, wall=74700
2022-09-08 20:52:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:52:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:52:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:52:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:52:37 | INFO | valid | epoch 566 | valid on 'valid' subset | loss 5.095 | nll_loss 2.492 | mask_loss 9.95728 | p_2 0.04884 | mask_ave 0.608 | ppl 5.63 | bleu 56.37 | wps 1513.6 | wpb 933.5 | bsz 59.6 | num_updates 45765 | best_bleu 57.52
2022-09-08 20:52:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 566 @ 45765 updates
2022-09-08 20:52:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint566.pt
2022-09-08 20:52:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint566.pt
2022-09-08 20:52:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint566.pt (epoch 566 @ 45765 updates, score 56.37) (writing took 2.443673714995384 seconds)
2022-09-08 20:52:39 | INFO | fairseq_cli.train | end of epoch 566 (average epoch stats below)
2022-09-08 20:52:39 | INFO | train | epoch 566 | loss 3.373 | nll_loss 0.34 | mask_loss 9.18657 | p_2 0.03511 | mask_ave 0.499 | ppl 1.27 | wps 3979.5 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 45765 | lr 0.00014782 | gnorm 0.246 | train_wall 97 | gb_free 9.1 | wall 74792
2022-09-08 20:52:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:52:40 | INFO | fairseq.trainer | begin training epoch 567
2022-09-08 20:52:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:53:23 | INFO | train_inner | epoch 567:     35 / 81 loss=3.373, nll_loss=0.339, mask_loss=9.16204, p_2=0.03537, mask_ave=0.5, ppl=1.27, wps=4055.3, ups=0.74, wpb=5502.3, bsz=358.4, num_updates=45800, lr=0.000147764, gnorm=0.241, train_wall=120, gb_free=8.9, wall=74835
2022-09-08 20:54:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:54:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:54:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:54:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:54:29 | INFO | valid | epoch 567 | valid on 'valid' subset | loss 5.108 | nll_loss 2.51 | mask_loss 10.0475 | p_2 0.04855 | mask_ave 0.618 | ppl 5.69 | bleu 56.5 | wps 1499.7 | wpb 933.5 | bsz 59.6 | num_updates 45846 | best_bleu 57.52
2022-09-08 20:54:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 567 @ 45846 updates
2022-09-08 20:54:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint567.pt
2022-09-08 20:54:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint567.pt
2022-09-08 20:54:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint567.pt (epoch 567 @ 45846 updates, score 56.5) (writing took 2.5142238549888134 seconds)
2022-09-08 20:54:32 | INFO | fairseq_cli.train | end of epoch 567 (average epoch stats below)
2022-09-08 20:54:32 | INFO | train | epoch 567 | loss 3.373 | nll_loss 0.339 | mask_loss 9.14598 | p_2 0.0352 | mask_ave 0.496 | ppl 1.27 | wps 3983 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 45846 | lr 0.000147689 | gnorm 0.251 | train_wall 97 | gb_free 9.1 | wall 74905
2022-09-08 20:54:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:54:32 | INFO | fairseq.trainer | begin training epoch 568
2022-09-08 20:54:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:55:38 | INFO | train_inner | epoch 568:     54 / 81 loss=3.373, nll_loss=0.34, mask_loss=9.16185, p_2=0.03511, mask_ave=0.496, ppl=1.27, wps=4093.4, ups=0.74, wpb=5545.1, bsz=358.1, num_updates=45900, lr=0.000147602, gnorm=0.256, train_wall=120, gb_free=9.1, wall=74971
2022-09-08 20:56:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:56:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:56:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:56:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:56:22 | INFO | valid | epoch 568 | valid on 'valid' subset | loss 5.099 | nll_loss 2.498 | mask_loss 9.98741 | p_2 0.04883 | mask_ave 0.609 | ppl 5.65 | bleu 56.85 | wps 1542.2 | wpb 933.5 | bsz 59.6 | num_updates 45927 | best_bleu 57.52
2022-09-08 20:56:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 568 @ 45927 updates
2022-09-08 20:56:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint568.pt
2022-09-08 20:56:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint568.pt
2022-09-08 20:56:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint568.pt (epoch 568 @ 45927 updates, score 56.85) (writing took 2.5618468523025513 seconds)
2022-09-08 20:56:24 | INFO | fairseq_cli.train | end of epoch 568 (average epoch stats below)
2022-09-08 20:56:24 | INFO | train | epoch 568 | loss 3.373 | nll_loss 0.339 | mask_loss 9.16526 | p_2 0.03515 | mask_ave 0.498 | ppl 1.27 | wps 3975.4 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 45927 | lr 0.000147559 | gnorm 0.248 | train_wall 97 | gb_free 9.3 | wall 75017
2022-09-08 20:56:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:56:24 | INFO | fairseq.trainer | begin training epoch 569
2022-09-08 20:56:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 20:57:53 | INFO | train_inner | epoch 569:     73 / 81 loss=3.372, nll_loss=0.339, mask_loss=9.17492, p_2=0.03507, mask_ave=0.498, ppl=1.26, wps=4099.5, ups=0.74, wpb=5523.7, bsz=355.5, num_updates=46000, lr=0.000147442, gnorm=0.245, train_wall=119, gb_free=9.1, wall=75106
2022-09-08 20:58:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 20:58:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 20:58:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 20:58:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 20:58:13 | INFO | valid | epoch 569 | valid on 'valid' subset | loss 5.107 | nll_loss 2.508 | mask_loss 9.89752 | p_2 0.04835 | mask_ave 0.622 | ppl 5.69 | bleu 56.68 | wps 1509.7 | wpb 933.5 | bsz 59.6 | num_updates 46008 | best_bleu 57.52
2022-09-08 20:58:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 569 @ 46008 updates
2022-09-08 20:58:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint569.pt
2022-09-08 20:58:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint569.pt
2022-09-08 20:58:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint569.pt (epoch 569 @ 46008 updates, score 56.68) (writing took 19.697252869606018 seconds)
2022-09-08 20:58:33 | INFO | fairseq_cli.train | end of epoch 569 (average epoch stats below)
2022-09-08 20:58:33 | INFO | train | epoch 569 | loss 3.372 | nll_loss 0.338 | mask_loss 9.17989 | p_2 0.03512 | mask_ave 0.499 | ppl 1.26 | wps 3475.6 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 46008 | lr 0.000147429 | gnorm 0.245 | train_wall 96 | gb_free 9.2 | wall 75146
2022-09-08 20:58:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 20:58:33 | INFO | fairseq.trainer | begin training epoch 570
2022-09-08 20:58:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:00:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:00:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:00:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:00:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:00:21 | INFO | valid | epoch 570 | valid on 'valid' subset | loss 5.112 | nll_loss 2.515 | mask_loss 10.0489 | p_2 0.04851 | mask_ave 0.619 | ppl 5.71 | bleu 56.91 | wps 1544.6 | wpb 933.5 | bsz 59.6 | num_updates 46089 | best_bleu 57.52
2022-09-08 21:00:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 570 @ 46089 updates
2022-09-08 21:00:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint570.pt
2022-09-08 21:00:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint570.pt
2022-09-08 21:00:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint570.pt (epoch 570 @ 46089 updates, score 56.91) (writing took 18.605085112154484 seconds)
2022-09-08 21:00:40 | INFO | fairseq_cli.train | end of epoch 570 (average epoch stats below)
2022-09-08 21:00:40 | INFO | train | epoch 570 | loss 3.373 | nll_loss 0.34 | mask_loss 9.14977 | p_2 0.03512 | mask_ave 0.499 | ppl 1.27 | wps 3528.3 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 46089 | lr 0.0001473 | gnorm 0.261 | train_wall 95 | gb_free 9.1 | wall 75273
2022-09-08 21:00:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:00:40 | INFO | fairseq.trainer | begin training epoch 571
2022-09-08 21:00:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:00:54 | INFO | train_inner | epoch 571:     11 / 81 loss=3.373, nll_loss=0.339, mask_loss=9.14667, p_2=0.035, mask_ave=0.498, ppl=1.27, wps=3048.8, ups=0.55, wpb=5518.1, bsz=356.6, num_updates=46100, lr=0.000147282, gnorm=0.257, train_wall=117, gb_free=9, wall=75287
2022-09-08 21:02:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:02:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:02:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:02:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:02:30 | INFO | valid | epoch 571 | valid on 'valid' subset | loss 5.102 | nll_loss 2.501 | mask_loss 9.96862 | p_2 0.04865 | mask_ave 0.614 | ppl 5.66 | bleu 56.64 | wps 1515.6 | wpb 933.5 | bsz 59.6 | num_updates 46170 | best_bleu 57.52
2022-09-08 21:02:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 571 @ 46170 updates
2022-09-08 21:02:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint571.pt
2022-09-08 21:02:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint571.pt
2022-09-08 21:02:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint571.pt (epoch 571 @ 46170 updates, score 56.64) (writing took 19.691333781927824 seconds)
2022-09-08 21:02:50 | INFO | fairseq_cli.train | end of epoch 571 (average epoch stats below)
2022-09-08 21:02:50 | INFO | train | epoch 571 | loss 3.372 | nll_loss 0.338 | mask_loss 9.17909 | p_2 0.03501 | mask_ave 0.502 | ppl 1.26 | wps 3443.7 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 46170 | lr 0.00014717 | gnorm 0.224 | train_wall 97 | gb_free 9 | wall 75403
2022-09-08 21:02:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:02:50 | INFO | fairseq.trainer | begin training epoch 572
2022-09-08 21:02:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:03:27 | INFO | train_inner | epoch 572:     30 / 81 loss=3.372, nll_loss=0.338, mask_loss=9.14309, p_2=0.03497, mask_ave=0.5, ppl=1.26, wps=3609.9, ups=0.65, wpb=5541.9, bsz=360.4, num_updates=46200, lr=0.000147122, gnorm=0.226, train_wall=121, gb_free=9, wall=75440
2022-09-08 21:04:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:04:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:04:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:04:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:04:40 | INFO | valid | epoch 572 | valid on 'valid' subset | loss 5.104 | nll_loss 2.503 | mask_loss 9.9879 | p_2 0.04884 | mask_ave 0.608 | ppl 5.67 | bleu 56.6 | wps 1529.5 | wpb 933.5 | bsz 59.6 | num_updates 46251 | best_bleu 57.52
2022-09-08 21:04:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 572 @ 46251 updates
2022-09-08 21:04:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint572.pt
2022-09-08 21:04:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint572.pt
2022-09-08 21:04:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint572.pt (epoch 572 @ 46251 updates, score 56.6) (writing took 18.21353429555893 seconds)
2022-09-08 21:04:59 | INFO | fairseq_cli.train | end of epoch 572 (average epoch stats below)
2022-09-08 21:04:59 | INFO | train | epoch 572 | loss 3.372 | nll_loss 0.339 | mask_loss 9.03743 | p_2 0.03519 | mask_ave 0.497 | ppl 1.26 | wps 3465.2 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 46251 | lr 0.000147041 | gnorm 0.246 | train_wall 98 | gb_free 9.1 | wall 75532
2022-09-08 21:04:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:04:59 | INFO | fairseq.trainer | begin training epoch 573
2022-09-08 21:04:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:05:59 | INFO | train_inner | epoch 573:     49 / 81 loss=3.372, nll_loss=0.339, mask_loss=9.0787, p_2=0.03525, mask_ave=0.498, ppl=1.27, wps=3634, ups=0.66, wpb=5517.8, bsz=357.3, num_updates=46300, lr=0.000146964, gnorm=0.26, train_wall=121, gb_free=9.1, wall=75592
2022-09-08 21:06:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:06:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:06:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:06:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:06:49 | INFO | valid | epoch 573 | valid on 'valid' subset | loss 5.109 | nll_loss 2.512 | mask_loss 9.90675 | p_2 0.04898 | mask_ave 0.605 | ppl 5.7 | bleu 56.66 | wps 1499.5 | wpb 933.5 | bsz 59.6 | num_updates 46332 | best_bleu 57.52
2022-09-08 21:06:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 573 @ 46332 updates
2022-09-08 21:06:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint573.pt
2022-09-08 21:06:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint573.pt
2022-09-08 21:06:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint573.pt (epoch 573 @ 46332 updates, score 56.66) (writing took 2.510363131761551 seconds)
2022-09-08 21:06:51 | INFO | fairseq_cli.train | end of epoch 573 (average epoch stats below)
2022-09-08 21:06:51 | INFO | train | epoch 573 | loss 3.372 | nll_loss 0.339 | mask_loss 9.13773 | p_2 0.0352 | mask_ave 0.496 | ppl 1.26 | wps 3974 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 46332 | lr 0.000146913 | gnorm 0.264 | train_wall 97 | gb_free 9.2 | wall 75644
2022-09-08 21:06:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:06:52 | INFO | fairseq.trainer | begin training epoch 574
2022-09-08 21:06:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:08:14 | INFO | train_inner | epoch 574:     68 / 81 loss=3.372, nll_loss=0.339, mask_loss=9.0902, p_2=0.03534, mask_ave=0.498, ppl=1.26, wps=4102.2, ups=0.74, wpb=5517.4, bsz=359.8, num_updates=46400, lr=0.000146805, gnorm=0.239, train_wall=119, gb_free=9.1, wall=75727
2022-09-08 21:08:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:08:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:08:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:08:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:08:40 | INFO | valid | epoch 574 | valid on 'valid' subset | loss 5.121 | nll_loss 2.523 | mask_loss 10.1551 | p_2 0.04867 | mask_ave 0.614 | ppl 5.75 | bleu 56.19 | wps 1552.8 | wpb 933.5 | bsz 59.6 | num_updates 46413 | best_bleu 57.52
2022-09-08 21:08:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 574 @ 46413 updates
2022-09-08 21:08:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint574.pt
2022-09-08 21:08:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint574.pt
2022-09-08 21:08:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint574.pt (epoch 574 @ 46413 updates, score 56.19) (writing took 2.3737424686551094 seconds)
2022-09-08 21:08:43 | INFO | fairseq_cli.train | end of epoch 574 (average epoch stats below)
2022-09-08 21:08:43 | INFO | train | epoch 574 | loss 3.372 | nll_loss 0.339 | mask_loss 9.08575 | p_2 0.03514 | mask_ave 0.498 | ppl 1.26 | wps 4012.9 | ups 0.73 | wpb 5523.2 | bsz 358 | num_updates 46413 | lr 0.000146784 | gnorm 0.235 | train_wall 97 | gb_free 9.1 | wall 75756
2022-09-08 21:08:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:08:43 | INFO | fairseq.trainer | begin training epoch 575
2022-09-08 21:08:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:10:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:10:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:10:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:10:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:10:31 | INFO | valid | epoch 575 | valid on 'valid' subset | loss 5.094 | nll_loss 2.487 | mask_loss 10.0063 | p_2 0.04896 | mask_ave 0.606 | ppl 5.61 | bleu 57.21 | wps 1525.8 | wpb 933.5 | bsz 59.6 | num_updates 46494 | best_bleu 57.52
2022-09-08 21:10:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 575 @ 46494 updates
2022-09-08 21:10:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint575.pt
2022-09-08 21:10:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint575.pt
2022-09-08 21:10:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint575.pt (epoch 575 @ 46494 updates, score 57.21) (writing took 18.12306883558631 seconds)
2022-09-08 21:10:49 | INFO | fairseq_cli.train | end of epoch 575 (average epoch stats below)
2022-09-08 21:10:49 | INFO | train | epoch 575 | loss 3.372 | nll_loss 0.338 | mask_loss 9.12533 | p_2 0.03529 | mask_ave 0.494 | ppl 1.26 | wps 3539 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 46494 | lr 0.000146657 | gnorm 0.237 | train_wall 95 | gb_free 9.1 | wall 75882
2022-09-08 21:10:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:10:49 | INFO | fairseq.trainer | begin training epoch 576
2022-09-08 21:10:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:10:57 | INFO | train_inner | epoch 576:      6 / 81 loss=3.372, nll_loss=0.338, mask_loss=9.14251, p_2=0.03519, mask_ave=0.494, ppl=1.26, wps=3365.4, ups=0.61, wpb=5503.7, bsz=354.2, num_updates=46500, lr=0.000146647, gnorm=0.242, train_wall=118, gb_free=9, wall=75890
2022-09-08 21:12:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:12:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:12:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:12:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:12:39 | INFO | valid | epoch 576 | valid on 'valid' subset | loss 5.109 | nll_loss 2.506 | mask_loss 9.83552 | p_2 0.0488 | mask_ave 0.609 | ppl 5.68 | bleu 56.35 | wps 1484.3 | wpb 933.5 | bsz 59.6 | num_updates 46575 | best_bleu 57.52
2022-09-08 21:12:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 576 @ 46575 updates
2022-09-08 21:12:39 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint576.pt
2022-09-08 21:12:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint576.pt
2022-09-08 21:13:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint576.pt (epoch 576 @ 46575 updates, score 56.35) (writing took 21.75145348161459 seconds)
2022-09-08 21:13:01 | INFO | fairseq_cli.train | end of epoch 576 (average epoch stats below)
2022-09-08 21:13:01 | INFO | train | epoch 576 | loss 3.372 | nll_loss 0.339 | mask_loss 9.02751 | p_2 0.03518 | mask_ave 0.497 | ppl 1.27 | wps 3386.9 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 46575 | lr 0.000146529 | gnorm 0.264 | train_wall 97 | gb_free 9.3 | wall 76014
2022-09-08 21:13:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:13:02 | INFO | fairseq.trainer | begin training epoch 577
2022-09-08 21:13:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:13:32 | INFO | train_inner | epoch 577:     25 / 81 loss=3.372, nll_loss=0.339, mask_loss=9.00694, p_2=0.03515, mask_ave=0.497, ppl=1.27, wps=3577.4, ups=0.64, wpb=5546.6, bsz=360.5, num_updates=46600, lr=0.00014649, gnorm=0.257, train_wall=120, gb_free=9, wall=76045
2022-09-08 21:14:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:14:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:14:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:14:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:14:51 | INFO | valid | epoch 577 | valid on 'valid' subset | loss 5.122 | nll_loss 2.526 | mask_loss 10.0908 | p_2 0.04866 | mask_ave 0.613 | ppl 5.76 | bleu 56.48 | wps 1525.1 | wpb 933.5 | bsz 59.6 | num_updates 46656 | best_bleu 57.52
2022-09-08 21:14:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 577 @ 46656 updates
2022-09-08 21:14:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint577.pt
2022-09-08 21:14:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint577.pt
2022-09-08 21:15:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint577.pt (epoch 577 @ 46656 updates, score 56.48) (writing took 23.323569428175688 seconds)
2022-09-08 21:15:15 | INFO | fairseq_cli.train | end of epoch 577 (average epoch stats below)
2022-09-08 21:15:15 | INFO | train | epoch 577 | loss 3.372 | nll_loss 0.339 | mask_loss 9.05728 | p_2 0.03521 | mask_ave 0.496 | ppl 1.27 | wps 3357.6 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 46656 | lr 0.000146402 | gnorm 0.239 | train_wall 97 | gb_free 9.2 | wall 76148
2022-09-08 21:15:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:15:15 | INFO | fairseq.trainer | begin training epoch 578
2022-09-08 21:15:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:16:09 | INFO | train_inner | epoch 578:     44 / 81 loss=3.372, nll_loss=0.339, mask_loss=9.11516, p_2=0.03524, mask_ave=0.496, ppl=1.26, wps=3523.5, ups=0.64, wpb=5513, bsz=359.4, num_updates=46700, lr=0.000146333, gnorm=0.245, train_wall=120, gb_free=9.1, wall=76202
2022-09-08 21:16:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:16:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:16:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:16:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:17:04 | INFO | valid | epoch 578 | valid on 'valid' subset | loss 5.12 | nll_loss 2.524 | mask_loss 10.1352 | p_2 0.04893 | mask_ave 0.606 | ppl 5.75 | bleu 56.19 | wps 1526.7 | wpb 933.5 | bsz 59.6 | num_updates 46737 | best_bleu 57.52
2022-09-08 21:17:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 578 @ 46737 updates
2022-09-08 21:17:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint578.pt
2022-09-08 21:17:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint578.pt
2022-09-08 21:17:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint578.pt (epoch 578 @ 46737 updates, score 56.19) (writing took 20.982950139790773 seconds)
2022-09-08 21:17:25 | INFO | fairseq_cli.train | end of epoch 578 (average epoch stats below)
2022-09-08 21:17:25 | INFO | train | epoch 578 | loss 3.372 | nll_loss 0.339 | mask_loss 9.14097 | p_2 0.03518 | mask_ave 0.497 | ppl 1.27 | wps 3422.8 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 46737 | lr 0.000146275 | gnorm 0.257 | train_wall 97 | gb_free 9.1 | wall 76278
2022-09-08 21:17:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:17:26 | INFO | fairseq.trainer | begin training epoch 579
2022-09-08 21:17:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:18:43 | INFO | train_inner | epoch 579:     63 / 81 loss=3.372, nll_loss=0.339, mask_loss=9.18388, p_2=0.03515, mask_ave=0.498, ppl=1.27, wps=3592.7, ups=0.65, wpb=5528.4, bsz=355.4, num_updates=46800, lr=0.000146176, gnorm=0.238, train_wall=120, gb_free=9, wall=76355
2022-09-08 21:19:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:19:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:19:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:19:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:19:14 | INFO | valid | epoch 579 | valid on 'valid' subset | loss 5.109 | nll_loss 2.508 | mask_loss 10.1163 | p_2 0.04856 | mask_ave 0.616 | ppl 5.69 | bleu 56.24 | wps 1540.5 | wpb 933.5 | bsz 59.6 | num_updates 46818 | best_bleu 57.52
2022-09-08 21:19:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 579 @ 46818 updates
2022-09-08 21:19:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint579.pt
2022-09-08 21:19:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint579.pt
2022-09-08 21:19:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint579.pt (epoch 579 @ 46818 updates, score 56.24) (writing took 21.959571670740843 seconds)
2022-09-08 21:19:37 | INFO | fairseq_cli.train | end of epoch 579 (average epoch stats below)
2022-09-08 21:19:37 | INFO | train | epoch 579 | loss 3.372 | nll_loss 0.338 | mask_loss 9.19069 | p_2 0.03516 | mask_ave 0.498 | ppl 1.26 | wps 3408.1 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 46818 | lr 0.000146148 | gnorm 0.221 | train_wall 96 | gb_free 9.2 | wall 76410
2022-09-08 21:19:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:19:37 | INFO | fairseq.trainer | begin training epoch 580
2022-09-08 21:19:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:21:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:21:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:21:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:21:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:21:27 | INFO | valid | epoch 580 | valid on 'valid' subset | loss 5.099 | nll_loss 2.495 | mask_loss 10.2098 | p_2 0.04861 | mask_ave 0.615 | ppl 5.64 | bleu 56.12 | wps 1537.6 | wpb 933.5 | bsz 59.6 | num_updates 46899 | best_bleu 57.52
2022-09-08 21:21:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 580 @ 46899 updates
2022-09-08 21:21:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint580.pt
2022-09-08 21:21:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint580.pt
2022-09-08 21:21:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint580.pt (epoch 580 @ 46899 updates, score 56.12) (writing took 18.49123102426529 seconds)
2022-09-08 21:21:46 | INFO | fairseq_cli.train | end of epoch 580 (average epoch stats below)
2022-09-08 21:21:46 | INFO | train | epoch 580 | loss 3.372 | nll_loss 0.339 | mask_loss 9.16358 | p_2 0.0351 | mask_ave 0.499 | ppl 1.27 | wps 3470.3 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 46899 | lr 0.000146022 | gnorm 0.265 | train_wall 97 | gb_free 9 | wall 76538
2022-09-08 21:21:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:21:46 | INFO | fairseq.trainer | begin training epoch 581
2022-09-08 21:21:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:21:48 | INFO | train_inner | epoch 581:      1 / 81 loss=3.372, nll_loss=0.339, mask_loss=9.1541, p_2=0.03512, mask_ave=0.498, ppl=1.27, wps=2972.5, ups=0.54, wpb=5508.6, bsz=358.2, num_updates=46900, lr=0.00014602, gnorm=0.254, train_wall=119, gb_free=9.1, wall=76541
2022-09-08 21:23:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:23:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:23:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:23:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:23:35 | INFO | valid | epoch 581 | valid on 'valid' subset | loss 5.093 | nll_loss 2.491 | mask_loss 10.0262 | p_2 0.04858 | mask_ave 0.615 | ppl 5.62 | bleu 56.57 | wps 1485.7 | wpb 933.5 | bsz 59.6 | num_updates 46980 | best_bleu 57.52
2022-09-08 21:23:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 581 @ 46980 updates
2022-09-08 21:23:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint581.pt
2022-09-08 21:23:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint581.pt
2022-09-08 21:23:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint581.pt (epoch 581 @ 46980 updates, score 56.57) (writing took 16.13627080619335 seconds)
2022-09-08 21:23:52 | INFO | fairseq_cli.train | end of epoch 581 (average epoch stats below)
2022-09-08 21:23:52 | INFO | train | epoch 581 | loss 3.372 | nll_loss 0.339 | mask_loss 9.26042 | p_2 0.03509 | mask_ave 0.499 | ppl 1.26 | wps 3546.3 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 46980 | lr 0.000145896 | gnorm 0.226 | train_wall 97 | gb_free 9.2 | wall 76665
2022-09-08 21:23:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:23:52 | INFO | fairseq.trainer | begin training epoch 582
2022-09-08 21:23:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:24:16 | INFO | train_inner | epoch 582:     20 / 81 loss=3.372, nll_loss=0.338, mask_loss=9.24071, p_2=0.03524, mask_ave=0.501, ppl=1.26, wps=3717.8, ups=0.67, wpb=5521.4, bsz=360.2, num_updates=47000, lr=0.000145865, gnorm=0.229, train_wall=119, gb_free=9.2, wall=76689
2022-09-08 21:25:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:25:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:25:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:25:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:25:41 | INFO | valid | epoch 582 | valid on 'valid' subset | loss 5.099 | nll_loss 2.494 | mask_loss 10.3085 | p_2 0.04873 | mask_ave 0.611 | ppl 5.63 | bleu 56.9 | wps 1576.5 | wpb 933.5 | bsz 59.6 | num_updates 47061 | best_bleu 57.52
2022-09-08 21:25:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 582 @ 47061 updates
2022-09-08 21:25:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint582.pt
2022-09-08 21:25:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint582.pt
2022-09-08 21:25:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint582.pt (epoch 582 @ 47061 updates, score 56.9) (writing took 2.6283566243946552 seconds)
2022-09-08 21:25:44 | INFO | fairseq_cli.train | end of epoch 582 (average epoch stats below)
2022-09-08 21:25:44 | INFO | train | epoch 582 | loss 3.372 | nll_loss 0.339 | mask_loss 9.25014 | p_2 0.03515 | mask_ave 0.498 | ppl 1.26 | wps 3994.7 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 47061 | lr 0.00014577 | gnorm 0.238 | train_wall 97 | gb_free 9.2 | wall 76777
2022-09-08 21:25:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:25:44 | INFO | fairseq.trainer | begin training epoch 583
2022-09-08 21:25:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:26:31 | INFO | train_inner | epoch 583:     39 / 81 loss=3.372, nll_loss=0.339, mask_loss=9.2813, p_2=0.03533, mask_ave=0.497, ppl=1.26, wps=4092.1, ups=0.74, wpb=5524.2, bsz=359, num_updates=47100, lr=0.00014571, gnorm=0.247, train_wall=120, gb_free=9, wall=76824
2022-09-08 21:27:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:27:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:27:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:27:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:27:34 | INFO | valid | epoch 583 | valid on 'valid' subset | loss 5.098 | nll_loss 2.492 | mask_loss 10.1544 | p_2 0.04866 | mask_ave 0.613 | ppl 5.63 | bleu 56.53 | wps 1550.2 | wpb 933.5 | bsz 59.6 | num_updates 47142 | best_bleu 57.52
2022-09-08 21:27:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 583 @ 47142 updates
2022-09-08 21:27:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint583.pt
2022-09-08 21:27:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint583.pt
2022-09-08 21:28:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint583.pt (epoch 583 @ 47142 updates, score 56.53) (writing took 30.679726596921682 seconds)
2022-09-08 21:28:05 | INFO | fairseq_cli.train | end of epoch 583 (average epoch stats below)
2022-09-08 21:28:05 | INFO | train | epoch 583 | loss 3.372 | nll_loss 0.339 | mask_loss 9.25219 | p_2 0.03522 | mask_ave 0.496 | ppl 1.26 | wps 3176.3 | ups 0.58 | wpb 5523.2 | bsz 358 | num_updates 47142 | lr 0.000145645 | gnorm 0.263 | train_wall 97 | gb_free 9.1 | wall 76917
2022-09-08 21:28:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:28:05 | INFO | fairseq.trainer | begin training epoch 584
2022-09-08 21:28:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:29:16 | INFO | train_inner | epoch 584:     58 / 81 loss=3.372, nll_loss=0.339, mask_loss=9.28793, p_2=0.03466, mask_ave=0.497, ppl=1.26, wps=3370.4, ups=0.61, wpb=5550.7, bsz=356.2, num_updates=47200, lr=0.000145556, gnorm=0.26, train_wall=121, gb_free=9.1, wall=76989
2022-09-08 21:29:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:29:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:29:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:29:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:29:55 | INFO | valid | epoch 584 | valid on 'valid' subset | loss 5.1 | nll_loss 2.498 | mask_loss 9.99369 | p_2 0.04846 | mask_ave 0.619 | ppl 5.65 | bleu 56.56 | wps 1551.4 | wpb 933.5 | bsz 59.6 | num_updates 47223 | best_bleu 57.52
2022-09-08 21:29:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 584 @ 47223 updates
2022-09-08 21:29:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint584.pt
2022-09-08 21:29:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint584.pt
2022-09-08 21:29:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint584.pt (epoch 584 @ 47223 updates, score 56.56) (writing took 2.592069383710623 seconds)
2022-09-08 21:29:57 | INFO | fairseq_cli.train | end of epoch 584 (average epoch stats below)
2022-09-08 21:29:57 | INFO | train | epoch 584 | loss 3.372 | nll_loss 0.339 | mask_loss 9.24539 | p_2 0.03504 | mask_ave 0.501 | ppl 1.26 | wps 3970.8 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 47223 | lr 0.00014552 | gnorm 0.265 | train_wall 97 | gb_free 9.2 | wall 77030
2022-09-08 21:29:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:29:57 | INFO | fairseq.trainer | begin training epoch 585
2022-09-08 21:29:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:31:32 | INFO | train_inner | epoch 585:     77 / 81 loss=3.372, nll_loss=0.339, mask_loss=9.1695, p_2=0.03522, mask_ave=0.5, ppl=1.27, wps=4074.2, ups=0.74, wpb=5521, bsz=358.2, num_updates=47300, lr=0.000145402, gnorm=0.254, train_wall=120, gb_free=9, wall=77125
2022-09-08 21:31:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:31:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:31:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:31:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:31:47 | INFO | valid | epoch 585 | valid on 'valid' subset | loss 5.11 | nll_loss 2.51 | mask_loss 9.99182 | p_2 0.04836 | mask_ave 0.621 | ppl 5.69 | bleu 56.21 | wps 1505.2 | wpb 933.5 | bsz 59.6 | num_updates 47304 | best_bleu 57.52
2022-09-08 21:31:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 585 @ 47304 updates
2022-09-08 21:31:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint585.pt
2022-09-08 21:31:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint585.pt
2022-09-08 21:32:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint585.pt (epoch 585 @ 47304 updates, score 56.21) (writing took 26.13966754823923 seconds)
2022-09-08 21:32:14 | INFO | fairseq_cli.train | end of epoch 585 (average epoch stats below)
2022-09-08 21:32:14 | INFO | train | epoch 585 | loss 3.372 | nll_loss 0.339 | mask_loss 9.20667 | p_2 0.0351 | mask_ave 0.499 | ppl 1.26 | wps 3279.2 | ups 0.59 | wpb 5523.2 | bsz 358 | num_updates 47304 | lr 0.000145396 | gnorm 0.245 | train_wall 97 | gb_free 9.3 | wall 77167
2022-09-08 21:32:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:32:14 | INFO | fairseq.trainer | begin training epoch 586
2022-09-08 21:32:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:33:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:33:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:33:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:33:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:34:04 | INFO | valid | epoch 586 | valid on 'valid' subset | loss 5.103 | nll_loss 2.503 | mask_loss 10.0891 | p_2 0.04872 | mask_ave 0.611 | ppl 5.67 | bleu 56.69 | wps 1464.4 | wpb 933.5 | bsz 59.6 | num_updates 47385 | best_bleu 57.52
2022-09-08 21:34:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 586 @ 47385 updates
2022-09-08 21:34:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint586.pt
2022-09-08 21:34:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint586.pt
2022-09-08 21:34:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint586.pt (epoch 586 @ 47385 updates, score 56.69) (writing took 16.598778922110796 seconds)
2022-09-08 21:34:21 | INFO | fairseq_cli.train | end of epoch 586 (average epoch stats below)
2022-09-08 21:34:21 | INFO | train | epoch 586 | loss 3.371 | nll_loss 0.338 | mask_loss 9.18703 | p_2 0.03511 | mask_ave 0.499 | ppl 1.26 | wps 3519.9 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 47385 | lr 0.000145271 | gnorm 0.263 | train_wall 97 | gb_free 9.2 | wall 77294
2022-09-08 21:34:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:34:21 | INFO | fairseq.trainer | begin training epoch 587
2022-09-08 21:34:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:34:40 | INFO | train_inner | epoch 587:     15 / 81 loss=3.372, nll_loss=0.339, mask_loss=9.18109, p_2=0.03511, mask_ave=0.498, ppl=1.26, wps=2925.1, ups=0.53, wpb=5507.8, bsz=356.9, num_updates=47400, lr=0.000145248, gnorm=0.261, train_wall=119, gb_free=9, wall=77313
2022-09-08 21:35:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:36:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:36:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:36:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:36:11 | INFO | valid | epoch 587 | valid on 'valid' subset | loss 5.087 | nll_loss 2.485 | mask_loss 9.92043 | p_2 0.04864 | mask_ave 0.614 | ppl 5.6 | bleu 57.34 | wps 1496.6 | wpb 933.5 | bsz 59.6 | num_updates 47466 | best_bleu 57.52
2022-09-08 21:36:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 587 @ 47466 updates
2022-09-08 21:36:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint587.pt
2022-09-08 21:36:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint587.pt
2022-09-08 21:36:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint587.pt (epoch 587 @ 47466 updates, score 57.34) (writing took 28.220843207091093 seconds)
2022-09-08 21:36:39 | INFO | fairseq_cli.train | end of epoch 587 (average epoch stats below)
2022-09-08 21:36:39 | INFO | train | epoch 587 | loss 3.372 | nll_loss 0.339 | mask_loss 9.1773 | p_2 0.03516 | mask_ave 0.497 | ppl 1.27 | wps 3237.5 | ups 0.59 | wpb 5523.2 | bsz 358 | num_updates 47466 | lr 0.000145147 | gnorm 0.253 | train_wall 97 | gb_free 9.2 | wall 77432
2022-09-08 21:36:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:36:39 | INFO | fairseq.trainer | begin training epoch 588
2022-09-08 21:36:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:37:20 | INFO | train_inner | epoch 588:     34 / 81 loss=3.372, nll_loss=0.339, mask_loss=9.17547, p_2=0.03511, mask_ave=0.497, ppl=1.26, wps=3460.3, ups=0.62, wpb=5554.6, bsz=360.5, num_updates=47500, lr=0.000145095, gnorm=0.254, train_wall=119, gb_free=9, wall=77473
2022-09-08 21:38:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:38:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:38:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:38:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:38:28 | INFO | valid | epoch 588 | valid on 'valid' subset | loss 5.104 | nll_loss 2.504 | mask_loss 10.0128 | p_2 0.04879 | mask_ave 0.609 | ppl 5.67 | bleu 56.67 | wps 1516.7 | wpb 933.5 | bsz 59.6 | num_updates 47547 | best_bleu 57.52
2022-09-08 21:38:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 588 @ 47547 updates
2022-09-08 21:38:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint588.pt
2022-09-08 21:38:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint588.pt
2022-09-08 21:38:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint588.pt (epoch 588 @ 47547 updates, score 56.67) (writing took 26.505201887339354 seconds)
2022-09-08 21:38:55 | INFO | fairseq_cli.train | end of epoch 588 (average epoch stats below)
2022-09-08 21:38:55 | INFO | train | epoch 588 | loss 3.372 | nll_loss 0.339 | mask_loss 9.21193 | p_2 0.03516 | mask_ave 0.497 | ppl 1.27 | wps 3292.7 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 47547 | lr 0.000145024 | gnorm 0.291 | train_wall 96 | gb_free 9.3 | wall 77568
2022-09-08 21:38:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:38:55 | INFO | fairseq.trainer | begin training epoch 589
2022-09-08 21:38:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:40:00 | INFO | train_inner | epoch 589:     53 / 81 loss=3.373, nll_loss=0.34, mask_loss=9.23389, p_2=0.03502, mask_ave=0.496, ppl=1.27, wps=3461.9, ups=0.63, wpb=5509, bsz=354.2, num_updates=47600, lr=0.000144943, gnorm=0.306, train_wall=120, gb_free=9, wall=77632
2022-09-08 21:40:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:40:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:40:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:40:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:40:43 | INFO | valid | epoch 589 | valid on 'valid' subset | loss 5.113 | nll_loss 2.516 | mask_loss 10.0426 | p_2 0.04878 | mask_ave 0.61 | ppl 5.72 | bleu 56.59 | wps 1553.8 | wpb 933.5 | bsz 59.6 | num_updates 47628 | best_bleu 57.52
2022-09-08 21:40:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 589 @ 47628 updates
2022-09-08 21:40:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint589.pt
2022-09-08 21:40:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint589.pt
2022-09-08 21:41:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint589.pt (epoch 589 @ 47628 updates, score 56.59) (writing took 18.31186557933688 seconds)
2022-09-08 21:41:02 | INFO | fairseq_cli.train | end of epoch 589 (average epoch stats below)
2022-09-08 21:41:02 | INFO | train | epoch 589 | loss 3.373 | nll_loss 0.34 | mask_loss 9.17577 | p_2 0.03519 | mask_ave 0.496 | ppl 1.27 | wps 3523.9 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 47628 | lr 0.0001449 | gnorm 0.29 | train_wall 96 | gb_free 9.2 | wall 77695
2022-09-08 21:41:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:41:02 | INFO | fairseq.trainer | begin training epoch 590
2022-09-08 21:41:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:42:30 | INFO | train_inner | epoch 590:     72 / 81 loss=3.372, nll_loss=0.339, mask_loss=9.13726, p_2=0.03556, mask_ave=0.495, ppl=1.26, wps=3660.3, ups=0.66, wpb=5510.4, bsz=359.1, num_updates=47700, lr=0.000144791, gnorm=0.251, train_wall=119, gb_free=9.1, wall=77783
2022-09-08 21:42:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:42:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:42:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:42:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:42:52 | INFO | valid | epoch 590 | valid on 'valid' subset | loss 5.091 | nll_loss 2.49 | mask_loss 9.92278 | p_2 0.0488 | mask_ave 0.609 | ppl 5.62 | bleu 56.64 | wps 1493.5 | wpb 933.5 | bsz 59.6 | num_updates 47709 | best_bleu 57.52
2022-09-08 21:42:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 590 @ 47709 updates
2022-09-08 21:42:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint590.pt
2022-09-08 21:42:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint590.pt
2022-09-08 21:43:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint590.pt (epoch 590 @ 47709 updates, score 56.64) (writing took 19.065658017992973 seconds)
2022-09-08 21:43:11 | INFO | fairseq_cli.train | end of epoch 590 (average epoch stats below)
2022-09-08 21:43:11 | INFO | train | epoch 590 | loss 3.372 | nll_loss 0.338 | mask_loss 9.15102 | p_2 0.03534 | mask_ave 0.492 | ppl 1.26 | wps 3463.8 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 47709 | lr 0.000144777 | gnorm 0.242 | train_wall 97 | gb_free 9.2 | wall 77824
2022-09-08 21:43:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:43:11 | INFO | fairseq.trainer | begin training epoch 591
2022-09-08 21:43:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:44:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:44:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:44:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:44:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:45:00 | INFO | valid | epoch 591 | valid on 'valid' subset | loss 5.088 | nll_loss 2.482 | mask_loss 10.1025 | p_2 0.04855 | mask_ave 0.616 | ppl 5.59 | bleu 56.8 | wps 1497.4 | wpb 933.5 | bsz 59.6 | num_updates 47790 | best_bleu 57.52
2022-09-08 21:45:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 591 @ 47790 updates
2022-09-08 21:45:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint591.pt
2022-09-08 21:45:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint591.pt
2022-09-08 21:45:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint591.pt (epoch 591 @ 47790 updates, score 56.8) (writing took 23.95545281842351 seconds)
2022-09-08 21:45:24 | INFO | fairseq_cli.train | end of epoch 591 (average epoch stats below)
2022-09-08 21:45:24 | INFO | train | epoch 591 | loss 3.372 | nll_loss 0.339 | mask_loss 9.18884 | p_2 0.0353 | mask_ave 0.493 | ppl 1.26 | wps 3355.3 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 47790 | lr 0.000144654 | gnorm 0.262 | train_wall 96 | gb_free 9.1 | wall 77957
2022-09-08 21:45:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:45:25 | INFO | fairseq.trainer | begin training epoch 592
2022-09-08 21:45:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:45:37 | INFO | train_inner | epoch 592:     10 / 81 loss=3.372, nll_loss=0.338, mask_loss=9.17602, p_2=0.03526, mask_ave=0.492, ppl=1.26, wps=2949.9, ups=0.53, wpb=5518.5, bsz=357.6, num_updates=47800, lr=0.000144639, gnorm=0.259, train_wall=118, gb_free=9.1, wall=77970
2022-09-08 21:47:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:47:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:47:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:47:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:47:13 | INFO | valid | epoch 592 | valid on 'valid' subset | loss 5.089 | nll_loss 2.487 | mask_loss 9.94268 | p_2 0.04831 | mask_ave 0.623 | ppl 5.61 | bleu 56.91 | wps 1513.9 | wpb 933.5 | bsz 59.6 | num_updates 47871 | best_bleu 57.52
2022-09-08 21:47:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 592 @ 47871 updates
2022-09-08 21:47:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint592.pt
2022-09-08 21:47:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint592.pt
2022-09-08 21:47:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint592.pt (epoch 592 @ 47871 updates, score 56.91) (writing took 16.521174505352974 seconds)
2022-09-08 21:47:30 | INFO | fairseq_cli.train | end of epoch 592 (average epoch stats below)
2022-09-08 21:47:30 | INFO | train | epoch 592 | loss 3.372 | nll_loss 0.338 | mask_loss 9.0871 | p_2 0.03519 | mask_ave 0.497 | ppl 1.26 | wps 3557.5 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 47871 | lr 0.000144532 | gnorm 0.233 | train_wall 96 | gb_free 9.1 | wall 78083
2022-09-08 21:47:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:47:30 | INFO | fairseq.trainer | begin training epoch 593
2022-09-08 21:47:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:48:06 | INFO | train_inner | epoch 593:     29 / 81 loss=3.371, nll_loss=0.338, mask_loss=9.07569, p_2=0.03498, mask_ave=0.496, ppl=1.26, wps=3732.5, ups=0.67, wpb=5551, bsz=359.4, num_updates=47900, lr=0.000144488, gnorm=0.24, train_wall=119, gb_free=9, wall=78119
2022-09-08 21:49:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:49:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:49:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:49:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:49:20 | INFO | valid | epoch 593 | valid on 'valid' subset | loss 5.084 | nll_loss 2.481 | mask_loss 9.97353 | p_2 0.04879 | mask_ave 0.608 | ppl 5.58 | bleu 56.8 | wps 1517.7 | wpb 933.5 | bsz 59.6 | num_updates 47952 | best_bleu 57.52
2022-09-08 21:49:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 593 @ 47952 updates
2022-09-08 21:49:20 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint593.pt
2022-09-08 21:49:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint593.pt
2022-09-08 21:49:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint593.pt (epoch 593 @ 47952 updates, score 56.8) (writing took 15.985236540436745 seconds)
2022-09-08 21:49:36 | INFO | fairseq_cli.train | end of epoch 593 (average epoch stats below)
2022-09-08 21:49:36 | INFO | train | epoch 593 | loss 3.372 | nll_loss 0.339 | mask_loss 9.08476 | p_2 0.03519 | mask_ave 0.496 | ppl 1.27 | wps 3544.6 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 47952 | lr 0.00014441 | gnorm 0.262 | train_wall 97 | gb_free 9.1 | wall 78209
2022-09-08 21:49:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:49:37 | INFO | fairseq.trainer | begin training epoch 594
2022-09-08 21:49:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:50:36 | INFO | train_inner | epoch 594:     48 / 81 loss=3.372, nll_loss=0.339, mask_loss=9.06181, p_2=0.03542, mask_ave=0.496, ppl=1.27, wps=3685.7, ups=0.67, wpb=5512.3, bsz=358.7, num_updates=48000, lr=0.000144338, gnorm=0.25, train_wall=121, gb_free=9.1, wall=78268
2022-09-08 21:51:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:51:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:51:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:51:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:51:27 | INFO | valid | epoch 594 | valid on 'valid' subset | loss 5.09 | nll_loss 2.483 | mask_loss 10.0068 | p_2 0.04846 | mask_ave 0.619 | ppl 5.59 | bleu 57.05 | wps 1511.1 | wpb 933.5 | bsz 59.6 | num_updates 48033 | best_bleu 57.52
2022-09-08 21:51:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 594 @ 48033 updates
2022-09-08 21:51:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint594.pt
2022-09-08 21:51:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint594.pt
2022-09-08 21:51:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint594.pt (epoch 594 @ 48033 updates, score 57.05) (writing took 15.818845372647047 seconds)
2022-09-08 21:51:43 | INFO | fairseq_cli.train | end of epoch 594 (average epoch stats below)
2022-09-08 21:51:43 | INFO | train | epoch 594 | loss 3.372 | nll_loss 0.339 | mask_loss 9.03993 | p_2 0.03526 | mask_ave 0.494 | ppl 1.26 | wps 3532.5 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 48033 | lr 0.000144288 | gnorm 0.235 | train_wall 98 | gb_free 9.1 | wall 78336
2022-09-08 21:51:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:51:43 | INFO | fairseq.trainer | begin training epoch 595
2022-09-08 21:51:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:53:05 | INFO | train_inner | epoch 595:     67 / 81 loss=3.372, nll_loss=0.339, mask_loss=9.08921, p_2=0.03522, mask_ave=0.497, ppl=1.26, wps=3685, ups=0.67, wpb=5510, bsz=356.3, num_updates=48100, lr=0.000144187, gnorm=0.244, train_wall=120, gb_free=9.1, wall=78418
2022-09-08 21:53:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:53:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:53:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:53:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:53:33 | INFO | valid | epoch 595 | valid on 'valid' subset | loss 5.093 | nll_loss 2.488 | mask_loss 9.93602 | p_2 0.04856 | mask_ave 0.616 | ppl 5.61 | bleu 56.48 | wps 1487.1 | wpb 933.5 | bsz 59.6 | num_updates 48114 | best_bleu 57.52
2022-09-08 21:53:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 595 @ 48114 updates
2022-09-08 21:53:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint595.pt
2022-09-08 21:53:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint595.pt
2022-09-08 21:53:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint595.pt (epoch 595 @ 48114 updates, score 56.48) (writing took 16.54053743183613 seconds)
2022-09-08 21:53:50 | INFO | fairseq_cli.train | end of epoch 595 (average epoch stats below)
2022-09-08 21:53:50 | INFO | train | epoch 595 | loss 3.372 | nll_loss 0.339 | mask_loss 9.11477 | p_2 0.03516 | mask_ave 0.497 | ppl 1.26 | wps 3527.9 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 48114 | lr 0.000144166 | gnorm 0.242 | train_wall 97 | gb_free 9.1 | wall 78463
2022-09-08 21:53:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:53:50 | INFO | fairseq.trainer | begin training epoch 596
2022-09-08 21:53:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:55:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:55:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:55:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:55:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:55:40 | INFO | valid | epoch 596 | valid on 'valid' subset | loss 5.108 | nll_loss 2.506 | mask_loss 10.0239 | p_2 0.04863 | mask_ave 0.614 | ppl 5.68 | bleu 56.48 | wps 1497.6 | wpb 933.5 | bsz 59.6 | num_updates 48195 | best_bleu 57.52
2022-09-08 21:55:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 596 @ 48195 updates
2022-09-08 21:55:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint596.pt
2022-09-08 21:55:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint596.pt
2022-09-08 21:55:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint596.pt (epoch 596 @ 48195 updates, score 56.48) (writing took 15.912928611040115 seconds)
2022-09-08 21:55:56 | INFO | fairseq_cli.train | end of epoch 596 (average epoch stats below)
2022-09-08 21:55:56 | INFO | train | epoch 596 | loss 3.371 | nll_loss 0.338 | mask_loss 9.05671 | p_2 0.03527 | mask_ave 0.494 | ppl 1.26 | wps 3544.6 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 48195 | lr 0.000144045 | gnorm 0.247 | train_wall 97 | gb_free 9.1 | wall 78589
2022-09-08 21:55:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:55:56 | INFO | fairseq.trainer | begin training epoch 597
2022-09-08 21:55:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:56:03 | INFO | train_inner | epoch 597:      5 / 81 loss=3.371, nll_loss=0.338, mask_loss=9.06838, p_2=0.03531, mask_ave=0.495, ppl=1.26, wps=3097.7, ups=0.56, wpb=5509.6, bsz=359.1, num_updates=48200, lr=0.000144038, gnorm=0.241, train_wall=119, gb_free=9.2, wall=78596
2022-09-08 21:57:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:57:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:57:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:57:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 21:57:45 | INFO | valid | epoch 597 | valid on 'valid' subset | loss 5.101 | nll_loss 2.499 | mask_loss 9.9946 | p_2 0.04816 | mask_ave 0.627 | ppl 5.65 | bleu 56.24 | wps 1516.2 | wpb 933.5 | bsz 59.6 | num_updates 48276 | best_bleu 57.52
2022-09-08 21:57:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 597 @ 48276 updates
2022-09-08 21:57:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint597.pt
2022-09-08 21:57:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint597.pt
2022-09-08 21:58:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint597.pt (epoch 597 @ 48276 updates, score 56.24) (writing took 27.931437216699123 seconds)
2022-09-08 21:58:14 | INFO | fairseq_cli.train | end of epoch 597 (average epoch stats below)
2022-09-08 21:58:14 | INFO | train | epoch 597 | loss 3.372 | nll_loss 0.339 | mask_loss 9.21316 | p_2 0.03528 | mask_ave 0.494 | ppl 1.26 | wps 3251.7 | ups 0.59 | wpb 5523.2 | bsz 358 | num_updates 48276 | lr 0.000143924 | gnorm 0.25 | train_wall 97 | gb_free 9.2 | wall 78726
2022-09-08 21:58:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 21:58:14 | INFO | fairseq.trainer | begin training epoch 598
2022-09-08 21:58:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 21:58:43 | INFO | train_inner | epoch 598:     24 / 81 loss=3.372, nll_loss=0.339, mask_loss=9.18789, p_2=0.03522, mask_ave=0.496, ppl=1.26, wps=3447.6, ups=0.62, wpb=5526.5, bsz=358.7, num_updates=48300, lr=0.000143889, gnorm=0.256, train_wall=119, gb_free=9.1, wall=78756
2022-09-08 21:59:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 21:59:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 21:59:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 21:59:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 22:00:04 | INFO | valid | epoch 598 | valid on 'valid' subset | loss 5.098 | nll_loss 2.492 | mask_loss 10.0073 | p_2 0.04829 | mask_ave 0.623 | ppl 5.63 | bleu 56.75 | wps 1466.2 | wpb 933.5 | bsz 59.6 | num_updates 48357 | best_bleu 57.52
2022-09-08 22:00:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 598 @ 48357 updates
2022-09-08 22:00:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint598.pt
2022-09-08 22:00:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint598.pt
2022-09-08 22:00:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint598.pt (epoch 598 @ 48357 updates, score 56.75) (writing took 17.125719286501408 seconds)
2022-09-08 22:00:21 | INFO | fairseq_cli.train | end of epoch 598 (average epoch stats below)
2022-09-08 22:00:21 | INFO | train | epoch 598 | loss 3.372 | nll_loss 0.339 | mask_loss 9.13059 | p_2 0.03511 | mask_ave 0.499 | ppl 1.27 | wps 3502.6 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 48357 | lr 0.000143804 | gnorm 0.245 | train_wall 97 | gb_free 9.2 | wall 78854
2022-09-08 22:00:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 22:00:22 | INFO | fairseq.trainer | begin training epoch 599
2022-09-08 22:00:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 22:01:15 | INFO | train_inner | epoch 599:     43 / 81 loss=3.371, nll_loss=0.338, mask_loss=9.04639, p_2=0.0352, mask_ave=0.498, ppl=1.26, wps=3652, ups=0.66, wpb=5526.1, bsz=359.2, num_updates=48400, lr=0.00014374, gnorm=0.237, train_wall=121, gb_free=9.1, wall=78907
2022-09-08 22:02:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 22:02:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 22:02:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 22:02:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 22:02:13 | INFO | valid | epoch 599 | valid on 'valid' subset | loss 5.081 | nll_loss 2.473 | mask_loss 9.87485 | p_2 0.04839 | mask_ave 0.621 | ppl 5.55 | bleu 56.79 | wps 1432.3 | wpb 933.5 | bsz 59.6 | num_updates 48438 | best_bleu 57.52
2022-09-08 22:02:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 599 @ 48438 updates
2022-09-08 22:02:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint599.pt
2022-09-08 22:02:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint599.pt
2022-09-08 22:02:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint599.pt (epoch 599 @ 48438 updates, score 56.79) (writing took 2.5136389434337616 seconds)
2022-09-08 22:02:16 | INFO | fairseq_cli.train | end of epoch 599 (average epoch stats below)
2022-09-08 22:02:16 | INFO | train | epoch 599 | loss 3.371 | nll_loss 0.338 | mask_loss 8.94069 | p_2 0.03513 | mask_ave 0.498 | ppl 1.26 | wps 3917.2 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 48438 | lr 0.000143684 | gnorm 0.245 | train_wall 98 | gb_free 9.1 | wall 78968
2022-09-08 22:02:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 22:02:16 | INFO | fairseq.trainer | begin training epoch 600
2022-09-08 22:02:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 22:03:32 | INFO | train_inner | epoch 600:     62 / 81 loss=3.372, nll_loss=0.339, mask_loss=9.00792, p_2=0.03503, mask_ave=0.497, ppl=1.26, wps=4030.3, ups=0.73, wpb=5531.8, bsz=354.2, num_updates=48500, lr=0.000143592, gnorm=0.246, train_wall=121, gb_free=9.1, wall=79045
2022-09-08 22:03:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 22:03:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 22:03:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 22:03:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 22:04:06 | INFO | valid | epoch 600 | valid on 'valid' subset | loss 5.093 | nll_loss 2.486 | mask_loss 10.059 | p_2 0.04862 | mask_ave 0.614 | ppl 5.6 | bleu 56.18 | wps 1562.2 | wpb 933.5 | bsz 59.6 | num_updates 48519 | best_bleu 57.52
2022-09-08 22:04:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 600 @ 48519 updates
2022-09-08 22:04:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint600.pt
2022-09-08 22:04:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint600.pt
2022-09-08 22:04:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint600.pt (epoch 600 @ 48519 updates, score 56.18) (writing took 15.052584741264582 seconds)
2022-09-08 22:04:21 | INFO | fairseq_cli.train | end of epoch 600 (average epoch stats below)
2022-09-08 22:04:21 | INFO | train | epoch 600 | loss 3.371 | nll_loss 0.338 | mask_loss 9.00328 | p_2 0.0352 | mask_ave 0.496 | ppl 1.26 | wps 3571.4 | ups 0.65 | wpb 5523.2 | bsz 358 | num_updates 48519 | lr 0.000143564 | gnorm 0.241 | train_wall 97 | gb_free 9.1 | wall 79094
2022-09-08 22:04:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 22:04:21 | INFO | fairseq.trainer | begin training epoch 601
2022-09-08 22:04:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 22:05:58 | INFO | train_inner | epoch 601:     81 / 81 loss=3.372, nll_loss=0.339, mask_loss=9.04124, p_2=0.03527, mask_ave=0.495, ppl=1.26, wps=3759.9, ups=0.68, wpb=5512.1, bsz=359, num_updates=48600, lr=0.000143444, gnorm=0.249, train_wall=119, gb_free=9.1, wall=79191
2022-09-08 22:05:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 22:06:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 22:06:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 22:06:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 22:06:10 | INFO | valid | epoch 601 | valid on 'valid' subset | loss 5.103 | nll_loss 2.501 | mask_loss 10.0632 | p_2 0.04856 | mask_ave 0.615 | ppl 5.66 | bleu 55.79 | wps 1533.8 | wpb 933.5 | bsz 59.6 | num_updates 48600 | best_bleu 57.52
2022-09-08 22:06:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 601 @ 48600 updates
2022-09-08 22:06:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint601.pt
2022-09-08 22:06:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint601.pt
2022-09-08 22:06:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint601.pt (epoch 601 @ 48600 updates, score 55.79) (writing took 17.798629216849804 seconds)
2022-09-08 22:06:27 | INFO | fairseq_cli.train | end of epoch 601 (average epoch stats below)
2022-09-08 22:06:27 | INFO | train | epoch 601 | loss 3.372 | nll_loss 0.339 | mask_loss 9.06455 | p_2 0.03525 | mask_ave 0.495 | ppl 1.26 | wps 3532.2 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 48600 | lr 0.000143444 | gnorm 0.252 | train_wall 96 | gb_free 9.1 | wall 79220
2022-09-08 22:06:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 22:06:28 | INFO | fairseq.trainer | begin training epoch 602
2022-09-08 22:06:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 22:08:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 22:08:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 22:08:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 22:08:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 22:08:17 | INFO | valid | epoch 602 | valid on 'valid' subset | loss 5.104 | nll_loss 2.497 | mask_loss 10.0517 | p_2 0.04808 | mask_ave 0.629 | ppl 5.65 | bleu 56.57 | wps 1528.9 | wpb 933.5 | bsz 59.6 | num_updates 48681 | best_bleu 57.52
2022-09-08 22:08:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 602 @ 48681 updates
2022-09-08 22:08:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint602.pt
2022-09-08 22:08:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint602.pt
2022-09-08 22:08:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint602.pt (epoch 602 @ 48681 updates, score 56.57) (writing took 15.315519779920578 seconds)
2022-09-08 22:08:33 | INFO | fairseq_cli.train | end of epoch 602 (average epoch stats below)
2022-09-08 22:08:33 | INFO | train | epoch 602 | loss 3.371 | nll_loss 0.338 | mask_loss 9.23617 | p_2 0.03515 | mask_ave 0.498 | ppl 1.26 | wps 3575.8 | ups 0.65 | wpb 5523.2 | bsz 358 | num_updates 48681 | lr 0.000143324 | gnorm 0.233 | train_wall 97 | gb_free 9.2 | wall 79345
2022-09-08 22:08:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 22:08:33 | INFO | fairseq.trainer | begin training epoch 603
2022-09-08 22:08:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 22:08:56 | INFO | train_inner | epoch 603:     19 / 81 loss=3.371, nll_loss=0.338, mask_loss=9.21334, p_2=0.03517, mask_ave=0.498, ppl=1.26, wps=3109.3, ups=0.56, wpb=5532.8, bsz=361.4, num_updates=48700, lr=0.000143296, gnorm=0.238, train_wall=119, gb_free=9.1, wall=79369
2022-09-08 22:10:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 22:10:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 22:10:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 22:10:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 22:10:23 | INFO | valid | epoch 603 | valid on 'valid' subset | loss 5.105 | nll_loss 2.503 | mask_loss 10.2029 | p_2 0.04837 | mask_ave 0.621 | ppl 5.67 | bleu 56.32 | wps 1463.4 | wpb 933.5 | bsz 59.6 | num_updates 48762 | best_bleu 57.52
2022-09-08 22:10:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 603 @ 48762 updates
2022-09-08 22:10:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint603.pt
2022-09-08 22:10:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint603.pt
2022-09-08 22:10:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint603.pt (epoch 603 @ 48762 updates, score 56.32) (writing took 16.52768537774682 seconds)
2022-09-08 22:10:39 | INFO | fairseq_cli.train | end of epoch 603 (average epoch stats below)
2022-09-08 22:10:39 | INFO | train | epoch 603 | loss 3.371 | nll_loss 0.338 | mask_loss 9.22737 | p_2 0.03506 | mask_ave 0.5 | ppl 1.26 | wps 3529.7 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 48762 | lr 0.000143205 | gnorm 0.248 | train_wall 97 | gb_free 9.1 | wall 79472
2022-09-08 22:10:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 22:10:40 | INFO | fairseq.trainer | begin training epoch 604
2022-09-08 22:10:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 22:11:26 | INFO | train_inner | epoch 604:     38 / 81 loss=3.371, nll_loss=0.338, mask_loss=9.27998, p_2=0.03494, mask_ave=0.496, ppl=1.26, wps=3703, ups=0.67, wpb=5543.2, bsz=356.3, num_updates=48800, lr=0.00014315, gnorm=0.24, train_wall=120, gb_free=9.1, wall=79519
2022-09-08 22:12:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 22:12:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 22:12:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 22:12:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 22:12:30 | INFO | valid | epoch 604 | valid on 'valid' subset | loss 5.1 | nll_loss 2.5 | mask_loss 9.93995 | p_2 0.04834 | mask_ave 0.623 | ppl 5.66 | bleu 56.33 | wps 1475 | wpb 933.5 | bsz 59.6 | num_updates 48843 | best_bleu 57.52
2022-09-08 22:12:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 604 @ 48843 updates
2022-09-08 22:12:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint604.pt
2022-09-08 22:12:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint604.pt
2022-09-08 22:12:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint604.pt (epoch 604 @ 48843 updates, score 56.33) (writing took 16.39149608463049 seconds)
2022-09-08 22:12:46 | INFO | fairseq_cli.train | end of epoch 604 (average epoch stats below)
2022-09-08 22:12:46 | INFO | train | epoch 604 | loss 3.371 | nll_loss 0.338 | mask_loss 9.20977 | p_2 0.03518 | mask_ave 0.497 | ppl 1.26 | wps 3520 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 48843 | lr 0.000143087 | gnorm 0.25 | train_wall 97 | gb_free 9.1 | wall 79599
2022-09-08 22:12:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 22:12:47 | INFO | fairseq.trainer | begin training epoch 605
2022-09-08 22:12:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 22:13:55 | INFO | train_inner | epoch 605:     57 / 81 loss=3.372, nll_loss=0.339, mask_loss=9.18864, p_2=0.03503, mask_ave=0.501, ppl=1.27, wps=3693.6, ups=0.67, wpb=5509.1, bsz=353.7, num_updates=48900, lr=0.000143003, gnorm=0.272, train_wall=120, gb_free=9.2, wall=79668
2022-09-08 22:14:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 22:14:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 22:14:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 22:14:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 22:14:35 | INFO | valid | epoch 605 | valid on 'valid' subset | loss 5.091 | nll_loss 2.487 | mask_loss 10.0939 | p_2 0.04835 | mask_ave 0.622 | ppl 5.61 | bleu 56.95 | wps 1481.9 | wpb 933.5 | bsz 59.6 | num_updates 48924 | best_bleu 57.52
2022-09-08 22:14:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 605 @ 48924 updates
2022-09-08 22:14:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint605.pt
2022-09-08 22:14:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint605.pt
2022-09-08 22:14:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint605.pt (epoch 605 @ 48924 updates, score 56.95) (writing took 22.192825503647327 seconds)
2022-09-08 22:14:57 | INFO | fairseq_cli.train | end of epoch 605 (average epoch stats below)
2022-09-08 22:14:57 | INFO | train | epoch 605 | loss 3.372 | nll_loss 0.339 | mask_loss 9.15867 | p_2 0.03507 | mask_ave 0.5 | ppl 1.27 | wps 3414.1 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 48924 | lr 0.000142968 | gnorm 0.257 | train_wall 96 | gb_free 9.1 | wall 79730
2022-09-08 22:14:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 22:14:58 | INFO | fairseq.trainer | begin training epoch 606
2022-09-08 22:14:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 22:16:31 | INFO | train_inner | epoch 606:     76 / 81 loss=3.371, nll_loss=0.339, mask_loss=9.15974, p_2=0.03516, mask_ave=0.502, ppl=1.26, wps=3559.4, ups=0.64, wpb=5538.7, bsz=361.6, num_updates=49000, lr=0.000142857, gnorm=0.242, train_wall=120, gb_free=9.1, wall=79824
2022-09-08 22:16:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 22:16:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 22:16:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 22:16:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 22:16:47 | INFO | valid | epoch 606 | valid on 'valid' subset | loss 5.096 | nll_loss 2.492 | mask_loss 9.97218 | p_2 0.04812 | mask_ave 0.629 | ppl 5.63 | bleu 56.51 | wps 1486.7 | wpb 933.5 | bsz 59.6 | num_updates 49005 | best_bleu 57.52
2022-09-08 22:16:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 606 @ 49005 updates
2022-09-08 22:16:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint606.pt
2022-09-08 22:16:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint606.pt
2022-09-08 22:17:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint606.pt (epoch 606 @ 49005 updates, score 56.51) (writing took 16.361974511295557 seconds)
2022-09-08 22:17:04 | INFO | fairseq_cli.train | end of epoch 606 (average epoch stats below)
2022-09-08 22:17:04 | INFO | train | epoch 606 | loss 3.371 | nll_loss 0.339 | mask_loss 9.19725 | p_2 0.035 | mask_ave 0.502 | ppl 1.26 | wps 3537.4 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 49005 | lr 0.00014285 | gnorm 0.249 | train_wall 97 | gb_free 9 | wall 79857
2022-09-08 22:17:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 22:17:04 | INFO | fairseq.trainer | begin training epoch 607
2022-09-08 22:17:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 22:18:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-08 22:18:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-08 22:18:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-08 22:18:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-08 22:18:54 | INFO | valid | epoch 607 | valid on 'valid' subset | loss 5.104 | nll_loss 2.503 | mask_loss 10.0419 | p_2 0.04862 | mask_ave 0.614 | ppl 5.67 | bleu 56.3 | wps 1498.2 | wpb 933.5 | bsz 59.6 | num_updates 49086 | best_bleu 57.52
2022-09-08 22:18:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 607 @ 49086 updates
2022-09-08 22:18:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint607.pt
2022-09-08 22:18:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rg-ende-fixed/results/en-fr/mmt/checkpoint607.pt
2022-09-08 22:19:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint607.pt (epoch 607 @ 49086 updates, score 56.3) (writing took 14.060959193855524 seconds)
2022-09-08 22:19:08 | INFO | fairseq_cli.train | end of epoch 607 (average epoch stats below)
2022-09-08 22:19:08 | INFO | train | epoch 607 | loss 3.371 | nll_loss 0.339 | mask_loss 9.27885 | p_2 0.03503 | mask_ave 0.501 | ppl 1.26 | wps 3603.7 | ups 0.65 | wpb 5523.2 | bsz 358 | num_updates 49086 | lr 0.000142732 | gnorm 0.247 | train_wall 97 | gb_free 9 | wall 79981
2022-09-08 22:19:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-09-08 22:19:08 | INFO | fairseq.trainer | begin training epoch 608
2022-09-08 22:19:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-08 22:19:25 | INFO | train_inner | epoch 608:     14 / 81 loss=3.372, nll_loss=0.339, mask_loss=9.25519, p_2=0.03495, mask_ave=0.501, ppl=1.26, wps=3147.4, ups=0.57, wpb=5497.4, bsz=355, num_updates=49100, lr=0.000142712, gnorm=0.245, train_wall=118, gb_free=9, wall=79998
train.sh: line 24:  7211 Killed                  fairseq-train ${ACMMT_ROOT}/data_bin/$SRC-$TGT --user-dir ${RGMMT_ROOT} --criterion rgfix_criterion --task rgfix_translation_task --arch rgfix_model --optimizer rgfix_adam --adam-betas 0.9,0.98 --clip-norm 0 --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 4000 --reset-optimizer --lr 0.0005 --weight-decay 0.0001 --label-smoothing 0.2 --dropout 0.3 --max-tokens 1536 --no-progress-bar --log-interval 100 --stop-min-lr 1e-09 --max-update 150000 --keep-last-epochs 12 --update-freq 4 --eval-bleu --maximize-best-checkpoint-metric --save-dir ${SAVE_DIR} --share-decoder-input-output-embed --source-lang ${SRC} --target-lang ${TGT} --mask-loss-weight 0.03 --tensorboard-logdir ${SAVE_DIR}/bl_log1 --log-format simple --img-grid-prefix ${IMG_DATA_PREFIX}/resnet101-dlmmt --img-region-prefix ${IMG_DATA_PREFIX}/faster-dlmmt
