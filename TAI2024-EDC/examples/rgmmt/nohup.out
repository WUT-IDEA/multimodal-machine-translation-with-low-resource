/data1/home/turghun/project/acmmt/examples/rgmmt/tasks/data_loader.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  region_img_features = torch.tensor(torch.stack(region_img_features_tmp, dim=0))
/data1/home/turghun/project/acmmt/fairseq/utils.py:361: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
/data1/home/turghun/project/acmmt/examples/rgmmt/tasks/data_loader.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  region_img_features = torch.tensor(torch.stack(region_img_features_tmp, dim=0))
/data1/home/turghun/project/acmmt/fairseq/utils.py:361: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
Traceback (most recent call last):
  File "/data1/home/turghun/anaconda3/envs/tr1.10/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data1/home/turghun/project/acmmt/fairseq_cli/train.py", line 393, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/data1/home/turghun/project/acmmt/fairseq/distributed/utils.py", line 306, in call_main
    main(cfg, **kwargs)
  File "/data1/home/turghun/project/acmmt/fairseq_cli/train.py", line 136, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/data1/home/turghun/anaconda3/envs/tr1.10/lib/python3.6/contextlib.py", line 52, in inner
    return func(*args, **kwds)
  File "/data1/home/turghun/project/acmmt/fairseq_cli/train.py", line 235, in train
    valid_losses, should_stop = validate_and_save(cfg, trainer, task, epoch_itr, valid_subsets, end_of_epoch)
  File "/data1/home/turghun/project/acmmt/fairseq_cli/train.py", line 299, in validate_and_save
    checkpoint_utils.save_checkpoint(cfg.checkpoint, trainer, epoch_itr, valid_losses[0])
  File "/data1/home/turghun/project/acmmt/fairseq/checkpoint_utils.py", line 132, in save_checkpoint
    checkpoints = checkpoint_paths(cfg.save_dir, pattern=r"checkpoint(\d+){}\.pt".format(suffix))
  File "/data1/home/turghun/project/acmmt/fairseq/checkpoint_utils.py", line 406, in checkpoint_paths
    files = PathManager.ls(path)
  File "/data1/home/turghun/project/acmmt/fairseq/file_io.py", line 100, in ls
    return os.listdir(path)
FileNotFoundError: [Errno 2] No such file or directory: 'results/en-de/2mmt'
cp: cannot create regular file ‘results/en-de/2mmt/train.sh’: No such file or directory
train.sh: line 24:  7275 Aborted                 fairseq-train ${ACMMT_ROOT}/data_bin/$SRC-$TGT --user-dir ${RGMMT_ROOT} --criterion label_smoothed_cross_entropy --task rgmmt_translation_task --arch rgmmt_model --optimizer adam --adam-betas 0.9,0.98 --clip-norm 0 --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 4000 --reset-optimizer --lr 0.001 --weight-decay 0.0001 --label-smoothing 0.2 --dropout 0.3 --max-tokens 1536 --no-progress-bar --log-interval 100 --stop-min-lr 1e-09 --keep-last-epochs 12 --update-freq 4 --eval-bleu --maximize-best-checkpoint-metric --save-dir ${SAVE_DIR} --share-decoder-input-output-embed --source-lang ${SRC} --target-lang ${TGT} --tensorboard-logdir ${SAVE_DIR}/bl_log1 --log-format simple --img-grid-prefix ${IMG_DATA_PREFIX}/resnet101-dlmmt --img-region-prefix ${IMG_DATA_PREFIX}/faster-dlmmt > train-mmt1h.log
train.sh: line 25: g: command not found
2022-08-16 21:58:49 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': 'simple', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'results/en-de/mmt/bl_log1', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/data1/home/turghun/project/acmmt/examples/rgmmt', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1536, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1536, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [4], 'lr': [0.001], 'stop_min_lr': 1e-09, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'results/en-de/mmt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 12, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'rgmmt_model', 'activation_fn': relu, 'dropout': 0.3, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'adaptive_input': False, 'encoder': {'_name': None, 'embed_path': None, 'embed_dim': 512, 'ffn_embed_dim': 2048, 'layers': 6, 'attention_heads': 8, 'normalize_before': False, 'learned_pos': False, 'layerdrop': 0.0, 'layers_to_keep': None}, 'max_source_positions': 1024, 'decoder': {'_name': None, 'embed_path': None, 'embed_dim': 512, 'ffn_embed_dim': 2048, 'layers': 6, 'attention_heads': 8, 'normalize_before': False, 'learned_pos': False, 'layerdrop': 0.0, 'layers_to_keep': None, 'input_dim': 512, 'output_dim': 512}, 'max_target_positions': 1024, 'share_decoder_input_output_embed': True, 'share_all_embeddings': False, 'no_token_positional_embeddings': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'layernorm_embedding': False, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'no_cross_attention': False, 'cross_self_attention': False, 'quant_noise': {'_name': None, 'pq': 0.0, 'pq_block_size': 8, 'scalar': 0.0}, 'min_params_to_wrap': 100000000, 'char_inputs': False, 'relu_dropout': 0.0, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'export': False, 'no_decoder_final_norm': False, 'img_feature_dim': 512}, 'task': {'_name': 'rgmmt_translation_task', 'data': '/data1/home/turghun/project/acmmt/data_bin/en-de', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False, 'img_grid_prefix': '/data1/home/turghun/project/images/features/resnet101-dlmmt', 'img_region_prefix': '/data1/home/turghun/project/images/features/faster-dlmmt'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.2, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '0.9,0.98', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-08-16 21:58:49 | INFO | rgmmt.tasks.tasks | [en] dictionary: 8512 types
2022-08-16 21:58:49 | INFO | rgmmt.tasks.tasks | [de] dictionary: 9392 types
2022-08-16 21:58:50 | INFO | fairseq_cli.train | RGMMTTModel(
  (encoder): RGMMTEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(8512, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (region_img_project): Imgprojector(
      (linear): Linear(in_features=2048, out_features=512, bias=True)
    )
    (grid_img_project): Imgprojector(
      (linear): Linear(in_features=2048, out_features=512, bias=True)
    )
    (visual_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=512, out_features=512, bias=True)
      (v_proj): Linear(in_features=512, out_features=512, bias=True)
      (q_proj): Linear(in_features=512, out_features=512, bias=True)
      (out_proj): Linear(in_features=512, out_features=512, bias=True)
    )
    (merge_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=512, out_features=512, bias=True)
      (v_proj): Linear(in_features=512, out_features=512, bias=True)
      (q_proj): Linear(in_features=512, out_features=512, bias=True)
      (out_proj): Linear(in_features=512, out_features=512, bias=True)
    )
    (gate): Linear(in_features=1024, out_features=512, bias=True)
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(9392, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=9392, bias=False)
  )
)
2022-08-16 21:58:50 | INFO | fairseq_cli.train | task: RGMMTTask
2022-08-16 21:58:50 | INFO | fairseq_cli.train | model: RGMMTTModel
2022-08-16 21:58:50 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-08-16 21:58:50 | INFO | fairseq_cli.train | num. shared model params: 58,029,568 (num. trained: 58,029,568)
2022-08-16 21:58:50 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-08-16 21:58:50 | INFO | fairseq.data.data_utils | loaded 1,014 examples from: /data1/home/turghun/project/acmmt/data_bin/en-de/valid.en-de.en
2022-08-16 21:58:50 | INFO | fairseq.data.data_utils | loaded 1,014 examples from: /data1/home/turghun/project/acmmt/data_bin/en-de/valid.en-de.de
2022-08-16 21:58:51 | INFO | rgmmt.tasks.tasks | /data1/home/turghun/project/acmmt/data_bin/en-de valid en-de 1014 examples
2022-08-16 21:58:51 | INFO | rgmmt.tasks.tasks | load 1014 grid image examples for valid
2022-08-16 21:58:51 | INFO | rgmmt.tasks.tasks | load 1014 region image examples for valid
2022-08-16 21:58:54 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-08-16 21:58:54 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-08-16 21:58:54 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 11.910 GB ; name = TITAN Xp                                
2022-08-16 21:58:54 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-08-16 21:58:54 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-08-16 21:58:54 | INFO | fairseq_cli.train | max tokens per device = 1536 and max sentences per device = None
2022-08-16 21:58:54 | INFO | fairseq.trainer | Preparing to load checkpoint results/en-de/mmt/checkpoint_last.pt
2022-08-16 21:58:55 | INFO | fairseq.trainer | Loaded checkpoint results/en-de/mmt/checkpoint_last.pt (epoch 1037 @ 0 updates)
2022-08-16 21:58:55 | INFO | fairseq.trainer | loading train data for epoch 1037
2022-08-16 21:58:55 | INFO | fairseq.data.data_utils | loaded 29,000 examples from: /data1/home/turghun/project/acmmt/data_bin/en-de/train.en-de.en
2022-08-16 21:58:55 | INFO | fairseq.data.data_utils | loaded 29,000 examples from: /data1/home/turghun/project/acmmt/data_bin/en-de/train.en-de.de
2022-08-16 21:59:04 | INFO | rgmmt.tasks.tasks | /data1/home/turghun/project/acmmt/data_bin/en-de train en-de 29000 examples
2022-08-16 21:59:04 | INFO | rgmmt.tasks.tasks | load 29000 grid image examples for train
2022-08-16 21:59:04 | INFO | rgmmt.tasks.tasks | load 29000 region image examples for train
2022-08-16 21:59:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-08-16 21:59:04 | INFO | fairseq.trainer | begin training epoch 1037
2022-08-16 21:59:04 | INFO | fairseq_cli.train | Start iterating over samples
/data1/home/turghun/project/acmmt/examples/rgmmt/tasks/data_loader.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  region_img_features = torch.tensor(torch.stack(region_img_features_tmp, dim=0))
2022-08-16 21:59:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
/data1/home/turghun/project/acmmt/fairseq/utils.py:361: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2022-08-16 21:59:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 21:59:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 21:59:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 21:59:55 | INFO | valid | epoch 1037 | valid on 'valid' subset | loss 6.214 | nll_loss 3.841 | ppl 14.33 | bleu 35.27 | wps 2226.7 | wpb 1003.7 | bsz 67.6 | num_updates 78
2022-08-16 21:59:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1037 @ 78 updates
2022-08-16 21:59:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint1037.pt
2022-08-16 21:59:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint1037.pt
2022-08-16 22:01:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint1037.pt (epoch 1037 @ 78 updates, score 35.27) (writing took 91.63151180744171 seconds)
2022-08-16 22:01:26 | INFO | fairseq_cli.train | end of epoch 1037 (average epoch stats below)
2022-08-16 22:01:26 | INFO | train | epoch 1037 | loss 3.407 | nll_loss 0.347 | ppl 1.27 | wps 2931.3 | ups 0.55 | wpb 5319.8 | bsz 371.8 | num_updates 78 | lr 1.9598e-05 | gnorm 0.425 | train_wall 38 | gb_free 10.1 | wall 152
2022-08-16 22:01:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-08-16 22:01:27 | INFO | fairseq.trainer | begin training epoch 1038
2022-08-16 22:01:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:01:39 | INFO | train_inner | epoch 1038:     22 / 78 loss=3.406, nll_loss=0.347, ppl=1.27, wps=3451.8, ups=0.65, wpb=5312.7, bsz=370.9, num_updates=100, lr=2.50975e-05, gnorm=0.42, train_wall=48, gb_free=9.9, wall=165
2022-08-16 22:02:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:02:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:02:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:02:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:02:15 | INFO | valid | epoch 1038 | valid on 'valid' subset | loss 6.21 | nll_loss 3.836 | ppl 14.28 | bleu 35.11 | wps 2129.4 | wpb 1003.7 | bsz 67.6 | num_updates 156 | best_bleu 35.27
2022-08-16 22:02:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1038 @ 156 updates
2022-08-16 22:02:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint1038.pt
2022-08-16 22:02:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint1038.pt
2022-08-16 22:02:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint1038.pt (epoch 1038 @ 156 updates, score 35.11) (writing took 37.977945156395435 seconds)
2022-08-16 22:02:53 | INFO | fairseq_cli.train | end of epoch 1038 (average epoch stats below)
2022-08-16 22:02:53 | INFO | train | epoch 1038 | loss 3.404 | nll_loss 0.345 | ppl 1.27 | wps 4763 | ups 0.9 | wpb 5319.8 | bsz 371.8 | num_updates 156 | lr 3.90961e-05 | gnorm 0.383 | train_wall 36 | gb_free 10.1 | wall 239
2022-08-16 22:02:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-08-16 22:02:54 | INFO | fairseq.trainer | begin training epoch 1039
2022-08-16 22:02:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:03:19 | INFO | train_inner | epoch 1039:     44 / 78 loss=3.403, nll_loss=0.344, ppl=1.27, wps=5329.7, ups=1, wpb=5333.5, bsz=371.9, num_updates=200, lr=5.0095e-05, gnorm=0.352, train_wall=46, gb_free=10, wall=265
2022-08-16 22:03:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:03:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:03:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:03:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:03:46 | INFO | valid | epoch 1039 | valid on 'valid' subset | loss 6.196 | nll_loss 3.819 | ppl 14.11 | bleu 35.45 | wps 2129.8 | wpb 1003.7 | bsz 67.6 | num_updates 234 | best_bleu 35.45
2022-08-16 22:03:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1039 @ 234 updates
2022-08-16 22:03:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint1039.pt
2022-08-16 22:03:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint1039.pt
train.sh: line 24: 27379 Killed                  fairseq-train ${ACMMT_ROOT}/data_bin/$SRC-$TGT --user-dir ${RGMMT_ROOT} --criterion label_smoothed_cross_entropy --task rgmmt_translation_task --arch rgmmt_model --optimizer adam --adam-betas 0.9,0.98 --clip-norm 0 --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 4000 --reset-optimizer --lr 0.001 --weight-decay 0.0001 --label-smoothing 0.2 --dropout 0.3 --max-tokens 1536 --no-progress-bar --log-interval 100 --stop-min-lr 1e-09 --keep-last-epochs 12 --update-freq 4 --eval-bleu --maximize-best-checkpoint-metric --save-dir ${SAVE_DIR} --share-decoder-input-output-embed --source-lang ${SRC} --target-lang ${TGT} --tensorboard-logdir ${SAVE_DIR}/bl_log1 --log-format simple --img-grid-prefix ${IMG_DATA_PREFIX}/resnet101-dlmmt --img-region-prefix ${IMG_DATA_PREFIX}/faster-dlmmt
2022-08-16 22:08:03 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': 'simple', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'results/en-de/mmt/bl_log1', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/data1/home/turghun/project/acmmt/examples/rgmmt', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1536, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1536, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [4], 'lr': [0.001], 'stop_min_lr': 1e-09, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'results/en-de/mmt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 12, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'rgmmt_model', 'activation_fn': relu, 'dropout': 0.3, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'adaptive_input': False, 'encoder': {'_name': None, 'embed_path': None, 'embed_dim': 512, 'ffn_embed_dim': 2048, 'layers': 6, 'attention_heads': 8, 'normalize_before': False, 'learned_pos': False, 'layerdrop': 0.0, 'layers_to_keep': None}, 'max_source_positions': 1024, 'decoder': {'_name': None, 'embed_path': None, 'embed_dim': 512, 'ffn_embed_dim': 2048, 'layers': 6, 'attention_heads': 8, 'normalize_before': False, 'learned_pos': False, 'layerdrop': 0.0, 'layers_to_keep': None, 'input_dim': 512, 'output_dim': 512}, 'max_target_positions': 1024, 'share_decoder_input_output_embed': True, 'share_all_embeddings': False, 'no_token_positional_embeddings': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'layernorm_embedding': False, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'no_cross_attention': False, 'cross_self_attention': False, 'quant_noise': {'_name': None, 'pq': 0.0, 'pq_block_size': 8, 'scalar': 0.0}, 'min_params_to_wrap': 100000000, 'char_inputs': False, 'relu_dropout': 0.0, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'export': False, 'no_decoder_final_norm': False, 'img_feature_dim': 512}, 'task': {'_name': 'rgmmt_translation_task', 'data': '/data1/home/turghun/project/acmmt/data_bin/en-de', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False, 'img_grid_prefix': '/data1/home/turghun/project/images/features/resnet101-dlmmt', 'img_region_prefix': '/data1/home/turghun/project/images/features/faster-dlmmt'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.2, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '0.9,0.98', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-08-16 22:08:03 | INFO | rgmmt.tasks.tasks | [en] dictionary: 8512 types
2022-08-16 22:08:03 | INFO | rgmmt.tasks.tasks | [de] dictionary: 9392 types
2022-08-16 22:08:04 | INFO | fairseq_cli.train | RGMMTTModel(
  (encoder): RGMMTEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(8512, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (region_img_project): Imgprojector(
      (linear): Linear(in_features=2048, out_features=512, bias=True)
    )
    (grid_img_project): Imgprojector(
      (linear): Linear(in_features=2048, out_features=512, bias=True)
    )
    (visual_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=512, out_features=512, bias=True)
      (v_proj): Linear(in_features=512, out_features=512, bias=True)
      (q_proj): Linear(in_features=512, out_features=512, bias=True)
      (out_proj): Linear(in_features=512, out_features=512, bias=True)
    )
    (merge_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=512, out_features=512, bias=True)
      (v_proj): Linear(in_features=512, out_features=512, bias=True)
      (q_proj): Linear(in_features=512, out_features=512, bias=True)
      (out_proj): Linear(in_features=512, out_features=512, bias=True)
    )
    (gate): Linear(in_features=1024, out_features=512, bias=True)
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(9392, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=9392, bias=False)
  )
)
2022-08-16 22:08:04 | INFO | fairseq_cli.train | task: RGMMTTask
2022-08-16 22:08:04 | INFO | fairseq_cli.train | model: RGMMTTModel
2022-08-16 22:08:04 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-08-16 22:08:04 | INFO | fairseq_cli.train | num. shared model params: 58,029,568 (num. trained: 58,029,568)
2022-08-16 22:08:04 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-08-16 22:08:04 | INFO | fairseq.data.data_utils | loaded 1,014 examples from: /data1/home/turghun/project/acmmt/data_bin/en-de/valid.en-de.en
2022-08-16 22:08:04 | INFO | fairseq.data.data_utils | loaded 1,014 examples from: /data1/home/turghun/project/acmmt/data_bin/en-de/valid.en-de.de
2022-08-16 22:08:04 | INFO | rgmmt.tasks.tasks | /data1/home/turghun/project/acmmt/data_bin/en-de valid en-de 1014 examples
2022-08-16 22:08:04 | INFO | rgmmt.tasks.tasks | load 1014 grid image examples for valid
2022-08-16 22:08:04 | INFO | rgmmt.tasks.tasks | load 1014 region image examples for valid
2022-08-16 22:08:07 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-08-16 22:08:07 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-08-16 22:08:07 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 11.910 GB ; name = TITAN Xp                                
2022-08-16 22:08:07 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-08-16 22:08:07 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-08-16 22:08:07 | INFO | fairseq_cli.train | max tokens per device = 1536 and max sentences per device = None
2022-08-16 22:08:07 | INFO | fairseq.trainer | Preparing to load checkpoint results/en-de/mmt/checkpoint_last.pt
2022-08-16 22:08:07 | INFO | fairseq.trainer | No existing checkpoint found results/en-de/mmt/checkpoint_last.pt
2022-08-16 22:08:07 | INFO | fairseq.trainer | loading train data for epoch 1
2022-08-16 22:08:07 | INFO | fairseq.data.data_utils | loaded 29,000 examples from: /data1/home/turghun/project/acmmt/data_bin/en-de/train.en-de.en
2022-08-16 22:08:07 | INFO | fairseq.data.data_utils | loaded 29,000 examples from: /data1/home/turghun/project/acmmt/data_bin/en-de/train.en-de.de
2022-08-16 22:08:17 | INFO | rgmmt.tasks.tasks | /data1/home/turghun/project/acmmt/data_bin/en-de train en-de 29000 examples
2022-08-16 22:08:17 | INFO | rgmmt.tasks.tasks | load 29000 grid image examples for train
2022-08-16 22:08:17 | INFO | rgmmt.tasks.tasks | load 29000 region image examples for train
2022-08-16 22:08:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-08-16 22:08:17 | INFO | fairseq.trainer | begin training epoch 1
2022-08-16 22:08:17 | INFO | fairseq_cli.train | Start iterating over samples
/data1/home/turghun/project/acmmt/examples/rgmmt/tasks/data_loader.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  region_img_features = torch.tensor(torch.stack(region_img_features_tmp, dim=0))
2022-08-16 22:09:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
/data1/home/turghun/project/acmmt/fairseq/utils.py:361: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2022-08-16 22:09:07 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.762 | nll_loss 9.884 | ppl 944.79 | bleu 0.01 | wps 4147.3 | wpb 1003.7 | bsz 67.6 | num_updates 78
2022-08-16 22:09:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 78 updates
2022-08-16 22:09:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint1.pt
2022-08-16 22:09:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint1.pt
2022-08-16 22:09:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint1.pt (epoch 1 @ 78 updates, score 0.01) (writing took 2.8775444999337196 seconds)
2022-08-16 22:09:10 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-08-16 22:09:10 | INFO | train | epoch 001 | loss 11.942 | nll_loss 11.415 | ppl 2730.41 | wps 8217.7 | ups 1.55 | wpb 5319.8 | bsz 371.8 | num_updates 78 | lr 1.9598e-05 | gnorm 3.075 | train_wall 38 | gb_free 10 | wall 63
2022-08-16 22:09:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-08-16 22:09:10 | INFO | fairseq.trainer | begin training epoch 2
2022-08-16 22:09:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:09:23 | INFO | train_inner | epoch 002:     22 / 78 loss=11.649, nll_loss=11.042, ppl=2109.12, wps=8354.5, ups=1.56, wpb=5342, bsz=374.4, num_updates=100, lr=2.50975e-05, gnorm=2.71, train_wall=49, gb_free=10, wall=76
2022-08-16 22:09:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:09:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:09:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:09:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:10:07 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.671 | nll_loss 8.388 | ppl 334.89 | bleu 0.35 | wps 1061.9 | wpb 1003.7 | bsz 67.6 | num_updates 156 | best_bleu 0.35
2022-08-16 22:10:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 156 updates
2022-08-16 22:10:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint2.pt
2022-08-16 22:10:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint2.pt
train.sh: line 24: 30802 Killed                  fairseq-train ${ACMMT_ROOT}/data_bin/$SRC-$TGT --user-dir ${RGMMT_ROOT} --criterion label_smoothed_cross_entropy --task rgmmt_translation_task --arch rgmmt_model --optimizer adam --adam-betas 0.9,0.98 --clip-norm 0 --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 4000 --reset-optimizer --lr 0.001 --weight-decay 0.0001 --label-smoothing 0.2 --dropout 0.3 --max-tokens 1536 --no-progress-bar --log-interval 100 --stop-min-lr 1e-09 --keep-last-epochs 12 --update-freq 4 --eval-bleu --maximize-best-checkpoint-metric --save-dir ${SAVE_DIR} --share-decoder-input-output-embed --source-lang ${SRC} --target-lang ${TGT} --tensorboard-logdir ${SAVE_DIR}/bl_log1 --log-format simple --img-grid-prefix ${IMG_DATA_PREFIX}/resnet101-dlmmt --img-region-prefix ${IMG_DATA_PREFIX}/faster-dlmmt
2022-08-16 22:11:39 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': 'simple', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'results/en-fr/mmt/bl_log1', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/data1/home/turghun/project/acmmt/examples/rgmmt', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1536, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1536, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [4], 'lr': [0.001], 'stop_min_lr': 1e-09, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'results/en-fr/mmt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 12, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'rgmmt_model', 'activation_fn': relu, 'dropout': 0.3, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'adaptive_input': False, 'encoder': {'_name': None, 'embed_path': None, 'embed_dim': 512, 'ffn_embed_dim': 2048, 'layers': 6, 'attention_heads': 8, 'normalize_before': False, 'learned_pos': False, 'layerdrop': 0.0, 'layers_to_keep': None}, 'max_source_positions': 1024, 'decoder': {'_name': None, 'embed_path': None, 'embed_dim': 512, 'ffn_embed_dim': 2048, 'layers': 6, 'attention_heads': 8, 'normalize_before': False, 'learned_pos': False, 'layerdrop': 0.0, 'layers_to_keep': None, 'input_dim': 512, 'output_dim': 512}, 'max_target_positions': 1024, 'share_decoder_input_output_embed': True, 'share_all_embeddings': False, 'no_token_positional_embeddings': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'layernorm_embedding': False, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'no_cross_attention': False, 'cross_self_attention': False, 'quant_noise': {'_name': None, 'pq': 0.0, 'pq_block_size': 8, 'scalar': 0.0}, 'min_params_to_wrap': 100000000, 'char_inputs': False, 'relu_dropout': 0.0, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'export': False, 'no_decoder_final_norm': False, 'img_feature_dim': 512}, 'task': {'_name': 'rgmmt_translation_task', 'data': '/data1/home/turghun/project/acmmt/data_bin/en-fr', 'source_lang': 'en', 'target_lang': 'fr', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False, 'img_grid_prefix': '/data1/home/turghun/project/images/features/resnet101-dlmmt', 'img_region_prefix': '/data1/home/turghun/project/images/features/faster-dlmmt'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.2, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '0.9,0.98', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-08-16 22:11:39 | INFO | rgmmt.tasks.tasks | [en] dictionary: 8512 types
2022-08-16 22:11:39 | INFO | rgmmt.tasks.tasks | [fr] dictionary: 8752 types
2022-08-16 22:11:40 | INFO | fairseq_cli.train | RGMMTTModel(
  (encoder): RGMMTEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(8512, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (region_img_project): Imgprojector(
      (linear): Linear(in_features=2048, out_features=512, bias=True)
    )
    (grid_img_project): Imgprojector(
      (linear): Linear(in_features=2048, out_features=512, bias=True)
    )
    (visual_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=512, out_features=512, bias=True)
      (v_proj): Linear(in_features=512, out_features=512, bias=True)
      (q_proj): Linear(in_features=512, out_features=512, bias=True)
      (out_proj): Linear(in_features=512, out_features=512, bias=True)
    )
    (merge_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=512, out_features=512, bias=True)
      (v_proj): Linear(in_features=512, out_features=512, bias=True)
      (q_proj): Linear(in_features=512, out_features=512, bias=True)
      (out_proj): Linear(in_features=512, out_features=512, bias=True)
    )
    (gate): Linear(in_features=1024, out_features=512, bias=True)
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(8752, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=8752, bias=False)
  )
)
2022-08-16 22:11:40 | INFO | fairseq_cli.train | task: RGMMTTask
2022-08-16 22:11:40 | INFO | fairseq_cli.train | model: RGMMTTModel
2022-08-16 22:11:40 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-08-16 22:11:40 | INFO | fairseq_cli.train | num. shared model params: 57,701,888 (num. trained: 57,701,888)
2022-08-16 22:11:40 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-08-16 22:11:40 | INFO | fairseq.data.data_utils | loaded 1,014 examples from: /data1/home/turghun/project/acmmt/data_bin/en-fr/valid.en-fr.en
2022-08-16 22:11:40 | INFO | fairseq.data.data_utils | loaded 1,014 examples from: /data1/home/turghun/project/acmmt/data_bin/en-fr/valid.en-fr.fr
2022-08-16 22:11:40 | INFO | rgmmt.tasks.tasks | /data1/home/turghun/project/acmmt/data_bin/en-fr valid en-fr 1014 examples
2022-08-16 22:11:40 | INFO | rgmmt.tasks.tasks | load 1014 grid image examples for valid
2022-08-16 22:11:40 | INFO | rgmmt.tasks.tasks | load 1014 region image examples for valid
2022-08-16 22:11:44 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-08-16 22:11:44 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-08-16 22:11:44 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 11.910 GB ; name = TITAN Xp                                
2022-08-16 22:11:44 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-08-16 22:11:44 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-08-16 22:11:44 | INFO | fairseq_cli.train | max tokens per device = 1536 and max sentences per device = None
2022-08-16 22:11:44 | INFO | fairseq.trainer | Preparing to load checkpoint results/en-fr/mmt/checkpoint_last.pt
2022-08-16 22:11:44 | INFO | fairseq.trainer | No existing checkpoint found results/en-fr/mmt/checkpoint_last.pt
2022-08-16 22:11:44 | INFO | fairseq.trainer | loading train data for epoch 1
2022-08-16 22:11:44 | INFO | fairseq.data.data_utils | loaded 29,000 examples from: /data1/home/turghun/project/acmmt/data_bin/en-fr/train.en-fr.en
2022-08-16 22:11:44 | INFO | fairseq.data.data_utils | loaded 29,000 examples from: /data1/home/turghun/project/acmmt/data_bin/en-fr/train.en-fr.fr
2022-08-16 22:11:53 | INFO | rgmmt.tasks.tasks | /data1/home/turghun/project/acmmt/data_bin/en-fr train en-fr 29000 examples
2022-08-16 22:11:53 | INFO | rgmmt.tasks.tasks | load 29000 grid image examples for train
2022-08-16 22:11:53 | INFO | rgmmt.tasks.tasks | load 29000 region image examples for train
2022-08-16 22:11:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 22:11:54 | INFO | fairseq.trainer | begin training epoch 1
2022-08-16 22:11:54 | INFO | fairseq_cli.train | Start iterating over samples
/data1/home/turghun/project/acmmt/examples/rgmmt/tasks/data_loader.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  region_img_features = torch.tensor(torch.stack(region_img_features_tmp, dim=0))
2022-08-16 22:12:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
/data1/home/turghun/project/acmmt/fairseq/utils.py:361: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2022-08-16 22:12:46 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.478 | nll_loss 9.556 | ppl 752.93 | bleu 0.01 | wps 2856.8 | wpb 933.5 | bsz 59.6 | num_updates 81
2022-08-16 22:12:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 81 updates
2022-08-16 22:12:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1.pt
2022-08-16 22:12:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1.pt
2022-08-16 22:12:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1.pt (epoch 1 @ 81 updates, score 0.01) (writing took 2.5219926610589027 seconds)
2022-08-16 22:12:49 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-08-16 22:12:49 | INFO | train | epoch 001 | loss 11.794 | nll_loss 11.257 | ppl 2447.37 | wps 8358.4 | ups 1.51 | wpb 5523.2 | bsz 358 | num_updates 81 | lr 2.0348e-05 | gnorm 3.109 | train_wall 38 | gb_free 10.1 | wall 65
2022-08-16 22:12:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 22:12:49 | INFO | fairseq.trainer | begin training epoch 2
2022-08-16 22:12:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:13:01 | INFO | train_inner | epoch 002:     19 / 81 loss=11.536, nll_loss=10.932, ppl=1953.04, wps=8468.9, ups=1.53, wpb=5541.6, bsz=359.6, num_updates=100, lr=2.50975e-05, gnorm=2.784, train_wall=48, gb_free=10.1, wall=77
2022-08-16 22:13:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:13:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:13:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:13:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:13:56 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.17 | nll_loss 7.802 | ppl 223.21 | bleu 0.09 | wps 702.8 | wpb 933.5 | bsz 59.6 | num_updates 162 | best_bleu 0.09
2022-08-16 22:13:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 162 updates
2022-08-16 22:13:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint2.pt
2022-08-16 22:13:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint2.pt
2022-08-16 22:14:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint2.pt (epoch 2 @ 162 updates, score 0.09) (writing took 38.899981431663036 seconds)
2022-08-16 22:14:35 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-08-16 22:14:35 | INFO | train | epoch 002 | loss 9.929 | nll_loss 8.883 | ppl 472.04 | wps 4216.9 | ups 0.76 | wpb 5523.2 | bsz 358 | num_updates 162 | lr 4.05959e-05 | gnorm 1.651 | train_wall 39 | gb_free 10.2 | wall 171
2022-08-16 22:14:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 22:14:35 | INFO | fairseq.trainer | begin training epoch 3
2022-08-16 22:14:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:14:57 | INFO | train_inner | epoch 003:     38 / 81 loss=9.489, nll_loss=8.297, ppl=314.58, wps=4733.5, ups=0.86, wpb=5499.6, bsz=355.1, num_updates=200, lr=5.0095e-05, gnorm=1.641, train_wall=47, gb_free=10.1, wall=193
2022-08-16 22:15:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:15:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:15:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:15:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:16:06 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 8.423 | nll_loss 6.697 | ppl 103.76 | bleu 1.02 | wps 324 | wpb 933.5 | bsz 59.6 | num_updates 243 | best_bleu 1.02
2022-08-16 22:16:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 243 updates
2022-08-16 22:16:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint3.pt
2022-08-16 22:16:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint3.pt
2022-08-16 22:17:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint3.pt (epoch 3 @ 243 updates, score 1.02) (writing took 65.17627596482635 seconds)
2022-08-16 22:17:12 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-08-16 22:17:12 | INFO | train | epoch 003 | loss 8.807 | nll_loss 7.366 | ppl 164.91 | wps 2859.1 | ups 0.52 | wpb 5523.2 | bsz 358 | num_updates 243 | lr 6.08439e-05 | gnorm 1.504 | train_wall 38 | gb_free 10.2 | wall 328
2022-08-16 22:17:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 22:17:12 | INFO | fairseq.trainer | begin training epoch 4
2022-08-16 22:17:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:17:40 | INFO | train_inner | epoch 004:     57 / 81 loss=8.373, nll_loss=6.769, ppl=109.09, wps=3388.1, ups=0.61, wpb=5535.6, bsz=359.6, num_updates=300, lr=7.50925e-05, gnorm=1.518, train_wall=47, gb_free=10, wall=357
2022-08-16 22:17:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:17:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:17:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:17:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:18:06 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.733 | nll_loss 5.731 | ppl 53.1 | bleu 9.6 | wps 1133.3 | wpb 933.5 | bsz 59.6 | num_updates 324 | best_bleu 9.6
2022-08-16 22:18:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 324 updates
2022-08-16 22:18:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint4.pt
2022-08-16 22:18:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint4.pt
2022-08-16 22:19:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint4.pt (epoch 4 @ 324 updates, score 9.6) (writing took 89.58557669445872 seconds)
2022-08-16 22:19:36 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-08-16 22:19:36 | INFO | train | epoch 004 | loss 8.113 | nll_loss 6.418 | ppl 85.51 | wps 3100.8 | ups 0.56 | wpb 5523.2 | bsz 358 | num_updates 324 | lr 8.10919e-05 | gnorm 1.518 | train_wall 38 | gb_free 10.2 | wall 472
2022-08-16 22:19:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 22:19:36 | INFO | fairseq.trainer | begin training epoch 5
2022-08-16 22:19:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:20:17 | INFO | train_inner | epoch 005:     76 / 81 loss=7.665, nll_loss=5.822, ppl=56.58, wps=3540.9, ups=0.64, wpb=5544.8, bsz=358.8, num_updates=400, lr=0.00010009, gnorm=1.46, train_wall=46, gb_free=10.1, wall=513
2022-08-16 22:20:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:20:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:20:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:20:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:20:41 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.288 | nll_loss 5.152 | ppl 35.55 | bleu 13.65 | wps 733.6 | wpb 933.5 | bsz 59.6 | num_updates 405 | best_bleu 13.65
2022-08-16 22:20:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 405 updates
2022-08-16 22:20:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint5.pt
2022-08-16 22:20:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint5.pt
2022-08-16 22:21:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint5.pt (epoch 5 @ 405 updates, score 13.65) (writing took 60.677159551531076 seconds)
2022-08-16 22:21:42 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-08-16 22:21:42 | INFO | train | epoch 005 | loss 7.577 | nll_loss 5.706 | ppl 52.22 | wps 3552.2 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 405 | lr 0.00010134 | gnorm 1.453 | train_wall 38 | gb_free 10.2 | wall 598
2022-08-16 22:21:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 22:21:42 | INFO | fairseq.trainer | begin training epoch 6
2022-08-16 22:21:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:22:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:22:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:22:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:22:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:22:33 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.999 | nll_loss 4.746 | ppl 26.84 | bleu 16.7 | wps 1579 | wpb 933.5 | bsz 59.6 | num_updates 486 | best_bleu 16.7
2022-08-16 22:22:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 486 updates
2022-08-16 22:22:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint6.pt
2022-08-16 22:22:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint6.pt
2022-08-16 22:23:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint6.pt (epoch 6 @ 486 updates, score 16.7) (writing took 65.75104202702641 seconds)
2022-08-16 22:23:39 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-08-16 22:23:39 | INFO | train | epoch 006 | loss 7.186 | nll_loss 5.19 | ppl 36.51 | wps 3808.3 | ups 0.69 | wpb 5523.2 | bsz 358 | num_updates 486 | lr 0.000121588 | gnorm 1.334 | train_wall 39 | gb_free 10.1 | wall 715
2022-08-16 22:23:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 22:23:39 | INFO | fairseq.trainer | begin training epoch 7
2022-08-16 22:23:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:23:47 | INFO | train_inner | epoch 007:     14 / 81 loss=7.168, nll_loss=5.166, ppl=35.9, wps=2614.4, ups=0.48, wpb=5500.8, bsz=355.9, num_updates=500, lr=0.000125087, gnorm=1.359, train_wall=47, gb_free=10.1, wall=724
2022-08-16 22:24:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:24:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:24:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:24:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:24:31 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.7 | nll_loss 4.348 | ppl 20.36 | bleu 18.57 | wps 1893.1 | wpb 933.5 | bsz 59.6 | num_updates 567 | best_bleu 18.57
2022-08-16 22:24:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 567 updates
2022-08-16 22:24:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint7.pt
2022-08-16 22:24:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint7.pt
2022-08-16 22:25:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint7.pt (epoch 7 @ 567 updates, score 18.57) (writing took 73.51082834228873 seconds)
2022-08-16 22:25:44 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-08-16 22:25:44 | INFO | train | epoch 007 | loss 6.886 | nll_loss 4.79 | ppl 27.66 | wps 3576.9 | ups 0.65 | wpb 5523.2 | bsz 358 | num_updates 567 | lr 0.000141836 | gnorm 1.422 | train_wall 38 | gb_free 10.1 | wall 841
2022-08-16 22:25:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 22:25:45 | INFO | fairseq.trainer | begin training epoch 8
2022-08-16 22:25:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:26:06 | INFO | train_inner | epoch 008:     33 / 81 loss=6.768, nll_loss=4.634, ppl=24.83, wps=3995.2, ups=0.72, wpb=5538.7, bsz=365.8, num_updates=600, lr=0.000150085, gnorm=1.359, train_wall=47, gb_free=10, wall=862
2022-08-16 22:26:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:26:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:26:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:26:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:26:40 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.593 | nll_loss 4.17 | ppl 18 | bleu 18.3 | wps 2498.8 | wpb 933.5 | bsz 59.6 | num_updates 648 | best_bleu 18.57
2022-08-16 22:26:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 648 updates
2022-08-16 22:26:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint8.pt
2022-08-16 22:26:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint8.pt
2022-08-16 22:27:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint8.pt (epoch 8 @ 648 updates, score 18.3) (writing took 26.445014845579863 seconds)
2022-08-16 22:27:07 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-08-16 22:27:07 | INFO | train | epoch 008 | loss 6.58 | nll_loss 4.382 | ppl 20.85 | wps 5434.8 | ups 0.98 | wpb 5523.2 | bsz 358 | num_updates 648 | lr 0.000162084 | gnorm 1.345 | train_wall 38 | gb_free 10.2 | wall 923
2022-08-16 22:27:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 22:27:07 | INFO | fairseq.trainer | begin training epoch 9
2022-08-16 22:27:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:27:32 | INFO | train_inner | epoch 009:     52 / 81 loss=6.481, nll_loss=4.249, ppl=19.01, wps=6380.2, ups=1.16, wpb=5508, bsz=346.5, num_updates=700, lr=0.000175082, gnorm=1.359, train_wall=47, gb_free=10, wall=949
2022-08-16 22:27:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:27:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:27:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:27:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:27:57 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.203 | nll_loss 3.648 | ppl 12.53 | bleu 26.21 | wps 1773.4 | wpb 933.5 | bsz 59.6 | num_updates 729 | best_bleu 26.21
2022-08-16 22:27:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 729 updates
2022-08-16 22:27:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint9.pt
2022-08-16 22:27:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint9.pt
2022-08-16 22:29:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint9.pt (epoch 9 @ 729 updates, score 26.21) (writing took 84.20581128075719 seconds)
2022-08-16 22:29:21 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-08-16 22:29:21 | INFO | train | epoch 009 | loss 6.322 | nll_loss 4.037 | ppl 16.42 | wps 3318.1 | ups 0.6 | wpb 5523.2 | bsz 358 | num_updates 729 | lr 0.000182332 | gnorm 1.344 | train_wall 38 | gb_free 10.4 | wall 1058
2022-08-16 22:29:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 22:29:22 | INFO | fairseq.trainer | begin training epoch 10
2022-08-16 22:29:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:29:59 | INFO | train_inner | epoch 010:     71 / 81 loss=6.094, nll_loss=3.737, ppl=13.33, wps=3770, ups=0.68, wpb=5542.4, bsz=365.3, num_updates=800, lr=0.00020008, gnorm=1.312, train_wall=47, gb_free=10.1, wall=1096
2022-08-16 22:30:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:30:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:30:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:30:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:30:13 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.987 | nll_loss 3.353 | ppl 10.22 | bleu 32.35 | wps 1980 | wpb 933.5 | bsz 59.6 | num_updates 810 | best_bleu 32.35
2022-08-16 22:30:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 810 updates
2022-08-16 22:30:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint10.pt
2022-08-16 22:30:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint10.pt
2022-08-16 22:30:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint10.pt (epoch 10 @ 810 updates, score 32.35) (writing took 43.68198551610112 seconds)
2022-08-16 22:30:57 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-08-16 22:30:57 | INFO | train | epoch 010 | loss 6.05 | nll_loss 3.678 | ppl 12.8 | wps 4707.8 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 810 | lr 0.00020258 | gnorm 1.295 | train_wall 38 | gb_free 10.1 | wall 1153
2022-08-16 22:30:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 22:30:57 | INFO | fairseq.trainer | begin training epoch 11
2022-08-16 22:30:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:31:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:31:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:31:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:31:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:31:51 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 5.764 | nll_loss 3.001 | ppl 8 | bleu 37.1 | wps 2116.9 | wpb 933.5 | bsz 59.6 | num_updates 891 | best_bleu 37.1
2022-08-16 22:31:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 891 updates
2022-08-16 22:31:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint11.pt
2022-08-16 22:31:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint11.pt
2022-08-16 22:32:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint11.pt (epoch 11 @ 891 updates, score 37.1) (writing took 67.60441217944026 seconds)
2022-08-16 22:32:59 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-08-16 22:32:59 | INFO | train | epoch 011 | loss 5.816 | nll_loss 3.369 | ppl 10.33 | wps 3663.9 | ups 0.66 | wpb 5523.2 | bsz 358 | num_updates 891 | lr 0.000222828 | gnorm 1.213 | train_wall 38 | gb_free 10.3 | wall 1275
2022-08-16 22:32:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 22:32:59 | INFO | fairseq.trainer | begin training epoch 12
2022-08-16 22:32:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:33:05 | INFO | train_inner | epoch 012:      9 / 81 loss=5.81, nll_loss=3.36, ppl=10.27, wps=2977.1, ups=0.54, wpb=5510.9, bsz=357, num_updates=900, lr=0.000225077, gnorm=1.236, train_wall=47, gb_free=10.1, wall=1281
2022-08-16 22:33:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:33:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:33:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:33:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:33:54 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 5.634 | nll_loss 2.837 | ppl 7.14 | bleu 38.81 | wps 1112.9 | wpb 933.5 | bsz 59.6 | num_updates 972 | best_bleu 38.81
2022-08-16 22:33:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 972 updates
2022-08-16 22:33:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint12.pt
2022-08-16 22:33:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint12.pt
2022-08-16 22:34:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint12.pt (epoch 12 @ 972 updates, score 38.81) (writing took 61.8543001934886 seconds)
2022-08-16 22:34:56 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-08-16 22:34:56 | INFO | train | epoch 012 | loss 5.598 | nll_loss 3.081 | ppl 8.46 | wps 3826.8 | ups 0.69 | wpb 5523.2 | bsz 358 | num_updates 972 | lr 0.000243076 | gnorm 1.172 | train_wall 38 | gb_free 10.2 | wall 1392
2022-08-16 22:34:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 22:34:56 | INFO | fairseq.trainer | begin training epoch 13
2022-08-16 22:34:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:35:11 | INFO | train_inner | epoch 013:     28 / 81 loss=5.551, nll_loss=3.018, ppl=8.1, wps=4385.4, ups=0.79, wpb=5541.3, bsz=361.6, num_updates=1000, lr=0.000250075, gnorm=1.151, train_wall=47, gb_free=10.1, wall=1407
2022-08-16 22:35:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:35:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:35:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:35:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:35:45 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 5.602 | nll_loss 2.822 | ppl 7.07 | bleu 36.77 | wps 2200.5 | wpb 933.5 | bsz 59.6 | num_updates 1053 | best_bleu 38.81
2022-08-16 22:35:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 1053 updates
2022-08-16 22:35:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint13.pt
2022-08-16 22:35:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint13.pt
2022-08-16 22:36:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint13.pt (epoch 13 @ 1053 updates, score 36.77) (writing took 30.390248365700245 seconds)
2022-08-16 22:36:16 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-08-16 22:36:16 | INFO | train | epoch 013 | loss 5.418 | nll_loss 2.843 | ppl 7.18 | wps 5594.1 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 1053 | lr 0.000263324 | gnorm 1.116 | train_wall 38 | gb_free 10.1 | wall 1472
2022-08-16 22:36:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 22:36:16 | INFO | fairseq.trainer | begin training epoch 14
2022-08-16 22:36:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:36:43 | INFO | train_inner | epoch 014:     47 / 81 loss=5.373, nll_loss=2.783, ppl=6.88, wps=5960.2, ups=1.08, wpb=5505.4, bsz=350.6, num_updates=1100, lr=0.000275072, gnorm=1.146, train_wall=47, gb_free=10.1, wall=1500
2022-08-16 22:37:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:37:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:37:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:37:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:37:09 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 5.392 | nll_loss 2.496 | ppl 5.64 | bleu 44.74 | wps 1997.2 | wpb 933.5 | bsz 59.6 | num_updates 1134 | best_bleu 44.74
2022-08-16 22:37:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 1134 updates
2022-08-16 22:37:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint14.pt
2022-08-16 22:37:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint14.pt
2022-08-16 22:38:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint14.pt (epoch 14 @ 1134 updates, score 44.74) (writing took 78.57649016007781 seconds)
2022-08-16 22:38:28 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-08-16 22:38:28 | INFO | train | epoch 014 | loss 5.285 | nll_loss 2.668 | ppl 6.35 | wps 3370.9 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 1134 | lr 0.000283572 | gnorm 1.116 | train_wall 38 | gb_free 10.1 | wall 1604
2022-08-16 22:38:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 22:38:28 | INFO | fairseq.trainer | begin training epoch 15
2022-08-16 22:38:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:39:02 | INFO | train_inner | epoch 015:     66 / 81 loss=5.159, nll_loss=2.501, ppl=5.66, wps=3989.3, ups=0.72, wpb=5532.5, bsz=362.6, num_updates=1200, lr=0.00030007, gnorm=0.997, train_wall=47, gb_free=10.1, wall=1638
2022-08-16 22:39:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:39:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:39:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:39:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:39:18 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 5.293 | nll_loss 2.36 | ppl 5.13 | bleu 45.31 | wps 1922.8 | wpb 933.5 | bsz 59.6 | num_updates 1215 | best_bleu 45.31
2022-08-16 22:39:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 1215 updates
2022-08-16 22:39:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint15.pt
2022-08-16 22:39:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint15.pt
2022-08-16 22:40:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint15.pt (epoch 15 @ 1215 updates, score 45.31) (writing took 94.3265154287219 seconds)
2022-08-16 22:40:53 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-08-16 22:40:53 | INFO | train | epoch 015 | loss 5.128 | nll_loss 2.46 | ppl 5.5 | wps 3100.8 | ups 0.56 | wpb 5523.2 | bsz 358 | num_updates 1215 | lr 0.00030382 | gnorm 0.988 | train_wall 37 | gb_free 10.1 | wall 1749
2022-08-16 22:40:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 22:40:53 | INFO | fairseq.trainer | begin training epoch 16
2022-08-16 22:40:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:41:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:41:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:41:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:41:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:41:48 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 5.225 | nll_loss 2.249 | ppl 4.75 | bleu 47.88 | wps 2015.8 | wpb 933.5 | bsz 59.6 | num_updates 1296 | best_bleu 47.88
2022-08-16 22:41:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 1296 updates
2022-08-16 22:41:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint16.pt
2022-08-16 22:41:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint16.pt
2022-08-16 22:43:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint16.pt (epoch 16 @ 1296 updates, score 47.88) (writing took 92.78811328113079 seconds)
2022-08-16 22:43:21 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-08-16 22:43:21 | INFO | train | epoch 016 | loss 5.024 | nll_loss 2.324 | ppl 5.01 | wps 3012 | ups 0.55 | wpb 5523.2 | bsz 358 | num_updates 1296 | lr 0.000324068 | gnorm 0.969 | train_wall 36 | gb_free 10.1 | wall 1897
2022-08-16 22:43:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 22:43:21 | INFO | fairseq.trainer | begin training epoch 17
2022-08-16 22:43:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:43:24 | INFO | train_inner | epoch 017:      4 / 81 loss=5.036, nll_loss=2.339, ppl=5.06, wps=2098.6, ups=0.38, wpb=5505.8, bsz=357.8, num_updates=1300, lr=0.000325067, gnorm=0.981, train_wall=45, gb_free=10, wall=1901
2022-08-16 22:44:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:44:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:44:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:44:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:44:12 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 5.162 | nll_loss 2.2 | ppl 4.59 | bleu 50.04 | wps 1931.1 | wpb 933.5 | bsz 59.6 | num_updates 1377 | best_bleu 50.04
2022-08-16 22:44:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1377 updates
2022-08-16 22:44:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint17.pt
2022-08-16 22:44:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint17.pt
2022-08-16 22:45:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint17.pt (epoch 17 @ 1377 updates, score 50.04) (writing took 79.3807916752994 seconds)
2022-08-16 22:45:32 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-08-16 22:45:32 | INFO | train | epoch 017 | loss 4.904 | nll_loss 2.166 | ppl 4.49 | wps 3422.8 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 1377 | lr 0.000344316 | gnorm 0.899 | train_wall 36 | gb_free 10.2 | wall 2028
2022-08-16 22:45:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 22:45:32 | INFO | fairseq.trainer | begin training epoch 18
2022-08-16 22:45:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:45:44 | INFO | train_inner | epoch 018:     23 / 81 loss=4.887, nll_loss=2.143, ppl=4.42, wps=3950.8, ups=0.71, wpb=5526.4, bsz=356.7, num_updates=1400, lr=0.000350065, gnorm=0.885, train_wall=44, gb_free=10.1, wall=2040
2022-08-16 22:46:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:46:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:46:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:46:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:46:22 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 5.11 | nll_loss 2.136 | ppl 4.4 | bleu 49.48 | wps 1963.4 | wpb 933.5 | bsz 59.6 | num_updates 1458 | best_bleu 50.04
2022-08-16 22:46:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1458 updates
2022-08-16 22:46:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint18.pt
2022-08-16 22:46:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint18.pt
2022-08-16 22:46:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint18.pt (epoch 18 @ 1458 updates, score 49.48) (writing took 24.022037617862225 seconds)
2022-08-16 22:46:46 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-08-16 22:46:46 | INFO | train | epoch 018 | loss 4.804 | nll_loss 2.033 | ppl 4.09 | wps 6037.2 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 1458 | lr 0.000364564 | gnorm 0.869 | train_wall 36 | gb_free 10.2 | wall 2102
2022-08-16 22:46:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 22:46:46 | INFO | fairseq.trainer | begin training epoch 19
2022-08-16 22:46:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:47:07 | INFO | train_inner | epoch 019:     42 / 81 loss=4.762, nll_loss=1.979, ppl=3.94, wps=6635.5, ups=1.2, wpb=5509.9, bsz=357.4, num_updates=1500, lr=0.000375062, gnorm=0.856, train_wall=46, gb_free=10.1, wall=2123
2022-08-16 22:47:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:47:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:47:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:47:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:47:35 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 5.086 | nll_loss 2.102 | ppl 4.29 | bleu 49.67 | wps 1796.8 | wpb 933.5 | bsz 59.6 | num_updates 1539 | best_bleu 50.04
2022-08-16 22:47:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1539 updates
2022-08-16 22:47:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint19.pt
2022-08-16 22:47:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint19.pt
2022-08-16 22:48:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint19.pt (epoch 19 @ 1539 updates, score 49.67) (writing took 29.608568463474512 seconds)
2022-08-16 22:48:05 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-08-16 22:48:05 | INFO | train | epoch 019 | loss 4.707 | nll_loss 1.907 | ppl 3.75 | wps 5649.8 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 1539 | lr 0.000384812 | gnorm 0.821 | train_wall 38 | gb_free 10.2 | wall 2181
2022-08-16 22:48:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 22:48:05 | INFO | fairseq.trainer | begin training epoch 20
2022-08-16 22:48:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:48:39 | INFO | train_inner | epoch 020:     61 / 81 loss=4.652, nll_loss=1.835, ppl=3.57, wps=6064.2, ups=1.09, wpb=5547.3, bsz=362.8, num_updates=1600, lr=0.00040006, gnorm=0.793, train_wall=47, gb_free=10.1, wall=2215
2022-08-16 22:48:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:48:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:48:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:48:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:48:58 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 5.015 | nll_loss 2.019 | ppl 4.05 | bleu 51.75 | wps 1901 | wpb 933.5 | bsz 59.6 | num_updates 1620 | best_bleu 51.75
2022-08-16 22:48:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 1620 updates
2022-08-16 22:48:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint20.pt
2022-08-16 22:48:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint20.pt
2022-08-16 22:50:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint20.pt (epoch 20 @ 1620 updates, score 51.75) (writing took 65.1825380474329 seconds)
2022-08-16 22:50:03 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-08-16 22:50:03 | INFO | train | epoch 020 | loss 4.63 | nll_loss 1.806 | ppl 3.5 | wps 3785.7 | ups 0.69 | wpb 5523.2 | bsz 358 | num_updates 1620 | lr 0.000405059 | gnorm 0.773 | train_wall 38 | gb_free 10.2 | wall 2299
2022-08-16 22:50:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 22:50:03 | INFO | fairseq.trainer | begin training epoch 21
2022-08-16 22:50:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:50:47 | INFO | train_inner | epoch 021:     80 / 81 loss=4.572, nll_loss=1.729, ppl=3.31, wps=4317.7, ups=0.78, wpb=5531.6, bsz=355.3, num_updates=1700, lr=0.000425057, gnorm=0.765, train_wall=47, gb_free=10.1, wall=2343
2022-08-16 22:50:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:50:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:50:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:50:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:50:56 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 5.028 | nll_loss 2.039 | ppl 4.11 | bleu 51.8 | wps 1957.9 | wpb 933.5 | bsz 59.6 | num_updates 1701 | best_bleu 51.8
2022-08-16 22:50:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 1701 updates
2022-08-16 22:50:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint21.pt
2022-08-16 22:50:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint21.pt
2022-08-16 22:51:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint21.pt (epoch 21 @ 1701 updates, score 51.8) (writing took 27.9509520791471 seconds)
2022-08-16 22:51:24 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-08-16 22:51:24 | INFO | train | epoch 021 | loss 4.55 | nll_loss 1.7 | ppl 3.25 | wps 5529.5 | ups 1 | wpb 5523.2 | bsz 358 | num_updates 1701 | lr 0.000425307 | gnorm 0.771 | train_wall 38 | gb_free 10.1 | wall 2380
2022-08-16 22:51:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 22:51:24 | INFO | fairseq.trainer | begin training epoch 22
2022-08-16 22:51:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:52:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:52:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:52:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:52:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:52:19 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 4.981 | nll_loss 1.992 | ppl 3.98 | bleu 52.2 | wps 1236.1 | wpb 933.5 | bsz 59.6 | num_updates 1782 | best_bleu 52.2
2022-08-16 22:52:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 1782 updates
2022-08-16 22:52:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint22.pt
2022-08-16 22:52:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint22.pt
2022-08-16 22:53:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint22.pt (epoch 22 @ 1782 updates, score 52.2) (writing took 67.65328925848007 seconds)
2022-08-16 22:53:27 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-08-16 22:53:27 | INFO | train | epoch 022 | loss 4.488 | nll_loss 1.618 | ppl 3.07 | wps 3645.8 | ups 0.66 | wpb 5523.2 | bsz 358 | num_updates 1782 | lr 0.000445555 | gnorm 0.751 | train_wall 38 | gb_free 10.1 | wall 2503
2022-08-16 22:53:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 22:53:27 | INFO | fairseq.trainer | begin training epoch 23
2022-08-16 22:53:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:53:37 | INFO | train_inner | epoch 023:     18 / 81 loss=4.471, nll_loss=1.596, ppl=3.02, wps=3234.9, ups=0.59, wpb=5505.4, bsz=357, num_updates=1800, lr=0.000450055, gnorm=0.754, train_wall=47, gb_free=10.1, wall=2513
2022-08-16 22:54:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:54:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:54:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:54:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:54:18 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 4.961 | nll_loss 1.951 | ppl 3.87 | bleu 52.78 | wps 1501.9 | wpb 933.5 | bsz 59.6 | num_updates 1863 | best_bleu 52.78
2022-08-16 22:54:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 1863 updates
2022-08-16 22:54:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint23.pt
2022-08-16 22:54:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint23.pt
2022-08-16 22:55:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint23.pt (epoch 23 @ 1863 updates, score 52.78) (writing took 73.44912073761225 seconds)
2022-08-16 22:55:32 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-08-16 22:55:32 | INFO | train | epoch 023 | loss 4.403 | nll_loss 1.508 | ppl 2.84 | wps 3572.8 | ups 0.65 | wpb 5523.2 | bsz 358 | num_updates 1863 | lr 0.000465803 | gnorm 0.697 | train_wall 38 | gb_free 10.1 | wall 2628
2022-08-16 22:55:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 22:55:32 | INFO | fairseq.trainer | begin training epoch 24
2022-08-16 22:55:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:55:52 | INFO | train_inner | epoch 024:     37 / 81 loss=4.374, nll_loss=1.469, ppl=2.77, wps=4107.9, ups=0.74, wpb=5535.2, bsz=358, num_updates=1900, lr=0.000475052, gnorm=0.668, train_wall=46, gb_free=10, wall=2648
2022-08-16 22:56:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:56:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:56:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:56:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:56:23 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 4.96 | nll_loss 1.903 | ppl 3.74 | bleu 53.51 | wps 1969.5 | wpb 933.5 | bsz 59.6 | num_updates 1944 | best_bleu 53.51
2022-08-16 22:56:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 1944 updates
2022-08-16 22:56:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint24.pt
2022-08-16 22:56:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint24.pt
2022-08-16 22:57:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint24.pt (epoch 24 @ 1944 updates, score 53.51) (writing took 62.384328577667475 seconds)
2022-08-16 22:57:26 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-08-16 22:57:26 | INFO | train | epoch 024 | loss 4.341 | nll_loss 1.426 | ppl 2.69 | wps 3933.4 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 1944 | lr 0.000486051 | gnorm 0.682 | train_wall 38 | gb_free 10.1 | wall 2742
2022-08-16 22:57:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 22:57:26 | INFO | fairseq.trainer | begin training epoch 25
2022-08-16 22:57:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:57:54 | INFO | train_inner | epoch 025:     56 / 81 loss=4.304, nll_loss=1.378, ppl=2.6, wps=4548.2, ups=0.82, wpb=5557.8, bsz=360.3, num_updates=2000, lr=0.00050005, gnorm=0.665, train_wall=46, gb_free=10.1, wall=2770
2022-08-16 22:58:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:58:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:58:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:58:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:58:15 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 5 | nll_loss 1.983 | ppl 3.95 | bleu 51.41 | wps 2125.4 | wpb 933.5 | bsz 59.6 | num_updates 2025 | best_bleu 53.51
2022-08-16 22:58:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 2025 updates
2022-08-16 22:58:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint25.pt
2022-08-16 22:58:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint25.pt
2022-08-16 22:58:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint25.pt (epoch 25 @ 2025 updates, score 51.41) (writing took 36.98631693422794 seconds)
2022-08-16 22:58:52 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-08-16 22:58:52 | INFO | train | epoch 025 | loss 4.28 | nll_loss 1.346 | ppl 2.54 | wps 5201.4 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 2025 | lr 0.000506299 | gnorm 0.659 | train_wall 37 | gb_free 10.3 | wall 2828
2022-08-16 22:58:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 22:58:52 | INFO | fairseq.trainer | begin training epoch 26
2022-08-16 22:58:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 22:59:39 | INFO | train_inner | epoch 026:     75 / 81 loss=4.274, nll_loss=1.336, ppl=2.52, wps=5252.3, ups=0.95, wpb=5513.1, bsz=355.8, num_updates=2100, lr=0.000525047, gnorm=0.699, train_wall=49, gb_free=10.1, wall=2875
2022-08-16 22:59:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 22:59:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 22:59:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 22:59:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 22:59:52 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 4.942 | nll_loss 1.907 | ppl 3.75 | bleu 53.31 | wps 1833 | wpb 933.5 | bsz 59.6 | num_updates 2106 | best_bleu 53.51
2022-08-16 22:59:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 2106 updates
2022-08-16 22:59:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint26.pt
2022-08-16 22:59:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint26.pt
2022-08-16 23:00:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint26.pt (epoch 26 @ 2106 updates, score 53.31) (writing took 58.899196203798056 seconds)
2022-08-16 23:00:51 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-08-16 23:00:51 | INFO | train | epoch 026 | loss 4.253 | nll_loss 1.308 | ppl 2.48 | wps 3766.2 | ups 0.68 | wpb 5523.2 | bsz 358 | num_updates 2106 | lr 0.000526547 | gnorm 0.689 | train_wall 40 | gb_free 10.2 | wall 2947
2022-08-16 23:00:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:00:51 | INFO | fairseq.trainer | begin training epoch 27
2022-08-16 23:00:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:01:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:01:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:01:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:01:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:01:47 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 4.934 | nll_loss 1.946 | ppl 3.85 | bleu 52.53 | wps 2073.4 | wpb 933.5 | bsz 59.6 | num_updates 2187 | best_bleu 53.51
2022-08-16 23:01:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 2187 updates
2022-08-16 23:01:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint27.pt
2022-08-16 23:01:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint27.pt
2022-08-16 23:02:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint27.pt (epoch 27 @ 2187 updates, score 52.53) (writing took 13.615833453834057 seconds)
2022-08-16 23:02:01 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-08-16 23:02:01 | INFO | train | epoch 027 | loss 4.187 | nll_loss 1.222 | ppl 2.33 | wps 6345.5 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 2187 | lr 0.000546795 | gnorm 0.667 | train_wall 38 | gb_free 10.1 | wall 3017
2022-08-16 23:02:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:02:01 | INFO | fairseq.trainer | begin training epoch 28
2022-08-16 23:02:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:02:09 | INFO | train_inner | epoch 028:     13 / 81 loss=4.174, nll_loss=1.206, ppl=2.31, wps=3655.8, ups=0.67, wpb=5473.4, bsz=358.5, num_updates=2200, lr=0.000550045, gnorm=0.66, train_wall=47, gb_free=10.1, wall=3025
2022-08-16 23:02:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:02:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:02:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:02:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:02:50 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 4.937 | nll_loss 1.945 | ppl 3.85 | bleu 53.46 | wps 2071.8 | wpb 933.5 | bsz 59.6 | num_updates 2268 | best_bleu 53.51
2022-08-16 23:02:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 2268 updates
2022-08-16 23:02:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint28.pt
2022-08-16 23:02:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint28.pt
2022-08-16 23:03:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint28.pt (epoch 28 @ 2268 updates, score 53.46) (writing took 32.67760405316949 seconds)
2022-08-16 23:03:23 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-08-16 23:03:23 | INFO | train | epoch 028 | loss 4.14 | nll_loss 1.162 | ppl 2.24 | wps 5482.8 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 2268 | lr 0.000567043 | gnorm 0.634 | train_wall 38 | gb_free 10.1 | wall 3099
2022-08-16 23:03:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:03:23 | INFO | fairseq.trainer | begin training epoch 29
2022-08-16 23:03:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:03:40 | INFO | train_inner | epoch 029:     32 / 81 loss=4.116, nll_loss=1.131, ppl=2.19, wps=6090.5, ups=1.09, wpb=5565.7, bsz=362.4, num_updates=2300, lr=0.000575042, gnorm=0.624, train_wall=47, gb_free=10, wall=3116
2022-08-16 23:04:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:04:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:04:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:04:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:04:13 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 4.907 | nll_loss 1.924 | ppl 3.79 | bleu 53.93 | wps 1999.2 | wpb 933.5 | bsz 59.6 | num_updates 2349 | best_bleu 53.93
2022-08-16 23:04:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 2349 updates
2022-08-16 23:04:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint29.pt
2022-08-16 23:04:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint29.pt
2022-08-16 23:05:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint29.pt (epoch 29 @ 2349 updates, score 53.93) (writing took 80.62799849361181 seconds)
2022-08-16 23:05:33 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-08-16 23:05:33 | INFO | train | epoch 029 | loss 4.087 | nll_loss 1.093 | ppl 2.13 | wps 3425.8 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 2349 | lr 0.000587291 | gnorm 0.62 | train_wall 38 | gb_free 10.2 | wall 3230
2022-08-16 23:05:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:05:34 | INFO | fairseq.trainer | begin training epoch 30
2022-08-16 23:05:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:06:01 | INFO | train_inner | epoch 030:     51 / 81 loss=4.078, nll_loss=1.08, ppl=2.11, wps=3917.9, ups=0.71, wpb=5507.6, bsz=353.1, num_updates=2400, lr=0.00060004, gnorm=0.626, train_wall=49, gb_free=10.1, wall=3257
2022-08-16 23:06:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:06:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:06:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:06:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:06:24 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 4.896 | nll_loss 1.919 | ppl 3.78 | bleu 54.02 | wps 1930 | wpb 933.5 | bsz 59.6 | num_updates 2430 | best_bleu 54.02
2022-08-16 23:06:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 2430 updates
2022-08-16 23:06:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint30.pt
2022-08-16 23:06:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint30.pt
2022-08-16 23:07:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint30.pt (epoch 30 @ 2430 updates, score 54.02) (writing took 77.98162996396422 seconds)
2022-08-16 23:07:43 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-08-16 23:07:43 | INFO | train | epoch 030 | loss 4.057 | nll_loss 1.054 | ppl 2.08 | wps 3463.5 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 2430 | lr 0.000607539 | gnorm 0.625 | train_wall 40 | gb_free 10.2 | wall 3359
2022-08-16 23:07:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:07:43 | INFO | fairseq.trainer | begin training epoch 31
2022-08-16 23:07:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:08:19 | INFO | train_inner | epoch 031:     70 / 81 loss=4.03, nll_loss=1.02, ppl=2.03, wps=3996.5, ups=0.72, wpb=5523.2, bsz=357.9, num_updates=2500, lr=0.000625037, gnorm=0.616, train_wall=47, gb_free=10, wall=3395
2022-08-16 23:08:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:08:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:08:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:08:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:08:34 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 4.908 | nll_loss 1.928 | ppl 3.81 | bleu 53.34 | wps 1743 | wpb 933.5 | bsz 59.6 | num_updates 2511 | best_bleu 54.02
2022-08-16 23:08:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 2511 updates
2022-08-16 23:08:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint31.pt
2022-08-16 23:08:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint31.pt
2022-08-16 23:08:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint31.pt (epoch 31 @ 2511 updates, score 53.34) (writing took 15.886218748986721 seconds)
2022-08-16 23:08:50 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-08-16 23:08:50 | INFO | train | epoch 031 | loss 4.015 | nll_loss 1.001 | ppl 2 | wps 6625.2 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 2511 | lr 0.000627787 | gnorm 0.603 | train_wall 38 | gb_free 10.1 | wall 3426
2022-08-16 23:08:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:08:50 | INFO | fairseq.trainer | begin training epoch 32
2022-08-16 23:08:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:09:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:09:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:09:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:09:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:09:40 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 4.921 | nll_loss 1.943 | ppl 3.84 | bleu 53.66 | wps 2020.2 | wpb 933.5 | bsz 59.6 | num_updates 2592 | best_bleu 54.02
2022-08-16 23:09:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 2592 updates
2022-08-16 23:09:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint32.pt
2022-08-16 23:09:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint32.pt
2022-08-16 23:10:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint32.pt (epoch 32 @ 2592 updates, score 53.66) (writing took 47.89737021550536 seconds)
2022-08-16 23:10:28 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-08-16 23:10:28 | INFO | train | epoch 032 | loss 3.995 | nll_loss 0.974 | ppl 1.96 | wps 4560.2 | ups 0.83 | wpb 5523.2 | bsz 358 | num_updates 2592 | lr 0.000648035 | gnorm 0.628 | train_wall 38 | gb_free 10.1 | wall 3524
2022-08-16 23:10:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:10:28 | INFO | fairseq.trainer | begin training epoch 33
2022-08-16 23:10:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:10:33 | INFO | train_inner | epoch 033:      8 / 81 loss=3.996, nll_loss=0.976, ppl=1.97, wps=4098.4, ups=0.74, wpb=5505, bsz=355.8, num_updates=2600, lr=0.000650035, gnorm=0.621, train_wall=47, gb_free=10.1, wall=3529
2022-08-16 23:11:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:11:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:11:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:11:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:11:18 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 4.898 | nll_loss 1.928 | ppl 3.81 | bleu 54.54 | wps 1980.4 | wpb 933.5 | bsz 59.6 | num_updates 2673 | best_bleu 54.54
2022-08-16 23:11:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 2673 updates
2022-08-16 23:11:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint33.pt
2022-08-16 23:11:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint33.pt
2022-08-16 23:12:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint33.pt (epoch 33 @ 2673 updates, score 54.54) (writing took 82.53179317712784 seconds)
2022-08-16 23:12:40 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-08-16 23:12:40 | INFO | train | epoch 033 | loss 3.952 | nll_loss 0.922 | ppl 1.89 | wps 3387 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 2673 | lr 0.000668283 | gnorm 0.585 | train_wall 37 | gb_free 10.2 | wall 3656
2022-08-16 23:12:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:12:40 | INFO | fairseq.trainer | begin training epoch 34
2022-08-16 23:12:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:12:55 | INFO | train_inner | epoch 034:     27 / 81 loss=3.941, nll_loss=0.908, ppl=1.88, wps=3887.4, ups=0.7, wpb=5516.6, bsz=359, num_updates=2700, lr=0.000675032, gnorm=0.58, train_wall=46, gb_free=10, wall=3671
2022-08-16 23:13:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:13:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:13:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:13:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:13:32 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 4.908 | nll_loss 1.942 | ppl 3.84 | bleu 54.36 | wps 2112.3 | wpb 933.5 | bsz 59.6 | num_updates 2754 | best_bleu 54.54
2022-08-16 23:13:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 2754 updates
2022-08-16 23:13:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint34.pt
2022-08-16 23:13:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint34.pt
2022-08-16 23:13:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint34.pt (epoch 34 @ 2754 updates, score 54.36) (writing took 21.8203851506114 seconds)
2022-08-16 23:13:54 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-08-16 23:13:54 | INFO | train | epoch 034 | loss 3.925 | nll_loss 0.888 | ppl 1.85 | wps 6094.8 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 2754 | lr 0.000688531 | gnorm 0.57 | train_wall 38 | gb_free 10.2 | wall 3730
2022-08-16 23:13:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:13:54 | INFO | fairseq.trainer | begin training epoch 35
2022-08-16 23:13:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:14:18 | INFO | train_inner | epoch 035:     46 / 81 loss=3.919, nll_loss=0.881, ppl=1.84, wps=6689.6, ups=1.21, wpb=5540.5, bsz=360.5, num_updates=2800, lr=0.00070003, gnorm=0.579, train_wall=47, gb_free=10.1, wall=3754
2022-08-16 23:14:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:14:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:14:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:14:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:14:44 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 4.915 | nll_loss 1.965 | ppl 3.9 | bleu 53.78 | wps 2092.5 | wpb 933.5 | bsz 59.6 | num_updates 2835 | best_bleu 54.54
2022-08-16 23:14:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 2835 updates
2022-08-16 23:14:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint35.pt
2022-08-16 23:14:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint35.pt
2022-08-16 23:15:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint35.pt (epoch 35 @ 2835 updates, score 53.78) (writing took 25.862260546535254 seconds)
2022-08-16 23:15:10 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-08-16 23:15:10 | INFO | train | epoch 035 | loss 3.91 | nll_loss 0.871 | ppl 1.83 | wps 5861.9 | ups 1.06 | wpb 5523.2 | bsz 358 | num_updates 2835 | lr 0.000708779 | gnorm 0.587 | train_wall 39 | gb_free 10.1 | wall 3806
2022-08-16 23:15:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:15:10 | INFO | fairseq.trainer | begin training epoch 36
2022-08-16 23:15:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:15:46 | INFO | train_inner | epoch 036:     65 / 81 loss=3.896, nll_loss=0.854, ppl=1.81, wps=6317.7, ups=1.14, wpb=5534.3, bsz=359.1, num_updates=2900, lr=0.000725027, gnorm=0.576, train_wall=48, gb_free=10.1, wall=3842
2022-08-16 23:15:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:15:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:15:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:15:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:16:01 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 4.948 | nll_loss 2.009 | ppl 4.03 | bleu 52.08 | wps 2184.6 | wpb 933.5 | bsz 59.6 | num_updates 2916 | best_bleu 54.54
2022-08-16 23:16:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 2916 updates
2022-08-16 23:16:01 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint36.pt
2022-08-16 23:16:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint36.pt
2022-08-16 23:16:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint36.pt (epoch 36 @ 2916 updates, score 52.08) (writing took 33.63181594759226 seconds)
2022-08-16 23:16:35 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-08-16 23:16:35 | INFO | train | epoch 036 | loss 3.889 | nll_loss 0.845 | ppl 1.8 | wps 5276.4 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 2916 | lr 0.000729027 | gnorm 0.576 | train_wall 38 | gb_free 10.1 | wall 3891
2022-08-16 23:16:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:16:35 | INFO | fairseq.trainer | begin training epoch 37
2022-08-16 23:16:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:17:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:17:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:17:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:17:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:17:27 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 4.909 | nll_loss 1.974 | ppl 3.93 | bleu 54.5 | wps 1611.7 | wpb 933.5 | bsz 59.6 | num_updates 2997 | best_bleu 54.54
2022-08-16 23:17:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 2997 updates
2022-08-16 23:17:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint37.pt
2022-08-16 23:17:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint37.pt
2022-08-16 23:18:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint37.pt (epoch 37 @ 2997 updates, score 54.5) (writing took 36.13623324781656 seconds)
2022-08-16 23:18:04 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-08-16 23:18:04 | INFO | train | epoch 037 | loss 3.867 | nll_loss 0.819 | ppl 1.76 | wps 5033.2 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 2997 | lr 0.000749275 | gnorm 0.557 | train_wall 40 | gb_free 10.1 | wall 3980
2022-08-16 23:18:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:18:04 | INFO | fairseq.trainer | begin training epoch 38
2022-08-16 23:18:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:18:06 | INFO | train_inner | epoch 038:      3 / 81 loss=3.874, nll_loss=0.828, ppl=1.78, wps=3909.3, ups=0.71, wpb=5504.5, bsz=355, num_updates=3000, lr=0.000750025, gnorm=0.564, train_wall=48, gb_free=10, wall=3983
2022-08-16 23:18:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:18:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:18:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:18:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:18:56 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 4.949 | nll_loss 2.003 | ppl 4.01 | bleu 54.9 | wps 1956.7 | wpb 933.5 | bsz 59.6 | num_updates 3078 | best_bleu 54.9
2022-08-16 23:18:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 3078 updates
2022-08-16 23:18:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint38.pt
2022-08-16 23:18:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint38.pt
2022-08-16 23:19:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint38.pt (epoch 38 @ 3078 updates, score 54.9) (writing took 48.14880395308137 seconds)
2022-08-16 23:19:45 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-08-16 23:19:45 | INFO | train | epoch 038 | loss 3.858 | nll_loss 0.811 | ppl 1.75 | wps 4430.6 | ups 0.8 | wpb 5523.2 | bsz 358 | num_updates 3078 | lr 0.000769523 | gnorm 0.562 | train_wall 38 | gb_free 10.1 | wall 4081
2022-08-16 23:19:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:19:45 | INFO | fairseq.trainer | begin training epoch 39
2022-08-16 23:19:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:19:58 | INFO | train_inner | epoch 039:     22 / 81 loss=3.851, nll_loss=0.801, ppl=1.74, wps=4992.8, ups=0.9, wpb=5550.5, bsz=358, num_updates=3100, lr=0.000775022, gnorm=0.558, train_wall=47, gb_free=10, wall=4094
2022-08-16 23:20:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:20:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:20:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:20:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:20:37 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 4.933 | nll_loss 2.01 | ppl 4.03 | bleu 53.35 | wps 2055.7 | wpb 933.5 | bsz 59.6 | num_updates 3159 | best_bleu 54.9
2022-08-16 23:20:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 3159 updates
2022-08-16 23:20:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint39.pt
2022-08-16 23:20:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint39.pt
2022-08-16 23:21:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint39.pt (epoch 39 @ 3159 updates, score 53.35) (writing took 25.445389714092016 seconds)
2022-08-16 23:21:03 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-08-16 23:21:03 | INFO | train | epoch 039 | loss 3.842 | nll_loss 0.791 | ppl 1.73 | wps 5743.7 | ups 1.04 | wpb 5523.2 | bsz 358 | num_updates 3159 | lr 0.000789771 | gnorm 0.558 | train_wall 38 | gb_free 10.1 | wall 4159
2022-08-16 23:21:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:21:03 | INFO | fairseq.trainer | begin training epoch 40
2022-08-16 23:21:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:21:25 | INFO | train_inner | epoch 040:     41 / 81 loss=3.841, nll_loss=0.791, ppl=1.73, wps=6286.9, ups=1.15, wpb=5487.1, bsz=356.2, num_updates=3200, lr=0.00080002, gnorm=0.566, train_wall=47, gb_free=10, wall=4181
2022-08-16 23:21:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:21:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:21:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:21:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:21:55 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 4.912 | nll_loss 1.998 | ppl 3.99 | bleu 55.04 | wps 2149.2 | wpb 933.5 | bsz 59.6 | num_updates 3240 | best_bleu 55.04
2022-08-16 23:21:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 3240 updates
2022-08-16 23:21:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint40.pt
2022-08-16 23:21:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint40.pt
2022-08-16 23:22:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint40.pt (epoch 40 @ 3240 updates, score 55.04) (writing took 38.8563418649137 seconds)
2022-08-16 23:22:35 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-08-16 23:22:35 | INFO | train | epoch 040 | loss 3.838 | nll_loss 0.789 | ppl 1.73 | wps 4864.4 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 3240 | lr 0.000810019 | gnorm 0.566 | train_wall 38 | gb_free 10.1 | wall 4251
2022-08-16 23:22:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:22:35 | INFO | fairseq.trainer | begin training epoch 41
2022-08-16 23:22:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:23:06 | INFO | train_inner | epoch 041:     60 / 81 loss=3.831, nll_loss=0.781, ppl=1.72, wps=5476.2, ups=0.99, wpb=5543.4, bsz=361, num_updates=3300, lr=0.000825017, gnorm=0.561, train_wall=47, gb_free=10.1, wall=4282
2022-08-16 23:23:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:23:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:23:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:23:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:23:25 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 4.92 | nll_loss 2.028 | ppl 4.08 | bleu 53.94 | wps 2096.4 | wpb 933.5 | bsz 59.6 | num_updates 3321 | best_bleu 55.04
2022-08-16 23:23:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 3321 updates
2022-08-16 23:23:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint41.pt
2022-08-16 23:23:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint41.pt
2022-08-16 23:23:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint41.pt (epoch 41 @ 3321 updates, score 53.94) (writing took 26.718489553779364 seconds)
2022-08-16 23:23:52 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-08-16 23:23:52 | INFO | train | epoch 041 | loss 3.826 | nll_loss 0.775 | ppl 1.71 | wps 5772.9 | ups 1.05 | wpb 5523.2 | bsz 358 | num_updates 3321 | lr 0.000830267 | gnorm 0.558 | train_wall 37 | gb_free 10.1 | wall 4328
2022-08-16 23:23:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:23:52 | INFO | fairseq.trainer | begin training epoch 42
2022-08-16 23:23:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:24:34 | INFO | train_inner | epoch 042:     79 / 81 loss=3.814, nll_loss=0.763, ppl=1.7, wps=6318.6, ups=1.14, wpb=5534.9, bsz=359.7, num_updates=3400, lr=0.000850015, gnorm=0.535, train_wall=47, gb_free=10.1, wall=4370
2022-08-16 23:24:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:24:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:24:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:24:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:24:43 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 4.92 | nll_loss 1.996 | ppl 3.99 | bleu 54.42 | wps 2123.7 | wpb 933.5 | bsz 59.6 | num_updates 3402 | best_bleu 55.04
2022-08-16 23:24:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 3402 updates
2022-08-16 23:24:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint42.pt
2022-08-16 23:24:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint42.pt
2022-08-16 23:25:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint42.pt (epoch 42 @ 3402 updates, score 54.42) (writing took 39.32329122722149 seconds)
2022-08-16 23:25:22 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-08-16 23:25:22 | INFO | train | epoch 042 | loss 3.808 | nll_loss 0.755 | ppl 1.69 | wps 4953.8 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 3402 | lr 0.000850515 | gnorm 0.536 | train_wall 38 | gb_free 10.2 | wall 4419
2022-08-16 23:25:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:25:23 | INFO | fairseq.trainer | begin training epoch 43
2022-08-16 23:25:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:26:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:26:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:26:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:26:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:26:14 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 4.913 | nll_loss 2.041 | ppl 4.12 | bleu 54.48 | wps 2044.7 | wpb 933.5 | bsz 59.6 | num_updates 3483 | best_bleu 55.04
2022-08-16 23:26:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 3483 updates
2022-08-16 23:26:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint43.pt
2022-08-16 23:26:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint43.pt
2022-08-16 23:26:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint43.pt (epoch 43 @ 3483 updates, score 54.48) (writing took 20.41678401082754 seconds)
2022-08-16 23:26:35 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-08-16 23:26:35 | INFO | train | epoch 043 | loss 3.808 | nll_loss 0.756 | ppl 1.69 | wps 6156.8 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 3483 | lr 0.000870763 | gnorm 0.556 | train_wall 38 | gb_free 10.2 | wall 4491
2022-08-16 23:26:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:26:35 | INFO | fairseq.trainer | begin training epoch 44
2022-08-16 23:26:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:26:45 | INFO | train_inner | epoch 044:     17 / 81 loss=3.806, nll_loss=0.752, ppl=1.68, wps=4206.6, ups=0.76, wpb=5503.4, bsz=355.3, num_updates=3500, lr=0.000875012, gnorm=0.561, train_wall=47, gb_free=10.1, wall=4501
2022-08-16 23:27:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:27:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:27:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:27:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:27:23 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 4.909 | nll_loss 2.022 | ppl 4.06 | bleu 54.36 | wps 2055.5 | wpb 933.5 | bsz 59.6 | num_updates 3564 | best_bleu 55.04
2022-08-16 23:27:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 3564 updates
2022-08-16 23:27:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint44.pt
2022-08-16 23:27:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint44.pt
2022-08-16 23:27:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint44.pt (epoch 44 @ 3564 updates, score 54.36) (writing took 16.225957740098238 seconds)
2022-08-16 23:27:40 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-08-16 23:27:40 | INFO | train | epoch 044 | loss 3.798 | nll_loss 0.745 | ppl 1.68 | wps 6899.8 | ups 1.25 | wpb 5523.2 | bsz 358 | num_updates 3564 | lr 0.000891011 | gnorm 0.544 | train_wall 38 | gb_free 10.1 | wall 4556
2022-08-16 23:27:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:27:40 | INFO | fairseq.trainer | begin training epoch 45
2022-08-16 23:27:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:28:04 | INFO | train_inner | epoch 045:     36 / 81 loss=3.791, nll_loss=0.736, ppl=1.67, wps=6932.6, ups=1.26, wpb=5507.2, bsz=358.1, num_updates=3600, lr=0.00090001, gnorm=0.537, train_wall=48, gb_free=10.1, wall=4580
2022-08-16 23:28:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:28:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:28:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:28:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:28:40 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 4.936 | nll_loss 2.051 | ppl 4.14 | bleu 54.33 | wps 1963.6 | wpb 933.5 | bsz 59.6 | num_updates 3645 | best_bleu 55.04
2022-08-16 23:28:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 3645 updates
2022-08-16 23:28:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint45.pt
2022-08-16 23:28:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint45.pt
2022-08-16 23:29:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint45.pt (epoch 45 @ 3645 updates, score 54.33) (writing took 28.865917712450027 seconds)
2022-08-16 23:29:09 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-08-16 23:29:09 | INFO | train | epoch 045 | loss 3.793 | nll_loss 0.74 | ppl 1.67 | wps 5026 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 3645 | lr 0.000911259 | gnorm 0.552 | train_wall 38 | gb_free 10 | wall 4645
2022-08-16 23:29:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:29:09 | INFO | fairseq.trainer | begin training epoch 46
2022-08-16 23:29:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:29:40 | INFO | train_inner | epoch 046:     55 / 81 loss=3.791, nll_loss=0.739, ppl=1.67, wps=5748.2, ups=1.04, wpb=5535.1, bsz=358.9, num_updates=3700, lr=0.000925007, gnorm=0.551, train_wall=48, gb_free=10.1, wall=4676
2022-08-16 23:29:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:29:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:29:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:29:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:30:02 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 4.916 | nll_loss 2.043 | ppl 4.12 | bleu 53.89 | wps 1961.6 | wpb 933.5 | bsz 59.6 | num_updates 3726 | best_bleu 55.04
2022-08-16 23:30:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 3726 updates
2022-08-16 23:30:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint46.pt
2022-08-16 23:30:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint46.pt
2022-08-16 23:30:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint46.pt (epoch 46 @ 3726 updates, score 53.89) (writing took 41.36550705879927 seconds)
2022-08-16 23:30:43 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-08-16 23:30:43 | INFO | train | epoch 046 | loss 3.786 | nll_loss 0.733 | ppl 1.66 | wps 4729.5 | ups 0.86 | wpb 5523.2 | bsz 358 | num_updates 3726 | lr 0.000931507 | gnorm 0.544 | train_wall 40 | gb_free 10.1 | wall 4740
2022-08-16 23:30:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:30:44 | INFO | fairseq.trainer | begin training epoch 47
2022-08-16 23:30:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:31:23 | INFO | train_inner | epoch 047:     74 / 81 loss=3.785, nll_loss=0.733, ppl=1.66, wps=5398, ups=0.97, wpb=5553.2, bsz=358.8, num_updates=3800, lr=0.000950005, gnorm=0.545, train_wall=47, gb_free=10, wall=4779
2022-08-16 23:31:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:31:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:31:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:31:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:31:35 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 4.917 | nll_loss 2.041 | ppl 4.11 | bleu 53.98 | wps 2129.1 | wpb 933.5 | bsz 59.6 | num_updates 3807 | best_bleu 55.04
2022-08-16 23:31:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 3807 updates
2022-08-16 23:31:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint47.pt
2022-08-16 23:31:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint47.pt
2022-08-16 23:31:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint47.pt (epoch 47 @ 3807 updates, score 53.98) (writing took 20.203738778829575 seconds)
2022-08-16 23:31:55 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-08-16 23:31:55 | INFO | train | epoch 047 | loss 3.778 | nll_loss 0.724 | ppl 1.65 | wps 6256.4 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 3807 | lr 0.000951755 | gnorm 0.542 | train_wall 37 | gb_free 10.2 | wall 4811
2022-08-16 23:31:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:31:55 | INFO | fairseq.trainer | begin training epoch 48
2022-08-16 23:31:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:32:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:32:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:32:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:32:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:32:47 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 4.905 | nll_loss 2.014 | ppl 4.04 | bleu 54.24 | wps 1882.3 | wpb 933.5 | bsz 59.6 | num_updates 3888 | best_bleu 55.04
2022-08-16 23:32:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 3888 updates
2022-08-16 23:32:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint48.pt
2022-08-16 23:32:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint48.pt
2022-08-16 23:33:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint48.pt (epoch 48 @ 3888 updates, score 54.24) (writing took 14.182558916509151 seconds)
2022-08-16 23:33:01 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-08-16 23:33:01 | INFO | train | epoch 048 | loss 3.781 | nll_loss 0.73 | ppl 1.66 | wps 6759.3 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 3888 | lr 0.000972003 | gnorm 0.559 | train_wall 39 | gb_free 10.1 | wall 4877
2022-08-16 23:33:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:33:01 | INFO | fairseq.trainer | begin training epoch 49
2022-08-16 23:33:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:33:10 | INFO | train_inner | epoch 049:     12 / 81 loss=3.779, nll_loss=0.726, ppl=1.65, wps=5138.8, ups=0.94, wpb=5477.3, bsz=355.4, num_updates=3900, lr=0.000975002, gnorm=0.559, train_wall=48, gb_free=10.1, wall=4886
2022-08-16 23:33:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:33:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:33:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:33:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:33:53 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 4.903 | nll_loss 2.033 | ppl 4.09 | bleu 54.56 | wps 2041.9 | wpb 933.5 | bsz 59.6 | num_updates 3969 | best_bleu 55.04
2022-08-16 23:33:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 3969 updates
2022-08-16 23:33:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint49.pt
2022-08-16 23:33:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint49.pt
2022-08-16 23:34:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint49.pt (epoch 49 @ 3969 updates, score 54.56) (writing took 27.388774063438177 seconds)
2022-08-16 23:34:20 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-08-16 23:34:20 | INFO | train | epoch 049 | loss 3.78 | nll_loss 0.73 | ppl 1.66 | wps 5642.6 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 3969 | lr 0.000992251 | gnorm 0.574 | train_wall 40 | gb_free 10.2 | wall 4957
2022-08-16 23:34:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:34:21 | INFO | fairseq.trainer | begin training epoch 50
2022-08-16 23:34:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:34:38 | INFO | train_inner | epoch 050:     31 / 81 loss=3.771, nll_loss=0.719, ppl=1.65, wps=6308, ups=1.14, wpb=5546.7, bsz=362.3, num_updates=4000, lr=0.001, gnorm=0.559, train_wall=49, gb_free=10, wall=4974
2022-08-16 23:35:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:35:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:35:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:35:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:35:15 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 4.899 | nll_loss 2.025 | ppl 4.07 | bleu 54.87 | wps 1920.9 | wpb 933.5 | bsz 59.6 | num_updates 4050 | best_bleu 55.04
2022-08-16 23:35:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 4050 updates
2022-08-16 23:35:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint50.pt
2022-08-16 23:35:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint50.pt
2022-08-16 23:35:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint50.pt (epoch 50 @ 4050 updates, score 54.87) (writing took 2.1980459950864315 seconds)
2022-08-16 23:35:17 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-08-16 23:35:17 | INFO | train | epoch 050 | loss 3.769 | nll_loss 0.717 | ppl 1.64 | wps 7936.7 | ups 1.44 | wpb 5523.2 | bsz 358 | num_updates 4050 | lr 0.000993808 | gnorm 0.544 | train_wall 40 | gb_free 10.1 | wall 5013
2022-08-16 23:35:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:35:17 | INFO | fairseq.trainer | begin training epoch 51
2022-08-16 23:35:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:35:43 | INFO | train_inner | epoch 051:     50 / 81 loss=3.769, nll_loss=0.716, ppl=1.64, wps=8492.9, ups=1.54, wpb=5514.7, bsz=356.2, num_updates=4100, lr=0.00098773, gnorm=0.555, train_wall=48, gb_free=10.1, wall=5039
2022-08-16 23:35:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:35:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:35:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:35:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:36:07 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 4.916 | nll_loss 2.043 | ppl 4.12 | bleu 54.51 | wps 1969.8 | wpb 933.5 | bsz 59.6 | num_updates 4131 | best_bleu 55.04
2022-08-16 23:36:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 4131 updates
2022-08-16 23:36:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint51.pt
2022-08-16 23:36:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint51.pt
2022-08-16 23:36:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint51.pt (epoch 51 @ 4131 updates, score 54.51) (writing took 30.98972586169839 seconds)
2022-08-16 23:36:38 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-08-16 23:36:38 | INFO | train | epoch 051 | loss 3.76 | nll_loss 0.707 | ppl 1.63 | wps 5501.9 | ups 1 | wpb 5523.2 | bsz 358 | num_updates 4131 | lr 0.000984017 | gnorm 0.551 | train_wall 38 | gb_free 10.2 | wall 5094
2022-08-16 23:36:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:36:38 | INFO | fairseq.trainer | begin training epoch 52
2022-08-16 23:36:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:37:15 | INFO | train_inner | epoch 052:     69 / 81 loss=3.754, nll_loss=0.702, ppl=1.63, wps=5991.6, ups=1.08, wpb=5528.1, bsz=356.8, num_updates=4200, lr=0.0009759, gnorm=0.549, train_wall=47, gb_free=10.1, wall=5131
2022-08-16 23:37:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:37:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:37:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:37:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:37:29 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 4.922 | nll_loss 2.059 | ppl 4.17 | bleu 54.52 | wps 1986 | wpb 933.5 | bsz 59.6 | num_updates 4212 | best_bleu 55.04
2022-08-16 23:37:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 4212 updates
2022-08-16 23:37:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint52.pt
2022-08-16 23:37:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint52.pt
2022-08-16 23:38:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint52.pt (epoch 52 @ 4212 updates, score 54.52) (writing took 34.60665701702237 seconds)
2022-08-16 23:38:04 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-08-16 23:38:04 | INFO | train | epoch 052 | loss 3.747 | nll_loss 0.692 | ppl 1.62 | wps 5219.6 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 4212 | lr 0.000974509 | gnorm 0.553 | train_wall 38 | gb_free 10.1 | wall 5180
2022-08-16 23:38:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:38:04 | INFO | fairseq.trainer | begin training epoch 53
2022-08-16 23:38:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:38:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:38:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:38:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:38:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:38:56 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 4.913 | nll_loss 2.067 | ppl 4.19 | bleu 55.49 | wps 1956.1 | wpb 933.5 | bsz 59.6 | num_updates 4293 | best_bleu 55.49
2022-08-16 23:38:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 4293 updates
2022-08-16 23:38:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint53.pt
2022-08-16 23:38:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint53.pt
2022-08-16 23:39:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint53.pt (epoch 53 @ 4293 updates, score 55.49) (writing took 59.52429924532771 seconds)
2022-08-16 23:39:56 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-08-16 23:39:56 | INFO | train | epoch 053 | loss 3.736 | nll_loss 0.679 | ppl 1.6 | wps 3999.5 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 4293 | lr 0.000965272 | gnorm 0.553 | train_wall 39 | gb_free 10.1 | wall 5292
2022-08-16 23:39:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:39:56 | INFO | fairseq.trainer | begin training epoch 54
2022-08-16 23:39:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:40:01 | INFO | train_inner | epoch 054:      7 / 81 loss=3.735, nll_loss=0.678, ppl=1.6, wps=3328.1, ups=0.6, wpb=5521, bsz=360.2, num_updates=4300, lr=0.000964486, gnorm=0.552, train_wall=47, gb_free=10.2, wall=5297
2022-08-16 23:40:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:40:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:40:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:40:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:40:47 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 4.913 | nll_loss 2.058 | ppl 4.16 | bleu 54.08 | wps 1808.6 | wpb 933.5 | bsz 59.6 | num_updates 4374 | best_bleu 55.49
2022-08-16 23:40:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 4374 updates
2022-08-16 23:40:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint54.pt
2022-08-16 23:40:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint54.pt
2022-08-16 23:41:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint54.pt (epoch 54 @ 4374 updates, score 54.08) (writing took 16.62334133684635 seconds)
2022-08-16 23:41:04 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-08-16 23:41:04 | INFO | train | epoch 054 | loss 3.719 | nll_loss 0.662 | ppl 1.58 | wps 6562.2 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 4374 | lr 0.000956292 | gnorm 0.534 | train_wall 38 | gb_free 10.3 | wall 5360
2022-08-16 23:41:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:41:04 | INFO | fairseq.trainer | begin training epoch 55
2022-08-16 23:41:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:41:18 | INFO | train_inner | epoch 055:     26 / 81 loss=3.714, nll_loss=0.655, ppl=1.57, wps=7170.8, ups=1.3, wpb=5528.5, bsz=359, num_updates=4400, lr=0.000953463, gnorm=0.531, train_wall=46, gb_free=10, wall=5374
2022-08-16 23:41:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:41:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:41:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:41:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:41:54 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 4.923 | nll_loss 2.05 | ppl 4.14 | bleu 55.46 | wps 2111.5 | wpb 933.5 | bsz 59.6 | num_updates 4455 | best_bleu 55.49
2022-08-16 23:41:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 4455 updates
2022-08-16 23:41:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint55.pt
2022-08-16 23:41:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint55.pt
2022-08-16 23:42:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint55.pt (epoch 55 @ 4455 updates, score 55.46) (writing took 20.576500095427036 seconds)
2022-08-16 23:42:15 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-08-16 23:42:15 | INFO | train | epoch 055 | loss 3.708 | nll_loss 0.649 | ppl 1.57 | wps 6330 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 4455 | lr 0.000947559 | gnorm 0.527 | train_wall 38 | gb_free 10.1 | wall 5431
2022-08-16 23:42:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:42:15 | INFO | fairseq.trainer | begin training epoch 56
2022-08-16 23:42:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:42:40 | INFO | train_inner | epoch 056:     45 / 81 loss=3.699, nll_loss=0.639, ppl=1.56, wps=6801, ups=1.22, wpb=5555.1, bsz=357.8, num_updates=4500, lr=0.000942809, gnorm=0.518, train_wall=50, gb_free=10.1, wall=5456
2022-08-16 23:42:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:42:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:42:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:42:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:43:05 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 4.922 | nll_loss 2.082 | ppl 4.23 | bleu 54.4 | wps 2180.8 | wpb 933.5 | bsz 59.6 | num_updates 4536 | best_bleu 55.49
2022-08-16 23:43:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 4536 updates
2022-08-16 23:43:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint56.pt
2022-08-16 23:43:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint56.pt
2022-08-16 23:43:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint56.pt (epoch 56 @ 4536 updates, score 54.4) (writing took 34.10091262310743 seconds)
2022-08-16 23:43:39 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-08-16 23:43:39 | INFO | train | epoch 056 | loss 3.691 | nll_loss 0.63 | ppl 1.55 | wps 5284.1 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 4536 | lr 0.00093906 | gnorm 0.514 | train_wall 41 | gb_free 10.1 | wall 5515
2022-08-16 23:43:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:43:39 | INFO | fairseq.trainer | begin training epoch 57
2022-08-16 23:43:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:44:13 | INFO | train_inner | epoch 057:     64 / 81 loss=3.691, nll_loss=0.631, ppl=1.55, wps=5859.1, ups=1.06, wpb=5502.6, bsz=355.4, num_updates=4600, lr=0.000932505, gnorm=0.519, train_wall=48, gb_free=10.1, wall=5550
2022-08-16 23:44:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:44:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:44:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:44:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:44:31 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 4.917 | nll_loss 2.075 | ppl 4.21 | bleu 54.42 | wps 1965.3 | wpb 933.5 | bsz 59.6 | num_updates 4617 | best_bleu 55.49
2022-08-16 23:44:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 4617 updates
2022-08-16 23:44:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint57.pt
2022-08-16 23:44:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint57.pt
2022-08-16 23:44:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint57.pt (epoch 57 @ 4617 updates, score 54.42) (writing took 28.638057701289654 seconds)
2022-08-16 23:44:59 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-08-16 23:44:59 | INFO | train | epoch 057 | loss 3.685 | nll_loss 0.625 | ppl 1.54 | wps 5578.5 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 4617 | lr 0.000930786 | gnorm 0.52 | train_wall 39 | gb_free 10.1 | wall 5596
2022-08-16 23:45:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:45:00 | INFO | fairseq.trainer | begin training epoch 58
2022-08-16 23:45:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:45:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:45:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:45:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:45:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:45:52 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 4.893 | nll_loss 2.048 | ppl 4.14 | bleu 55.93 | wps 1832.3 | wpb 933.5 | bsz 59.6 | num_updates 4698 | best_bleu 55.93
2022-08-16 23:45:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 4698 updates
2022-08-16 23:45:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint58.pt
2022-08-16 23:45:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint58.pt
2022-08-16 23:46:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint58.pt (epoch 58 @ 4698 updates, score 55.93) (writing took 40.00532815977931 seconds)
2022-08-16 23:46:32 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-08-16 23:46:32 | INFO | train | epoch 058 | loss 3.674 | nll_loss 0.613 | ppl 1.53 | wps 4815.5 | ups 0.87 | wpb 5523.2 | bsz 358 | num_updates 4698 | lr 0.000922728 | gnorm 0.511 | train_wall 39 | gb_free 10.2 | wall 5689
2022-08-16 23:46:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:46:33 | INFO | fairseq.trainer | begin training epoch 59
2022-08-16 23:46:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:46:35 | INFO | train_inner | epoch 059:      2 / 81 loss=3.677, nll_loss=0.617, ppl=1.53, wps=3898.2, ups=0.71, wpb=5498.7, bsz=357.8, num_updates=4700, lr=0.000922531, gnorm=0.517, train_wall=48, gb_free=10.1, wall=5691
2022-08-16 23:47:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:47:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:47:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:47:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:47:25 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 4.924 | nll_loss 2.109 | ppl 4.31 | bleu 54.33 | wps 2164.7 | wpb 933.5 | bsz 59.6 | num_updates 4779 | best_bleu 55.93
2022-08-16 23:47:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 4779 updates
2022-08-16 23:47:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint59.pt
2022-08-16 23:47:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint59.pt
2022-08-16 23:47:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint59.pt (epoch 59 @ 4779 updates, score 54.33) (writing took 13.679168671369553 seconds)
2022-08-16 23:47:39 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-08-16 23:47:39 | INFO | train | epoch 059 | loss 3.669 | nll_loss 0.608 | ppl 1.52 | wps 6754.5 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 4779 | lr 0.000914874 | gnorm 0.525 | train_wall 38 | gb_free 10.2 | wall 5755
2022-08-16 23:47:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:47:39 | INFO | fairseq.trainer | begin training epoch 60
2022-08-16 23:47:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:47:51 | INFO | train_inner | epoch 060:     21 / 81 loss=3.666, nll_loss=0.604, ppl=1.52, wps=7211.2, ups=1.31, wpb=5513.8, bsz=354.6, num_updates=4800, lr=0.000912871, gnorm=0.523, train_wall=47, gb_free=10.1, wall=5767
2022-08-16 23:48:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:48:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:48:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:48:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:48:30 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 4.899 | nll_loss 2.079 | ppl 4.22 | bleu 55.47 | wps 1900.9 | wpb 933.5 | bsz 59.6 | num_updates 4860 | best_bleu 55.93
2022-08-16 23:48:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 4860 updates
2022-08-16 23:48:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint60.pt
2022-08-16 23:48:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint60.pt
2022-08-16 23:48:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint60.pt (epoch 60 @ 4860 updates, score 55.47) (writing took 19.736742705106735 seconds)
2022-08-16 23:48:50 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-08-16 23:48:50 | INFO | train | epoch 060 | loss 3.653 | nll_loss 0.59 | ppl 1.51 | wps 6238.8 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 4860 | lr 0.000907218 | gnorm 0.5 | train_wall 39 | gb_free 10.1 | wall 5827
2022-08-16 23:48:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:48:50 | INFO | fairseq.trainer | begin training epoch 61
2022-08-16 23:48:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:49:12 | INFO | train_inner | epoch 061:     40 / 81 loss=3.65, nll_loss=0.587, ppl=1.5, wps=6835.5, ups=1.24, wpb=5520.7, bsz=356.2, num_updates=4900, lr=0.000903508, gnorm=0.494, train_wall=48, gb_free=10.1, wall=5848
2022-08-16 23:49:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:49:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:49:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:49:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:49:42 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 4.915 | nll_loss 2.084 | ppl 4.24 | bleu 55.3 | wps 2056 | wpb 933.5 | bsz 59.6 | num_updates 4941 | best_bleu 55.93
2022-08-16 23:49:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 4941 updates
2022-08-16 23:49:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint61.pt
2022-08-16 23:49:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint61.pt
2022-08-16 23:50:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint61.pt (epoch 61 @ 4941 updates, score 55.3) (writing took 27.317085675895214 seconds)
2022-08-16 23:50:09 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-08-16 23:50:09 | INFO | train | epoch 061 | loss 3.645 | nll_loss 0.582 | ppl 1.5 | wps 5675.9 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 4941 | lr 0.000899751 | gnorm 0.497 | train_wall 39 | gb_free 10.2 | wall 5905
2022-08-16 23:50:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:50:09 | INFO | fairseq.trainer | begin training epoch 62
2022-08-16 23:50:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:50:41 | INFO | train_inner | epoch 062:     59 / 81 loss=3.638, nll_loss=0.575, ppl=1.49, wps=6263.4, ups=1.13, wpb=5560.2, bsz=364.2, num_updates=5000, lr=0.000894427, gnorm=0.49, train_wall=48, gb_free=10.1, wall=5937
2022-08-16 23:50:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:50:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:50:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:50:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:51:00 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 4.92 | nll_loss 2.104 | ppl 4.3 | bleu 54.63 | wps 2048.7 | wpb 933.5 | bsz 59.6 | num_updates 5022 | best_bleu 55.93
2022-08-16 23:51:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 5022 updates
2022-08-16 23:51:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint62.pt
2022-08-16 23:51:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint62.pt
2022-08-16 23:51:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint62.pt (epoch 62 @ 5022 updates, score 54.63) (writing took 18.617507603019476 seconds)
2022-08-16 23:51:19 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-08-16 23:51:19 | INFO | train | epoch 062 | loss 3.637 | nll_loss 0.574 | ppl 1.49 | wps 6399.5 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 5022 | lr 0.000892466 | gnorm 0.487 | train_wall 38 | gb_free 10 | wall 5975
2022-08-16 23:51:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:51:19 | INFO | fairseq.trainer | begin training epoch 63
2022-08-16 23:51:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:51:59 | INFO | train_inner | epoch 063:     78 / 81 loss=3.634, nll_loss=0.571, ppl=1.49, wps=7040.9, ups=1.28, wpb=5517.5, bsz=357.5, num_updates=5100, lr=0.000885615, gnorm=0.486, train_wall=47, gb_free=10.1, wall=6015
2022-08-16 23:52:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:52:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:52:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:52:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:52:09 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 4.924 | nll_loss 2.094 | ppl 4.27 | bleu 56.03 | wps 1943.4 | wpb 933.5 | bsz 59.6 | num_updates 5103 | best_bleu 56.03
2022-08-16 23:52:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 5103 updates
2022-08-16 23:52:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint63.pt
2022-08-16 23:52:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint63.pt
2022-08-16 23:52:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint63.pt (epoch 63 @ 5103 updates, score 56.03) (writing took 31.808336064219475 seconds)
2022-08-16 23:52:41 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-08-16 23:52:41 | INFO | train | epoch 063 | loss 3.629 | nll_loss 0.566 | ppl 1.48 | wps 5445.4 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 5103 | lr 0.000885355 | gnorm 0.482 | train_wall 39 | gb_free 10.2 | wall 6057
2022-08-16 23:52:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:52:41 | INFO | fairseq.trainer | begin training epoch 64
2022-08-16 23:52:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:53:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:53:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:53:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:53:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:53:30 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 4.919 | nll_loss 2.09 | ppl 4.26 | bleu 54.72 | wps 2098.8 | wpb 933.5 | bsz 59.6 | num_updates 5184 | best_bleu 56.03
2022-08-16 23:53:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 5184 updates
2022-08-16 23:53:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint64.pt
2022-08-16 23:53:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint64.pt
2022-08-16 23:53:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint64.pt (epoch 64 @ 5184 updates, score 54.72) (writing took 21.154456194490194 seconds)
2022-08-16 23:53:52 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-08-16 23:53:52 | INFO | train | epoch 064 | loss 3.624 | nll_loss 0.562 | ppl 1.48 | wps 6354.2 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 5184 | lr 0.00087841 | gnorm 0.485 | train_wall 38 | gb_free 10.2 | wall 6128
2022-08-16 23:53:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:53:52 | INFO | fairseq.trainer | begin training epoch 65
2022-08-16 23:53:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:54:01 | INFO | train_inner | epoch 065:     16 / 81 loss=3.62, nll_loss=0.557, ppl=1.47, wps=4499.5, ups=0.82, wpb=5514.1, bsz=361.1, num_updates=5200, lr=0.000877058, gnorm=0.479, train_wall=47, gb_free=10.1, wall=6138
2022-08-16 23:54:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:54:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:54:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:54:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:54:44 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 4.915 | nll_loss 2.091 | ppl 4.26 | bleu 55.75 | wps 2019 | wpb 933.5 | bsz 59.6 | num_updates 5265 | best_bleu 56.03
2022-08-16 23:54:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 5265 updates
2022-08-16 23:54:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint65.pt
2022-08-16 23:54:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint65.pt
2022-08-16 23:54:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint65.pt (epoch 65 @ 5265 updates, score 55.75) (writing took 14.198530353605747 seconds)
2022-08-16 23:54:58 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-08-16 23:54:58 | INFO | train | epoch 065 | loss 3.617 | nll_loss 0.554 | ppl 1.47 | wps 6723 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 5265 | lr 0.000871627 | gnorm 0.49 | train_wall 40 | gb_free 10.1 | wall 6194
2022-08-16 23:54:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:54:58 | INFO | fairseq.trainer | begin training epoch 66
2022-08-16 23:54:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:55:18 | INFO | train_inner | epoch 066:     35 / 81 loss=3.619, nll_loss=0.556, ppl=1.47, wps=7248.1, ups=1.31, wpb=5520.3, bsz=352.5, num_updates=5300, lr=0.000868744, gnorm=0.515, train_wall=49, gb_free=10.1, wall=6214
2022-08-16 23:55:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:55:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:55:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:55:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:55:50 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 4.946 | nll_loss 2.151 | ppl 4.44 | bleu 55.86 | wps 2116.8 | wpb 933.5 | bsz 59.6 | num_updates 5346 | best_bleu 56.03
2022-08-16 23:55:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 5346 updates
2022-08-16 23:55:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint66.pt
2022-08-16 23:55:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint66.pt
2022-08-16 23:56:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint66.pt (epoch 66 @ 5346 updates, score 55.86) (writing took 12.937362965196371 seconds)
2022-08-16 23:56:03 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-08-16 23:56:03 | INFO | train | epoch 066 | loss 3.613 | nll_loss 0.55 | ppl 1.46 | wps 6918.5 | ups 1.25 | wpb 5523.2 | bsz 358 | num_updates 5346 | lr 0.000864999 | gnorm 0.5 | train_wall 38 | gb_free 10.3 | wall 6259
2022-08-16 23:56:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:56:03 | INFO | fairseq.trainer | begin training epoch 67
2022-08-16 23:56:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:56:32 | INFO | train_inner | epoch 067:     54 / 81 loss=3.604, nll_loss=0.54, ppl=1.45, wps=7437, ups=1.35, wpb=5523.6, bsz=357.7, num_updates=5400, lr=0.000860663, gnorm=0.464, train_wall=47, gb_free=10.1, wall=6288
2022-08-16 23:56:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:56:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:56:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:56:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:56:54 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 4.927 | nll_loss 2.122 | ppl 4.35 | bleu 56.1 | wps 2116.6 | wpb 933.5 | bsz 59.6 | num_updates 5427 | best_bleu 56.1
2022-08-16 23:56:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 5427 updates
2022-08-16 23:56:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint67.pt
2022-08-16 23:56:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint67.pt
2022-08-16 23:57:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint67.pt (epoch 67 @ 5427 updates, score 56.1) (writing took 59.06986080110073 seconds)
2022-08-16 23:57:53 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-08-16 23:57:53 | INFO | train | epoch 067 | loss 3.599 | nll_loss 0.535 | ppl 1.45 | wps 4063.2 | ups 0.74 | wpb 5523.2 | bsz 358 | num_updates 5427 | lr 0.000858519 | gnorm 0.461 | train_wall 38 | gb_free 10.2 | wall 6369
2022-08-16 23:57:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:57:53 | INFO | fairseq.trainer | begin training epoch 68
2022-08-16 23:57:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-16 23:58:32 | INFO | train_inner | epoch 068:     73 / 81 loss=3.596, nll_loss=0.533, ppl=1.45, wps=4619.8, ups=0.83, wpb=5533, bsz=361, num_updates=5500, lr=0.000852803, gnorm=0.461, train_wall=47, gb_free=10.1, wall=6408
2022-08-16 23:58:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-16 23:58:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-16 23:58:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-16 23:58:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-16 23:58:43 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 4.938 | nll_loss 2.131 | ppl 4.38 | bleu 56.18 | wps 2161.9 | wpb 933.5 | bsz 59.6 | num_updates 5508 | best_bleu 56.18
2022-08-16 23:58:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 5508 updates
2022-08-16 23:58:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint68.pt
2022-08-16 23:58:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint68.pt
2022-08-16 23:59:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint68.pt (epoch 68 @ 5508 updates, score 56.18) (writing took 67.59568148851395 seconds)
2022-08-16 23:59:51 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-08-16 23:59:51 | INFO | train | epoch 068 | loss 3.595 | nll_loss 0.531 | ppl 1.45 | wps 3798.8 | ups 0.69 | wpb 5523.2 | bsz 358 | num_updates 5508 | lr 0.000852183 | gnorm 0.461 | train_wall 38 | gb_free 10.1 | wall 6487
2022-08-16 23:59:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-16 23:59:51 | INFO | fairseq.trainer | begin training epoch 69
2022-08-16 23:59:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:00:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:00:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:00:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:00:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:00:41 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 4.923 | nll_loss 2.133 | ppl 4.38 | bleu 55.52 | wps 1976.2 | wpb 933.5 | bsz 59.6 | num_updates 5589 | best_bleu 56.18
2022-08-17 00:00:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 5589 updates
2022-08-17 00:00:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint69.pt
2022-08-17 00:00:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint69.pt
2022-08-17 00:01:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint69.pt (epoch 69 @ 5589 updates, score 55.52) (writing took 38.443411365151405 seconds)
2022-08-17 00:01:19 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-08-17 00:01:19 | INFO | train | epoch 069 | loss 3.591 | nll_loss 0.528 | ppl 1.44 | wps 5044.8 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 5589 | lr 0.000845986 | gnorm 0.473 | train_wall 38 | gb_free 10.1 | wall 6576
2022-08-17 00:01:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:01:20 | INFO | fairseq.trainer | begin training epoch 70
2022-08-17 00:01:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:01:27 | INFO | train_inner | epoch 070:     11 / 81 loss=3.592, nll_loss=0.528, ppl=1.44, wps=3141.6, ups=0.57, wpb=5506.9, bsz=355.4, num_updates=5600, lr=0.000845154, gnorm=0.474, train_wall=46, gb_free=10.1, wall=6583
2022-08-17 00:02:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:02:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:02:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:02:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:02:16 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 4.942 | nll_loss 2.156 | ppl 4.46 | bleu 55.16 | wps 2097.9 | wpb 933.5 | bsz 59.6 | num_updates 5670 | best_bleu 56.18
2022-08-17 00:02:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 5670 updates
2022-08-17 00:02:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint70.pt
2022-08-17 00:02:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint70.pt
2022-08-17 00:02:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint70.pt (epoch 70 @ 5670 updates, score 55.16) (writing took 14.783053375780582 seconds)
2022-08-17 00:02:31 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-08-17 00:02:31 | INFO | train | epoch 070 | loss 3.585 | nll_loss 0.522 | ppl 1.44 | wps 6251.9 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 5670 | lr 0.000839921 | gnorm 0.463 | train_wall 38 | gb_free 10.1 | wall 6647
2022-08-17 00:02:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:02:31 | INFO | fairseq.trainer | begin training epoch 71
2022-08-17 00:02:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:02:47 | INFO | train_inner | epoch 071:     30 / 81 loss=3.581, nll_loss=0.516, ppl=1.43, wps=6893.5, ups=1.24, wpb=5546.2, bsz=361.4, num_updates=5700, lr=0.000837708, gnorm=0.454, train_wall=47, gb_free=10.1, wall=6664
2022-08-17 00:03:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:03:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:03:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:03:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:03:23 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 4.937 | nll_loss 2.149 | ppl 4.43 | bleu 56.24 | wps 1940.5 | wpb 933.5 | bsz 59.6 | num_updates 5751 | best_bleu 56.24
2022-08-17 00:03:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 5751 updates
2022-08-17 00:03:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint71.pt
2022-08-17 00:03:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint71.pt
2022-08-17 00:03:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint71.pt (epoch 71 @ 5751 updates, score 56.24) (writing took 26.329582192003727 seconds)
2022-08-17 00:03:49 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-08-17 00:03:49 | INFO | train | epoch 071 | loss 3.578 | nll_loss 0.515 | ppl 1.43 | wps 5699.5 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 5751 | lr 0.000833985 | gnorm 0.456 | train_wall 38 | gb_free 10.1 | wall 6726
2022-08-17 00:03:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:03:50 | INFO | fairseq.trainer | begin training epoch 72
2022-08-17 00:03:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:04:19 | INFO | train_inner | epoch 072:     49 / 81 loss=3.576, nll_loss=0.513, ppl=1.43, wps=6035, ups=1.09, wpb=5520.7, bsz=358.5, num_updates=5800, lr=0.000830455, gnorm=0.458, train_wall=48, gb_free=10, wall=6755
2022-08-17 00:04:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:04:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:04:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:04:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:04:46 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 4.952 | nll_loss 2.162 | ppl 4.48 | bleu 56.11 | wps 2125.4 | wpb 933.5 | bsz 59.6 | num_updates 5832 | best_bleu 56.24
2022-08-17 00:04:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 5832 updates
2022-08-17 00:04:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint72.pt
2022-08-17 00:04:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint72.pt
2022-08-17 00:05:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint72.pt (epoch 72 @ 5832 updates, score 56.11) (writing took 26.630070604383945 seconds)
2022-08-17 00:05:12 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-08-17 00:05:12 | INFO | train | epoch 072 | loss 3.574 | nll_loss 0.511 | ppl 1.43 | wps 5393.7 | ups 0.98 | wpb 5523.2 | bsz 358 | num_updates 5832 | lr 0.000828173 | gnorm 0.462 | train_wall 39 | gb_free 10.1 | wall 6809
2022-08-17 00:05:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:05:13 | INFO | fairseq.trainer | begin training epoch 73
2022-08-17 00:05:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:05:50 | INFO | train_inner | epoch 073:     68 / 81 loss=3.573, nll_loss=0.511, ppl=1.42, wps=6089.7, ups=1.1, wpb=5546.1, bsz=356.2, num_updates=5900, lr=0.000823387, gnorm=0.454, train_wall=48, gb_free=10.2, wall=6846
2022-08-17 00:05:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:05:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:05:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:05:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:06:04 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 4.949 | nll_loss 2.143 | ppl 4.42 | bleu 55.59 | wps 1980.9 | wpb 933.5 | bsz 59.6 | num_updates 5913 | best_bleu 56.24
2022-08-17 00:06:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 5913 updates
2022-08-17 00:06:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint73.pt
2022-08-17 00:06:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint73.pt
2022-08-17 00:06:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint73.pt (epoch 73 @ 5913 updates, score 55.59) (writing took 36.838901687413454 seconds)
2022-08-17 00:06:41 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-08-17 00:06:41 | INFO | train | epoch 073 | loss 3.569 | nll_loss 0.506 | ppl 1.42 | wps 5023.4 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 5913 | lr 0.000822481 | gnorm 0.442 | train_wall 39 | gb_free 10.1 | wall 6898
2022-08-17 00:06:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:06:42 | INFO | fairseq.trainer | begin training epoch 74
2022-08-17 00:06:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:07:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:07:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:07:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:07:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:07:34 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 4.933 | nll_loss 2.159 | ppl 4.46 | bleu 55.64 | wps 1922.4 | wpb 933.5 | bsz 59.6 | num_updates 5994 | best_bleu 56.24
2022-08-17 00:07:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 5994 updates
2022-08-17 00:07:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint74.pt
2022-08-17 00:07:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint74.pt
2022-08-17 00:07:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint74.pt (epoch 74 @ 5994 updates, score 55.64) (writing took 18.076716352254152 seconds)
2022-08-17 00:07:52 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-08-17 00:07:52 | INFO | train | epoch 074 | loss 3.567 | nll_loss 0.505 | ppl 1.42 | wps 6344.4 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 5994 | lr 0.000816905 | gnorm 0.453 | train_wall 38 | gb_free 10.2 | wall 6968
2022-08-17 00:07:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:07:52 | INFO | fairseq.trainer | begin training epoch 75
2022-08-17 00:07:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:07:56 | INFO | train_inner | epoch 075:      6 / 81 loss=3.567, nll_loss=0.505, ppl=1.42, wps=4339.5, ups=0.79, wpb=5480.6, bsz=356.7, num_updates=6000, lr=0.000816497, gnorm=0.452, train_wall=46, gb_free=10.1, wall=6973
2022-08-17 00:08:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:08:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:08:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:08:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:08:44 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 4.932 | nll_loss 2.148 | ppl 4.43 | bleu 55.46 | wps 1953.9 | wpb 933.5 | bsz 59.6 | num_updates 6075 | best_bleu 56.24
2022-08-17 00:08:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 6075 updates
2022-08-17 00:08:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint75.pt
2022-08-17 00:08:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint75.pt
2022-08-17 00:09:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint75.pt (epoch 75 @ 6075 updates, score 55.46) (writing took 19.154749751091003 seconds)
2022-08-17 00:09:04 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-08-17 00:09:04 | INFO | train | epoch 075 | loss 3.562 | nll_loss 0.499 | ppl 1.41 | wps 6257.4 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 6075 | lr 0.000811441 | gnorm 0.434 | train_wall 39 | gb_free 10.1 | wall 7040
2022-08-17 00:09:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:09:04 | INFO | fairseq.trainer | begin training epoch 76
2022-08-17 00:09:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:09:21 | INFO | train_inner | epoch 076:     25 / 81 loss=3.558, nll_loss=0.495, ppl=1.41, wps=6561.4, ups=1.19, wpb=5524.4, bsz=363.4, num_updates=6100, lr=0.000809776, gnorm=0.427, train_wall=48, gb_free=10.1, wall=7057
2022-08-17 00:09:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:09:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:09:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:09:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:10:02 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 4.949 | nll_loss 2.167 | ppl 4.49 | bleu 55.55 | wps 2065.3 | wpb 933.5 | bsz 59.6 | num_updates 6156 | best_bleu 56.24
2022-08-17 00:10:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 6156 updates
2022-08-17 00:10:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint76.pt
2022-08-17 00:10:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint76.pt
2022-08-17 00:10:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint76.pt (epoch 76 @ 6156 updates, score 55.55) (writing took 29.905617266893387 seconds)
2022-08-17 00:10:32 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-08-17 00:10:32 | INFO | train | epoch 076 | loss 3.555 | nll_loss 0.492 | ppl 1.41 | wps 5041.8 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 6156 | lr 0.000806085 | gnorm 0.425 | train_wall 39 | gb_free 10.1 | wall 7128
2022-08-17 00:10:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:10:32 | INFO | fairseq.trainer | begin training epoch 77
2022-08-17 00:10:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:10:56 | INFO | train_inner | epoch 077:     44 / 81 loss=3.556, nll_loss=0.494, ppl=1.41, wps=5755, ups=1.05, wpb=5506.7, bsz=350.3, num_updates=6200, lr=0.000803219, gnorm=0.526, train_wall=47, gb_free=10, wall=7152
2022-08-17 00:11:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:11:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:11:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:11:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:11:24 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 4.943 | nll_loss 2.173 | ppl 4.51 | bleu 55.71 | wps 2009 | wpb 933.5 | bsz 59.6 | num_updates 6237 | best_bleu 56.24
2022-08-17 00:11:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 6237 updates
2022-08-17 00:11:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint77.pt
2022-08-17 00:11:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint77.pt
2022-08-17 00:12:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint77.pt (epoch 77 @ 6237 updates, score 55.71) (writing took 35.540762826800346 seconds)
2022-08-17 00:12:00 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-08-17 00:12:00 | INFO | train | epoch 077 | loss 3.556 | nll_loss 0.495 | ppl 1.41 | wps 5102.2 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 6237 | lr 0.000800833 | gnorm 0.571 | train_wall 37 | gb_free 10.1 | wall 7216
2022-08-17 00:12:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:12:00 | INFO | fairseq.trainer | begin training epoch 78
2022-08-17 00:12:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:12:35 | INFO | train_inner | epoch 078:     63 / 81 loss=3.552, nll_loss=0.491, ppl=1.41, wps=5624.8, ups=1.01, wpb=5567, bsz=363, num_updates=6300, lr=0.000796819, gnorm=0.457, train_wall=47, gb_free=10.1, wall=7251
2022-08-17 00:12:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:12:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:12:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:12:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:12:53 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 4.943 | nll_loss 2.166 | ppl 4.49 | bleu 55.5 | wps 2035.2 | wpb 933.5 | bsz 59.6 | num_updates 6318 | best_bleu 56.24
2022-08-17 00:12:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 6318 updates
2022-08-17 00:12:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint78.pt
2022-08-17 00:12:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint78.pt
2022-08-17 00:13:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint78.pt (epoch 78 @ 6318 updates, score 55.5) (writing took 23.707704462110996 seconds)
2022-08-17 00:13:17 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-08-17 00:13:17 | INFO | train | epoch 078 | loss 3.549 | nll_loss 0.487 | ppl 1.4 | wps 5830.2 | ups 1.06 | wpb 5523.2 | bsz 358 | num_updates 6318 | lr 0.000795683 | gnorm 0.44 | train_wall 39 | gb_free 10 | wall 7293
2022-08-17 00:13:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:13:17 | INFO | fairseq.trainer | begin training epoch 79
2022-08-17 00:13:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:13:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:13:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:13:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:13:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:14:05 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 4.94 | nll_loss 2.172 | ppl 4.51 | bleu 55.19 | wps 2138.3 | wpb 933.5 | bsz 59.6 | num_updates 6399 | best_bleu 56.24
2022-08-17 00:14:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 6399 updates
2022-08-17 00:14:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint79.pt
2022-08-17 00:14:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint79.pt
2022-08-17 00:14:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint79.pt (epoch 79 @ 6399 updates, score 55.19) (writing took 16.489795494824648 seconds)
2022-08-17 00:14:22 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-08-17 00:14:22 | INFO | train | epoch 079 | loss 3.545 | nll_loss 0.483 | ppl 1.4 | wps 6843.9 | ups 1.24 | wpb 5523.2 | bsz 358 | num_updates 6399 | lr 0.000790631 | gnorm 0.435 | train_wall 39 | gb_free 10.1 | wall 7358
2022-08-17 00:14:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:14:22 | INFO | fairseq.trainer | begin training epoch 80
2022-08-17 00:14:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:14:24 | INFO | train_inner | epoch 080:      1 / 81 loss=3.547, nll_loss=0.486, ppl=1.4, wps=5062.4, ups=0.92, wpb=5488.9, bsz=355.3, num_updates=6400, lr=0.000790569, gnorm=0.435, train_wall=48, gb_free=10.1, wall=7360
2022-08-17 00:15:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:15:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:15:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:15:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:15:16 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 4.948 | nll_loss 2.182 | ppl 4.54 | bleu 55.78 | wps 2056.9 | wpb 933.5 | bsz 59.6 | num_updates 6480 | best_bleu 56.24
2022-08-17 00:15:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 6480 updates
2022-08-17 00:15:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint80.pt
2022-08-17 00:15:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint80.pt
2022-08-17 00:15:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint80.pt (epoch 80 @ 6480 updates, score 55.78) (writing took 24.39087723568082 seconds)
2022-08-17 00:15:40 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-08-17 00:15:40 | INFO | train | epoch 080 | loss 3.543 | nll_loss 0.481 | ppl 1.4 | wps 5707.3 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 6480 | lr 0.000785674 | gnorm 0.44 | train_wall 39 | gb_free 10.1 | wall 7437
2022-08-17 00:15:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:15:41 | INFO | fairseq.trainer | begin training epoch 81
2022-08-17 00:15:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:15:53 | INFO | train_inner | epoch 081:     20 / 81 loss=3.541, nll_loss=0.478, ppl=1.39, wps=6161.9, ups=1.12, wpb=5519.3, bsz=358.8, num_updates=6500, lr=0.000784465, gnorm=0.434, train_wall=48, gb_free=10.1, wall=7449
2022-08-17 00:16:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:16:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:16:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:16:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:16:34 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 4.929 | nll_loss 2.163 | ppl 4.48 | bleu 56.09 | wps 2013.3 | wpb 933.5 | bsz 59.6 | num_updates 6561 | best_bleu 56.24
2022-08-17 00:16:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 6561 updates
2022-08-17 00:16:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint81.pt
2022-08-17 00:16:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint81.pt
2022-08-17 00:16:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint81.pt (epoch 81 @ 6561 updates, score 56.09) (writing took 20.073764361441135 seconds)
2022-08-17 00:16:54 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-08-17 00:16:54 | INFO | train | epoch 081 | loss 3.54 | nll_loss 0.479 | ppl 1.39 | wps 6082.4 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 6561 | lr 0.000780809 | gnorm 0.466 | train_wall 40 | gb_free 10.1 | wall 7510
2022-08-17 00:16:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:16:54 | INFO | fairseq.trainer | begin training epoch 82
2022-08-17 00:16:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:17:16 | INFO | train_inner | epoch 082:     39 / 81 loss=3.539, nll_loss=0.478, ppl=1.39, wps=6705.8, ups=1.21, wpb=5553.5, bsz=356, num_updates=6600, lr=0.000778499, gnorm=0.462, train_wall=49, gb_free=10.1, wall=7532
2022-08-17 00:17:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:17:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:17:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:17:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:17:46 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 4.934 | nll_loss 2.17 | ppl 4.5 | bleu 56.39 | wps 2058.1 | wpb 933.5 | bsz 59.6 | num_updates 6642 | best_bleu 56.39
2022-08-17 00:17:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 6642 updates
2022-08-17 00:17:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint82.pt
2022-08-17 00:17:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint82.pt
2022-08-17 00:18:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint82.pt (epoch 82 @ 6642 updates, score 56.39) (writing took 35.14592860266566 seconds)
2022-08-17 00:18:21 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-08-17 00:18:21 | INFO | train | epoch 082 | loss 3.536 | nll_loss 0.475 | ppl 1.39 | wps 5116.8 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 6642 | lr 0.000776034 | gnorm 0.424 | train_wall 39 | gb_free 10.1 | wall 7598
2022-08-17 00:18:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:18:22 | INFO | fairseq.trainer | begin training epoch 83
2022-08-17 00:18:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:18:53 | INFO | train_inner | epoch 083:     58 / 81 loss=3.531, nll_loss=0.471, ppl=1.39, wps=5712.1, ups=1.03, wpb=5549.1, bsz=364.3, num_updates=6700, lr=0.000772667, gnorm=0.411, train_wall=47, gb_free=10, wall=7629
2022-08-17 00:19:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:19:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:19:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:19:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:19:13 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 4.954 | nll_loss 2.19 | ppl 4.56 | bleu 55.56 | wps 2201.2 | wpb 933.5 | bsz 59.6 | num_updates 6723 | best_bleu 56.39
2022-08-17 00:19:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 6723 updates
2022-08-17 00:19:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint83.pt
2022-08-17 00:19:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint83.pt
2022-08-17 00:19:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint83.pt (epoch 83 @ 6723 updates, score 55.56) (writing took 33.39760912954807 seconds)
2022-08-17 00:19:46 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-08-17 00:19:46 | INFO | train | epoch 083 | loss 3.53 | nll_loss 0.469 | ppl 1.38 | wps 5283.6 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 6723 | lr 0.000771345 | gnorm 0.41 | train_wall 37 | gb_free 10.2 | wall 7682
2022-08-17 00:19:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:19:46 | INFO | fairseq.trainer | begin training epoch 84
2022-08-17 00:19:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:20:26 | INFO | train_inner | epoch 084:     77 / 81 loss=3.532, nll_loss=0.472, ppl=1.39, wps=5938.4, ups=1.08, wpb=5509.6, bsz=355, num_updates=6800, lr=0.000766965, gnorm=0.423, train_wall=47, gb_free=10.1, wall=7722
2022-08-17 00:20:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:20:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:20:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:20:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:20:37 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 4.927 | nll_loss 2.157 | ppl 4.46 | bleu 55.75 | wps 1907.7 | wpb 933.5 | bsz 59.6 | num_updates 6804 | best_bleu 56.39
2022-08-17 00:20:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 6804 updates
2022-08-17 00:20:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint84.pt
2022-08-17 00:20:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint84.pt
2022-08-17 00:21:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint84.pt (epoch 84 @ 6804 updates, score 55.75) (writing took 33.222221322357655 seconds)
2022-08-17 00:21:10 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-08-17 00:21:10 | INFO | train | epoch 084 | loss 3.53 | nll_loss 0.471 | ppl 1.39 | wps 5330.4 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 6804 | lr 0.00076674 | gnorm 0.424 | train_wall 38 | gb_free 10.1 | wall 7766
2022-08-17 00:21:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:21:10 | INFO | fairseq.trainer | begin training epoch 85
2022-08-17 00:21:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:21:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:21:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:21:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:21:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:22:01 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 4.939 | nll_loss 2.191 | ppl 4.57 | bleu 56.39 | wps 1947.7 | wpb 933.5 | bsz 59.6 | num_updates 6885 | best_bleu 56.39
2022-08-17 00:22:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 6885 updates
2022-08-17 00:22:01 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint85.pt
2022-08-17 00:22:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint85.pt
2022-08-17 00:22:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint85.pt (epoch 85 @ 6885 updates, score 56.39) (writing took 49.01393301039934 seconds)
2022-08-17 00:22:50 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-08-17 00:22:50 | INFO | train | epoch 085 | loss 3.528 | nll_loss 0.467 | ppl 1.38 | wps 4455.9 | ups 0.81 | wpb 5523.2 | bsz 358 | num_updates 6885 | lr 0.000762216 | gnorm 0.406 | train_wall 38 | gb_free 10.1 | wall 7867
2022-08-17 00:22:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:22:51 | INFO | fairseq.trainer | begin training epoch 86
2022-08-17 00:22:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:23:00 | INFO | train_inner | epoch 086:     15 / 81 loss=3.525, nll_loss=0.465, ppl=1.38, wps=3574.8, ups=0.65, wpb=5501.7, bsz=360.2, num_updates=6900, lr=0.000761387, gnorm=0.406, train_wall=46, gb_free=10, wall=7876
2022-08-17 00:23:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:23:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:23:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:23:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:23:40 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 4.947 | nll_loss 2.189 | ppl 4.56 | bleu 55.67 | wps 2149.9 | wpb 933.5 | bsz 59.6 | num_updates 6966 | best_bleu 56.39
2022-08-17 00:23:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 6966 updates
2022-08-17 00:23:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint86.pt
2022-08-17 00:23:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint86.pt
2022-08-17 00:23:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint86.pt (epoch 86 @ 6966 updates, score 55.67) (writing took 14.60788158327341 seconds)
2022-08-17 00:23:55 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-08-17 00:23:55 | INFO | train | epoch 086 | loss 3.524 | nll_loss 0.464 | ppl 1.38 | wps 6964.5 | ups 1.26 | wpb 5523.2 | bsz 358 | num_updates 6966 | lr 0.000757771 | gnorm 0.405 | train_wall 37 | gb_free 10.1 | wall 7931
2022-08-17 00:23:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:23:55 | INFO | fairseq.trainer | begin training epoch 87
2022-08-17 00:23:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:24:14 | INFO | train_inner | epoch 087:     34 / 81 loss=3.523, nll_loss=0.463, ppl=1.38, wps=7505.1, ups=1.35, wpb=5541, bsz=359.1, num_updates=7000, lr=0.000755929, gnorm=0.4, train_wall=46, gb_free=10.1, wall=7950
2022-08-17 00:24:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:24:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:24:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:24:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:24:45 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 4.946 | nll_loss 2.195 | ppl 4.58 | bleu 55.18 | wps 1907.5 | wpb 933.5 | bsz 59.6 | num_updates 7047 | best_bleu 56.39
2022-08-17 00:24:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 7047 updates
2022-08-17 00:24:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint87.pt
2022-08-17 00:24:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint87.pt
2022-08-17 00:25:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint87.pt (epoch 87 @ 7047 updates, score 55.18) (writing took 26.342469785362482 seconds)
2022-08-17 00:25:11 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-08-17 00:25:11 | INFO | train | epoch 087 | loss 3.522 | nll_loss 0.462 | ppl 1.38 | wps 5830.2 | ups 1.06 | wpb 5523.2 | bsz 358 | num_updates 7047 | lr 0.000753404 | gnorm 0.41 | train_wall 38 | gb_free 10.2 | wall 8008
2022-08-17 00:25:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:25:12 | INFO | fairseq.trainer | begin training epoch 88
2022-08-17 00:25:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:25:40 | INFO | train_inner | epoch 088:     53 / 81 loss=3.521, nll_loss=0.462, ppl=1.38, wps=6376.5, ups=1.16, wpb=5486.2, bsz=355, num_updates=7100, lr=0.000750587, gnorm=0.416, train_wall=46, gb_free=10.1, wall=8036
2022-08-17 00:25:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:25:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:25:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:25:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:26:02 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 4.945 | nll_loss 2.199 | ppl 4.59 | bleu 55.57 | wps 2044.1 | wpb 933.5 | bsz 59.6 | num_updates 7128 | best_bleu 56.39
2022-08-17 00:26:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 7128 updates
2022-08-17 00:26:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint88.pt
2022-08-17 00:26:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint88.pt
2022-08-17 00:26:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint88.pt (epoch 88 @ 7128 updates, score 55.57) (writing took 34.57403499633074 seconds)
2022-08-17 00:26:37 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-08-17 00:26:37 | INFO | train | epoch 088 | loss 3.519 | nll_loss 0.46 | ppl 1.38 | wps 5228.8 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 7128 | lr 0.000749111 | gnorm 0.411 | train_wall 37 | gb_free 10.2 | wall 8093
2022-08-17 00:26:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:26:37 | INFO | fairseq.trainer | begin training epoch 89
2022-08-17 00:26:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:27:14 | INFO | train_inner | epoch 089:     72 / 81 loss=3.522, nll_loss=0.463, ppl=1.38, wps=5894.3, ups=1.06, wpb=5566.6, bsz=357.2, num_updates=7200, lr=0.000745356, gnorm=0.673, train_wall=47, gb_free=10.1, wall=8130
2022-08-17 00:27:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:27:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:27:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:27:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:27:27 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 4.971 | nll_loss 2.225 | ppl 4.67 | bleu 56.17 | wps 2089.1 | wpb 933.5 | bsz 59.6 | num_updates 7209 | best_bleu 56.39
2022-08-17 00:27:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 7209 updates
2022-08-17 00:27:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint89.pt
2022-08-17 00:27:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint89.pt
2022-08-17 00:27:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint89.pt (epoch 89 @ 7209 updates, score 56.17) (writing took 28.92810057476163 seconds)
2022-08-17 00:27:56 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-08-17 00:27:56 | INFO | train | epoch 089 | loss 3.52 | nll_loss 0.461 | ppl 1.38 | wps 5671.9 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 7209 | lr 0.000744891 | gnorm 0.728 | train_wall 38 | gb_free 10.2 | wall 8172
2022-08-17 00:27:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:27:56 | INFO | fairseq.trainer | begin training epoch 90
2022-08-17 00:27:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:28:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:28:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:28:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:28:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:28:47 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 4.941 | nll_loss 2.19 | ppl 4.56 | bleu 56.16 | wps 2002.2 | wpb 933.5 | bsz 59.6 | num_updates 7290 | best_bleu 56.39
2022-08-17 00:28:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 7290 updates
2022-08-17 00:28:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint90.pt
2022-08-17 00:28:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint90.pt
2022-08-17 00:29:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint90.pt (epoch 90 @ 7290 updates, score 56.16) (writing took 13.579319380223751 seconds)
2022-08-17 00:29:01 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-08-17 00:29:01 | INFO | train | epoch 090 | loss 3.514 | nll_loss 0.455 | ppl 1.37 | wps 6923.2 | ups 1.25 | wpb 5523.2 | bsz 358 | num_updates 7290 | lr 0.000740741 | gnorm 0.397 | train_wall 39 | gb_free 10.2 | wall 8237
2022-08-17 00:29:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:29:01 | INFO | fairseq.trainer | begin training epoch 91
2022-08-17 00:29:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:29:07 | INFO | train_inner | epoch 091:     10 / 81 loss=3.513, nll_loss=0.453, ppl=1.37, wps=4865.3, ups=0.89, wpb=5481.1, bsz=357, num_updates=7300, lr=0.000740233, gnorm=0.393, train_wall=48, gb_free=10.2, wall=8243
2022-08-17 00:29:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:29:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:29:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:29:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:29:57 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 4.952 | nll_loss 2.204 | ppl 4.61 | bleu 56.12 | wps 2001.9 | wpb 933.5 | bsz 59.6 | num_updates 7371 | best_bleu 56.39
2022-08-17 00:29:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 7371 updates
2022-08-17 00:29:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint91.pt
2022-08-17 00:29:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint91.pt
2022-08-17 00:30:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint91.pt (epoch 91 @ 7371 updates, score 56.12) (writing took 20.499867737293243 seconds)
2022-08-17 00:30:18 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-08-17 00:30:18 | INFO | train | epoch 091 | loss 3.509 | nll_loss 0.45 | ppl 1.37 | wps 5790.5 | ups 1.05 | wpb 5523.2 | bsz 358 | num_updates 7371 | lr 0.000736659 | gnorm 0.379 | train_wall 39 | gb_free 10.2 | wall 8314
2022-08-17 00:30:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:30:18 | INFO | fairseq.trainer | begin training epoch 92
2022-08-17 00:30:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:30:34 | INFO | train_inner | epoch 092:     29 / 81 loss=3.508, nll_loss=0.449, ppl=1.37, wps=6315, ups=1.14, wpb=5527.8, bsz=356.2, num_updates=7400, lr=0.000735215, gnorm=0.379, train_wall=48, gb_free=10.1, wall=8331
2022-08-17 00:31:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:31:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:31:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:31:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:31:10 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 4.953 | nll_loss 2.213 | ppl 4.64 | bleu 56.08 | wps 1938.1 | wpb 933.5 | bsz 59.6 | num_updates 7452 | best_bleu 56.39
2022-08-17 00:31:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 7452 updates
2022-08-17 00:31:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint92.pt
2022-08-17 00:31:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint92.pt
2022-08-17 00:31:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint92.pt (epoch 92 @ 7452 updates, score 56.08) (writing took 32.56391140446067 seconds)
2022-08-17 00:31:43 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-08-17 00:31:43 | INFO | train | epoch 092 | loss 3.508 | nll_loss 0.449 | ppl 1.37 | wps 5250.1 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 7452 | lr 0.000732645 | gnorm 0.387 | train_wall 39 | gb_free 10.2 | wall 8399
2022-08-17 00:31:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:31:43 | INFO | fairseq.trainer | begin training epoch 93
2022-08-17 00:31:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:32:11 | INFO | train_inner | epoch 093:     48 / 81 loss=3.507, nll_loss=0.448, ppl=1.36, wps=5749.4, ups=1.03, wpb=5559.4, bsz=363.9, num_updates=7500, lr=0.000730297, gnorm=0.441, train_wall=48, gb_free=10.1, wall=8427
2022-08-17 00:32:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:32:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:32:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:32:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:32:37 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 4.973 | nll_loss 2.225 | ppl 4.68 | bleu 55.51 | wps 2032.4 | wpb 933.5 | bsz 59.6 | num_updates 7533 | best_bleu 56.39
2022-08-17 00:32:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 7533 updates
2022-08-17 00:32:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint93.pt
2022-08-17 00:32:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint93.pt
2022-08-17 00:32:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint93.pt (epoch 93 @ 7533 updates, score 55.51) (writing took 2.2717753387987614 seconds)
2022-08-17 00:32:39 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-08-17 00:32:39 | INFO | train | epoch 093 | loss 3.507 | nll_loss 0.45 | ppl 1.37 | wps 7944.7 | ups 1.44 | wpb 5523.2 | bsz 358 | num_updates 7533 | lr 0.000728695 | gnorm 0.492 | train_wall 38 | gb_free 10.1 | wall 8456
2022-08-17 00:32:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:32:39 | INFO | fairseq.trainer | begin training epoch 94
2022-08-17 00:32:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:33:21 | INFO | train_inner | epoch 094:     67 / 81 loss=3.508, nll_loss=0.451, ppl=1.37, wps=7879.6, ups=1.43, wpb=5512.6, bsz=355.5, num_updates=7600, lr=0.000725476, gnorm=0.51, train_wall=47, gb_free=10.1, wall=8497
2022-08-17 00:33:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:33:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:33:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:33:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:33:38 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 4.967 | nll_loss 2.227 | ppl 4.68 | bleu 55.34 | wps 2072 | wpb 933.5 | bsz 59.6 | num_updates 7614 | best_bleu 56.39
2022-08-17 00:33:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 7614 updates
2022-08-17 00:33:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint94.pt
2022-08-17 00:33:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint94.pt
2022-08-17 00:34:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint94.pt (epoch 94 @ 7614 updates, score 55.34) (writing took 25.363816894590855 seconds)
2022-08-17 00:34:03 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-08-17 00:34:04 | INFO | train | epoch 094 | loss 3.506 | nll_loss 0.448 | ppl 1.36 | wps 5313.4 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 7614 | lr 0.000724809 | gnorm 0.525 | train_wall 38 | gb_free 10.2 | wall 8540
2022-08-17 00:34:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:34:04 | INFO | fairseq.trainer | begin training epoch 95
2022-08-17 00:34:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:34:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:34:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:34:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:34:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:34:55 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 4.968 | nll_loss 2.23 | ppl 4.69 | bleu 55.5 | wps 1948.2 | wpb 933.5 | bsz 59.6 | num_updates 7695 | best_bleu 56.39
2022-08-17 00:34:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 7695 updates
2022-08-17 00:34:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint95.pt
2022-08-17 00:34:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint95.pt
2022-08-17 00:35:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint95.pt (epoch 95 @ 7695 updates, score 55.5) (writing took 30.960070751607418 seconds)
2022-08-17 00:35:26 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-08-17 00:35:26 | INFO | train | epoch 095 | loss 3.503 | nll_loss 0.445 | ppl 1.36 | wps 5437.4 | ups 0.98 | wpb 5523.2 | bsz 358 | num_updates 7695 | lr 0.000720984 | gnorm 0.404 | train_wall 38 | gb_free 10.1 | wall 8622
2022-08-17 00:35:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:35:26 | INFO | fairseq.trainer | begin training epoch 96
2022-08-17 00:35:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:35:30 | INFO | train_inner | epoch 096:      5 / 81 loss=3.504, nll_loss=0.446, ppl=1.36, wps=4270.1, ups=0.78, wpb=5487.2, bsz=355, num_updates=7700, lr=0.00072075, gnorm=0.426, train_wall=47, gb_free=10.1, wall=8626
2022-08-17 00:36:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:36:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:36:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:36:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:36:17 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 4.947 | nll_loss 2.208 | ppl 4.62 | bleu 55.91 | wps 2070.2 | wpb 933.5 | bsz 59.6 | num_updates 7776 | best_bleu 56.39
2022-08-17 00:36:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 7776 updates
2022-08-17 00:36:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint96.pt
2022-08-17 00:36:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint96.pt
2022-08-17 00:36:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint96.pt (epoch 96 @ 7776 updates, score 55.91) (writing took 17.47658045962453 seconds)
2022-08-17 00:36:35 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-08-17 00:36:35 | INFO | train | epoch 096 | loss 3.499 | nll_loss 0.442 | ppl 1.36 | wps 6456 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 7776 | lr 0.000717219 | gnorm 0.387 | train_wall 38 | gb_free 10.2 | wall 8691
2022-08-17 00:36:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:36:35 | INFO | fairseq.trainer | begin training epoch 97
2022-08-17 00:36:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:36:48 | INFO | train_inner | epoch 097:     24 / 81 loss=3.499, nll_loss=0.441, ppl=1.36, wps=7107.9, ups=1.28, wpb=5568.9, bsz=357.8, num_updates=7800, lr=0.000716115, gnorm=0.385, train_wall=46, gb_free=10.1, wall=8704
2022-08-17 00:37:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:37:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:37:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:37:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:37:24 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 4.965 | nll_loss 2.224 | ppl 4.67 | bleu 56.03 | wps 2004.1 | wpb 933.5 | bsz 59.6 | num_updates 7857 | best_bleu 56.39
2022-08-17 00:37:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 7857 updates
2022-08-17 00:37:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint97.pt
2022-08-17 00:37:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint97.pt
2022-08-17 00:37:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint97.pt (epoch 97 @ 7857 updates, score 56.03) (writing took 21.499228846281767 seconds)
2022-08-17 00:37:46 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-08-17 00:37:46 | INFO | train | epoch 097 | loss 3.5 | nll_loss 0.443 | ppl 1.36 | wps 6302.6 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 7857 | lr 0.000713513 | gnorm 0.389 | train_wall 38 | gb_free 10 | wall 8762
2022-08-17 00:37:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:37:46 | INFO | fairseq.trainer | begin training epoch 98
2022-08-17 00:37:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:38:10 | INFO | train_inner | epoch 098:     43 / 81 loss=3.5, nll_loss=0.444, ppl=1.36, wps=6705.8, ups=1.22, wpb=5498.3, bsz=361.5, num_updates=7900, lr=0.000711568, gnorm=0.422, train_wall=46, gb_free=10, wall=8786
2022-08-17 00:38:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:38:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:38:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:38:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:38:37 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 4.951 | nll_loss 2.21 | ppl 4.63 | bleu 56.78 | wps 2122.7 | wpb 933.5 | bsz 59.6 | num_updates 7938 | best_bleu 56.78
2022-08-17 00:38:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 7938 updates
2022-08-17 00:38:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint98.pt
2022-08-17 00:38:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint98.pt
2022-08-17 00:39:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint98.pt (epoch 98 @ 7938 updates, score 56.78) (writing took 56.87096453085542 seconds)
2022-08-17 00:39:34 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-08-17 00:39:34 | INFO | train | epoch 098 | loss 3.498 | nll_loss 0.441 | ppl 1.36 | wps 4152.9 | ups 0.75 | wpb 5523.2 | bsz 358 | num_updates 7938 | lr 0.000709863 | gnorm 0.417 | train_wall 37 | gb_free 10.1 | wall 8870
2022-08-17 00:39:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:39:34 | INFO | fairseq.trainer | begin training epoch 99
2022-08-17 00:39:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:40:06 | INFO | train_inner | epoch 099:     62 / 81 loss=3.493, nll_loss=0.435, ppl=1.35, wps=4761.3, ups=0.86, wpb=5531.2, bsz=355.5, num_updates=8000, lr=0.000707107, gnorm=0.373, train_wall=46, gb_free=10.1, wall=8902
2022-08-17 00:40:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:40:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:40:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:40:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:40:24 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 4.984 | nll_loss 2.248 | ppl 4.75 | bleu 56.21 | wps 2190.6 | wpb 933.5 | bsz 59.6 | num_updates 8019 | best_bleu 56.78
2022-08-17 00:40:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 8019 updates
2022-08-17 00:40:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint99.pt
2022-08-17 00:40:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint99.pt
2022-08-17 00:40:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint99.pt (epoch 99 @ 8019 updates, score 56.21) (writing took 25.158327028155327 seconds)
2022-08-17 00:40:49 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-08-17 00:40:49 | INFO | train | epoch 099 | loss 3.492 | nll_loss 0.434 | ppl 1.35 | wps 5960.1 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 8019 | lr 0.000706269 | gnorm 0.378 | train_wall 38 | gb_free 10.1 | wall 8945
2022-08-17 00:40:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:40:49 | INFO | fairseq.trainer | begin training epoch 100
2022-08-17 00:40:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:41:30 | INFO | train_inner | epoch 100:     81 / 81 loss=3.494, nll_loss=0.438, ppl=1.35, wps=6543.8, ups=1.19, wpb=5506.4, bsz=359.1, num_updates=8100, lr=0.000702728, gnorm=0.427, train_wall=48, gb_free=10, wall=8986
2022-08-17 00:41:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:41:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:41:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:41:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:41:39 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 4.971 | nll_loss 2.237 | ppl 4.71 | bleu 56.15 | wps 1955 | wpb 933.5 | bsz 59.6 | num_updates 8100 | best_bleu 56.78
2022-08-17 00:41:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 8100 updates
2022-08-17 00:41:39 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint100.pt
2022-08-17 00:41:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint100.pt
2022-08-17 00:41:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint100.pt (epoch 100 @ 8100 updates, score 56.15) (writing took 2.262010708451271 seconds)
2022-08-17 00:41:41 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-08-17 00:41:41 | INFO | train | epoch 100 | loss 3.493 | nll_loss 0.437 | ppl 1.35 | wps 8518.9 | ups 1.54 | wpb 5523.2 | bsz 358 | num_updates 8100 | lr 0.000702728 | gnorm 0.437 | train_wall 38 | gb_free 10 | wall 8998
2022-08-17 00:41:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:41:42 | INFO | fairseq.trainer | begin training epoch 101
2022-08-17 00:41:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:42:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:42:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:42:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:42:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:42:32 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 4.952 | nll_loss 2.221 | ppl 4.66 | bleu 56.07 | wps 2059.2 | wpb 933.5 | bsz 59.6 | num_updates 8181 | best_bleu 56.78
2022-08-17 00:42:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 8181 updates
2022-08-17 00:42:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint101.pt
2022-08-17 00:42:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint101.pt
2022-08-17 00:42:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint101.pt (epoch 101 @ 8181 updates, score 56.07) (writing took 19.883715711534023 seconds)
2022-08-17 00:42:52 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-08-17 00:42:52 | INFO | train | epoch 101 | loss 3.491 | nll_loss 0.435 | ppl 1.35 | wps 6362.2 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 8181 | lr 0.000699241 | gnorm 0.371 | train_wall 39 | gb_free 10.2 | wall 9068
2022-08-17 00:42:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:42:52 | INFO | fairseq.trainer | begin training epoch 102
2022-08-17 00:42:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:43:03 | INFO | train_inner | epoch 102:     19 / 81 loss=3.489, nll_loss=0.432, ppl=1.35, wps=5948.6, ups=1.08, wpb=5530.1, bsz=358.7, num_updates=8200, lr=0.00069843, gnorm=0.368, train_wall=47, gb_free=10.1, wall=9079
2022-08-17 00:43:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:43:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:43:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:43:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:43:43 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 4.964 | nll_loss 2.225 | ppl 4.67 | bleu 56.1 | wps 2115.7 | wpb 933.5 | bsz 59.6 | num_updates 8262 | best_bleu 56.78
2022-08-17 00:43:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 8262 updates
2022-08-17 00:43:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint102.pt
2022-08-17 00:43:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint102.pt
2022-08-17 00:44:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint102.pt (epoch 102 @ 8262 updates, score 56.1) (writing took 24.100863713771105 seconds)
2022-08-17 00:44:08 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-08-17 00:44:08 | INFO | train | epoch 102 | loss 3.49 | nll_loss 0.435 | ppl 1.35 | wps 5883.9 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 8262 | lr 0.000695805 | gnorm 0.373 | train_wall 38 | gb_free 10.2 | wall 9144
2022-08-17 00:44:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:44:08 | INFO | fairseq.trainer | begin training epoch 103
2022-08-17 00:44:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:44:29 | INFO | train_inner | epoch 103:     38 / 81 loss=3.491, nll_loss=0.435, ppl=1.35, wps=6424.1, ups=1.16, wpb=5524.5, bsz=357.2, num_updates=8300, lr=0.00069421, gnorm=0.375, train_wall=47, gb_free=10, wall=9165
2022-08-17 00:44:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:44:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:44:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:44:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:45:00 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 4.985 | nll_loss 2.257 | ppl 4.78 | bleu 55.49 | wps 2129.5 | wpb 933.5 | bsz 59.6 | num_updates 8343 | best_bleu 56.78
2022-08-17 00:45:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 8343 updates
2022-08-17 00:45:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint103.pt
2022-08-17 00:45:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint103.pt
2022-08-17 00:45:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint103.pt (epoch 103 @ 8343 updates, score 55.49) (writing took 13.875450689345598 seconds)
2022-08-17 00:45:14 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-08-17 00:45:14 | INFO | train | epoch 103 | loss 3.488 | nll_loss 0.432 | ppl 1.35 | wps 6705.5 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 8343 | lr 0.000692419 | gnorm 0.401 | train_wall 39 | gb_free 10 | wall 9211
2022-08-17 00:45:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:45:15 | INFO | fairseq.trainer | begin training epoch 104
2022-08-17 00:45:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:45:45 | INFO | train_inner | epoch 104:     57 / 81 loss=3.488, nll_loss=0.433, ppl=1.35, wps=7298.1, ups=1.32, wpb=5530, bsz=355.3, num_updates=8400, lr=0.000690066, gnorm=0.422, train_wall=48, gb_free=10.2, wall=9241
2022-08-17 00:45:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:46:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:46:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:46:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:46:07 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 4.965 | nll_loss 2.237 | ppl 4.72 | bleu 56.4 | wps 2047.4 | wpb 933.5 | bsz 59.6 | num_updates 8424 | best_bleu 56.78
2022-08-17 00:46:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 8424 updates
2022-08-17 00:46:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint104.pt
2022-08-17 00:46:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint104.pt
2022-08-17 00:46:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint104.pt (epoch 104 @ 8424 updates, score 56.4) (writing took 13.560221742838621 seconds)
2022-08-17 00:46:21 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-08-17 00:46:21 | INFO | train | epoch 104 | loss 3.487 | nll_loss 0.431 | ppl 1.35 | wps 6776.4 | ups 1.23 | wpb 5523.2 | bsz 358 | num_updates 8424 | lr 0.000689082 | gnorm 0.408 | train_wall 40 | gb_free 10.1 | wall 9277
2022-08-17 00:46:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:46:21 | INFO | fairseq.trainer | begin training epoch 105
2022-08-17 00:46:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:47:01 | INFO | train_inner | epoch 105:     76 / 81 loss=3.487, nll_loss=0.432, ppl=1.35, wps=7265.5, ups=1.31, wpb=5532.7, bsz=363.1, num_updates=8500, lr=0.000685994, gnorm=0.406, train_wall=51, gb_free=10.1, wall=9317
2022-08-17 00:47:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:47:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:47:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:47:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:47:12 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 4.966 | nll_loss 2.231 | ppl 4.69 | bleu 55.27 | wps 2070.7 | wpb 933.5 | bsz 59.6 | num_updates 8505 | best_bleu 56.78
2022-08-17 00:47:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 8505 updates
2022-08-17 00:47:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint105.pt
2022-08-17 00:47:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint105.pt
2022-08-17 00:47:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint105.pt (epoch 105 @ 8505 updates, score 55.27) (writing took 24.53890186175704 seconds)
2022-08-17 00:47:37 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-08-17 00:47:37 | INFO | train | epoch 105 | loss 3.488 | nll_loss 0.433 | ppl 1.35 | wps 5885.8 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 8505 | lr 0.000685793 | gnorm 0.413 | train_wall 41 | gb_free 10.3 | wall 9353
2022-08-17 00:47:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:47:37 | INFO | fairseq.trainer | begin training epoch 106
2022-08-17 00:47:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:48:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:48:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:48:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:48:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:48:35 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 4.969 | nll_loss 2.243 | ppl 4.73 | bleu 56.21 | wps 1901.2 | wpb 933.5 | bsz 59.6 | num_updates 8586 | best_bleu 56.78
2022-08-17 00:48:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 8586 updates
2022-08-17 00:48:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint106.pt
2022-08-17 00:48:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint106.pt
2022-08-17 00:49:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint106.pt (epoch 106 @ 8586 updates, score 56.21) (writing took 36.98436698317528 seconds)
2022-08-17 00:49:12 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-08-17 00:49:12 | INFO | train | epoch 106 | loss 3.485 | nll_loss 0.43 | ppl 1.35 | wps 4699 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 8586 | lr 0.00068255 | gnorm 0.458 | train_wall 39 | gb_free 10.2 | wall 9448
2022-08-17 00:49:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:49:12 | INFO | fairseq.trainer | begin training epoch 107
2022-08-17 00:49:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:49:21 | INFO | train_inner | epoch 107:     14 / 81 loss=3.484, nll_loss=0.429, ppl=1.35, wps=3923.7, ups=0.72, wpb=5480.7, bsz=354.2, num_updates=8600, lr=0.000681994, gnorm=0.443, train_wall=48, gb_free=10.2, wall=9457
2022-08-17 00:49:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:49:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:49:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:49:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:50:05 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 4.972 | nll_loss 2.257 | ppl 4.78 | bleu 56.17 | wps 2081.9 | wpb 933.5 | bsz 59.6 | num_updates 8667 | best_bleu 56.78
2022-08-17 00:50:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 8667 updates
2022-08-17 00:50:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint107.pt
2022-08-17 00:50:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint107.pt
2022-08-17 00:50:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint107.pt (epoch 107 @ 8667 updates, score 56.17) (writing took 23.52480187639594 seconds)
2022-08-17 00:50:29 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-08-17 00:50:29 | INFO | train | epoch 107 | loss 3.483 | nll_loss 0.428 | ppl 1.34 | wps 5798 | ups 1.05 | wpb 5523.2 | bsz 358 | num_updates 8667 | lr 0.000679353 | gnorm 0.392 | train_wall 38 | gb_free 10.1 | wall 9525
2022-08-17 00:50:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:50:29 | INFO | fairseq.trainer | begin training epoch 108
2022-08-17 00:50:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:50:46 | INFO | train_inner | epoch 108:     33 / 81 loss=3.481, nll_loss=0.426, ppl=1.34, wps=6520.9, ups=1.17, wpb=5572.1, bsz=362.2, num_updates=8700, lr=0.000678064, gnorm=0.385, train_wall=47, gb_free=10, wall=9542
2022-08-17 00:51:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:51:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:51:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:51:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:51:19 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 4.971 | nll_loss 2.24 | ppl 4.72 | bleu 55.82 | wps 2014.9 | wpb 933.5 | bsz 59.6 | num_updates 8748 | best_bleu 56.78
2022-08-17 00:51:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 8748 updates
2022-08-17 00:51:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint108.pt
2022-08-17 00:51:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint108.pt
2022-08-17 00:51:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint108.pt (epoch 108 @ 8748 updates, score 55.82) (writing took 18.58566788956523 seconds)
2022-08-17 00:51:38 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-08-17 00:51:38 | INFO | train | epoch 108 | loss 3.481 | nll_loss 0.426 | ppl 1.34 | wps 6467.5 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 8748 | lr 0.000676201 | gnorm 0.369 | train_wall 37 | gb_free 10.1 | wall 9594
2022-08-17 00:51:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:51:38 | INFO | fairseq.trainer | begin training epoch 109
2022-08-17 00:51:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:52:07 | INFO | train_inner | epoch 109:     52 / 81 loss=3.481, nll_loss=0.426, ppl=1.34, wps=6813.1, ups=1.24, wpb=5492.3, bsz=354.6, num_updates=8800, lr=0.0006742, gnorm=0.379, train_wall=46, gb_free=10.1, wall=9623
2022-08-17 00:52:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:52:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:52:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:52:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:52:30 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 5.004 | nll_loss 2.284 | ppl 4.87 | bleu 55.61 | wps 1864.7 | wpb 933.5 | bsz 59.6 | num_updates 8829 | best_bleu 56.78
2022-08-17 00:52:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 8829 updates
2022-08-17 00:52:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint109.pt
2022-08-17 00:52:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint109.pt
2022-08-17 00:52:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint109.pt (epoch 109 @ 8829 updates, score 55.61) (writing took 28.306883867830038 seconds)
2022-08-17 00:52:58 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-08-17 00:52:58 | INFO | train | epoch 109 | loss 3.48 | nll_loss 0.426 | ppl 1.34 | wps 5564 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 8829 | lr 0.000673092 | gnorm 0.384 | train_wall 39 | gb_free 10.1 | wall 9675
2022-08-17 00:52:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:52:59 | INFO | fairseq.trainer | begin training epoch 110
2022-08-17 00:52:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:53:43 | INFO | train_inner | epoch 110:     71 / 81 loss=3.479, nll_loss=0.425, ppl=1.34, wps=5782.9, ups=1.04, wpb=5552.5, bsz=361.7, num_updates=8900, lr=0.000670402, gnorm=0.393, train_wall=47, gb_free=10.1, wall=9719
2022-08-17 00:53:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:53:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:53:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:53:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:53:57 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 4.983 | nll_loss 2.26 | ppl 4.79 | bleu 56.09 | wps 2066.1 | wpb 933.5 | bsz 59.6 | num_updates 8910 | best_bleu 56.78
2022-08-17 00:53:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 8910 updates
2022-08-17 00:53:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint110.pt
2022-08-17 00:53:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint110.pt
2022-08-17 00:54:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint110.pt (epoch 110 @ 8910 updates, score 56.09) (writing took 38.189230144023895 seconds)
2022-08-17 00:54:35 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-08-17 00:54:35 | INFO | train | epoch 110 | loss 3.478 | nll_loss 0.424 | ppl 1.34 | wps 4633 | ups 0.84 | wpb 5523.2 | bsz 358 | num_updates 8910 | lr 0.000670025 | gnorm 0.396 | train_wall 38 | gb_free 10.1 | wall 9771
2022-08-17 00:54:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:54:35 | INFO | fairseq.trainer | begin training epoch 111
2022-08-17 00:54:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:55:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:55:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:55:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:55:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:55:31 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 4.982 | nll_loss 2.261 | ppl 4.79 | bleu 55.88 | wps 2090.1 | wpb 933.5 | bsz 59.6 | num_updates 8991 | best_bleu 56.78
2022-08-17 00:55:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 8991 updates
2022-08-17 00:55:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint111.pt
2022-08-17 00:55:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint111.pt
2022-08-17 00:55:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint111.pt (epoch 111 @ 8991 updates, score 55.88) (writing took 16.345242902636528 seconds)
2022-08-17 00:55:48 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-08-17 00:55:48 | INFO | train | epoch 111 | loss 3.477 | nll_loss 0.423 | ppl 1.34 | wps 6133.9 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 8991 | lr 0.000667 | gnorm 0.396 | train_wall 38 | gb_free 10.1 | wall 9844
2022-08-17 00:55:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:55:48 | INFO | fairseq.trainer | begin training epoch 112
2022-08-17 00:55:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:55:53 | INFO | train_inner | epoch 112:      9 / 81 loss=3.478, nll_loss=0.423, ppl=1.34, wps=4198.5, ups=0.77, wpb=5477.2, bsz=356.5, num_updates=9000, lr=0.000666667, gnorm=0.392, train_wall=47, gb_free=10, wall=9850
2022-08-17 00:56:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:56:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:56:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:56:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:56:38 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 4.975 | nll_loss 2.254 | ppl 4.77 | bleu 56.14 | wps 2104 | wpb 933.5 | bsz 59.6 | num_updates 9072 | best_bleu 56.78
2022-08-17 00:56:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 9072 updates
2022-08-17 00:56:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint112.pt
2022-08-17 00:56:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint112.pt
2022-08-17 00:56:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint112.pt (epoch 112 @ 9072 updates, score 56.14) (writing took 14.837073039263487 seconds)
2022-08-17 00:56:53 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-08-17 00:56:53 | INFO | train | epoch 112 | loss 3.477 | nll_loss 0.423 | ppl 1.34 | wps 6833.5 | ups 1.24 | wpb 5523.2 | bsz 358 | num_updates 9072 | lr 0.000664016 | gnorm 0.418 | train_wall 38 | gb_free 10.1 | wall 9910
2022-08-17 00:56:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:56:54 | INFO | fairseq.trainer | begin training epoch 113
2022-08-17 00:56:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:57:09 | INFO | train_inner | epoch 113:     28 / 81 loss=3.474, nll_loss=0.42, ppl=1.34, wps=7300.3, ups=1.31, wpb=5552.9, bsz=360.6, num_updates=9100, lr=0.000662994, gnorm=0.408, train_wall=48, gb_free=10, wall=9926
2022-08-17 00:57:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:57:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:57:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:57:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:57:44 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 4.971 | nll_loss 2.252 | ppl 4.76 | bleu 55.96 | wps 1911.4 | wpb 933.5 | bsz 59.6 | num_updates 9153 | best_bleu 56.78
2022-08-17 00:57:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 9153 updates
2022-08-17 00:57:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint113.pt
2022-08-17 00:57:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint113.pt
2022-08-17 00:58:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint113.pt (epoch 113 @ 9153 updates, score 55.96) (writing took 23.365577094256878 seconds)
2022-08-17 00:58:08 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-08-17 00:58:08 | INFO | train | epoch 113 | loss 3.474 | nll_loss 0.42 | ppl 1.34 | wps 6003.4 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 9153 | lr 0.000661071 | gnorm 0.365 | train_wall 38 | gb_free 10.1 | wall 9984
2022-08-17 00:58:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:58:08 | INFO | fairseq.trainer | begin training epoch 114
2022-08-17 00:58:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:58:34 | INFO | train_inner | epoch 114:     47 / 81 loss=3.475, nll_loss=0.421, ppl=1.34, wps=6525.1, ups=1.18, wpb=5528.6, bsz=353.5, num_updates=9200, lr=0.00065938, gnorm=0.358, train_wall=49, gb_free=10, wall=10010
2022-08-17 00:58:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 00:58:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 00:58:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 00:58:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 00:59:00 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 4.955 | nll_loss 2.226 | ppl 4.68 | bleu 56.28 | wps 2127.3 | wpb 933.5 | bsz 59.6 | num_updates 9234 | best_bleu 56.78
2022-08-17 00:59:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 9234 updates
2022-08-17 00:59:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint114.pt
2022-08-17 00:59:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint114.pt
2022-08-17 00:59:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint114.pt (epoch 114 @ 9234 updates, score 56.28) (writing took 19.386132024228573 seconds)
2022-08-17 00:59:20 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-08-17 00:59:20 | INFO | train | epoch 114 | loss 3.472 | nll_loss 0.418 | ppl 1.34 | wps 6249.9 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 9234 | lr 0.000658165 | gnorm 0.347 | train_wall 40 | gb_free 10.1 | wall 10056
2022-08-17 00:59:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 00:59:20 | INFO | fairseq.trainer | begin training epoch 115
2022-08-17 00:59:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 00:59:54 | INFO | train_inner | epoch 115:     66 / 81 loss=3.471, nll_loss=0.417, ppl=1.34, wps=6912.1, ups=1.25, wpb=5523.2, bsz=357.6, num_updates=9300, lr=0.000655826, gnorm=0.347, train_wall=49, gb_free=10, wall=10090
2022-08-17 01:00:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:00:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:00:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:00:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:00:10 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 4.977 | nll_loss 2.256 | ppl 4.78 | bleu 56.91 | wps 2088.1 | wpb 933.5 | bsz 59.6 | num_updates 9315 | best_bleu 56.91
2022-08-17 01:00:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 9315 updates
2022-08-17 01:00:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint115.pt
2022-08-17 01:00:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint115.pt
2022-08-17 01:00:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint115.pt (epoch 115 @ 9315 updates, score 56.91) (writing took 37.46812193468213 seconds)
2022-08-17 01:00:48 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-08-17 01:00:48 | INFO | train | epoch 115 | loss 3.471 | nll_loss 0.417 | ppl 1.34 | wps 5059.5 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 9315 | lr 0.000655298 | gnorm 0.35 | train_wall 40 | gb_free 10.3 | wall 10144
2022-08-17 01:00:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:00:48 | INFO | fairseq.trainer | begin training epoch 116
2022-08-17 01:00:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:01:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:01:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:01:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:01:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:01:40 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 4.987 | nll_loss 2.266 | ppl 4.81 | bleu 56.1 | wps 1984 | wpb 933.5 | bsz 59.6 | num_updates 9396 | best_bleu 56.91
2022-08-17 01:01:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 9396 updates
2022-08-17 01:01:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint116.pt
2022-08-17 01:01:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint116.pt
2022-08-17 01:01:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint116.pt (epoch 116 @ 9396 updates, score 56.1) (writing took 2.3165789172053337 seconds)
2022-08-17 01:01:42 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-08-17 01:01:42 | INFO | train | epoch 116 | loss 3.473 | nll_loss 0.419 | ppl 1.34 | wps 8285.2 | ups 1.5 | wpb 5523.2 | bsz 358 | num_updates 9396 | lr 0.000652467 | gnorm 0.38 | train_wall 38 | gb_free 10.1 | wall 10198
2022-08-17 01:01:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:01:42 | INFO | fairseq.trainer | begin training epoch 117
2022-08-17 01:01:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:01:45 | INFO | train_inner | epoch 117:      4 / 81 loss=3.472, nll_loss=0.419, ppl=1.34, wps=4965.4, ups=0.9, wpb=5510.1, bsz=360.3, num_updates=9400, lr=0.000652328, gnorm=0.372, train_wall=47, gb_free=10, wall=10201
2022-08-17 01:02:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:02:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:02:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:02:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:02:37 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 4.976 | nll_loss 2.252 | ppl 4.76 | bleu 55.92 | wps 2106.1 | wpb 933.5 | bsz 59.6 | num_updates 9477 | best_bleu 56.91
2022-08-17 01:02:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 9477 updates
2022-08-17 01:02:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint117.pt
2022-08-17 01:02:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint117.pt
2022-08-17 01:03:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint117.pt (epoch 117 @ 9477 updates, score 55.92) (writing took 23.41823025792837 seconds)
2022-08-17 01:03:00 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-08-17 01:03:00 | INFO | train | epoch 117 | loss 3.468 | nll_loss 0.414 | ppl 1.33 | wps 5715.6 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 9477 | lr 0.000649673 | gnorm 0.347 | train_wall 39 | gb_free 10.1 | wall 10276
2022-08-17 01:03:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:03:00 | INFO | fairseq.trainer | begin training epoch 118
2022-08-17 01:03:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:03:13 | INFO | train_inner | epoch 118:     23 / 81 loss=3.469, nll_loss=0.415, ppl=1.33, wps=6278.3, ups=1.14, wpb=5528.3, bsz=355.3, num_updates=9500, lr=0.000648886, gnorm=0.354, train_wall=48, gb_free=10.1, wall=10289
2022-08-17 01:03:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:03:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:03:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:03:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:03:51 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 4.976 | nll_loss 2.259 | ppl 4.79 | bleu 55.36 | wps 1941.4 | wpb 933.5 | bsz 59.6 | num_updates 9558 | best_bleu 56.91
2022-08-17 01:03:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 9558 updates
2022-08-17 01:03:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint118.pt
2022-08-17 01:03:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint118.pt
2022-08-17 01:04:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint118.pt (epoch 118 @ 9558 updates, score 55.36) (writing took 32.6844374909997 seconds)
2022-08-17 01:04:24 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-08-17 01:04:24 | INFO | train | epoch 118 | loss 3.468 | nll_loss 0.415 | ppl 1.33 | wps 5365.1 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 9558 | lr 0.000646914 | gnorm 0.348 | train_wall 39 | gb_free 10.1 | wall 10360
2022-08-17 01:04:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:04:24 | INFO | fairseq.trainer | begin training epoch 119
2022-08-17 01:04:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:04:51 | INFO | train_inner | epoch 119:     42 / 81 loss=3.466, nll_loss=0.413, ppl=1.33, wps=5647, ups=1.03, wpb=5503.9, bsz=360.2, num_updates=9600, lr=0.000645497, gnorm=0.337, train_wall=48, gb_free=10.1, wall=10387
2022-08-17 01:05:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:05:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:05:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:05:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:05:23 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 4.977 | nll_loss 2.266 | ppl 4.81 | bleu 55.76 | wps 1978.5 | wpb 933.5 | bsz 59.6 | num_updates 9639 | best_bleu 56.91
2022-08-17 01:05:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 9639 updates
2022-08-17 01:05:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint119.pt
2022-08-17 01:05:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint119.pt
2022-08-17 01:06:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint119.pt (epoch 119 @ 9639 updates, score 55.76) (writing took 37.076210379600525 seconds)
2022-08-17 01:06:00 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-08-17 01:06:00 | INFO | train | epoch 119 | loss 3.466 | nll_loss 0.413 | ppl 1.33 | wps 4629.8 | ups 0.84 | wpb 5523.2 | bsz 358 | num_updates 9639 | lr 0.00064419 | gnorm 0.34 | train_wall 39 | gb_free 10.1 | wall 10457
2022-08-17 01:06:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:06:00 | INFO | fairseq.trainer | begin training epoch 120
2022-08-17 01:06:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:06:33 | INFO | train_inner | epoch 120:     61 / 81 loss=3.467, nll_loss=0.414, ppl=1.33, wps=5418, ups=0.97, wpb=5558.8, bsz=352.6, num_updates=9700, lr=0.000642161, gnorm=0.346, train_wall=50, gb_free=10.1, wall=10489
2022-08-17 01:06:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:06:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:06:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:06:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:06:52 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 4.986 | nll_loss 2.275 | ppl 4.84 | bleu 55.86 | wps 2040 | wpb 933.5 | bsz 59.6 | num_updates 9720 | best_bleu 56.91
2022-08-17 01:06:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 9720 updates
2022-08-17 01:06:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint120.pt
2022-08-17 01:06:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint120.pt
2022-08-17 01:07:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint120.pt (epoch 120 @ 9720 updates, score 55.86) (writing took 17.742669325321913 seconds)
2022-08-17 01:07:10 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-08-17 01:07:10 | INFO | train | epoch 120 | loss 3.465 | nll_loss 0.412 | ppl 1.33 | wps 6460.4 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 9720 | lr 0.0006415 | gnorm 0.36 | train_wall 41 | gb_free 10.3 | wall 10526
2022-08-17 01:07:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:07:10 | INFO | fairseq.trainer | begin training epoch 121
2022-08-17 01:07:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:07:53 | INFO | train_inner | epoch 121:     80 / 81 loss=3.465, nll_loss=0.413, ppl=1.33, wps=6934.6, ups=1.25, wpb=5527.6, bsz=365.4, num_updates=9800, lr=0.000638877, gnorm=0.378, train_wall=48, gb_free=10.1, wall=10569
2022-08-17 01:07:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:07:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:07:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:07:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:08:01 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 4.981 | nll_loss 2.279 | ppl 4.85 | bleu 55.02 | wps 2139.8 | wpb 933.5 | bsz 59.6 | num_updates 9801 | best_bleu 56.91
2022-08-17 01:08:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 9801 updates
2022-08-17 01:08:01 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint121.pt
2022-08-17 01:08:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint121.pt
2022-08-17 01:08:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint121.pt (epoch 121 @ 9801 updates, score 55.02) (writing took 27.19603279978037 seconds)
2022-08-17 01:08:29 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-08-17 01:08:29 | INFO | train | epoch 121 | loss 3.465 | nll_loss 0.413 | ppl 1.33 | wps 5656.3 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 9801 | lr 0.000638844 | gnorm 0.374 | train_wall 38 | gb_free 10.3 | wall 10605
2022-08-17 01:08:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:08:29 | INFO | fairseq.trainer | begin training epoch 122
2022-08-17 01:08:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:09:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:09:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:09:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:09:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:09:21 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 4.989 | nll_loss 2.275 | ppl 4.84 | bleu 56.18 | wps 1899.8 | wpb 933.5 | bsz 59.6 | num_updates 9882 | best_bleu 56.91
2022-08-17 01:09:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 9882 updates
2022-08-17 01:09:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint122.pt
2022-08-17 01:09:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint122.pt
2022-08-17 01:09:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint122.pt (epoch 122 @ 9882 updates, score 56.18) (writing took 21.546405874192715 seconds)
2022-08-17 01:09:43 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-08-17 01:09:43 | INFO | train | epoch 122 | loss 3.466 | nll_loss 0.414 | ppl 1.33 | wps 6000.5 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 9882 | lr 0.00063622 | gnorm 0.444 | train_wall 37 | gb_free 10.1 | wall 10679
2022-08-17 01:09:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:09:43 | INFO | fairseq.trainer | begin training epoch 123
2022-08-17 01:09:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:09:56 | INFO | train_inner | epoch 123:     18 / 81 loss=3.466, nll_loss=0.415, ppl=1.33, wps=4476, ups=0.82, wpb=5488.5, bsz=355.1, num_updates=9900, lr=0.000635642, gnorm=0.861, train_wall=45, gb_free=10.1, wall=10692
2022-08-17 01:10:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:10:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:10:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:10:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:10:41 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 4.978 | nll_loss 2.264 | ppl 4.8 | bleu 56.19 | wps 2008.5 | wpb 933.5 | bsz 59.6 | num_updates 9963 | best_bleu 56.91
2022-08-17 01:10:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 9963 updates
2022-08-17 01:10:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint123.pt
2022-08-17 01:10:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint123.pt
2022-08-17 01:11:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint123.pt (epoch 123 @ 9963 updates, score 56.19) (writing took 36.09809974581003 seconds)
2022-08-17 01:11:17 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-08-17 01:11:17 | INFO | train | epoch 123 | loss 3.466 | nll_loss 0.415 | ppl 1.33 | wps 4767.7 | ups 0.86 | wpb 5523.2 | bsz 358 | num_updates 9963 | lr 0.000633629 | gnorm 0.903 | train_wall 38 | gb_free 10.2 | wall 10773
2022-08-17 01:11:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:11:17 | INFO | fairseq.trainer | begin training epoch 124
2022-08-17 01:11:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:11:37 | INFO | train_inner | epoch 124:     37 / 81 loss=3.463, nll_loss=0.41, ppl=1.33, wps=5446, ups=0.99, wpb=5523.9, bsz=355.4, num_updates=10000, lr=0.000632456, gnorm=0.362, train_wall=47, gb_free=10.1, wall=10793
2022-08-17 01:11:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:12:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:12:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:12:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:12:07 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 4.972 | nll_loss 2.26 | ppl 4.79 | bleu 56.47 | wps 2005.7 | wpb 933.5 | bsz 59.6 | num_updates 10044 | best_bleu 56.91
2022-08-17 01:12:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 10044 updates
2022-08-17 01:12:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint124.pt
2022-08-17 01:12:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint124.pt
2022-08-17 01:12:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint124.pt (epoch 124 @ 10044 updates, score 56.47) (writing took 20.82541785016656 seconds)
2022-08-17 01:12:28 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-08-17 01:12:28 | INFO | train | epoch 124 | loss 3.459 | nll_loss 0.406 | ppl 1.32 | wps 6274.8 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 10044 | lr 0.000631069 | gnorm 0.345 | train_wall 39 | gb_free 10.2 | wall 10845
2022-08-17 01:12:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:12:29 | INFO | fairseq.trainer | begin training epoch 125
2022-08-17 01:12:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:12:57 | INFO | train_inner | epoch 125:     56 / 81 loss=3.459, nll_loss=0.407, ppl=1.33, wps=6855.5, ups=1.24, wpb=5514.6, bsz=360.1, num_updates=10100, lr=0.000629317, gnorm=0.345, train_wall=48, gb_free=10.1, wall=10874
2022-08-17 01:13:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:13:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:13:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:13:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:13:20 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 4.99 | nll_loss 2.281 | ppl 4.86 | bleu 56.39 | wps 1978 | wpb 933.5 | bsz 59.6 | num_updates 10125 | best_bleu 56.91
2022-08-17 01:13:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 10125 updates
2022-08-17 01:13:20 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint125.pt
2022-08-17 01:13:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint125.pt
2022-08-17 01:13:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint125.pt (epoch 125 @ 10125 updates, score 56.39) (writing took 14.750705998390913 seconds)
2022-08-17 01:13:34 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-08-17 01:13:34 | INFO | train | epoch 125 | loss 3.459 | nll_loss 0.407 | ppl 1.33 | wps 6772.7 | ups 1.23 | wpb 5523.2 | bsz 358 | num_updates 10125 | lr 0.000628539 | gnorm 0.339 | train_wall 40 | gb_free 10.1 | wall 10911
2022-08-17 01:13:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:13:35 | INFO | fairseq.trainer | begin training epoch 126
2022-08-17 01:13:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:14:14 | INFO | train_inner | epoch 126:     75 / 81 loss=3.46, nll_loss=0.409, ppl=1.33, wps=7270.4, ups=1.31, wpb=5565.6, bsz=362.3, num_updates=10200, lr=0.000626224, gnorm=0.41, train_wall=50, gb_free=10, wall=10950
2022-08-17 01:14:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:14:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:14:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:14:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:14:25 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 4.977 | nll_loss 2.264 | ppl 4.8 | bleu 56.23 | wps 2054.3 | wpb 933.5 | bsz 59.6 | num_updates 10206 | best_bleu 56.91
2022-08-17 01:14:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 10206 updates
2022-08-17 01:14:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint126.pt
2022-08-17 01:14:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint126.pt
2022-08-17 01:14:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint126.pt (epoch 126 @ 10206 updates, score 56.23) (writing took 25.64387060701847 seconds)
2022-08-17 01:14:51 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-08-17 01:14:51 | INFO | train | epoch 126 | loss 3.46 | nll_loss 0.409 | ppl 1.33 | wps 5847 | ups 1.06 | wpb 5523.2 | bsz 358 | num_updates 10206 | lr 0.00062604 | gnorm 0.434 | train_wall 40 | gb_free 10.2 | wall 10987
2022-08-17 01:14:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:14:51 | INFO | fairseq.trainer | begin training epoch 127
2022-08-17 01:14:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:15:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:15:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:15:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:15:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:15:44 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 5.004 | nll_loss 2.298 | ppl 4.92 | bleu 56.02 | wps 1895.1 | wpb 933.5 | bsz 59.6 | num_updates 10287 | best_bleu 56.91
2022-08-17 01:15:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 10287 updates
2022-08-17 01:15:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint127.pt
2022-08-17 01:15:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint127.pt
2022-08-17 01:15:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint127.pt (epoch 127 @ 10287 updates, score 56.02) (writing took 2.477678071707487 seconds)
2022-08-17 01:15:47 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-08-17 01:15:47 | INFO | train | epoch 127 | loss 3.459 | nll_loss 0.408 | ppl 1.33 | wps 8009.4 | ups 1.45 | wpb 5523.2 | bsz 358 | num_updates 10287 | lr 0.000623571 | gnorm 0.438 | train_wall 38 | gb_free 10.3 | wall 11043
2022-08-17 01:15:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:15:47 | INFO | fairseq.trainer | begin training epoch 128
2022-08-17 01:15:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:15:54 | INFO | train_inner | epoch 128:     13 / 81 loss=3.459, nll_loss=0.407, ppl=1.33, wps=5493.7, ups=1, wpb=5489.6, bsz=355.3, num_updates=10300, lr=0.000623177, gnorm=0.432, train_wall=46, gb_free=10.1, wall=11050
2022-08-17 01:16:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:16:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:16:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:16:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:16:38 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 4.983 | nll_loss 2.271 | ppl 4.83 | bleu 55.41 | wps 1972.8 | wpb 933.5 | bsz 59.6 | num_updates 10368 | best_bleu 56.91
2022-08-17 01:16:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 10368 updates
2022-08-17 01:16:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint128.pt
2022-08-17 01:16:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint128.pt
2022-08-17 01:17:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint128.pt (epoch 128 @ 10368 updates, score 55.41) (writing took 27.63326543942094 seconds)
2022-08-17 01:17:06 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-08-17 01:17:06 | INFO | train | epoch 128 | loss 3.457 | nll_loss 0.406 | ppl 1.32 | wps 5664.4 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 10368 | lr 0.00062113 | gnorm 0.343 | train_wall 37 | gb_free 10.1 | wall 11122
2022-08-17 01:17:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:17:06 | INFO | fairseq.trainer | begin training epoch 129
2022-08-17 01:17:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:17:25 | INFO | train_inner | epoch 129:     32 / 81 loss=3.456, nll_loss=0.404, ppl=1.32, wps=6063.2, ups=1.09, wpb=5548.9, bsz=361, num_updates=10400, lr=0.000620174, gnorm=0.343, train_wall=47, gb_free=10.1, wall=11142
2022-08-17 01:17:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:17:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:17:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:17:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:17:59 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 4.987 | nll_loss 2.279 | ppl 4.85 | bleu 55.69 | wps 2110.8 | wpb 933.5 | bsz 59.6 | num_updates 10449 | best_bleu 56.91
2022-08-17 01:17:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 10449 updates
2022-08-17 01:17:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint129.pt
2022-08-17 01:18:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint129.pt
2022-08-17 01:18:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint129.pt (epoch 129 @ 10449 updates, score 55.69) (writing took 31.769491352140903 seconds)
2022-08-17 01:18:30 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-08-17 01:18:31 | INFO | train | epoch 129 | loss 3.457 | nll_loss 0.406 | ppl 1.32 | wps 5280.7 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 10449 | lr 0.000618718 | gnorm 0.355 | train_wall 38 | gb_free 10.1 | wall 11207
2022-08-17 01:18:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:18:31 | INFO | fairseq.trainer | begin training epoch 130
2022-08-17 01:18:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:19:03 | INFO | train_inner | epoch 130:     51 / 81 loss=3.457, nll_loss=0.405, ppl=1.32, wps=5643, ups=1.02, wpb=5521.5, bsz=354.7, num_updates=10500, lr=0.000617213, gnorm=0.361, train_wall=47, gb_free=10.1, wall=11239
2022-08-17 01:19:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:19:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:19:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:19:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:19:28 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 4.984 | nll_loss 2.277 | ppl 4.85 | bleu 56.03 | wps 2101.9 | wpb 933.5 | bsz 59.6 | num_updates 10530 | best_bleu 56.91
2022-08-17 01:19:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 10530 updates
2022-08-17 01:19:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint130.pt
2022-08-17 01:19:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint130.pt
2022-08-17 01:19:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint130.pt (epoch 130 @ 10530 updates, score 56.03) (writing took 29.137993916869164 seconds)
2022-08-17 01:19:57 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-08-17 01:19:58 | INFO | train | epoch 130 | loss 3.455 | nll_loss 0.404 | ppl 1.32 | wps 5143.4 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 10530 | lr 0.000616334 | gnorm 0.358 | train_wall 37 | gb_free 10.1 | wall 11294
2022-08-17 01:19:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:19:58 | INFO | fairseq.trainer | begin training epoch 131
2022-08-17 01:19:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:20:41 | INFO | train_inner | epoch 131:     70 / 81 loss=3.457, nll_loss=0.406, ppl=1.33, wps=5621.5, ups=1.02, wpb=5508.6, bsz=357.4, num_updates=10600, lr=0.000614295, gnorm=0.368, train_wall=47, gb_free=10, wall=11337
2022-08-17 01:20:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:20:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:20:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:20:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:20:56 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 4.99 | nll_loss 2.273 | ppl 4.83 | bleu 55.45 | wps 1970.6 | wpb 933.5 | bsz 59.6 | num_updates 10611 | best_bleu 56.91
2022-08-17 01:20:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 10611 updates
2022-08-17 01:20:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint131.pt
2022-08-17 01:20:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint131.pt
2022-08-17 01:21:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint131.pt (epoch 131 @ 10611 updates, score 55.45) (writing took 37.25984972715378 seconds)
2022-08-17 01:21:34 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-08-17 01:21:34 | INFO | train | epoch 131 | loss 3.456 | nll_loss 0.406 | ppl 1.32 | wps 4642.6 | ups 0.84 | wpb 5523.2 | bsz 358 | num_updates 10611 | lr 0.000613977 | gnorm 0.376 | train_wall 38 | gb_free 10.2 | wall 11390
2022-08-17 01:21:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:21:34 | INFO | fairseq.trainer | begin training epoch 132
2022-08-17 01:21:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:22:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:22:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:22:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:22:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:22:26 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 4.993 | nll_loss 2.29 | ppl 4.89 | bleu 55.36 | wps 2085.4 | wpb 933.5 | bsz 59.6 | num_updates 10692 | best_bleu 56.91
2022-08-17 01:22:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 10692 updates
2022-08-17 01:22:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint132.pt
2022-08-17 01:22:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint132.pt
2022-08-17 01:22:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint132.pt (epoch 132 @ 10692 updates, score 55.36) (writing took 2.285447593778372 seconds)
2022-08-17 01:22:29 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-08-17 01:22:29 | INFO | train | epoch 132 | loss 3.456 | nll_loss 0.406 | ppl 1.32 | wps 8171.5 | ups 1.48 | wpb 5523.2 | bsz 358 | num_updates 10692 | lr 0.000611647 | gnorm 0.455 | train_wall 39 | gb_free 10.1 | wall 11445
2022-08-17 01:22:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:22:29 | INFO | fairseq.trainer | begin training epoch 133
2022-08-17 01:22:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:22:34 | INFO | train_inner | epoch 133:      8 / 81 loss=3.455, nll_loss=0.405, ppl=1.32, wps=4886.8, ups=0.89, wpb=5512.7, bsz=358.4, num_updates=10700, lr=0.000611418, gnorm=0.432, train_wall=48, gb_free=10.1, wall=11450
2022-08-17 01:23:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:23:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:23:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:23:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:23:19 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 5 | nll_loss 2.292 | ppl 4.9 | bleu 55.71 | wps 1958.8 | wpb 933.5 | bsz 59.6 | num_updates 10773 | best_bleu 56.91
2022-08-17 01:23:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 10773 updates
2022-08-17 01:23:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint133.pt
2022-08-17 01:23:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint133.pt
2022-08-17 01:23:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint133.pt (epoch 133 @ 10773 updates, score 55.71) (writing took 20.79567002505064 seconds)
2022-08-17 01:23:40 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-08-17 01:23:40 | INFO | train | epoch 133 | loss 3.453 | nll_loss 0.402 | ppl 1.32 | wps 6263.4 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 10773 | lr 0.000609343 | gnorm 0.337 | train_wall 38 | gb_free 10.1 | wall 11516
2022-08-17 01:23:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:23:40 | INFO | fairseq.trainer | begin training epoch 134
2022-08-17 01:23:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:23:55 | INFO | train_inner | epoch 134:     27 / 81 loss=3.453, nll_loss=0.401, ppl=1.32, wps=6883.9, ups=1.24, wpb=5545.5, bsz=359.7, num_updates=10800, lr=0.000608581, gnorm=0.332, train_wall=47, gb_free=10.2, wall=11531
2022-08-17 01:24:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:24:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:24:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:24:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:24:31 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 4.985 | nll_loss 2.276 | ppl 4.84 | bleu 55.92 | wps 2031.9 | wpb 933.5 | bsz 59.6 | num_updates 10854 | best_bleu 56.91
2022-08-17 01:24:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 10854 updates
2022-08-17 01:24:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint134.pt
2022-08-17 01:24:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint134.pt
2022-08-17 01:25:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint134.pt (epoch 134 @ 10854 updates, score 55.92) (writing took 34.17375070974231 seconds)
2022-08-17 01:25:05 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-08-17 01:25:05 | INFO | train | epoch 134 | loss 3.451 | nll_loss 0.4 | ppl 1.32 | wps 5260.5 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 10854 | lr 0.000607065 | gnorm 0.358 | train_wall 39 | gb_free 10.1 | wall 11601
2022-08-17 01:25:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:25:05 | INFO | fairseq.trainer | begin training epoch 135
2022-08-17 01:25:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:25:29 | INFO | train_inner | epoch 135:     46 / 81 loss=3.451, nll_loss=0.401, ppl=1.32, wps=5866.1, ups=1.06, wpb=5510.6, bsz=354.2, num_updates=10900, lr=0.000605783, gnorm=0.357, train_wall=47, gb_free=10.1, wall=11625
2022-08-17 01:25:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:25:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:25:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:25:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:25:55 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 4.981 | nll_loss 2.276 | ppl 4.84 | bleu 56.26 | wps 2070.5 | wpb 933.5 | bsz 59.6 | num_updates 10935 | best_bleu 56.91
2022-08-17 01:25:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 10935 updates
2022-08-17 01:25:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint135.pt
2022-08-17 01:25:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint135.pt
2022-08-17 01:26:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint135.pt (epoch 135 @ 10935 updates, score 56.26) (writing took 14.003415562212467 seconds)
2022-08-17 01:26:09 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-08-17 01:26:09 | INFO | train | epoch 135 | loss 3.451 | nll_loss 0.401 | ppl 1.32 | wps 6992.6 | ups 1.27 | wpb 5523.2 | bsz 358 | num_updates 10935 | lr 0.000604812 | gnorm 0.33 | train_wall 38 | gb_free 10.1 | wall 11665
2022-08-17 01:26:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:26:09 | INFO | fairseq.trainer | begin training epoch 136
2022-08-17 01:26:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:26:43 | INFO | train_inner | epoch 136:     65 / 81 loss=3.451, nll_loss=0.402, ppl=1.32, wps=7470, ups=1.35, wpb=5538.6, bsz=364.6, num_updates=11000, lr=0.000603023, gnorm=0.933, train_wall=47, gb_free=10, wall=11699
2022-08-17 01:26:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:26:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:26:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:26:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:26:59 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 5.002 | nll_loss 2.298 | ppl 4.92 | bleu 56.3 | wps 1993.9 | wpb 933.5 | bsz 59.6 | num_updates 11016 | best_bleu 56.91
2022-08-17 01:26:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 11016 updates
2022-08-17 01:26:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint136.pt
2022-08-17 01:27:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint136.pt
2022-08-17 01:27:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint136.pt (epoch 136 @ 11016 updates, score 56.3) (writing took 19.312764901667833 seconds)
2022-08-17 01:27:19 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-08-17 01:27:19 | INFO | train | epoch 136 | loss 3.453 | nll_loss 0.403 | ppl 1.32 | wps 6421.1 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 11016 | lr 0.000602585 | gnorm 1.085 | train_wall 38 | gb_free 10.1 | wall 11735
2022-08-17 01:27:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:27:19 | INFO | fairseq.trainer | begin training epoch 137
2022-08-17 01:27:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:28:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:28:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:28:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:28:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:28:10 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 4.983 | nll_loss 2.276 | ppl 4.84 | bleu 56.08 | wps 1937.3 | wpb 933.5 | bsz 59.6 | num_updates 11097 | best_bleu 56.91
2022-08-17 01:28:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 11097 updates
2022-08-17 01:28:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint137.pt
2022-08-17 01:28:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint137.pt
2022-08-17 01:28:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint137.pt (epoch 137 @ 11097 updates, score 56.08) (writing took 46.48479624837637 seconds)
2022-08-17 01:28:57 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-08-17 01:28:57 | INFO | train | epoch 137 | loss 3.45 | nll_loss 0.399 | ppl 1.32 | wps 4545.6 | ups 0.82 | wpb 5523.2 | bsz 358 | num_updates 11097 | lr 0.000600381 | gnorm 0.362 | train_wall 38 | gb_free 10.2 | wall 11833
2022-08-17 01:28:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:28:57 | INFO | fairseq.trainer | begin training epoch 138
2022-08-17 01:28:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:29:00 | INFO | train_inner | epoch 138:      3 / 81 loss=3.451, nll_loss=0.401, ppl=1.32, wps=4003.8, ups=0.73, wpb=5502.1, bsz=355, num_updates=11100, lr=0.0006003, gnorm=0.363, train_wall=47, gb_free=10.1, wall=11836
2022-08-17 01:29:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:29:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:29:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:29:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:29:48 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 4.974 | nll_loss 2.273 | ppl 4.83 | bleu 56.83 | wps 2025.8 | wpb 933.5 | bsz 59.6 | num_updates 11178 | best_bleu 56.91
2022-08-17 01:29:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 11178 updates
2022-08-17 01:29:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint138.pt
2022-08-17 01:29:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint138.pt
2022-08-17 01:30:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint138.pt (epoch 138 @ 11178 updates, score 56.83) (writing took 16.26501839980483 seconds)
2022-08-17 01:30:04 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-08-17 01:30:04 | INFO | train | epoch 138 | loss 3.45 | nll_loss 0.4 | ppl 1.32 | wps 6648.7 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 11178 | lr 0.000598202 | gnorm 0.326 | train_wall 38 | gb_free 10 | wall 11901
2022-08-17 01:30:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:30:05 | INFO | fairseq.trainer | begin training epoch 139
2022-08-17 01:30:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:30:17 | INFO | train_inner | epoch 139:     22 / 81 loss=3.449, nll_loss=0.399, ppl=1.32, wps=7213.5, ups=1.31, wpb=5523.1, bsz=357.4, num_updates=11200, lr=0.000597614, gnorm=0.327, train_wall=47, gb_free=10, wall=11913
2022-08-17 01:30:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:30:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:30:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:30:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:30:55 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 4.975 | nll_loss 2.271 | ppl 4.83 | bleu 56.42 | wps 2077.9 | wpb 933.5 | bsz 59.6 | num_updates 11259 | best_bleu 56.91
2022-08-17 01:30:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 11259 updates
2022-08-17 01:30:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint139.pt
2022-08-17 01:30:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint139.pt
2022-08-17 01:31:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint139.pt (epoch 139 @ 11259 updates, score 56.42) (writing took 15.618205562233925 seconds)
2022-08-17 01:31:11 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-08-17 01:31:11 | INFO | train | epoch 139 | loss 3.447 | nll_loss 0.397 | ppl 1.32 | wps 6710.9 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 11259 | lr 0.000596046 | gnorm 0.343 | train_wall 38 | gb_free 10.1 | wall 11967
2022-08-17 01:31:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:31:11 | INFO | fairseq.trainer | begin training epoch 140
2022-08-17 01:31:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:31:33 | INFO | train_inner | epoch 140:     41 / 81 loss=3.447, nll_loss=0.397, ppl=1.32, wps=7218.2, ups=1.31, wpb=5528.6, bsz=358.6, num_updates=11300, lr=0.000594964, gnorm=0.344, train_wall=48, gb_free=10.1, wall=11990
2022-08-17 01:31:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:31:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:31:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:31:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:32:00 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 4.994 | nll_loss 2.293 | ppl 4.9 | bleu 56.11 | wps 2041.8 | wpb 933.5 | bsz 59.6 | num_updates 11340 | best_bleu 56.91
2022-08-17 01:32:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 11340 updates
2022-08-17 01:32:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint140.pt
2022-08-17 01:32:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint140.pt
2022-08-17 01:32:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint140.pt (epoch 140 @ 11340 updates, score 56.11) (writing took 19.170264944434166 seconds)
2022-08-17 01:32:20 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-08-17 01:32:20 | INFO | train | epoch 140 | loss 3.447 | nll_loss 0.397 | ppl 1.32 | wps 6515.5 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 11340 | lr 0.000593914 | gnorm 0.332 | train_wall 38 | gb_free 10.1 | wall 12036
2022-08-17 01:32:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:32:20 | INFO | fairseq.trainer | begin training epoch 141
2022-08-17 01:32:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:32:53 | INFO | train_inner | epoch 141:     60 / 81 loss=3.447, nll_loss=0.398, ppl=1.32, wps=6962.9, ups=1.26, wpb=5538.7, bsz=358.9, num_updates=11400, lr=0.000592349, gnorm=0.322, train_wall=48, gb_free=10.1, wall=12069
2022-08-17 01:33:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:33:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:33:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:33:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:33:12 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 4.998 | nll_loss 2.298 | ppl 4.92 | bleu 56.05 | wps 1968.3 | wpb 933.5 | bsz 59.6 | num_updates 11421 | best_bleu 56.91
2022-08-17 01:33:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 11421 updates
2022-08-17 01:33:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint141.pt
2022-08-17 01:33:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint141.pt
2022-08-17 01:33:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint141.pt (epoch 141 @ 11421 updates, score 56.05) (writing took 14.466108422726393 seconds)
2022-08-17 01:33:27 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-08-17 01:33:27 | INFO | train | epoch 141 | loss 3.448 | nll_loss 0.399 | ppl 1.32 | wps 6676.9 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 11421 | lr 0.000591804 | gnorm 0.331 | train_wall 40 | gb_free 10.2 | wall 12103
2022-08-17 01:33:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:33:27 | INFO | fairseq.trainer | begin training epoch 142
2022-08-17 01:33:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:34:08 | INFO | train_inner | epoch 142:     79 / 81 loss=3.447, nll_loss=0.398, ppl=1.32, wps=7362.9, ups=1.33, wpb=5529.6, bsz=357, num_updates=11500, lr=0.000589768, gnorm=0.334, train_wall=47, gb_free=10, wall=12144
2022-08-17 01:34:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:34:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:34:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:34:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:34:18 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 4.995 | nll_loss 2.288 | ppl 4.88 | bleu 56.06 | wps 1963.4 | wpb 933.5 | bsz 59.6 | num_updates 11502 | best_bleu 56.91
2022-08-17 01:34:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 11502 updates
2022-08-17 01:34:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint142.pt
2022-08-17 01:34:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint142.pt
2022-08-17 01:34:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint142.pt (epoch 142 @ 11502 updates, score 56.06) (writing took 13.426382835954428 seconds)
2022-08-17 01:34:31 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-08-17 01:34:31 | INFO | train | epoch 142 | loss 3.446 | nll_loss 0.396 | ppl 1.32 | wps 6946.7 | ups 1.26 | wpb 5523.2 | bsz 358 | num_updates 11502 | lr 0.000589717 | gnorm 0.328 | train_wall 37 | gb_free 10.1 | wall 12167
2022-08-17 01:34:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:34:31 | INFO | fairseq.trainer | begin training epoch 143
2022-08-17 01:34:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:35:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:35:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:35:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:35:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:35:22 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 4.988 | nll_loss 2.285 | ppl 4.87 | bleu 56.28 | wps 2082.6 | wpb 933.5 | bsz 59.6 | num_updates 11583 | best_bleu 56.91
2022-08-17 01:35:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 11583 updates
2022-08-17 01:35:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint143.pt
2022-08-17 01:35:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint143.pt
2022-08-17 01:35:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint143.pt (epoch 143 @ 11583 updates, score 56.28) (writing took 26.00045032426715 seconds)
2022-08-17 01:35:48 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-08-17 01:35:48 | INFO | train | epoch 143 | loss 3.445 | nll_loss 0.396 | ppl 1.32 | wps 5805.2 | ups 1.05 | wpb 5523.2 | bsz 358 | num_updates 11583 | lr 0.000587651 | gnorm 0.341 | train_wall 37 | gb_free 10.1 | wall 12245
2022-08-17 01:35:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:35:48 | INFO | fairseq.trainer | begin training epoch 144
2022-08-17 01:35:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:35:59 | INFO | train_inner | epoch 144:     17 / 81 loss=3.445, nll_loss=0.396, ppl=1.32, wps=4954.2, ups=0.9, wpb=5491.8, bsz=356.9, num_updates=11600, lr=0.00058722, gnorm=0.371, train_wall=46, gb_free=10.1, wall=12255
2022-08-17 01:36:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:36:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:36:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:36:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:36:41 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 5.015 | nll_loss 2.312 | ppl 4.97 | bleu 56.13 | wps 1897.5 | wpb 933.5 | bsz 59.6 | num_updates 11664 | best_bleu 56.91
2022-08-17 01:36:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 11664 updates
2022-08-17 01:36:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint144.pt
2022-08-17 01:36:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint144.pt
2022-08-17 01:36:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint144.pt (epoch 144 @ 11664 updates, score 56.13) (writing took 2.2749469466507435 seconds)
2022-08-17 01:36:43 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-08-17 01:36:43 | INFO | train | epoch 144 | loss 3.446 | nll_loss 0.397 | ppl 1.32 | wps 8117 | ups 1.47 | wpb 5523.2 | bsz 358 | num_updates 11664 | lr 0.000585607 | gnorm 0.373 | train_wall 39 | gb_free 10.3 | wall 12300
2022-08-17 01:36:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:36:44 | INFO | fairseq.trainer | begin training epoch 145
2022-08-17 01:36:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:37:06 | INFO | train_inner | epoch 145:     36 / 81 loss=3.444, nll_loss=0.395, ppl=1.32, wps=8247.5, ups=1.49, wpb=5540.5, bsz=361.1, num_updates=11700, lr=0.000584705, gnorm=0.327, train_wall=48, gb_free=10.1, wall=12322
2022-08-17 01:37:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:37:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:37:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:37:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:37:42 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 4.992 | nll_loss 2.294 | ppl 4.9 | bleu 56.13 | wps 2099 | wpb 933.5 | bsz 59.6 | num_updates 11745 | best_bleu 56.91
2022-08-17 01:37:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 11745 updates
2022-08-17 01:37:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint145.pt
2022-08-17 01:37:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint145.pt
2022-08-17 01:37:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint145.pt (epoch 145 @ 11745 updates, score 56.13) (writing took 16.482055224478245 seconds)
2022-08-17 01:37:59 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-08-17 01:37:59 | INFO | train | epoch 145 | loss 3.442 | nll_loss 0.393 | ppl 1.31 | wps 5918.2 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 11745 | lr 0.000583584 | gnorm 0.316 | train_wall 38 | gb_free 10.1 | wall 12375
2022-08-17 01:37:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:37:59 | INFO | fairseq.trainer | begin training epoch 146
2022-08-17 01:37:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:38:27 | INFO | train_inner | epoch 146:     55 / 81 loss=3.443, nll_loss=0.393, ppl=1.31, wps=6771.8, ups=1.23, wpb=5512.1, bsz=356.5, num_updates=11800, lr=0.000582223, gnorm=0.319, train_wall=46, gb_free=10.1, wall=12404
2022-08-17 01:38:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:38:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:38:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:38:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:38:51 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 4.985 | nll_loss 2.277 | ppl 4.85 | bleu 56.81 | wps 1620.3 | wpb 933.5 | bsz 59.6 | num_updates 11826 | best_bleu 56.91
2022-08-17 01:38:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 11826 updates
2022-08-17 01:38:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint146.pt
2022-08-17 01:38:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint146.pt
2022-08-17 01:39:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint146.pt (epoch 146 @ 11826 updates, score 56.81) (writing took 24.79239869490266 seconds)
2022-08-17 01:39:15 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-08-17 01:39:16 | INFO | train | epoch 146 | loss 3.443 | nll_loss 0.394 | ppl 1.31 | wps 5849.2 | ups 1.06 | wpb 5523.2 | bsz 358 | num_updates 11826 | lr 0.000581582 | gnorm 0.486 | train_wall 36 | gb_free 10.1 | wall 12452
2022-08-17 01:39:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:39:16 | INFO | fairseq.trainer | begin training epoch 147
2022-08-17 01:39:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:39:55 | INFO | train_inner | epoch 147:     74 / 81 loss=3.444, nll_loss=0.396, ppl=1.32, wps=6325, ups=1.14, wpb=5564, bsz=358.2, num_updates=11900, lr=0.000579771, gnorm=0.457, train_wall=49, gb_free=10.1, wall=12492
2022-08-17 01:39:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:40:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:40:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:40:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:40:07 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 5.003 | nll_loss 2.305 | ppl 4.94 | bleu 56.16 | wps 2020.1 | wpb 933.5 | bsz 59.6 | num_updates 11907 | best_bleu 56.91
2022-08-17 01:40:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 11907 updates
2022-08-17 01:40:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint147.pt
2022-08-17 01:40:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint147.pt
2022-08-17 01:40:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint147.pt (epoch 147 @ 11907 updates, score 56.16) (writing took 21.991813864558935 seconds)
2022-08-17 01:40:29 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-08-17 01:40:29 | INFO | train | epoch 147 | loss 3.443 | nll_loss 0.395 | ppl 1.32 | wps 6050 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 11907 | lr 0.000579601 | gnorm 0.326 | train_wall 41 | gb_free 10.1 | wall 12526
2022-08-17 01:40:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:40:30 | INFO | fairseq.trainer | begin training epoch 148
2022-08-17 01:40:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:41:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:41:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:41:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:41:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:41:21 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 4.991 | nll_loss 2.29 | ppl 4.89 | bleu 56.52 | wps 2081.9 | wpb 933.5 | bsz 59.6 | num_updates 11988 | best_bleu 56.91
2022-08-17 01:41:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 148 @ 11988 updates
2022-08-17 01:41:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint148.pt
2022-08-17 01:41:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint148.pt
2022-08-17 01:41:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint148.pt (epoch 148 @ 11988 updates, score 56.52) (writing took 12.717336159199476 seconds)
2022-08-17 01:41:34 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2022-08-17 01:41:34 | INFO | train | epoch 148 | loss 3.443 | nll_loss 0.394 | ppl 1.31 | wps 6912.9 | ups 1.25 | wpb 5523.2 | bsz 358 | num_updates 11988 | lr 0.000577639 | gnorm 0.324 | train_wall 39 | gb_free 10.3 | wall 12590
2022-08-17 01:41:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:41:34 | INFO | fairseq.trainer | begin training epoch 149
2022-08-17 01:41:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:41:42 | INFO | train_inner | epoch 149:     12 / 81 loss=3.442, nll_loss=0.394, ppl=1.31, wps=5158, ups=0.94, wpb=5485.4, bsz=355, num_updates=12000, lr=0.00057735, gnorm=0.335, train_wall=48, gb_free=10.1, wall=12598
2022-08-17 01:42:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:42:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:42:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:42:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:42:26 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 5.011 | nll_loss 2.31 | ppl 4.96 | bleu 56.44 | wps 2055.4 | wpb 933.5 | bsz 59.6 | num_updates 12069 | best_bleu 56.91
2022-08-17 01:42:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 12069 updates
2022-08-17 01:42:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint149.pt
2022-08-17 01:42:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint149.pt
2022-08-17 01:42:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint149.pt (epoch 149 @ 12069 updates, score 56.44) (writing took 25.029337782412767 seconds)
2022-08-17 01:42:51 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2022-08-17 01:42:51 | INFO | train | epoch 149 | loss 3.441 | nll_loss 0.393 | ppl 1.31 | wps 5842.5 | ups 1.06 | wpb 5523.2 | bsz 358 | num_updates 12069 | lr 0.000575698 | gnorm 0.343 | train_wall 38 | gb_free 10.2 | wall 12667
2022-08-17 01:42:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:42:51 | INFO | fairseq.trainer | begin training epoch 150
2022-08-17 01:42:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:43:08 | INFO | train_inner | epoch 150:     31 / 81 loss=3.441, nll_loss=0.393, ppl=1.31, wps=6424.7, ups=1.16, wpb=5539.8, bsz=360, num_updates=12100, lr=0.00057496, gnorm=0.34, train_wall=49, gb_free=10.1, wall=12684
2022-08-17 01:43:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:43:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:43:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:43:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:43:42 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 4.98 | nll_loss 2.275 | ppl 4.84 | bleu 56.03 | wps 1806.3 | wpb 933.5 | bsz 59.6 | num_updates 12150 | best_bleu 56.91
2022-08-17 01:43:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 150 @ 12150 updates
2022-08-17 01:43:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint150.pt
2022-08-17 01:43:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint150.pt
2022-08-17 01:44:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint150.pt (epoch 150 @ 12150 updates, score 56.03) (writing took 36.05075192824006 seconds)
2022-08-17 01:44:18 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2022-08-17 01:44:18 | INFO | train | epoch 150 | loss 3.443 | nll_loss 0.395 | ppl 1.31 | wps 5103.2 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 12150 | lr 0.000573775 | gnorm 0.423 | train_wall 40 | gb_free 10.1 | wall 12755
2022-08-17 01:44:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:44:19 | INFO | fairseq.trainer | begin training epoch 151
2022-08-17 01:44:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:44:43 | INFO | train_inner | epoch 151:     50 / 81 loss=3.442, nll_loss=0.394, ppl=1.31, wps=5781.9, ups=1.05, wpb=5522.8, bsz=354.5, num_updates=12200, lr=0.000572598, gnorm=0.392, train_wall=47, gb_free=10.1, wall=12780
2022-08-17 01:44:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:45:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:45:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:45:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:45:08 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 5.003 | nll_loss 2.309 | ppl 4.96 | bleu 56.08 | wps 2010.7 | wpb 933.5 | bsz 59.6 | num_updates 12231 | best_bleu 56.91
2022-08-17 01:45:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 151 @ 12231 updates
2022-08-17 01:45:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint151.pt
2022-08-17 01:45:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint151.pt
2022-08-17 01:45:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint151.pt (epoch 151 @ 12231 updates, score 56.08) (writing took 15.767322104424238 seconds)
2022-08-17 01:45:24 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2022-08-17 01:45:24 | INFO | train | epoch 151 | loss 3.44 | nll_loss 0.392 | ppl 1.31 | wps 6843.5 | ups 1.24 | wpb 5523.2 | bsz 358 | num_updates 12231 | lr 0.000571872 | gnorm 0.32 | train_wall 38 | gb_free 10.1 | wall 12820
2022-08-17 01:45:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:45:24 | INFO | fairseq.trainer | begin training epoch 152
2022-08-17 01:45:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:46:00 | INFO | train_inner | epoch 152:     69 / 81 loss=3.441, nll_loss=0.393, ppl=1.31, wps=7206.4, ups=1.3, wpb=5532.3, bsz=359.9, num_updates=12300, lr=0.000570266, gnorm=0.339, train_wall=48, gb_free=10, wall=12856
2022-08-17 01:46:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:46:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:46:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:46:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:46:15 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 5.006 | nll_loss 2.307 | ppl 4.95 | bleu 55.8 | wps 1929.4 | wpb 933.5 | bsz 59.6 | num_updates 12312 | best_bleu 56.91
2022-08-17 01:46:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 152 @ 12312 updates
2022-08-17 01:46:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint152.pt
2022-08-17 01:46:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint152.pt
2022-08-17 01:46:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint152.pt (epoch 152 @ 12312 updates, score 55.8) (writing took 18.02593719214201 seconds)
2022-08-17 01:46:33 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2022-08-17 01:46:33 | INFO | train | epoch 152 | loss 3.44 | nll_loss 0.392 | ppl 1.31 | wps 6449.9 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 12312 | lr 0.000569988 | gnorm 0.34 | train_wall 39 | gb_free 10.1 | wall 12889
2022-08-17 01:46:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:46:33 | INFO | fairseq.trainer | begin training epoch 153
2022-08-17 01:46:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:47:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:47:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:47:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:47:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:47:26 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 5.007 | nll_loss 2.319 | ppl 4.99 | bleu 55.8 | wps 1898.5 | wpb 933.5 | bsz 59.6 | num_updates 12393 | best_bleu 56.91
2022-08-17 01:47:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 153 @ 12393 updates
2022-08-17 01:47:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint153.pt
2022-08-17 01:47:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint153.pt
2022-08-17 01:48:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint153.pt (epoch 153 @ 12393 updates, score 55.8) (writing took 42.824206817895174 seconds)
2022-08-17 01:48:09 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2022-08-17 01:48:09 | INFO | train | epoch 153 | loss 3.441 | nll_loss 0.393 | ppl 1.31 | wps 4652.2 | ups 0.84 | wpb 5523.2 | bsz 358 | num_updates 12393 | lr 0.000568122 | gnorm 0.372 | train_wall 38 | gb_free 10.2 | wall 12986
2022-08-17 01:48:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:48:10 | INFO | fairseq.trainer | begin training epoch 154
2022-08-17 01:48:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:48:14 | INFO | train_inner | epoch 154:      7 / 81 loss=3.44, nll_loss=0.392, ppl=1.31, wps=4104.8, ups=0.75, wpb=5491.4, bsz=358.4, num_updates=12400, lr=0.000567962, gnorm=0.363, train_wall=47, gb_free=10.1, wall=12990
2022-08-17 01:48:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:48:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:48:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:48:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:49:00 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 5.008 | nll_loss 2.314 | ppl 4.97 | bleu 55.88 | wps 2048.2 | wpb 933.5 | bsz 59.6 | num_updates 12474 | best_bleu 56.91
2022-08-17 01:49:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 154 @ 12474 updates
2022-08-17 01:49:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint154.pt
2022-08-17 01:49:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint154.pt
2022-08-17 01:49:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint154.pt (epoch 154 @ 12474 updates, score 55.88) (writing took 19.73679694533348 seconds)
2022-08-17 01:49:20 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2022-08-17 01:49:20 | INFO | train | epoch 154 | loss 3.438 | nll_loss 0.39 | ppl 1.31 | wps 6313.4 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 12474 | lr 0.000566275 | gnorm 0.328 | train_wall 35 | gb_free 10.1 | wall 13056
2022-08-17 01:49:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:49:20 | INFO | fairseq.trainer | begin training epoch 155
2022-08-17 01:49:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:49:37 | INFO | train_inner | epoch 155:     26 / 81 loss=3.437, nll_loss=0.389, ppl=1.31, wps=6699.4, ups=1.21, wpb=5537.3, bsz=359.8, num_updates=12500, lr=0.000565685, gnorm=0.328, train_wall=44, gb_free=10.2, wall=13073
2022-08-17 01:50:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:50:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:50:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:50:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:50:12 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 4.996 | nll_loss 2.3 | ppl 4.92 | bleu 55.95 | wps 2105.3 | wpb 933.5 | bsz 59.6 | num_updates 12555 | best_bleu 56.91
2022-08-17 01:50:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 155 @ 12555 updates
2022-08-17 01:50:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint155.pt
2022-08-17 01:50:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint155.pt
2022-08-17 01:50:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint155.pt (epoch 155 @ 12555 updates, score 55.95) (writing took 13.996363889425993 seconds)
2022-08-17 01:50:27 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2022-08-17 01:50:27 | INFO | train | epoch 155 | loss 3.439 | nll_loss 0.391 | ppl 1.31 | wps 6736.8 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 12555 | lr 0.000564445 | gnorm 0.343 | train_wall 38 | gb_free 10 | wall 13123
2022-08-17 01:50:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:50:27 | INFO | fairseq.trainer | begin training epoch 156
2022-08-17 01:50:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:50:49 | INFO | train_inner | epoch 156:     45 / 81 loss=3.44, nll_loss=0.393, ppl=1.31, wps=7600.4, ups=1.38, wpb=5498.6, bsz=354.6, num_updates=12600, lr=0.000563436, gnorm=0.343, train_wall=47, gb_free=10.1, wall=13145
2022-08-17 01:51:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:51:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:51:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:51:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:51:15 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 5.009 | nll_loss 2.324 | ppl 5.01 | bleu 55.49 | wps 1992.2 | wpb 933.5 | bsz 59.6 | num_updates 12636 | best_bleu 56.91
2022-08-17 01:51:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 156 @ 12636 updates
2022-08-17 01:51:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint156.pt
2022-08-17 01:51:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint156.pt
2022-08-17 01:51:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint156.pt (epoch 156 @ 12636 updates, score 55.49) (writing took 37.047665912657976 seconds)
2022-08-17 01:51:52 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2022-08-17 01:51:52 | INFO | train | epoch 156 | loss 3.438 | nll_loss 0.39 | ppl 1.31 | wps 5232.1 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 12636 | lr 0.000562633 | gnorm 0.344 | train_wall 38 | gb_free 10.1 | wall 13208
2022-08-17 01:51:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:51:52 | INFO | fairseq.trainer | begin training epoch 157
2022-08-17 01:51:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:52:25 | INFO | train_inner | epoch 157:     64 / 81 loss=3.436, nll_loss=0.388, ppl=1.31, wps=5770.6, ups=1.04, wpb=5555.5, bsz=362.2, num_updates=12700, lr=0.000561214, gnorm=0.365, train_wall=47, gb_free=10.1, wall=13242
2022-08-17 01:52:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:52:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:52:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:52:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:52:42 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 5.011 | nll_loss 2.322 | ppl 5 | bleu 55.82 | wps 2025.1 | wpb 933.5 | bsz 59.6 | num_updates 12717 | best_bleu 56.91
2022-08-17 01:52:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 157 @ 12717 updates
2022-08-17 01:52:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint157.pt
2022-08-17 01:52:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint157.pt
2022-08-17 01:52:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint157.pt (epoch 157 @ 12717 updates, score 55.82) (writing took 15.969562355428934 seconds)
2022-08-17 01:52:58 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2022-08-17 01:52:58 | INFO | train | epoch 157 | loss 3.436 | nll_loss 0.388 | ppl 1.31 | wps 6758.5 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 12717 | lr 0.000560838 | gnorm 0.362 | train_wall 39 | gb_free 10.2 | wall 13275
2022-08-17 01:52:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:52:59 | INFO | fairseq.trainer | begin training epoch 158
2022-08-17 01:52:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:53:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:53:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:53:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:53:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:53:50 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 4.984 | nll_loss 2.291 | ppl 4.89 | bleu 56.09 | wps 1819.9 | wpb 933.5 | bsz 59.6 | num_updates 12798 | best_bleu 56.91
2022-08-17 01:53:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 158 @ 12798 updates
2022-08-17 01:53:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint158.pt
2022-08-17 01:53:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint158.pt
2022-08-17 01:54:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint158.pt (epoch 158 @ 12798 updates, score 56.09) (writing took 14.763418104499578 seconds)
2022-08-17 01:54:05 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2022-08-17 01:54:05 | INFO | train | epoch 158 | loss 3.437 | nll_loss 0.39 | ppl 1.31 | wps 6679.6 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 12798 | lr 0.000559061 | gnorm 0.357 | train_wall 38 | gb_free 10.1 | wall 13342
2022-08-17 01:54:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:54:05 | INFO | fairseq.trainer | begin training epoch 159
2022-08-17 01:54:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:54:08 | INFO | train_inner | epoch 159:      2 / 81 loss=3.438, nll_loss=0.391, ppl=1.31, wps=5385.2, ups=0.98, wpb=5507.3, bsz=355.4, num_updates=12800, lr=0.000559017, gnorm=0.352, train_wall=48, gb_free=10.1, wall=13344
2022-08-17 01:54:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:54:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:54:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:54:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:54:57 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 5.002 | nll_loss 2.311 | ppl 4.96 | bleu 55.78 | wps 2035.7 | wpb 933.5 | bsz 59.6 | num_updates 12879 | best_bleu 56.91
2022-08-17 01:54:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 159 @ 12879 updates
2022-08-17 01:54:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint159.pt
2022-08-17 01:54:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint159.pt
2022-08-17 01:55:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint159.pt (epoch 159 @ 12879 updates, score 55.78) (writing took 37.21935585513711 seconds)
2022-08-17 01:55:35 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2022-08-17 01:55:35 | INFO | train | epoch 159 | loss 3.436 | nll_loss 0.388 | ppl 1.31 | wps 5012.9 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 12879 | lr 0.0005573 | gnorm 0.35 | train_wall 38 | gb_free 10.3 | wall 13431
2022-08-17 01:55:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:55:35 | INFO | fairseq.trainer | begin training epoch 160
2022-08-17 01:55:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:55:46 | INFO | train_inner | epoch 160:     21 / 81 loss=3.435, nll_loss=0.387, ppl=1.31, wps=5561.5, ups=1.01, wpb=5492.3, bsz=357.1, num_updates=12900, lr=0.000556846, gnorm=0.35, train_wall=47, gb_free=10.2, wall=13443
2022-08-17 01:56:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:56:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:56:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:56:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:56:27 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 5.01 | nll_loss 2.316 | ppl 4.98 | bleu 56.2 | wps 2007.8 | wpb 933.5 | bsz 59.6 | num_updates 12960 | best_bleu 56.91
2022-08-17 01:56:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 12960 updates
2022-08-17 01:56:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint160.pt
2022-08-17 01:56:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint160.pt
2022-08-17 01:56:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint160.pt (epoch 160 @ 12960 updates, score 56.2) (writing took 19.49904404953122 seconds)
2022-08-17 01:56:46 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2022-08-17 01:56:46 | INFO | train | epoch 160 | loss 3.435 | nll_loss 0.388 | ppl 1.31 | wps 6221.3 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 12960 | lr 0.000555556 | gnorm 0.371 | train_wall 39 | gb_free 10.1 | wall 13503
2022-08-17 01:56:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:56:47 | INFO | fairseq.trainer | begin training epoch 161
2022-08-17 01:56:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:57:08 | INFO | train_inner | epoch 161:     40 / 81 loss=3.434, nll_loss=0.386, ppl=1.31, wps=6800.9, ups=1.22, wpb=5578.8, bsz=361.8, num_updates=13000, lr=0.0005547, gnorm=0.347, train_wall=48, gb_free=10.1, wall=13525
2022-08-17 01:57:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:57:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:57:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:57:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:57:38 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 4.995 | nll_loss 2.301 | ppl 4.93 | bleu 55.94 | wps 1983.9 | wpb 933.5 | bsz 59.6 | num_updates 13041 | best_bleu 56.91
2022-08-17 01:57:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 161 @ 13041 updates
2022-08-17 01:57:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint161.pt
2022-08-17 01:57:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint161.pt
2022-08-17 01:57:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint161.pt (epoch 161 @ 13041 updates, score 55.94) (writing took 14.83935746923089 seconds)
2022-08-17 01:57:53 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2022-08-17 01:57:53 | INFO | train | epoch 161 | loss 3.434 | nll_loss 0.386 | ppl 1.31 | wps 6713.5 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 13041 | lr 0.000553828 | gnorm 0.323 | train_wall 39 | gb_free 10.2 | wall 13569
2022-08-17 01:57:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:57:53 | INFO | fairseq.trainer | begin training epoch 162
2022-08-17 01:57:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 01:58:26 | INFO | train_inner | epoch 162:     59 / 81 loss=3.435, nll_loss=0.388, ppl=1.31, wps=7134.7, ups=1.29, wpb=5513, bsz=358.6, num_updates=13100, lr=0.000552579, gnorm=0.329, train_wall=48, gb_free=10, wall=13602
2022-08-17 01:58:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 01:58:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 01:58:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 01:58:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 01:58:45 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 5.011 | nll_loss 2.326 | ppl 5.02 | bleu 56.16 | wps 1917.3 | wpb 933.5 | bsz 59.6 | num_updates 13122 | best_bleu 56.91
2022-08-17 01:58:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 162 @ 13122 updates
2022-08-17 01:58:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint162.pt
2022-08-17 01:58:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint162.pt
2022-08-17 01:59:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint162.pt (epoch 162 @ 13122 updates, score 56.16) (writing took 31.478599421679974 seconds)
2022-08-17 01:59:17 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2022-08-17 01:59:17 | INFO | train | epoch 162 | loss 3.434 | nll_loss 0.387 | ppl 1.31 | wps 5360.9 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 13122 | lr 0.000552116 | gnorm 0.321 | train_wall 39 | gb_free 10.1 | wall 13653
2022-08-17 01:59:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 01:59:17 | INFO | fairseq.trainer | begin training epoch 163
2022-08-17 01:59:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:00:18 | INFO | train_inner | epoch 163:     78 / 81 loss=3.433, nll_loss=0.386, ppl=1.31, wps=4924.9, ups=0.89, wpb=5533.4, bsz=357.4, num_updates=13200, lr=0.000550482, gnorm=0.309, train_wall=68, gb_free=10.2, wall=13714
2022-08-17 02:00:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:00:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:00:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:00:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:00:34 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 5.023 | nll_loss 2.328 | ppl 5.02 | bleu 55.65 | wps 1257.8 | wpb 933.5 | bsz 59.6 | num_updates 13203 | best_bleu 56.91
2022-08-17 02:00:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 163 @ 13203 updates
2022-08-17 02:00:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint163.pt
2022-08-17 02:00:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint163.pt
2022-08-17 02:00:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint163.pt (epoch 163 @ 13203 updates, score 55.65) (writing took 19.271471243351698 seconds)
2022-08-17 02:00:53 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2022-08-17 02:00:53 | INFO | train | epoch 163 | loss 3.433 | nll_loss 0.385 | ppl 1.31 | wps 4632.6 | ups 0.84 | wpb 5523.2 | bsz 358 | num_updates 13203 | lr 0.000550419 | gnorm 0.306 | train_wall 61 | gb_free 10.2 | wall 13749
2022-08-17 02:00:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:00:53 | INFO | fairseq.trainer | begin training epoch 164
2022-08-17 02:00:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:02:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:02:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:02:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:02:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:02:14 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 5.005 | nll_loss 2.312 | ppl 4.97 | bleu 56.44 | wps 1191.5 | wpb 933.5 | bsz 59.6 | num_updates 13284 | best_bleu 56.91
2022-08-17 02:02:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 164 @ 13284 updates
2022-08-17 02:02:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint164.pt
2022-08-17 02:02:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint164.pt
2022-08-17 02:02:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint164.pt (epoch 164 @ 13284 updates, score 56.44) (writing took 2.4083458334207535 seconds)
2022-08-17 02:02:16 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2022-08-17 02:02:17 | INFO | train | epoch 164 | loss 3.434 | nll_loss 0.388 | ppl 1.31 | wps 5367.6 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 13284 | lr 0.000548739 | gnorm 0.373 | train_wall 65 | gb_free 10.1 | wall 13833
2022-08-17 02:02:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:02:17 | INFO | fairseq.trainer | begin training epoch 165
2022-08-17 02:02:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:02:29 | INFO | train_inner | epoch 165:     16 / 81 loss=3.434, nll_loss=0.387, ppl=1.31, wps=4221.3, ups=0.77, wpb=5508.3, bsz=355.5, num_updates=13300, lr=0.000548408, gnorm=0.363, train_wall=78, gb_free=10.1, wall=13845
2022-08-17 02:03:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:03:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:03:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:03:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:03:34 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 5.017 | nll_loss 2.331 | ppl 5.03 | bleu 56.04 | wps 1192.3 | wpb 933.5 | bsz 59.6 | num_updates 13365 | best_bleu 56.91
2022-08-17 02:03:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 165 @ 13365 updates
2022-08-17 02:03:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint165.pt
2022-08-17 02:03:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint165.pt
2022-08-17 02:04:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint165.pt (epoch 165 @ 13365 updates, score 56.04) (writing took 33.39076351374388 seconds)
2022-08-17 02:04:08 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2022-08-17 02:04:08 | INFO | train | epoch 165 | loss 3.433 | nll_loss 0.386 | ppl 1.31 | wps 4019.5 | ups 0.73 | wpb 5523.2 | bsz 358 | num_updates 13365 | lr 0.000547073 | gnorm 0.314 | train_wall 62 | gb_free 10.1 | wall 13944
2022-08-17 02:04:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:04:08 | INFO | fairseq.trainer | begin training epoch 166
2022-08-17 02:04:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:04:40 | INFO | train_inner | epoch 166:     35 / 81 loss=3.432, nll_loss=0.385, ppl=1.31, wps=4215.5, ups=0.76, wpb=5543, bsz=360, num_updates=13400, lr=0.000546358, gnorm=0.31, train_wall=82, gb_free=10.1, wall=13976
2022-08-17 02:05:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:05:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:05:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:05:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:05:29 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 5.02 | nll_loss 2.336 | ppl 5.05 | bleu 55.81 | wps 1842.8 | wpb 933.5 | bsz 59.6 | num_updates 13446 | best_bleu 56.91
2022-08-17 02:05:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 166 @ 13446 updates
2022-08-17 02:05:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint166.pt
2022-08-17 02:05:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint166.pt
2022-08-17 02:05:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint166.pt (epoch 166 @ 13446 updates, score 55.81) (writing took 28.429288521409035 seconds)
2022-08-17 02:05:58 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2022-08-17 02:05:58 | INFO | train | epoch 166 | loss 3.43 | nll_loss 0.383 | ppl 1.3 | wps 4059.9 | ups 0.74 | wpb 5523.2 | bsz 358 | num_updates 13446 | lr 0.000545423 | gnorm 0.301 | train_wall 71 | gb_free 10.4 | wall 14054
2022-08-17 02:05:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:05:58 | INFO | fairseq.trainer | begin training epoch 167
2022-08-17 02:05:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:06:44 | INFO | train_inner | epoch 167:     54 / 81 loss=3.432, nll_loss=0.385, ppl=1.31, wps=4431.2, ups=0.81, wpb=5503.7, bsz=354, num_updates=13500, lr=0.000544331, gnorm=0.416, train_wall=85, gb_free=10.1, wall=14100
2022-08-17 02:07:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:07:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:07:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:07:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:07:16 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 5.022 | nll_loss 2.339 | ppl 5.06 | bleu 56.13 | wps 1084.3 | wpb 933.5 | bsz 59.6 | num_updates 13527 | best_bleu 56.91
2022-08-17 02:07:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 167 @ 13527 updates
2022-08-17 02:07:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint167.pt
2022-08-17 02:07:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint167.pt
2022-08-17 02:07:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint167.pt (epoch 167 @ 13527 updates, score 56.13) (writing took 26.735391430556774 seconds)
2022-08-17 02:07:43 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2022-08-17 02:07:43 | INFO | train | epoch 167 | loss 3.432 | nll_loss 0.386 | ppl 1.31 | wps 4266.2 | ups 0.77 | wpb 5523.2 | bsz 358 | num_updates 13527 | lr 0.000543788 | gnorm 0.444 | train_wall 61 | gb_free 10.1 | wall 14159
2022-08-17 02:07:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:07:43 | INFO | fairseq.trainer | begin training epoch 168
2022-08-17 02:07:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:08:41 | INFO | train_inner | epoch 168:     73 / 81 loss=3.43, nll_loss=0.384, ppl=1.3, wps=4728.9, ups=0.86, wpb=5520.4, bsz=360.2, num_updates=13600, lr=0.000542326, gnorm=0.316, train_wall=72, gb_free=10.1, wall=14217
2022-08-17 02:08:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:08:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:08:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:08:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:09:02 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 5.012 | nll_loss 2.324 | ppl 5.01 | bleu 56.5 | wps 1162.3 | wpb 933.5 | bsz 59.6 | num_updates 13608 | best_bleu 56.91
2022-08-17 02:09:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 168 @ 13608 updates
2022-08-17 02:09:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint168.pt
2022-08-17 02:09:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint168.pt
2022-08-17 02:09:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint168.pt (epoch 168 @ 13608 updates, score 56.5) (writing took 2.361591476947069 seconds)
2022-08-17 02:09:04 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2022-08-17 02:09:04 | INFO | train | epoch 168 | loss 3.43 | nll_loss 0.384 | ppl 1.3 | wps 5494.7 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 13608 | lr 0.000542167 | gnorm 0.322 | train_wall 62 | gb_free 10.1 | wall 14241
2022-08-17 02:09:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:09:04 | INFO | fairseq.trainer | begin training epoch 169
2022-08-17 02:09:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:10:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:10:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:10:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:10:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:10:22 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 5.009 | nll_loss 2.328 | ppl 5.02 | bleu 56.4 | wps 1196.7 | wpb 933.5 | bsz 59.6 | num_updates 13689 | best_bleu 56.91
2022-08-17 02:10:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 169 @ 13689 updates
2022-08-17 02:10:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint169.pt
2022-08-17 02:10:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint169.pt
2022-08-17 02:10:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint169.pt (epoch 169 @ 13689 updates, score 56.4) (writing took 26.099782716482878 seconds)
2022-08-17 02:10:48 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2022-08-17 02:10:48 | INFO | train | epoch 169 | loss 3.431 | nll_loss 0.385 | ppl 1.31 | wps 4304.1 | ups 0.78 | wpb 5523.2 | bsz 358 | num_updates 13689 | lr 0.00054056 | gnorm 0.544 | train_wall 62 | gb_free 10.1 | wall 14344
2022-08-17 02:10:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:10:48 | INFO | fairseq.trainer | begin training epoch 170
2022-08-17 02:10:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:10:59 | INFO | train_inner | epoch 170:     11 / 81 loss=3.431, nll_loss=0.385, ppl=1.31, wps=3988.1, ups=0.72, wpb=5510.4, bsz=356.5, num_updates=13700, lr=0.000540343, gnorm=0.503, train_wall=78, gb_free=10.1, wall=14355
2022-08-17 02:11:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:11:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:11:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:11:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:12:06 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 5.026 | nll_loss 2.338 | ppl 5.06 | bleu 55.31 | wps 1181.8 | wpb 933.5 | bsz 59.6 | num_updates 13770 | best_bleu 56.91
2022-08-17 02:12:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 170 @ 13770 updates
2022-08-17 02:12:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint170.pt
2022-08-17 02:12:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint170.pt
2022-08-17 02:12:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint170.pt (epoch 170 @ 13770 updates, score 55.31) (writing took 19.28558361902833 seconds)
2022-08-17 02:12:26 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2022-08-17 02:12:26 | INFO | train | epoch 170 | loss 3.432 | nll_loss 0.386 | ppl 1.31 | wps 4599.7 | ups 0.83 | wpb 5523.2 | bsz 358 | num_updates 13770 | lr 0.000538968 | gnorm 0.327 | train_wall 62 | gb_free 10.2 | wall 14442
2022-08-17 02:12:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:12:26 | INFO | fairseq.trainer | begin training epoch 171
2022-08-17 02:12:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:12:44 | INFO | train_inner | epoch 171:     30 / 81 loss=3.431, nll_loss=0.384, ppl=1.31, wps=5293.9, ups=0.96, wpb=5542.8, bsz=359.8, num_updates=13800, lr=0.000538382, gnorm=0.327, train_wall=69, gb_free=10.1, wall=14460
2022-08-17 02:13:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:13:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:13:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:13:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:13:42 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 5.033 | nll_loss 2.353 | ppl 5.11 | bleu 55.1 | wps 1202.8 | wpb 933.5 | bsz 59.6 | num_updates 13851 | best_bleu 56.91
2022-08-17 02:13:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 171 @ 13851 updates
2022-08-17 02:13:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint171.pt
2022-08-17 02:13:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint171.pt
2022-08-17 02:14:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint171.pt (epoch 171 @ 13851 updates, score 55.1) (writing took 31.918779954314232 seconds)
2022-08-17 02:14:15 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2022-08-17 02:14:15 | INFO | train | epoch 171 | loss 3.43 | nll_loss 0.384 | ppl 1.3 | wps 4103.7 | ups 0.74 | wpb 5523.2 | bsz 358 | num_updates 13851 | lr 0.00053739 | gnorm 0.575 | train_wall 61 | gb_free 10.1 | wall 14551
2022-08-17 02:14:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:14:15 | INFO | fairseq.trainer | begin training epoch 172
2022-08-17 02:14:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:14:57 | INFO | train_inner | epoch 172:     49 / 81 loss=3.429, nll_loss=0.383, ppl=1.3, wps=4130.9, ups=0.75, wpb=5506.6, bsz=360.7, num_updates=13900, lr=0.000536442, gnorm=0.518, train_wall=86, gb_free=10.1, wall=14593
2022-08-17 02:15:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:15:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:15:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:15:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:15:36 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 4.989 | nll_loss 2.294 | ppl 4.91 | bleu 57.08 | wps 1599.8 | wpb 933.5 | bsz 59.6 | num_updates 13932 | best_bleu 57.08
2022-08-17 02:15:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 172 @ 13932 updates
2022-08-17 02:15:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint172.pt
2022-08-17 02:15:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint172.pt
2022-08-17 02:15:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint172.pt (epoch 172 @ 13932 updates, score 57.08) (writing took 21.422741789370775 seconds)
2022-08-17 02:15:57 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2022-08-17 02:15:57 | INFO | train | epoch 172 | loss 3.429 | nll_loss 0.383 | ppl 1.3 | wps 4353.5 | ups 0.79 | wpb 5523.2 | bsz 358 | num_updates 13932 | lr 0.000535825 | gnorm 0.316 | train_wall 69 | gb_free 10.2 | wall 14654
2022-08-17 02:15:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:15:58 | INFO | fairseq.trainer | begin training epoch 173
2022-08-17 02:15:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:16:57 | INFO | train_inner | epoch 173:     68 / 81 loss=3.43, nll_loss=0.384, ppl=1.31, wps=4613.4, ups=0.83, wpb=5543.6, bsz=355, num_updates=14000, lr=0.000534522, gnorm=0.337, train_wall=86, gb_free=10.1, wall=14714
2022-08-17 02:17:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:17:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:17:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:17:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:17:17 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 5.03 | nll_loss 2.347 | ppl 5.09 | bleu 56.11 | wps 1233.1 | wpb 933.5 | bsz 59.6 | num_updates 14013 | best_bleu 57.08
2022-08-17 02:17:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 173 @ 14013 updates
2022-08-17 02:17:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint173.pt
2022-08-17 02:17:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint173.pt
2022-08-17 02:17:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint173.pt (epoch 173 @ 14013 updates, score 56.11) (writing took 13.258858691900969 seconds)
2022-08-17 02:17:31 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2022-08-17 02:17:31 | INFO | train | epoch 173 | loss 3.429 | nll_loss 0.383 | ppl 1.3 | wps 4780.4 | ups 0.87 | wpb 5523.2 | bsz 358 | num_updates 14013 | lr 0.000534274 | gnorm 0.333 | train_wall 65 | gb_free 10.2 | wall 14747
2022-08-17 02:17:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:17:31 | INFO | fairseq.trainer | begin training epoch 174
2022-08-17 02:17:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:18:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:18:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:18:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:18:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:18:49 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 5.022 | nll_loss 2.342 | ppl 5.07 | bleu 56.53 | wps 1238.2 | wpb 933.5 | bsz 59.6 | num_updates 14094 | best_bleu 57.08
2022-08-17 02:18:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 174 @ 14094 updates
2022-08-17 02:18:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint174.pt
2022-08-17 02:18:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint174.pt
2022-08-17 02:19:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint174.pt (epoch 174 @ 14094 updates, score 56.53) (writing took 24.350320372730494 seconds)
2022-08-17 02:19:13 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2022-08-17 02:19:13 | INFO | train | epoch 174 | loss 3.427 | nll_loss 0.381 | ppl 1.3 | wps 4380.3 | ups 0.79 | wpb 5523.2 | bsz 358 | num_updates 14094 | lr 0.000532737 | gnorm 0.351 | train_wall 63 | gb_free 10.2 | wall 14849
2022-08-17 02:19:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:19:13 | INFO | fairseq.trainer | begin training epoch 175
2022-08-17 02:19:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:19:20 | INFO | train_inner | epoch 175:      6 / 81 loss=3.427, nll_loss=0.381, ppl=1.3, wps=3857.9, ups=0.7, wpb=5505.1, bsz=361.2, num_updates=14100, lr=0.000532624, gnorm=0.344, train_wall=75, gb_free=10.1, wall=14856
2022-08-17 02:20:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:20:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:20:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:20:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:20:31 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 5.036 | nll_loss 2.358 | ppl 5.13 | bleu 56.03 | wps 1184 | wpb 933.5 | bsz 59.6 | num_updates 14175 | best_bleu 57.08
2022-08-17 02:20:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 175 @ 14175 updates
2022-08-17 02:20:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint175.pt
2022-08-17 02:20:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint175.pt
2022-08-17 02:20:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint175.pt (epoch 175 @ 14175 updates, score 56.03) (writing took 25.926856145262718 seconds)
2022-08-17 02:20:57 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2022-08-17 02:20:57 | INFO | train | epoch 175 | loss 3.428 | nll_loss 0.382 | ppl 1.3 | wps 4302.2 | ups 0.78 | wpb 5523.2 | bsz 358 | num_updates 14175 | lr 0.000531213 | gnorm 0.368 | train_wall 62 | gb_free 10.2 | wall 14953
2022-08-17 02:20:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:20:57 | INFO | fairseq.trainer | begin training epoch 176
2022-08-17 02:20:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:21:17 | INFO | train_inner | epoch 176:     25 / 81 loss=3.429, nll_loss=0.382, ppl=1.3, wps=4749.2, ups=0.86, wpb=5532.2, bsz=354.4, num_updates=14200, lr=0.000530745, gnorm=0.364, train_wall=75, gb_free=10, wall=14973
2022-08-17 02:21:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:22:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:22:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:22:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:22:14 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 5.041 | nll_loss 2.355 | ppl 5.12 | bleu 55.54 | wps 1171.5 | wpb 933.5 | bsz 59.6 | num_updates 14256 | best_bleu 57.08
2022-08-17 02:22:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 176 @ 14256 updates
2022-08-17 02:22:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint176.pt
2022-08-17 02:22:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint176.pt
2022-08-17 02:22:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint176.pt (epoch 176 @ 14256 updates, score 55.54) (writing took 28.154281355440617 seconds)
2022-08-17 02:22:42 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2022-08-17 02:22:42 | INFO | train | epoch 176 | loss 3.429 | nll_loss 0.384 | ppl 1.31 | wps 4265.2 | ups 0.77 | wpb 5523.2 | bsz 358 | num_updates 14256 | lr 0.000529701 | gnorm 0.447 | train_wall 61 | gb_free 10 | wall 15058
2022-08-17 02:22:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:22:42 | INFO | fairseq.trainer | begin training epoch 177
2022-08-17 02:22:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:23:18 | INFO | train_inner | epoch 177:     44 / 81 loss=3.429, nll_loss=0.383, ppl=1.3, wps=4562, ups=0.83, wpb=5524.5, bsz=360.9, num_updates=14300, lr=0.000528886, gnorm=0.411, train_wall=77, gb_free=10.1, wall=15094
2022-08-17 02:23:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:23:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:23:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:23:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:24:04 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 5.036 | nll_loss 2.356 | ppl 5.12 | bleu 56.66 | wps 1186.4 | wpb 933.5 | bsz 59.6 | num_updates 14337 | best_bleu 57.08
2022-08-17 02:24:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 177 @ 14337 updates
2022-08-17 02:24:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint177.pt
2022-08-17 02:24:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint177.pt
2022-08-17 02:24:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint177.pt (epoch 177 @ 14337 updates, score 56.66) (writing took 2.3568842224776745 seconds)
2022-08-17 02:24:06 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2022-08-17 02:24:06 | INFO | train | epoch 177 | loss 3.427 | nll_loss 0.381 | ppl 1.3 | wps 5299.1 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 14337 | lr 0.000528203 | gnorm 0.305 | train_wall 66 | gb_free 10.2 | wall 15143
2022-08-17 02:24:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:24:07 | INFO | fairseq.trainer | begin training epoch 178
2022-08-17 02:24:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:24:54 | INFO | train_inner | epoch 178:     63 / 81 loss=3.426, nll_loss=0.381, ppl=1.3, wps=5718.4, ups=1.04, wpb=5510.6, bsz=353.8, num_updates=14400, lr=0.000527046, gnorm=0.313, train_wall=78, gb_free=10, wall=15190
2022-08-17 02:25:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:25:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:25:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:25:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:25:24 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 5.031 | nll_loss 2.351 | ppl 5.1 | bleu 56.49 | wps 1237.9 | wpb 933.5 | bsz 59.6 | num_updates 14418 | best_bleu 57.08
2022-08-17 02:25:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 178 @ 14418 updates
2022-08-17 02:25:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint178.pt
2022-08-17 02:25:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint178.pt
2022-08-17 02:25:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint178.pt (epoch 178 @ 14418 updates, score 56.49) (writing took 14.419515002518892 seconds)
2022-08-17 02:25:38 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2022-08-17 02:25:38 | INFO | train | epoch 178 | loss 3.427 | nll_loss 0.381 | ppl 1.3 | wps 4866 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 14418 | lr 0.000526717 | gnorm 0.318 | train_wall 62 | gb_free 10 | wall 15235
2022-08-17 02:25:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:25:38 | INFO | fairseq.trainer | begin training epoch 179
2022-08-17 02:25:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:26:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:26:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:26:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:26:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:26:55 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 5.011 | nll_loss 2.333 | ppl 5.04 | bleu 56.12 | wps 1224.5 | wpb 933.5 | bsz 59.6 | num_updates 14499 | best_bleu 57.08
2022-08-17 02:26:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 179 @ 14499 updates
2022-08-17 02:26:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint179.pt
2022-08-17 02:26:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint179.pt
2022-08-17 02:27:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint179.pt (epoch 179 @ 14499 updates, score 56.12) (writing took 16.382489498704672 seconds)
2022-08-17 02:27:12 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)
2022-08-17 02:27:12 | INFO | train | epoch 179 | loss 3.427 | nll_loss 0.382 | ppl 1.3 | wps 4788.3 | ups 0.87 | wpb 5523.2 | bsz 358 | num_updates 14499 | lr 0.000525244 | gnorm 0.339 | train_wall 62 | gb_free 10.2 | wall 15328
2022-08-17 02:27:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:27:12 | INFO | fairseq.trainer | begin training epoch 180
2022-08-17 02:27:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:27:14 | INFO | train_inner | epoch 180:      1 / 81 loss=3.427, nll_loss=0.382, ppl=1.3, wps=3958.3, ups=0.72, wpb=5525.4, bsz=360.2, num_updates=14500, lr=0.000525226, gnorm=0.335, train_wall=79, gb_free=10.1, wall=15330
2022-08-17 02:28:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:28:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:28:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:28:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:28:31 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 5.022 | nll_loss 2.336 | ppl 5.05 | bleu 55.91 | wps 1142.2 | wpb 933.5 | bsz 59.6 | num_updates 14580 | best_bleu 57.08
2022-08-17 02:28:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 180 @ 14580 updates
2022-08-17 02:28:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint180.pt
2022-08-17 02:28:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint180.pt
2022-08-17 02:28:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint180.pt (epoch 180 @ 14580 updates, score 55.91) (writing took 12.936370119452477 seconds)
2022-08-17 02:28:44 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)
2022-08-17 02:28:44 | INFO | train | epoch 180 | loss 3.426 | nll_loss 0.381 | ppl 1.3 | wps 4829.8 | ups 0.87 | wpb 5523.2 | bsz 358 | num_updates 14580 | lr 0.000523783 | gnorm 0.313 | train_wall 63 | gb_free 10.1 | wall 15421
2022-08-17 02:28:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:28:45 | INFO | fairseq.trainer | begin training epoch 181
2022-08-17 02:28:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:28:58 | INFO | train_inner | epoch 181:     20 / 81 loss=3.425, nll_loss=0.38, ppl=1.3, wps=5289, ups=0.96, wpb=5504.1, bsz=359.2, num_updates=14600, lr=0.000523424, gnorm=0.326, train_wall=74, gb_free=10.1, wall=15434
2022-08-17 02:29:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:29:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:29:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:29:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:30:06 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 4.997 | nll_loss 2.312 | ppl 4.97 | bleu 56.19 | wps 1190.9 | wpb 933.5 | bsz 59.6 | num_updates 14661 | best_bleu 57.08
2022-08-17 02:30:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 181 @ 14661 updates
2022-08-17 02:30:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint181.pt
2022-08-17 02:30:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint181.pt
2022-08-17 02:30:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint181.pt (epoch 181 @ 14661 updates, score 56.19) (writing took 45.43428597599268 seconds)
2022-08-17 02:30:51 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)
2022-08-17 02:30:51 | INFO | train | epoch 181 | loss 3.426 | nll_loss 0.381 | ppl 1.3 | wps 3530.6 | ups 0.64 | wpb 5523.2 | bsz 358 | num_updates 14661 | lr 0.000522334 | gnorm 0.347 | train_wall 65 | gb_free 10.2 | wall 15547
2022-08-17 02:30:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:30:51 | INFO | fairseq.trainer | begin training epoch 182
2022-08-17 02:30:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:31:26 | INFO | train_inner | epoch 182:     39 / 81 loss=3.426, nll_loss=0.381, ppl=1.3, wps=3709.2, ups=0.67, wpb=5515.9, bsz=358.8, num_updates=14700, lr=0.000521641, gnorm=0.325, train_wall=88, gb_free=10.1, wall=15583
2022-08-17 02:31:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:31:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:31:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:31:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:32:08 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 5.023 | nll_loss 2.34 | ppl 5.06 | bleu 56 | wps 1236.5 | wpb 933.5 | bsz 59.6 | num_updates 14742 | best_bleu 57.08
2022-08-17 02:32:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 182 @ 14742 updates
2022-08-17 02:32:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint182.pt
2022-08-17 02:32:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint182.pt
2022-08-17 02:32:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint182.pt (epoch 182 @ 14742 updates, score 56.0) (writing took 2.2990286014974117 seconds)
2022-08-17 02:32:10 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)
2022-08-17 02:32:10 | INFO | train | epoch 182 | loss 3.425 | nll_loss 0.38 | ppl 1.3 | wps 5664.1 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 14742 | lr 0.000520897 | gnorm 0.292 | train_wall 61 | gb_free 10.2 | wall 15626
2022-08-17 02:32:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:32:10 | INFO | fairseq.trainer | begin training epoch 183
2022-08-17 02:32:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:33:02 | INFO | train_inner | epoch 183:     58 / 81 loss=3.424, nll_loss=0.379, ppl=1.3, wps=5867.7, ups=1.05, wpb=5583.4, bsz=361.1, num_updates=14800, lr=0.000519875, gnorm=0.299, train_wall=78, gb_free=10, wall=15678
2022-08-17 02:33:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:33:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:33:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:33:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:33:28 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 5.021 | nll_loss 2.343 | ppl 5.07 | bleu 56.16 | wps 1383 | wpb 933.5 | bsz 59.6 | num_updates 14823 | best_bleu 57.08
2022-08-17 02:33:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 183 @ 14823 updates
2022-08-17 02:33:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint183.pt
2022-08-17 02:33:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint183.pt
2022-08-17 02:34:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint183.pt (epoch 183 @ 14823 updates, score 56.16) (writing took 35.215500961989164 seconds)
2022-08-17 02:34:03 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)
2022-08-17 02:34:03 | INFO | train | epoch 183 | loss 3.424 | nll_loss 0.379 | ppl 1.3 | wps 3958.8 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 14823 | lr 0.000519472 | gnorm 0.322 | train_wall 64 | gb_free 10.2 | wall 15739
2022-08-17 02:34:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:34:03 | INFO | fairseq.trainer | begin training epoch 184
2022-08-17 02:34:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:35:05 | INFO | train_inner | epoch 184:     77 / 81 loss=3.426, nll_loss=0.381, ppl=1.3, wps=4474.5, ups=0.81, wpb=5510.1, bsz=353.9, num_updates=14900, lr=0.000518128, gnorm=0.417, train_wall=74, gb_free=10.1, wall=15801
2022-08-17 02:35:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:35:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:35:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:35:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:35:21 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 5.013 | nll_loss 2.334 | ppl 5.04 | bleu 55.99 | wps 1252.8 | wpb 933.5 | bsz 59.6 | num_updates 14904 | best_bleu 57.08
2022-08-17 02:35:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 184 @ 14904 updates
2022-08-17 02:35:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint184.pt
2022-08-17 02:35:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint184.pt
2022-08-17 02:35:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint184.pt (epoch 184 @ 14904 updates, score 55.99) (writing took 16.618039660155773 seconds)
2022-08-17 02:35:38 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)
2022-08-17 02:35:38 | INFO | train | epoch 184 | loss 3.425 | nll_loss 0.38 | ppl 1.3 | wps 4724.5 | ups 0.86 | wpb 5523.2 | bsz 358 | num_updates 14904 | lr 0.000518058 | gnorm 0.435 | train_wall 63 | gb_free 10.2 | wall 15834
2022-08-17 02:35:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:35:38 | INFO | fairseq.trainer | begin training epoch 185
2022-08-17 02:35:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:36:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:36:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:36:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:36:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:36:54 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 5.033 | nll_loss 2.357 | ppl 5.12 | bleu 56.19 | wps 1196.1 | wpb 933.5 | bsz 59.6 | num_updates 14985 | best_bleu 57.08
2022-08-17 02:36:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 185 @ 14985 updates
2022-08-17 02:36:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint185.pt
2022-08-17 02:36:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint185.pt
2022-08-17 02:37:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint185.pt (epoch 185 @ 14985 updates, score 56.19) (writing took 17.522078704088926 seconds)
2022-08-17 02:37:12 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)
2022-08-17 02:37:12 | INFO | train | epoch 185 | loss 3.425 | nll_loss 0.38 | ppl 1.3 | wps 4772.3 | ups 0.86 | wpb 5523.2 | bsz 358 | num_updates 14985 | lr 0.000516656 | gnorm 0.303 | train_wall 61 | gb_free 10.1 | wall 15928
2022-08-17 02:37:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:37:12 | INFO | fairseq.trainer | begin training epoch 186
2022-08-17 02:37:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:37:25 | INFO | train_inner | epoch 186:     15 / 81 loss=3.425, nll_loss=0.38, ppl=1.3, wps=3930.4, ups=0.71, wpb=5510.6, bsz=356.2, num_updates=15000, lr=0.000516398, gnorm=0.303, train_wall=76, gb_free=10, wall=15941
2022-08-17 02:38:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:38:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:38:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:38:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:38:29 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 5.046 | nll_loss 2.368 | ppl 5.16 | bleu 56.18 | wps 1204.9 | wpb 933.5 | bsz 59.6 | num_updates 15066 | best_bleu 57.08
2022-08-17 02:38:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 186 @ 15066 updates
2022-08-17 02:38:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint186.pt
2022-08-17 02:38:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint186.pt
2022-08-17 02:38:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint186.pt (epoch 186 @ 15066 updates, score 56.18) (writing took 20.697563886642456 seconds)
2022-08-17 02:38:49 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)
2022-08-17 02:38:50 | INFO | train | epoch 186 | loss 3.423 | nll_loss 0.377 | ppl 1.3 | wps 4568.1 | ups 0.83 | wpb 5523.2 | bsz 358 | num_updates 15066 | lr 0.000515265 | gnorm 0.294 | train_wall 62 | gb_free 10.1 | wall 16026
2022-08-17 02:38:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:38:50 | INFO | fairseq.trainer | begin training epoch 187
2022-08-17 02:38:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:39:12 | INFO | train_inner | epoch 187:     34 / 81 loss=3.422, nll_loss=0.377, ppl=1.3, wps=5185.6, ups=0.94, wpb=5533.6, bsz=361.4, num_updates=15100, lr=0.000514685, gnorm=0.286, train_wall=71, gb_free=10.1, wall=16048
2022-08-17 02:39:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:39:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:39:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:39:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:40:07 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 5.022 | nll_loss 2.34 | ppl 5.06 | bleu 55.99 | wps 1186.9 | wpb 933.5 | bsz 59.6 | num_updates 15147 | best_bleu 57.08
2022-08-17 02:40:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 187 @ 15147 updates
2022-08-17 02:40:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint187.pt
2022-08-17 02:40:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint187.pt
2022-08-17 02:40:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint187.pt (epoch 187 @ 15147 updates, score 55.99) (writing took 21.462616082280874 seconds)
2022-08-17 02:40:29 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)
2022-08-17 02:40:29 | INFO | train | epoch 187 | loss 3.424 | nll_loss 0.379 | ppl 1.3 | wps 4497.4 | ups 0.81 | wpb 5523.2 | bsz 358 | num_updates 15147 | lr 0.000513886 | gnorm 0.315 | train_wall 62 | gb_free 10.1 | wall 16125
2022-08-17 02:40:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:40:29 | INFO | fairseq.trainer | begin training epoch 188
2022-08-17 02:40:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:41:15 | INFO | train_inner | epoch 188:     53 / 81 loss=3.425, nll_loss=0.381, ppl=1.3, wps=4491.7, ups=0.81, wpb=5537.2, bsz=357.2, num_updates=15200, lr=0.000512989, gnorm=0.386, train_wall=85, gb_free=10.1, wall=16171
2022-08-17 02:41:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:41:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:41:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:41:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:41:54 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 5.03 | nll_loss 2.357 | ppl 5.12 | bleu 55.89 | wps 1183.9 | wpb 933.5 | bsz 59.6 | num_updates 15228 | best_bleu 57.08
2022-08-17 02:41:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 188 @ 15228 updates
2022-08-17 02:41:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint188.pt
2022-08-17 02:41:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint188.pt
2022-08-17 02:42:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint188.pt (epoch 188 @ 15228 updates, score 55.89) (writing took 26.17167530953884 seconds)
2022-08-17 02:42:20 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)
2022-08-17 02:42:20 | INFO | train | epoch 188 | loss 3.425 | nll_loss 0.381 | ppl 1.3 | wps 4024.5 | ups 0.73 | wpb 5523.2 | bsz 358 | num_updates 15228 | lr 0.000512517 | gnorm 0.4 | train_wall 69 | gb_free 10.1 | wall 16236
2022-08-17 02:42:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:42:20 | INFO | fairseq.trainer | begin training epoch 189
2022-08-17 02:42:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:43:23 | INFO | train_inner | epoch 189:     72 / 81 loss=3.424, nll_loss=0.379, ppl=1.3, wps=4282.7, ups=0.78, wpb=5503.5, bsz=358.2, num_updates=15300, lr=0.00051131, gnorm=0.326, train_wall=86, gb_free=10.1, wall=16300
2022-08-17 02:43:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:43:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:43:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:43:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:43:38 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 5.03 | nll_loss 2.356 | ppl 5.12 | bleu 56.47 | wps 1623.3 | wpb 933.5 | bsz 59.6 | num_updates 15309 | best_bleu 57.08
2022-08-17 02:43:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 189 @ 15309 updates
2022-08-17 02:43:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint189.pt
2022-08-17 02:43:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint189.pt
2022-08-17 02:43:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint189.pt (epoch 189 @ 15309 updates, score 56.47) (writing took 19.343856632709503 seconds)
2022-08-17 02:43:58 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)
2022-08-17 02:43:58 | INFO | train | epoch 189 | loss 3.423 | nll_loss 0.379 | ppl 1.3 | wps 4584 | ups 0.83 | wpb 5523.2 | bsz 358 | num_updates 15309 | lr 0.00051116 | gnorm 0.313 | train_wall 66 | gb_free 10.1 | wall 16334
2022-08-17 02:43:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:43:58 | INFO | fairseq.trainer | begin training epoch 190
2022-08-17 02:43:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:44:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:45:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:45:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:45:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:45:14 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 5.041 | nll_loss 2.363 | ppl 5.15 | bleu 55.69 | wps 1148.1 | wpb 933.5 | bsz 59.6 | num_updates 15390 | best_bleu 57.08
2022-08-17 02:45:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 190 @ 15390 updates
2022-08-17 02:45:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint190.pt
2022-08-17 02:45:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint190.pt
2022-08-17 02:45:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint190.pt (epoch 190 @ 15390 updates, score 55.69) (writing took 16.023887243121862 seconds)
2022-08-17 02:45:30 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)
2022-08-17 02:45:30 | INFO | train | epoch 190 | loss 3.421 | nll_loss 0.376 | ppl 1.3 | wps 4843.3 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 15390 | lr 0.000509813 | gnorm 0.293 | train_wall 60 | gb_free 10 | wall 16426
2022-08-17 02:45:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:45:30 | INFO | fairseq.trainer | begin training epoch 191
2022-08-17 02:45:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:45:40 | INFO | train_inner | epoch 191:     10 / 81 loss=3.421, nll_loss=0.376, ppl=1.3, wps=4030.5, ups=0.73, wpb=5509.8, bsz=355.7, num_updates=15400, lr=0.000509647, gnorm=0.294, train_wall=73, gb_free=10.1, wall=16436
2022-08-17 02:46:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:46:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:46:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:46:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:46:46 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 5.043 | nll_loss 2.372 | ppl 5.18 | bleu 56.46 | wps 1198.8 | wpb 933.5 | bsz 59.6 | num_updates 15471 | best_bleu 57.08
2022-08-17 02:46:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 191 @ 15471 updates
2022-08-17 02:46:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint191.pt
2022-08-17 02:46:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint191.pt
2022-08-17 02:47:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint191.pt (epoch 191 @ 15471 updates, score 56.46) (writing took 21.80967789515853 seconds)
2022-08-17 02:47:08 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)
2022-08-17 02:47:08 | INFO | train | epoch 191 | loss 3.42 | nll_loss 0.376 | ppl 1.3 | wps 4565.3 | ups 0.83 | wpb 5523.2 | bsz 358 | num_updates 15471 | lr 0.000508476 | gnorm 0.291 | train_wall 60 | gb_free 10.1 | wall 16524
2022-08-17 02:47:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:47:08 | INFO | fairseq.trainer | begin training epoch 192
2022-08-17 02:47:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:47:33 | INFO | train_inner | epoch 192:     29 / 81 loss=3.421, nll_loss=0.377, ppl=1.3, wps=4894.6, ups=0.88, wpb=5546.4, bsz=359.9, num_updates=15500, lr=0.000508001, gnorm=0.293, train_wall=76, gb_free=10.1, wall=16550
2022-08-17 02:48:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:48:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:48:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:48:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:48:24 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 5.033 | nll_loss 2.357 | ppl 5.12 | bleu 55.84 | wps 1223.8 | wpb 933.5 | bsz 59.6 | num_updates 15552 | best_bleu 57.08
2022-08-17 02:48:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 192 @ 15552 updates
2022-08-17 02:48:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint192.pt
2022-08-17 02:48:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint192.pt
2022-08-17 02:48:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint192.pt (epoch 192 @ 15552 updates, score 55.84) (writing took 19.72957143560052 seconds)
2022-08-17 02:48:44 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)
2022-08-17 02:48:44 | INFO | train | epoch 192 | loss 3.422 | nll_loss 0.378 | ppl 1.3 | wps 4684.7 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 15552 | lr 0.000507151 | gnorm 0.332 | train_wall 60 | gb_free 10.1 | wall 16620
2022-08-17 02:48:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:48:44 | INFO | fairseq.trainer | begin training epoch 193
2022-08-17 02:48:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:49:18 | INFO | train_inner | epoch 193:     48 / 81 loss=3.422, nll_loss=0.378, ppl=1.3, wps=5260.2, ups=0.95, wpb=5508.1, bsz=355.5, num_updates=15600, lr=0.00050637, gnorm=0.336, train_wall=70, gb_free=10, wall=16654
2022-08-17 02:49:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:49:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:49:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:49:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:49:59 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 5.029 | nll_loss 2.351 | ppl 5.1 | bleu 56.64 | wps 1184.6 | wpb 933.5 | bsz 59.6 | num_updates 15633 | best_bleu 57.08
2022-08-17 02:49:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 193 @ 15633 updates
2022-08-17 02:49:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint193.pt
2022-08-17 02:50:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint193.pt
2022-08-17 02:50:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint193.pt (epoch 193 @ 15633 updates, score 56.64) (writing took 36.509087927639484 seconds)
2022-08-17 02:50:36 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)
2022-08-17 02:50:36 | INFO | train | epoch 193 | loss 3.422 | nll_loss 0.378 | ppl 1.3 | wps 3976.1 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 15633 | lr 0.000505835 | gnorm 0.327 | train_wall 60 | gb_free 10.2 | wall 16732
2022-08-17 02:50:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:50:36 | INFO | fairseq.trainer | begin training epoch 194
2022-08-17 02:50:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:51:33 | INFO | train_inner | epoch 194:     67 / 81 loss=3.419, nll_loss=0.375, ppl=1.3, wps=4117.1, ups=0.74, wpb=5540.1, bsz=364.7, num_updates=15700, lr=0.000504754, gnorm=0.313, train_wall=82, gb_free=10, wall=16789
2022-08-17 02:51:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:51:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:51:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:51:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:51:59 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 5.042 | nll_loss 2.374 | ppl 5.18 | bleu 55.93 | wps 1190.4 | wpb 933.5 | bsz 59.6 | num_updates 15714 | best_bleu 57.08
2022-08-17 02:51:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 194 @ 15714 updates
2022-08-17 02:51:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint194.pt
2022-08-17 02:52:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint194.pt
2022-08-17 02:52:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint194.pt (epoch 194 @ 15714 updates, score 55.93) (writing took 34.27233190461993 seconds)
2022-08-17 02:52:33 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)
2022-08-17 02:52:33 | INFO | train | epoch 194 | loss 3.42 | nll_loss 0.375 | ppl 1.3 | wps 3820.2 | ups 0.69 | wpb 5523.2 | bsz 358 | num_updates 15714 | lr 0.00050453 | gnorm 0.302 | train_wall 67 | gb_free 10.1 | wall 16849
2022-08-17 02:52:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:52:33 | INFO | fairseq.trainer | begin training epoch 195
2022-08-17 02:52:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:53:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:53:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:53:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:53:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:53:51 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 5.038 | nll_loss 2.369 | ppl 5.16 | bleu 56.48 | wps 1250 | wpb 933.5 | bsz 59.6 | num_updates 15795 | best_bleu 57.08
2022-08-17 02:53:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 195 @ 15795 updates
2022-08-17 02:53:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint195.pt
2022-08-17 02:53:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint195.pt
2022-08-17 02:54:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint195.pt (epoch 195 @ 15795 updates, score 56.48) (writing took 15.616151861846447 seconds)
2022-08-17 02:54:07 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)
2022-08-17 02:54:07 | INFO | train | epoch 195 | loss 3.421 | nll_loss 0.377 | ppl 1.3 | wps 4781.5 | ups 0.87 | wpb 5523.2 | bsz 358 | num_updates 15795 | lr 0.000503234 | gnorm 0.317 | train_wall 63 | gb_free 10.1 | wall 16943
2022-08-17 02:54:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:54:07 | INFO | fairseq.trainer | begin training epoch 196
2022-08-17 02:54:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:54:12 | INFO | train_inner | epoch 196:      5 / 81 loss=3.421, nll_loss=0.377, ppl=1.3, wps=3455, ups=0.63, wpb=5511, bsz=353.6, num_updates=15800, lr=0.000503155, gnorm=0.319, train_wall=80, gb_free=10.1, wall=16948
2022-08-17 02:55:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:55:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:55:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:55:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:55:24 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 5.048 | nll_loss 2.381 | ppl 5.21 | bleu 56.23 | wps 1217.3 | wpb 933.5 | bsz 59.6 | num_updates 15876 | best_bleu 57.08
2022-08-17 02:55:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 196 @ 15876 updates
2022-08-17 02:55:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint196.pt
2022-08-17 02:55:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint196.pt
2022-08-17 02:55:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint196.pt (epoch 196 @ 15876 updates, score 56.23) (writing took 20.99834695085883 seconds)
2022-08-17 02:55:45 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)
2022-08-17 02:55:45 | INFO | train | epoch 196 | loss 3.42 | nll_loss 0.376 | ppl 1.3 | wps 4551.9 | ups 0.82 | wpb 5523.2 | bsz 358 | num_updates 15876 | lr 0.000501949 | gnorm 0.301 | train_wall 62 | gb_free 10.1 | wall 17041
2022-08-17 02:55:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:55:45 | INFO | fairseq.trainer | begin training epoch 197
2022-08-17 02:55:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:56:08 | INFO | train_inner | epoch 197:     24 / 81 loss=3.42, nll_loss=0.375, ppl=1.3, wps=4767.4, ups=0.86, wpb=5516.6, bsz=356.2, num_updates=15900, lr=0.00050157, gnorm=0.293, train_wall=79, gb_free=10.1, wall=17064
2022-08-17 02:56:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:56:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:56:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:56:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:57:02 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 5.047 | nll_loss 2.381 | ppl 5.21 | bleu 56.06 | wps 1286.6 | wpb 933.5 | bsz 59.6 | num_updates 15957 | best_bleu 57.08
2022-08-17 02:57:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 197 @ 15957 updates
2022-08-17 02:57:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint197.pt
2022-08-17 02:57:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint197.pt
2022-08-17 02:57:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint197.pt (epoch 197 @ 15957 updates, score 56.06) (writing took 13.929472800344229 seconds)
2022-08-17 02:57:16 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)
2022-08-17 02:57:16 | INFO | train | epoch 197 | loss 3.42 | nll_loss 0.376 | ppl 1.3 | wps 4917.3 | ups 0.89 | wpb 5523.2 | bsz 358 | num_updates 15957 | lr 0.000500673 | gnorm 0.298 | train_wall 62 | gb_free 10.1 | wall 17132
2022-08-17 02:57:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:57:16 | INFO | fairseq.trainer | begin training epoch 198
2022-08-17 02:57:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:57:55 | INFO | train_inner | epoch 198:     43 / 81 loss=3.42, nll_loss=0.377, ppl=1.3, wps=5166.4, ups=0.94, wpb=5517.4, bsz=355.8, num_updates=16000, lr=0.0005, gnorm=0.313, train_wall=78, gb_free=10, wall=17171
2022-08-17 02:58:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:58:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:58:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:58:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:58:33 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 5.035 | nll_loss 2.364 | ppl 5.15 | bleu 56.2 | wps 1166.7 | wpb 933.5 | bsz 59.6 | num_updates 16038 | best_bleu 57.08
2022-08-17 02:58:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 198 @ 16038 updates
2022-08-17 02:58:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint198.pt
2022-08-17 02:58:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint198.pt
2022-08-17 02:58:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint198.pt (epoch 198 @ 16038 updates, score 56.2) (writing took 2.3667171634733677 seconds)
2022-08-17 02:58:36 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)
2022-08-17 02:58:36 | INFO | train | epoch 198 | loss 3.42 | nll_loss 0.376 | ppl 1.3 | wps 5614.9 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 16038 | lr 0.000499407 | gnorm 0.315 | train_wall 61 | gb_free 10.2 | wall 17212
2022-08-17 02:58:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 02:58:36 | INFO | fairseq.trainer | begin training epoch 199
2022-08-17 02:58:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 02:59:31 | INFO | train_inner | epoch 199:     62 / 81 loss=3.418, nll_loss=0.374, ppl=1.3, wps=5763.4, ups=1.04, wpb=5552.5, bsz=363.8, num_updates=16100, lr=0.000498445, gnorm=0.294, train_wall=78, gb_free=10.1, wall=17267
2022-08-17 02:59:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 02:59:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 02:59:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 02:59:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 02:59:55 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 5.033 | nll_loss 2.364 | ppl 5.15 | bleu 56.49 | wps 1221.2 | wpb 933.5 | bsz 59.6 | num_updates 16119 | best_bleu 57.08
2022-08-17 02:59:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 199 @ 16119 updates
2022-08-17 02:59:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint199.pt
2022-08-17 02:59:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint199.pt
2022-08-17 03:00:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint199.pt (epoch 199 @ 16119 updates, score 56.49) (writing took 16.625371139496565 seconds)
2022-08-17 03:00:12 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)
2022-08-17 03:00:12 | INFO | train | epoch 199 | loss 3.417 | nll_loss 0.373 | ppl 1.3 | wps 4672.7 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 16119 | lr 0.000498151 | gnorm 0.292 | train_wall 64 | gb_free 10.1 | wall 17308
2022-08-17 03:00:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:00:12 | INFO | fairseq.trainer | begin training epoch 200
2022-08-17 03:00:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:01:14 | INFO | train_inner | epoch 200:     81 / 81 loss=3.418, nll_loss=0.374, ppl=1.3, wps=5339.2, ups=0.97, wpb=5493.5, bsz=355.1, num_updates=16200, lr=0.000496904, gnorm=0.285, train_wall=71, gb_free=10.2, wall=17370
2022-08-17 03:01:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:01:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:01:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:01:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:01:28 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 5.044 | nll_loss 2.372 | ppl 5.18 | bleu 55.88 | wps 1175.3 | wpb 933.5 | bsz 59.6 | num_updates 16200 | best_bleu 57.08
2022-08-17 03:01:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 200 @ 16200 updates
2022-08-17 03:01:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint200.pt
2022-08-17 03:01:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint200.pt
2022-08-17 03:02:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint200.pt (epoch 200 @ 16200 updates, score 55.88) (writing took 35.59548508003354 seconds)
2022-08-17 03:02:04 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)
2022-08-17 03:02:04 | INFO | train | epoch 200 | loss 3.417 | nll_loss 0.374 | ppl 1.3 | wps 3975.6 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 16200 | lr 0.000496904 | gnorm 0.278 | train_wall 61 | gb_free 10.2 | wall 17420
2022-08-17 03:02:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:02:04 | INFO | fairseq.trainer | begin training epoch 201
2022-08-17 03:02:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:03:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:03:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:03:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:03:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:03:22 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 5.035 | nll_loss 2.365 | ppl 5.15 | bleu 55.49 | wps 1194.9 | wpb 933.5 | bsz 59.6 | num_updates 16281 | best_bleu 57.08
2022-08-17 03:03:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 201 @ 16281 updates
2022-08-17 03:03:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint201.pt
2022-08-17 03:03:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint201.pt
2022-08-17 03:03:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint201.pt (epoch 201 @ 16281 updates, score 55.49) (writing took 2.2795658595860004 seconds)
2022-08-17 03:03:24 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)
2022-08-17 03:03:24 | INFO | train | epoch 201 | loss 3.417 | nll_loss 0.374 | ppl 1.3 | wps 5596.7 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 16281 | lr 0.000495666 | gnorm 0.303 | train_wall 62 | gb_free 10.2 | wall 17500
2022-08-17 03:03:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:03:24 | INFO | fairseq.trainer | begin training epoch 202
2022-08-17 03:03:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:03:42 | INFO | train_inner | epoch 202:     19 / 81 loss=3.417, nll_loss=0.373, ppl=1.3, wps=3724.5, ups=0.67, wpb=5527.1, bsz=360.2, num_updates=16300, lr=0.000495377, gnorm=0.31, train_wall=79, gb_free=10.1, wall=17519
2022-08-17 03:04:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:04:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:04:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:04:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:04:42 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 5.035 | nll_loss 2.356 | ppl 5.12 | bleu 56.56 | wps 1198.7 | wpb 933.5 | bsz 59.6 | num_updates 16362 | best_bleu 57.08
2022-08-17 03:04:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 202 @ 16362 updates
2022-08-17 03:04:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint202.pt
2022-08-17 03:04:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint202.pt
2022-08-17 03:05:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint202.pt (epoch 202 @ 16362 updates, score 56.56) (writing took 31.073855351656675 seconds)
2022-08-17 03:05:13 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)
2022-08-17 03:05:13 | INFO | train | epoch 202 | loss 3.42 | nll_loss 0.377 | ppl 1.3 | wps 4110.5 | ups 0.74 | wpb 5523.2 | bsz 358 | num_updates 16362 | lr 0.000494438 | gnorm 0.349 | train_wall 62 | gb_free 10.1 | wall 17609
2022-08-17 03:05:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:05:13 | INFO | fairseq.trainer | begin training epoch 203
2022-08-17 03:05:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:05:40 | INFO | train_inner | epoch 203:     38 / 81 loss=3.419, nll_loss=0.376, ppl=1.3, wps=4713.4, ups=0.85, wpb=5528.3, bsz=357.8, num_updates=16400, lr=0.000493865, gnorm=0.333, train_wall=71, gb_free=10.2, wall=17636
2022-08-17 03:06:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:06:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:06:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:06:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:06:30 | INFO | valid | epoch 203 | valid on 'valid' subset | loss 5.057 | nll_loss 2.391 | ppl 5.25 | bleu 56.3 | wps 1241.3 | wpb 933.5 | bsz 59.6 | num_updates 16443 | best_bleu 57.08
2022-08-17 03:06:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 203 @ 16443 updates
2022-08-17 03:06:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint203.pt
2022-08-17 03:06:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint203.pt
2022-08-17 03:06:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint203.pt (epoch 203 @ 16443 updates, score 56.3) (writing took 21.037416595965624 seconds)
2022-08-17 03:06:51 | INFO | fairseq_cli.train | end of epoch 203 (average epoch stats below)
2022-08-17 03:06:51 | INFO | train | epoch 203 | loss 3.417 | nll_loss 0.373 | ppl 1.3 | wps 4566.1 | ups 0.83 | wpb 5523.2 | bsz 358 | num_updates 16443 | lr 0.000493219 | gnorm 0.306 | train_wall 62 | gb_free 10.1 | wall 17707
2022-08-17 03:06:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:06:51 | INFO | fairseq.trainer | begin training epoch 204
2022-08-17 03:06:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:07:41 | INFO | train_inner | epoch 204:     57 / 81 loss=3.417, nll_loss=0.374, ppl=1.3, wps=4585.1, ups=0.83, wpb=5544.5, bsz=358.5, num_updates=16500, lr=0.000492366, gnorm=0.301, train_wall=84, gb_free=10.1, wall=17757
2022-08-17 03:08:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:08:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:08:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:08:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:08:15 | INFO | valid | epoch 204 | valid on 'valid' subset | loss 5.049 | nll_loss 2.388 | ppl 5.23 | bleu 56.89 | wps 1275.9 | wpb 933.5 | bsz 59.6 | num_updates 16524 | best_bleu 57.08
2022-08-17 03:08:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 204 @ 16524 updates
2022-08-17 03:08:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint204.pt
2022-08-17 03:08:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint204.pt
2022-08-17 03:08:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint204.pt (epoch 204 @ 16524 updates, score 56.89) (writing took 2.35824054479599 seconds)
2022-08-17 03:08:17 | INFO | fairseq_cli.train | end of epoch 204 (average epoch stats below)
2022-08-17 03:08:18 | INFO | train | epoch 204 | loss 3.418 | nll_loss 0.374 | ppl 1.3 | wps 5163.2 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 16524 | lr 0.000492008 | gnorm 0.313 | train_wall 69 | gb_free 10.2 | wall 17794
2022-08-17 03:08:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:08:18 | INFO | fairseq.trainer | begin training epoch 205
2022-08-17 03:08:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:09:22 | INFO | train_inner | epoch 205:     76 / 81 loss=3.418, nll_loss=0.375, ppl=1.3, wps=5464.7, ups=0.99, wpb=5515.3, bsz=356.1, num_updates=16600, lr=0.000490881, gnorm=0.313, train_wall=84, gb_free=10.1, wall=17858
2022-08-17 03:09:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:09:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:09:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:09:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:09:40 | INFO | valid | epoch 205 | valid on 'valid' subset | loss 5.043 | nll_loss 2.371 | ppl 5.17 | bleu 56.02 | wps 1144.4 | wpb 933.5 | bsz 59.6 | num_updates 16605 | best_bleu 57.08
2022-08-17 03:09:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 205 @ 16605 updates
2022-08-17 03:09:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint205.pt
2022-08-17 03:09:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint205.pt
2022-08-17 03:10:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint205.pt (epoch 205 @ 16605 updates, score 56.02) (writing took 36.15340520814061 seconds)
2022-08-17 03:10:16 | INFO | fairseq_cli.train | end of epoch 205 (average epoch stats below)
2022-08-17 03:10:16 | INFO | train | epoch 205 | loss 3.417 | nll_loss 0.374 | ppl 1.3 | wps 3772 | ups 0.68 | wpb 5523.2 | bsz 358 | num_updates 16605 | lr 0.000490807 | gnorm 0.301 | train_wall 66 | gb_free 10.2 | wall 17912
2022-08-17 03:10:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:10:16 | INFO | fairseq.trainer | begin training epoch 206
2022-08-17 03:10:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:11:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:11:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:11:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:11:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:11:32 | INFO | valid | epoch 206 | valid on 'valid' subset | loss 5.054 | nll_loss 2.385 | ppl 5.22 | bleu 56.13 | wps 1240.9 | wpb 933.5 | bsz 59.6 | num_updates 16686 | best_bleu 57.08
2022-08-17 03:11:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 206 @ 16686 updates
2022-08-17 03:11:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint206.pt
2022-08-17 03:11:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint206.pt
2022-08-17 03:11:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint206.pt (epoch 206 @ 16686 updates, score 56.13) (writing took 19.100436627864838 seconds)
2022-08-17 03:11:52 | INFO | fairseq_cli.train | end of epoch 206 (average epoch stats below)
2022-08-17 03:11:52 | INFO | train | epoch 206 | loss 3.418 | nll_loss 0.375 | ppl 1.3 | wps 4678.6 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 16686 | lr 0.000489614 | gnorm 0.571 | train_wall 62 | gb_free 10.1 | wall 18008
2022-08-17 03:11:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:11:52 | INFO | fairseq.trainer | begin training epoch 207
2022-08-17 03:11:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:12:05 | INFO | train_inner | epoch 207:     14 / 81 loss=3.418, nll_loss=0.375, ppl=1.3, wps=3352.3, ups=0.61, wpb=5482.5, bsz=357, num_updates=16700, lr=0.000489409, gnorm=0.521, train_wall=77, gb_free=10.1, wall=18021
2022-08-17 03:12:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:12:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:12:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:12:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:13:08 | INFO | valid | epoch 207 | valid on 'valid' subset | loss 5.04 | nll_loss 2.372 | ppl 5.18 | bleu 55.89 | wps 1247.3 | wpb 933.5 | bsz 59.6 | num_updates 16767 | best_bleu 57.08
2022-08-17 03:13:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 207 @ 16767 updates
2022-08-17 03:13:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint207.pt
2022-08-17 03:13:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint207.pt
2022-08-17 03:13:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint207.pt (epoch 207 @ 16767 updates, score 55.89) (writing took 25.738168757408857 seconds)
2022-08-17 03:13:34 | INFO | fairseq_cli.train | end of epoch 207 (average epoch stats below)
2022-08-17 03:13:34 | INFO | train | epoch 207 | loss 3.418 | nll_loss 0.375 | ppl 1.3 | wps 4383.2 | ups 0.79 | wpb 5523.2 | bsz 358 | num_updates 16767 | lr 0.00048843 | gnorm 0.351 | train_wall 61 | gb_free 10.3 | wall 18110
2022-08-17 03:13:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:13:34 | INFO | fairseq.trainer | begin training epoch 208
2022-08-17 03:13:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:14:02 | INFO | train_inner | epoch 208:     33 / 81 loss=3.419, nll_loss=0.376, ppl=1.3, wps=4727.9, ups=0.85, wpb=5542.2, bsz=356.5, num_updates=16800, lr=0.00048795, gnorm=0.401, train_wall=76, gb_free=10.1, wall=18139
2022-08-17 03:14:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:14:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:14:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:14:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:14:50 | INFO | valid | epoch 208 | valid on 'valid' subset | loss 5.034 | nll_loss 2.359 | ppl 5.13 | bleu 56.05 | wps 1221.8 | wpb 933.5 | bsz 59.6 | num_updates 16848 | best_bleu 57.08
2022-08-17 03:14:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 208 @ 16848 updates
2022-08-17 03:14:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint208.pt
2022-08-17 03:14:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint208.pt
2022-08-17 03:15:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint208.pt (epoch 208 @ 16848 updates, score 56.05) (writing took 21.22094664722681 seconds)
2022-08-17 03:15:12 | INFO | fairseq_cli.train | end of epoch 208 (average epoch stats below)
2022-08-17 03:15:12 | INFO | train | epoch 208 | loss 3.418 | nll_loss 0.375 | ppl 1.3 | wps 4574.2 | ups 0.83 | wpb 5523.2 | bsz 358 | num_updates 16848 | lr 0.000487254 | gnorm 0.395 | train_wall 61 | gb_free 10.1 | wall 18208
2022-08-17 03:15:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:15:12 | INFO | fairseq.trainer | begin training epoch 209
2022-08-17 03:15:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:15:49 | INFO | train_inner | epoch 209:     52 / 81 loss=3.415, nll_loss=0.372, ppl=1.29, wps=5209.5, ups=0.94, wpb=5562.1, bsz=364.2, num_updates=16900, lr=0.000486504, gnorm=0.329, train_wall=70, gb_free=10.1, wall=18245
2022-08-17 03:16:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:16:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:16:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:16:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:16:27 | INFO | valid | epoch 209 | valid on 'valid' subset | loss 5.051 | nll_loss 2.379 | ppl 5.2 | bleu 56.04 | wps 1208 | wpb 933.5 | bsz 59.6 | num_updates 16929 | best_bleu 57.08
2022-08-17 03:16:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 209 @ 16929 updates
2022-08-17 03:16:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint209.pt
2022-08-17 03:16:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint209.pt
2022-08-17 03:16:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint209.pt (epoch 209 @ 16929 updates, score 56.04) (writing took 12.840144339948893 seconds)
2022-08-17 03:16:40 | INFO | fairseq_cli.train | end of epoch 209 (average epoch stats below)
2022-08-17 03:16:40 | INFO | train | epoch 209 | loss 3.415 | nll_loss 0.372 | ppl 1.29 | wps 5044.9 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 16929 | lr 0.000486087 | gnorm 0.32 | train_wall 61 | gb_free 10 | wall 18297
2022-08-17 03:16:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:16:41 | INFO | fairseq.trainer | begin training epoch 210
2022-08-17 03:16:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:17:36 | INFO | train_inner | epoch 210:     71 / 81 loss=3.416, nll_loss=0.372, ppl=1.29, wps=5148.1, ups=0.94, wpb=5492.5, bsz=356.6, num_updates=17000, lr=0.000485071, gnorm=0.304, train_wall=78, gb_free=10.1, wall=18352
2022-08-17 03:17:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:17:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:17:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:17:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:17:58 | INFO | valid | epoch 210 | valid on 'valid' subset | loss 5.033 | nll_loss 2.37 | ppl 5.17 | bleu 56.39 | wps 1256.4 | wpb 933.5 | bsz 59.6 | num_updates 17010 | best_bleu 57.08
2022-08-17 03:17:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 210 @ 17010 updates
2022-08-17 03:17:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint210.pt
2022-08-17 03:18:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint210.pt
2022-08-17 03:18:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint210.pt (epoch 210 @ 17010 updates, score 56.39) (writing took 26.20671347156167 seconds)
2022-08-17 03:18:24 | INFO | fairseq_cli.train | end of epoch 210 (average epoch stats below)
2022-08-17 03:18:24 | INFO | train | epoch 210 | loss 3.415 | nll_loss 0.372 | ppl 1.29 | wps 4295.8 | ups 0.78 | wpb 5523.2 | bsz 358 | num_updates 17010 | lr 0.000484929 | gnorm 0.302 | train_wall 63 | gb_free 10.1 | wall 18401
2022-08-17 03:18:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:18:25 | INFO | fairseq.trainer | begin training epoch 211
2022-08-17 03:18:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:19:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:19:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:19:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:19:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:19:42 | INFO | valid | epoch 211 | valid on 'valid' subset | loss 5.029 | nll_loss 2.36 | ppl 5.13 | bleu 56.08 | wps 1253.5 | wpb 933.5 | bsz 59.6 | num_updates 17091 | best_bleu 57.08
2022-08-17 03:19:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 211 @ 17091 updates
2022-08-17 03:19:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint211.pt
2022-08-17 03:19:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint211.pt
2022-08-17 03:19:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint211.pt (epoch 211 @ 17091 updates, score 56.08) (writing took 16.53679459914565 seconds)
2022-08-17 03:19:58 | INFO | fairseq_cli.train | end of epoch 211 (average epoch stats below)
2022-08-17 03:19:58 | INFO | train | epoch 211 | loss 3.416 | nll_loss 0.373 | ppl 1.3 | wps 4757.9 | ups 0.86 | wpb 5523.2 | bsz 358 | num_updates 17091 | lr 0.000483778 | gnorm 0.356 | train_wall 62 | gb_free 10.2 | wall 18495
2022-08-17 03:19:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:19:59 | INFO | fairseq.trainer | begin training epoch 212
2022-08-17 03:19:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:20:04 | INFO | train_inner | epoch 212:      9 / 81 loss=3.416, nll_loss=0.374, ppl=1.3, wps=3715.2, ups=0.67, wpb=5515.6, bsz=355.1, num_updates=17100, lr=0.000483651, gnorm=0.345, train_wall=75, gb_free=10, wall=18500
2022-08-17 03:21:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:21:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:21:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:21:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:21:19 | INFO | valid | epoch 212 | valid on 'valid' subset | loss 5.026 | nll_loss 2.355 | ppl 5.11 | bleu 56.42 | wps 1165.1 | wpb 933.5 | bsz 59.6 | num_updates 17172 | best_bleu 57.08
2022-08-17 03:21:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 212 @ 17172 updates
2022-08-17 03:21:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint212.pt
2022-08-17 03:21:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint212.pt
2022-08-17 03:21:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint212.pt (epoch 212 @ 17172 updates, score 56.42) (writing took 29.17435074970126 seconds)
2022-08-17 03:21:49 | INFO | fairseq_cli.train | end of epoch 212 (average epoch stats below)
2022-08-17 03:21:49 | INFO | train | epoch 212 | loss 3.416 | nll_loss 0.374 | ppl 1.3 | wps 4061.5 | ups 0.74 | wpb 5523.2 | bsz 358 | num_updates 17172 | lr 0.000482636 | gnorm 0.317 | train_wall 65 | gb_free 10.1 | wall 18605
2022-08-17 03:21:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:21:49 | INFO | fairseq.trainer | begin training epoch 213
2022-08-17 03:21:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:22:13 | INFO | train_inner | epoch 213:     28 / 81 loss=3.416, nll_loss=0.374, ppl=1.3, wps=4301.4, ups=0.78, wpb=5531.6, bsz=357.4, num_updates=17200, lr=0.000482243, gnorm=0.333, train_wall=84, gb_free=10.2, wall=18629
2022-08-17 03:22:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:22:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:22:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:22:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:23:06 | INFO | valid | epoch 213 | valid on 'valid' subset | loss 5.032 | nll_loss 2.359 | ppl 5.13 | bleu 56.73 | wps 1631.8 | wpb 933.5 | bsz 59.6 | num_updates 17253 | best_bleu 57.08
2022-08-17 03:23:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 213 @ 17253 updates
2022-08-17 03:23:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint213.pt
2022-08-17 03:23:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint213.pt
2022-08-17 03:23:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint213.pt (epoch 213 @ 17253 updates, score 56.73) (writing took 12.370917081832886 seconds)
2022-08-17 03:23:18 | INFO | fairseq_cli.train | end of epoch 213 (average epoch stats below)
2022-08-17 03:23:18 | INFO | train | epoch 213 | loss 3.416 | nll_loss 0.373 | ppl 1.3 | wps 4985 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 17253 | lr 0.000481502 | gnorm 0.325 | train_wall 65 | gb_free 10.1 | wall 18695
2022-08-17 03:23:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:23:19 | INFO | fairseq.trainer | begin training epoch 214
2022-08-17 03:23:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:24:01 | INFO | train_inner | epoch 214:     47 / 81 loss=3.413, nll_loss=0.371, ppl=1.29, wps=5101.6, ups=0.92, wpb=5518.3, bsz=362.9, num_updates=17300, lr=0.000480847, gnorm=0.288, train_wall=84, gb_free=10.1, wall=18737
2022-08-17 03:24:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:24:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:24:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:24:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:24:37 | INFO | valid | epoch 214 | valid on 'valid' subset | loss 5.019 | nll_loss 2.345 | ppl 5.08 | bleu 56.25 | wps 1499.1 | wpb 933.5 | bsz 59.6 | num_updates 17334 | best_bleu 57.08
2022-08-17 03:24:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 214 @ 17334 updates
2022-08-17 03:24:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint214.pt
2022-08-17 03:24:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint214.pt
2022-08-17 03:24:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint214.pt (epoch 214 @ 17334 updates, score 56.25) (writing took 16.2892629429698 seconds)
2022-08-17 03:24:54 | INFO | fairseq_cli.train | end of epoch 214 (average epoch stats below)
2022-08-17 03:24:54 | INFO | train | epoch 214 | loss 3.414 | nll_loss 0.372 | ppl 1.29 | wps 4686.9 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 17334 | lr 0.000480375 | gnorm 0.291 | train_wall 66 | gb_free 10.1 | wall 18790
2022-08-17 03:24:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:24:54 | INFO | fairseq.trainer | begin training epoch 215
2022-08-17 03:24:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:25:52 | INFO | train_inner | epoch 215:     66 / 81 loss=3.416, nll_loss=0.374, ppl=1.3, wps=5012.2, ups=0.9, wpb=5540.5, bsz=356.7, num_updates=17400, lr=0.000479463, gnorm=0.34, train_wall=81, gb_free=10.1, wall=18848
2022-08-17 03:25:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:26:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:26:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:26:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:26:12 | INFO | valid | epoch 215 | valid on 'valid' subset | loss 5.032 | nll_loss 2.37 | ppl 5.17 | bleu 55.73 | wps 1276 | wpb 933.5 | bsz 59.6 | num_updates 17415 | best_bleu 57.08
2022-08-17 03:26:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 215 @ 17415 updates
2022-08-17 03:26:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint215.pt
2022-08-17 03:26:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint215.pt
2022-08-17 03:26:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint215.pt (epoch 215 @ 17415 updates, score 55.73) (writing took 17.134334564208984 seconds)
2022-08-17 03:26:29 | INFO | fairseq_cli.train | end of epoch 215 (average epoch stats below)
2022-08-17 03:26:29 | INFO | train | epoch 215 | loss 3.415 | nll_loss 0.373 | ppl 1.29 | wps 4691.5 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 17415 | lr 0.000479257 | gnorm 0.347 | train_wall 64 | gb_free 10.1 | wall 18885
2022-08-17 03:26:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:26:29 | INFO | fairseq.trainer | begin training epoch 216
2022-08-17 03:26:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:27:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:27:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:27:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:27:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:27:46 | INFO | valid | epoch 216 | valid on 'valid' subset | loss 5.028 | nll_loss 2.359 | ppl 5.13 | bleu 56.1 | wps 1199.1 | wpb 933.5 | bsz 59.6 | num_updates 17496 | best_bleu 57.08
2022-08-17 03:27:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 216 @ 17496 updates
2022-08-17 03:27:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint216.pt
2022-08-17 03:27:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint216.pt
2022-08-17 03:27:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint216.pt (epoch 216 @ 17496 updates, score 56.1) (writing took 2.3956654109060764 seconds)
2022-08-17 03:27:49 | INFO | fairseq_cli.train | end of epoch 216 (average epoch stats below)
2022-08-17 03:27:49 | INFO | train | epoch 216 | loss 3.414 | nll_loss 0.372 | ppl 1.29 | wps 5617.2 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 17496 | lr 0.000478146 | gnorm 0.319 | train_wall 62 | gb_free 10.1 | wall 18965
2022-08-17 03:27:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:27:49 | INFO | fairseq.trainer | begin training epoch 217
2022-08-17 03:27:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:27:53 | INFO | train_inner | epoch 217:      4 / 81 loss=3.415, nll_loss=0.372, ppl=1.29, wps=4522.1, ups=0.82, wpb=5511, bsz=355.7, num_updates=17500, lr=0.000478091, gnorm=0.336, train_wall=73, gb_free=10.1, wall=18970
2022-08-17 03:28:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:28:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:28:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:28:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:29:08 | INFO | valid | epoch 217 | valid on 'valid' subset | loss 5.021 | nll_loss 2.352 | ppl 5.1 | bleu 56.8 | wps 1458.6 | wpb 933.5 | bsz 59.6 | num_updates 17577 | best_bleu 57.08
2022-08-17 03:29:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 217 @ 17577 updates
2022-08-17 03:29:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint217.pt
2022-08-17 03:29:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint217.pt
2022-08-17 03:29:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint217.pt (epoch 217 @ 17577 updates, score 56.8) (writing took 27.400423113256693 seconds)
2022-08-17 03:29:35 | INFO | fairseq_cli.train | end of epoch 217 (average epoch stats below)
2022-08-17 03:29:35 | INFO | train | epoch 217 | loss 3.416 | nll_loss 0.374 | ppl 1.3 | wps 4200.9 | ups 0.76 | wpb 5523.2 | bsz 358 | num_updates 17577 | lr 0.000477043 | gnorm 0.492 | train_wall 66 | gb_free 10.1 | wall 19072
2022-08-17 03:29:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:29:36 | INFO | fairseq.trainer | begin training epoch 218
2022-08-17 03:29:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:29:56 | INFO | train_inner | epoch 218:     23 / 81 loss=3.416, nll_loss=0.373, ppl=1.3, wps=4500.8, ups=0.81, wpb=5522.6, bsz=356.6, num_updates=17600, lr=0.000476731, gnorm=0.445, train_wall=82, gb_free=10.1, wall=19092
2022-08-17 03:30:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:30:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:30:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:30:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:30:55 | INFO | valid | epoch 218 | valid on 'valid' subset | loss 5.036 | nll_loss 2.365 | ppl 5.15 | bleu 56.09 | wps 1215.7 | wpb 933.5 | bsz 59.6 | num_updates 17658 | best_bleu 57.08
2022-08-17 03:30:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 218 @ 17658 updates
2022-08-17 03:30:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint218.pt
2022-08-17 03:30:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint218.pt
2022-08-17 03:31:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint218.pt (epoch 218 @ 17658 updates, score 56.09) (writing took 19.193313915282488 seconds)
2022-08-17 03:31:14 | INFO | fairseq_cli.train | end of epoch 218 (average epoch stats below)
2022-08-17 03:31:14 | INFO | train | epoch 218 | loss 3.414 | nll_loss 0.372 | ppl 1.29 | wps 4540 | ups 0.82 | wpb 5523.2 | bsz 358 | num_updates 17658 | lr 0.000475948 | gnorm 0.34 | train_wall 64 | gb_free 10.1 | wall 19170
2022-08-17 03:31:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:31:14 | INFO | fairseq.trainer | begin training epoch 219
2022-08-17 03:31:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:31:49 | INFO | train_inner | epoch 219:     42 / 81 loss=3.412, nll_loss=0.37, ppl=1.29, wps=4942.8, ups=0.89, wpb=5558.8, bsz=362.6, num_updates=17700, lr=0.000475383, gnorm=0.332, train_wall=78, gb_free=10.1, wall=19205
2022-08-17 03:32:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:32:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:32:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:32:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:32:31 | INFO | valid | epoch 219 | valid on 'valid' subset | loss 5.023 | nll_loss 2.351 | ppl 5.1 | bleu 55.71 | wps 1244.5 | wpb 933.5 | bsz 59.6 | num_updates 17739 | best_bleu 57.08
2022-08-17 03:32:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 219 @ 17739 updates
2022-08-17 03:32:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint219.pt
2022-08-17 03:32:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint219.pt
2022-08-17 03:33:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint219.pt (epoch 219 @ 17739 updates, score 55.71) (writing took 35.70914659276605 seconds)
2022-08-17 03:33:07 | INFO | fairseq_cli.train | end of epoch 219 (average epoch stats below)
2022-08-17 03:33:07 | INFO | train | epoch 219 | loss 3.413 | nll_loss 0.37 | ppl 1.29 | wps 3964.8 | ups 0.72 | wpb 5523.2 | bsz 358 | num_updates 17739 | lr 0.00047486 | gnorm 0.309 | train_wall 62 | gb_free 10.1 | wall 19283
2022-08-17 03:33:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:33:07 | INFO | fairseq.trainer | begin training epoch 220
2022-08-17 03:33:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:33:54 | INFO | train_inner | epoch 220:     61 / 81 loss=3.412, nll_loss=0.369, ppl=1.29, wps=4387.5, ups=0.8, wpb=5514.9, bsz=359.1, num_updates=17800, lr=0.000474045, gnorm=0.297, train_wall=74, gb_free=10.1, wall=19331
2022-08-17 03:34:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:34:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:34:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:34:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:34:24 | INFO | valid | epoch 220 | valid on 'valid' subset | loss 5.032 | nll_loss 2.36 | ppl 5.14 | bleu 56.66 | wps 1230 | wpb 933.5 | bsz 59.6 | num_updates 17820 | best_bleu 57.08
2022-08-17 03:34:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 220 @ 17820 updates
2022-08-17 03:34:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint220.pt
2022-08-17 03:34:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint220.pt
2022-08-17 03:34:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint220.pt (epoch 220 @ 17820 updates, score 56.66) (writing took 14.934938982129097 seconds)
2022-08-17 03:34:39 | INFO | fairseq_cli.train | end of epoch 220 (average epoch stats below)
2022-08-17 03:34:39 | INFO | train | epoch 220 | loss 3.412 | nll_loss 0.369 | ppl 1.29 | wps 4824.9 | ups 0.87 | wpb 5523.2 | bsz 358 | num_updates 17820 | lr 0.000473779 | gnorm 0.297 | train_wall 62 | gb_free 10.3 | wall 19376
2022-08-17 03:34:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:34:40 | INFO | fairseq.trainer | begin training epoch 221
2022-08-17 03:34:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:35:46 | INFO | train_inner | epoch 221:     80 / 81 loss=3.414, nll_loss=0.372, ppl=1.29, wps=4922.8, ups=0.89, wpb=5513.2, bsz=354.9, num_updates=17900, lr=0.000472719, gnorm=0.324, train_wall=81, gb_free=10.1, wall=19443
2022-08-17 03:35:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:35:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:35:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:35:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:36:01 | INFO | valid | epoch 221 | valid on 'valid' subset | loss 5.032 | nll_loss 2.367 | ppl 5.16 | bleu 56.29 | wps 1196.5 | wpb 933.5 | bsz 59.6 | num_updates 17901 | best_bleu 57.08
2022-08-17 03:36:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 221 @ 17901 updates
2022-08-17 03:36:01 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint221.pt
2022-08-17 03:36:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint221.pt
2022-08-17 03:36:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint221.pt (epoch 221 @ 17901 updates, score 56.29) (writing took 19.659429527819157 seconds)
2022-08-17 03:36:21 | INFO | fairseq_cli.train | end of epoch 221 (average epoch stats below)
2022-08-17 03:36:21 | INFO | train | epoch 221 | loss 3.413 | nll_loss 0.371 | ppl 1.29 | wps 4424.3 | ups 0.8 | wpb 5523.2 | bsz 358 | num_updates 17901 | lr 0.000472706 | gnorm 0.326 | train_wall 65 | gb_free 10.1 | wall 19477
2022-08-17 03:36:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:36:21 | INFO | fairseq.trainer | begin training epoch 222
2022-08-17 03:36:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:37:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:37:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:37:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:37:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:37:44 | INFO | valid | epoch 222 | valid on 'valid' subset | loss 5.055 | nll_loss 2.391 | ppl 5.24 | bleu 55.97 | wps 1494.2 | wpb 933.5 | bsz 59.6 | num_updates 17982 | best_bleu 57.08
2022-08-17 03:37:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 222 @ 17982 updates
2022-08-17 03:37:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint222.pt
2022-08-17 03:37:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint222.pt
2022-08-17 03:38:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint222.pt (epoch 222 @ 17982 updates, score 55.97) (writing took 37.27395489066839 seconds)
2022-08-17 03:38:22 | INFO | fairseq_cli.train | end of epoch 222 (average epoch stats below)
2022-08-17 03:38:22 | INFO | train | epoch 222 | loss 3.413 | nll_loss 0.37 | ppl 1.29 | wps 3699.3 | ups 0.67 | wpb 5523.2 | bsz 358 | num_updates 17982 | lr 0.00047164 | gnorm 0.38 | train_wall 70 | gb_free 10.2 | wall 19598
2022-08-17 03:38:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:38:22 | INFO | fairseq.trainer | begin training epoch 223
2022-08-17 03:38:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:38:38 | INFO | train_inner | epoch 223:     18 / 81 loss=3.413, nll_loss=0.37, ppl=1.29, wps=3215.7, ups=0.58, wpb=5505.3, bsz=357.7, num_updates=18000, lr=0.000471405, gnorm=0.359, train_wall=86, gb_free=10.1, wall=19614
2022-08-17 03:39:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:39:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:39:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:39:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:39:40 | INFO | valid | epoch 223 | valid on 'valid' subset | loss 5.032 | nll_loss 2.363 | ppl 5.15 | bleu 56.17 | wps 1190.5 | wpb 933.5 | bsz 59.6 | num_updates 18063 | best_bleu 57.08
2022-08-17 03:39:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 223 @ 18063 updates
2022-08-17 03:39:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint223.pt
2022-08-17 03:39:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint223.pt
2022-08-17 03:39:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint223.pt (epoch 223 @ 18063 updates, score 56.17) (writing took 17.94860463961959 seconds)
2022-08-17 03:39:58 | INFO | fairseq_cli.train | end of epoch 223 (average epoch stats below)
2022-08-17 03:39:58 | INFO | train | epoch 223 | loss 3.411 | nll_loss 0.369 | ppl 1.29 | wps 4653.6 | ups 0.84 | wpb 5523.2 | bsz 358 | num_updates 18063 | lr 0.000470582 | gnorm 0.28 | train_wall 62 | gb_free 10 | wall 19694
2022-08-17 03:39:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:39:58 | INFO | fairseq.trainer | begin training epoch 224
2022-08-17 03:39:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:40:28 | INFO | train_inner | epoch 224:     37 / 81 loss=3.413, nll_loss=0.37, ppl=1.29, wps=4987.1, ups=0.91, wpb=5499.2, bsz=351.8, num_updates=18100, lr=0.0004701, gnorm=0.437, train_wall=76, gb_free=10.1, wall=19724
2022-08-17 03:41:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:41:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:41:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:41:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:41:16 | INFO | valid | epoch 224 | valid on 'valid' subset | loss 5.031 | nll_loss 2.37 | ppl 5.17 | bleu 56.17 | wps 1179.3 | wpb 933.5 | bsz 59.6 | num_updates 18144 | best_bleu 57.08
2022-08-17 03:41:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 224 @ 18144 updates
2022-08-17 03:41:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint224.pt
2022-08-17 03:41:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint224.pt
2022-08-17 03:41:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint224.pt (epoch 224 @ 18144 updates, score 56.17) (writing took 24.947835993021727 seconds)
2022-08-17 03:41:41 | INFO | fairseq_cli.train | end of epoch 224 (average epoch stats below)
2022-08-17 03:41:41 | INFO | train | epoch 224 | loss 3.414 | nll_loss 0.372 | ppl 1.29 | wps 4342.3 | ups 0.79 | wpb 5523.2 | bsz 358 | num_updates 18144 | lr 0.00046953 | gnorm 0.505 | train_wall 62 | gb_free 10.1 | wall 19797
2022-08-17 03:41:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:41:41 | INFO | fairseq.trainer | begin training epoch 225
2022-08-17 03:41:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:42:23 | INFO | train_inner | epoch 225:     56 / 81 loss=3.412, nll_loss=0.37, ppl=1.29, wps=4818.4, ups=0.87, wpb=5559.7, bsz=366.6, num_updates=18200, lr=0.000468807, gnorm=0.319, train_wall=74, gb_free=10, wall=19839
2022-08-17 03:42:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:42:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:42:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:42:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:42:58 | INFO | valid | epoch 225 | valid on 'valid' subset | loss 5.046 | nll_loss 2.383 | ppl 5.22 | bleu 56.18 | wps 1210 | wpb 933.5 | bsz 59.6 | num_updates 18225 | best_bleu 57.08
2022-08-17 03:42:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 225 @ 18225 updates
2022-08-17 03:42:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint225.pt
2022-08-17 03:42:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint225.pt
2022-08-17 03:43:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint225.pt (epoch 225 @ 18225 updates, score 56.18) (writing took 19.807876128703356 seconds)
2022-08-17 03:43:18 | INFO | fairseq_cli.train | end of epoch 225 (average epoch stats below)
2022-08-17 03:43:18 | INFO | train | epoch 225 | loss 3.412 | nll_loss 0.37 | ppl 1.29 | wps 4613.8 | ups 0.84 | wpb 5523.2 | bsz 358 | num_updates 18225 | lr 0.000468486 | gnorm 0.302 | train_wall 61 | gb_free 10.2 | wall 19894
2022-08-17 03:43:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:43:18 | INFO | fairseq.trainer | begin training epoch 226
2022-08-17 03:43:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:44:19 | INFO | train_inner | epoch 226:     75 / 81 loss=3.413, nll_loss=0.371, ppl=1.29, wps=4776.5, ups=0.86, wpb=5527.6, bsz=357, num_updates=18300, lr=0.000467525, gnorm=0.303, train_wall=79, gb_free=10.1, wall=19955
2022-08-17 03:44:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:44:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:44:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:44:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:44:37 | INFO | valid | epoch 226 | valid on 'valid' subset | loss 5.042 | nll_loss 2.375 | ppl 5.19 | bleu 56.19 | wps 1250.5 | wpb 933.5 | bsz 59.6 | num_updates 18306 | best_bleu 57.08
2022-08-17 03:44:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 226 @ 18306 updates
2022-08-17 03:44:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint226.pt
2022-08-17 03:44:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint226.pt
2022-08-17 03:44:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint226.pt (epoch 226 @ 18306 updates, score 56.19) (writing took 20.658664770424366 seconds)
2022-08-17 03:44:58 | INFO | fairseq_cli.train | end of epoch 226 (average epoch stats below)
2022-08-17 03:44:58 | INFO | train | epoch 226 | loss 3.413 | nll_loss 0.372 | ppl 1.29 | wps 4447.3 | ups 0.81 | wpb 5523.2 | bsz 358 | num_updates 18306 | lr 0.000467448 | gnorm 0.309 | train_wall 64 | gb_free 10.1 | wall 19995
2022-08-17 03:44:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:44:58 | INFO | fairseq.trainer | begin training epoch 227
2022-08-17 03:44:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:46:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:46:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:46:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:46:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:46:20 | INFO | valid | epoch 227 | valid on 'valid' subset | loss 5.044 | nll_loss 2.38 | ppl 5.2 | bleu 56.56 | wps 1484.7 | wpb 933.5 | bsz 59.6 | num_updates 18387 | best_bleu 57.08
2022-08-17 03:46:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 227 @ 18387 updates
2022-08-17 03:46:20 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint227.pt
2022-08-17 03:46:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint227.pt
2022-08-17 03:47:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint227.pt (epoch 227 @ 18387 updates, score 56.56) (writing took 39.808738514781 seconds)
2022-08-17 03:47:00 | INFO | fairseq_cli.train | end of epoch 227 (average epoch stats below)
2022-08-17 03:47:00 | INFO | train | epoch 227 | loss 3.41 | nll_loss 0.368 | ppl 1.29 | wps 3678.2 | ups 0.67 | wpb 5523.2 | bsz 358 | num_updates 18387 | lr 0.000466417 | gnorm 0.286 | train_wall 69 | gb_free 10.1 | wall 20116
2022-08-17 03:47:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:47:00 | INFO | fairseq.trainer | begin training epoch 228
2022-08-17 03:47:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:47:12 | INFO | train_inner | epoch 228:     13 / 81 loss=3.411, nll_loss=0.368, ppl=1.29, wps=3171.8, ups=0.58, wpb=5498.4, bsz=354, num_updates=18400, lr=0.000466252, gnorm=0.296, train_wall=85, gb_free=10, wall=20129
2022-08-17 03:48:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:48:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:48:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:48:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:48:15 | INFO | valid | epoch 228 | valid on 'valid' subset | loss 5.047 | nll_loss 2.375 | ppl 5.19 | bleu 55.77 | wps 1234 | wpb 933.5 | bsz 59.6 | num_updates 18468 | best_bleu 57.08
2022-08-17 03:48:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 228 @ 18468 updates
2022-08-17 03:48:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint228.pt
2022-08-17 03:48:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint228.pt
2022-08-17 03:48:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint228.pt (epoch 228 @ 18468 updates, score 55.77) (writing took 19.814364053308964 seconds)
2022-08-17 03:48:35 | INFO | fairseq_cli.train | end of epoch 228 (average epoch stats below)
2022-08-17 03:48:35 | INFO | train | epoch 228 | loss 3.411 | nll_loss 0.369 | ppl 1.29 | wps 4686.9 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 18468 | lr 0.000465393 | gnorm 0.296 | train_wall 60 | gb_free 10.2 | wall 20212
2022-08-17 03:48:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:48:36 | INFO | fairseq.trainer | begin training epoch 229
2022-08-17 03:48:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:49:04 | INFO | train_inner | epoch 229:     32 / 81 loss=3.411, nll_loss=0.369, ppl=1.29, wps=4995.3, ups=0.9, wpb=5574.5, bsz=359.8, num_updates=18500, lr=0.000464991, gnorm=0.287, train_wall=77, gb_free=10, wall=20240
2022-08-17 03:49:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:49:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:49:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:49:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:49:52 | INFO | valid | epoch 229 | valid on 'valid' subset | loss 5.028 | nll_loss 2.353 | ppl 5.11 | bleu 56.87 | wps 1156.6 | wpb 933.5 | bsz 59.6 | num_updates 18549 | best_bleu 57.08
2022-08-17 03:49:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 229 @ 18549 updates
2022-08-17 03:49:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint229.pt
2022-08-17 03:49:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint229.pt
2022-08-17 03:50:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint229.pt (epoch 229 @ 18549 updates, score 56.87) (writing took 17.76775997504592 seconds)
2022-08-17 03:50:10 | INFO | fairseq_cli.train | end of epoch 229 (average epoch stats below)
2022-08-17 03:50:10 | INFO | train | epoch 229 | loss 3.41 | nll_loss 0.368 | ppl 1.29 | wps 4719.5 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 18549 | lr 0.000464376 | gnorm 0.285 | train_wall 61 | gb_free 10.1 | wall 20306
2022-08-17 03:50:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:50:10 | INFO | fairseq.trainer | begin training epoch 230
2022-08-17 03:50:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:50:47 | INFO | train_inner | epoch 230:     51 / 81 loss=3.41, nll_loss=0.368, ppl=1.29, wps=5296.3, ups=0.97, wpb=5478.4, bsz=360.1, num_updates=18600, lr=0.000463739, gnorm=0.296, train_wall=69, gb_free=10.1, wall=20344
2022-08-17 03:51:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:51:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:51:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:51:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:51:26 | INFO | valid | epoch 230 | valid on 'valid' subset | loss 5.04 | nll_loss 2.376 | ppl 5.19 | bleu 56.89 | wps 1211.7 | wpb 933.5 | bsz 59.6 | num_updates 18630 | best_bleu 57.08
2022-08-17 03:51:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 230 @ 18630 updates
2022-08-17 03:51:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint230.pt
2022-08-17 03:51:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint230.pt
2022-08-17 03:51:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint230.pt (epoch 230 @ 18630 updates, score 56.89) (writing took 20.997062254697084 seconds)
2022-08-17 03:51:47 | INFO | fairseq_cli.train | end of epoch 230 (average epoch stats below)
2022-08-17 03:51:47 | INFO | train | epoch 230 | loss 3.411 | nll_loss 0.369 | ppl 1.29 | wps 4635.6 | ups 0.84 | wpb 5523.2 | bsz 358 | num_updates 18630 | lr 0.000463365 | gnorm 0.304 | train_wall 60 | gb_free 10.2 | wall 20403
2022-08-17 03:51:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:51:47 | INFO | fairseq.trainer | begin training epoch 231
2022-08-17 03:51:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:52:39 | INFO | train_inner | epoch 231:     70 / 81 loss=3.412, nll_loss=0.37, ppl=1.29, wps=4946.7, ups=0.89, wpb=5534.3, bsz=356.2, num_updates=18700, lr=0.000462497, gnorm=0.292, train_wall=75, gb_free=10, wall=20455
2022-08-17 03:52:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:52:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:52:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:52:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:53:03 | INFO | valid | epoch 231 | valid on 'valid' subset | loss 5.028 | nll_loss 2.361 | ppl 5.14 | bleu 56.64 | wps 1227.1 | wpb 933.5 | bsz 59.6 | num_updates 18711 | best_bleu 57.08
2022-08-17 03:53:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 231 @ 18711 updates
2022-08-17 03:53:03 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint231.pt
2022-08-17 03:53:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint231.pt
2022-08-17 03:53:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint231.pt (epoch 231 @ 18711 updates, score 56.64) (writing took 18.861201781779528 seconds)
2022-08-17 03:53:22 | INFO | fairseq_cli.train | end of epoch 231 (average epoch stats below)
2022-08-17 03:53:22 | INFO | train | epoch 231 | loss 3.411 | nll_loss 0.369 | ppl 1.29 | wps 4698.4 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 18711 | lr 0.000462361 | gnorm 0.289 | train_wall 61 | gb_free 10.1 | wall 20498
2022-08-17 03:53:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:53:22 | INFO | fairseq.trainer | begin training epoch 232
2022-08-17 03:53:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:54:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:54:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:54:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:54:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:54:42 | INFO | valid | epoch 232 | valid on 'valid' subset | loss 5.035 | nll_loss 2.372 | ppl 5.18 | bleu 56.42 | wps 1165.1 | wpb 933.5 | bsz 59.6 | num_updates 18792 | best_bleu 57.08
2022-08-17 03:54:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 232 @ 18792 updates
2022-08-17 03:54:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint232.pt
2022-08-17 03:54:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint232.pt
2022-08-17 03:55:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint232.pt (epoch 232 @ 18792 updates, score 56.42) (writing took 27.384491480886936 seconds)
2022-08-17 03:55:09 | INFO | fairseq_cli.train | end of epoch 232 (average epoch stats below)
2022-08-17 03:55:09 | INFO | train | epoch 232 | loss 3.41 | nll_loss 0.368 | ppl 1.29 | wps 4161 | ups 0.75 | wpb 5523.2 | bsz 358 | num_updates 18792 | lr 0.000461364 | gnorm 0.305 | train_wall 64 | gb_free 10.3 | wall 20606
2022-08-17 03:55:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:55:10 | INFO | fairseq.trainer | begin training epoch 233
2022-08-17 03:55:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:55:17 | INFO | train_inner | epoch 233:      8 / 81 loss=3.41, nll_loss=0.368, ppl=1.29, wps=3475.8, ups=0.63, wpb=5499.7, bsz=360.8, num_updates=18800, lr=0.000461266, gnorm=0.309, train_wall=80, gb_free=10.1, wall=20614
2022-08-17 03:56:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:56:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:56:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:56:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:56:30 | INFO | valid | epoch 233 | valid on 'valid' subset | loss 5.031 | nll_loss 2.364 | ppl 5.15 | bleu 56.49 | wps 1903.8 | wpb 933.5 | bsz 59.6 | num_updates 18873 | best_bleu 57.08
2022-08-17 03:56:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 233 @ 18873 updates
2022-08-17 03:56:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint233.pt
2022-08-17 03:56:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint233.pt
2022-08-17 03:56:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint233.pt (epoch 233 @ 18873 updates, score 56.49) (writing took 22.15804222598672 seconds)
2022-08-17 03:56:53 | INFO | fairseq_cli.train | end of epoch 233 (average epoch stats below)
2022-08-17 03:56:53 | INFO | train | epoch 233 | loss 3.412 | nll_loss 0.371 | ppl 1.29 | wps 4330.6 | ups 0.78 | wpb 5523.2 | bsz 358 | num_updates 18873 | lr 0.000460373 | gnorm 0.362 | train_wall 70 | gb_free 10.1 | wall 20709
2022-08-17 03:56:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:56:53 | INFO | fairseq.trainer | begin training epoch 234
2022-08-17 03:56:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:57:17 | INFO | train_inner | epoch 234:     27 / 81 loss=3.412, nll_loss=0.37, ppl=1.29, wps=4662.9, ups=0.84, wpb=5558.6, bsz=355.8, num_updates=18900, lr=0.000460044, gnorm=0.363, train_wall=86, gb_free=10, wall=20733
2022-08-17 03:57:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:57:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:57:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:57:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:58:09 | INFO | valid | epoch 234 | valid on 'valid' subset | loss 5.052 | nll_loss 2.384 | ppl 5.22 | bleu 56.37 | wps 1236.7 | wpb 933.5 | bsz 59.6 | num_updates 18954 | best_bleu 57.08
2022-08-17 03:58:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 234 @ 18954 updates
2022-08-17 03:58:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint234.pt
2022-08-17 03:58:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint234.pt
2022-08-17 03:58:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint234.pt (epoch 234 @ 18954 updates, score 56.37) (writing took 25.083202112466097 seconds)
2022-08-17 03:58:34 | INFO | fairseq_cli.train | end of epoch 234 (average epoch stats below)
2022-08-17 03:58:34 | INFO | train | epoch 234 | loss 3.41 | nll_loss 0.368 | ppl 1.29 | wps 4409.7 | ups 0.8 | wpb 5523.2 | bsz 358 | num_updates 18954 | lr 0.000459388 | gnorm 0.328 | train_wall 62 | gb_free 10.1 | wall 20810
2022-08-17 03:58:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 03:58:34 | INFO | fairseq.trainer | begin training epoch 235
2022-08-17 03:58:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 03:59:12 | INFO | train_inner | epoch 235:     46 / 81 loss=3.409, nll_loss=0.368, ppl=1.29, wps=4798.9, ups=0.87, wpb=5538.5, bsz=359.8, num_updates=19000, lr=0.000458831, gnorm=0.294, train_wall=75, gb_free=10, wall=20848
2022-08-17 03:59:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 03:59:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 03:59:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 03:59:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 03:59:49 | INFO | valid | epoch 235 | valid on 'valid' subset | loss 5.039 | nll_loss 2.375 | ppl 5.19 | bleu 56.15 | wps 1212.9 | wpb 933.5 | bsz 59.6 | num_updates 19035 | best_bleu 57.08
2022-08-17 03:59:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 235 @ 19035 updates
2022-08-17 03:59:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint235.pt
2022-08-17 03:59:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint235.pt
2022-08-17 04:00:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint235.pt (epoch 235 @ 19035 updates, score 56.15) (writing took 18.925361528992653 seconds)
2022-08-17 04:00:09 | INFO | fairseq_cli.train | end of epoch 235 (average epoch stats below)
2022-08-17 04:00:09 | INFO | train | epoch 235 | loss 3.409 | nll_loss 0.367 | ppl 1.29 | wps 4743.4 | ups 0.86 | wpb 5523.2 | bsz 358 | num_updates 19035 | lr 0.000458409 | gnorm 0.291 | train_wall 60 | gb_free 10.2 | wall 20905
2022-08-17 04:00:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:00:09 | INFO | fairseq.trainer | begin training epoch 236
2022-08-17 04:00:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:00:58 | INFO | train_inner | epoch 236:     65 / 81 loss=3.408, nll_loss=0.367, ppl=1.29, wps=5206.5, ups=0.95, wpb=5503.7, bsz=356.5, num_updates=19100, lr=0.000457629, gnorm=0.294, train_wall=71, gb_free=10, wall=20954
2022-08-17 04:01:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:01:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:01:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:01:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:01:26 | INFO | valid | epoch 236 | valid on 'valid' subset | loss 5.037 | nll_loss 2.376 | ppl 5.19 | bleu 56.39 | wps 1207.9 | wpb 933.5 | bsz 59.6 | num_updates 19116 | best_bleu 57.08
2022-08-17 04:01:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 236 @ 19116 updates
2022-08-17 04:01:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint236.pt
2022-08-17 04:01:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint236.pt
2022-08-17 04:01:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint236.pt (epoch 236 @ 19116 updates, score 56.39) (writing took 21.687863923609257 seconds)
2022-08-17 04:01:48 | INFO | fairseq_cli.train | end of epoch 236 (average epoch stats below)
2022-08-17 04:01:48 | INFO | train | epoch 236 | loss 3.409 | nll_loss 0.367 | ppl 1.29 | wps 4499.4 | ups 0.81 | wpb 5523.2 | bsz 358 | num_updates 19116 | lr 0.000457437 | gnorm 0.292 | train_wall 62 | gb_free 10.2 | wall 21004
2022-08-17 04:01:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:01:48 | INFO | fairseq.trainer | begin training epoch 237
2022-08-17 04:01:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:02:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:02:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:02:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:02:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:03:07 | INFO | valid | epoch 237 | valid on 'valid' subset | loss 5.048 | nll_loss 2.386 | ppl 5.23 | bleu 55.95 | wps 1200.3 | wpb 933.5 | bsz 59.6 | num_updates 19197 | best_bleu 57.08
2022-08-17 04:03:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 237 @ 19197 updates
2022-08-17 04:03:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint237.pt
2022-08-17 04:03:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint237.pt
2022-08-17 04:03:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint237.pt (epoch 237 @ 19197 updates, score 55.95) (writing took 23.867145769298077 seconds)
2022-08-17 04:03:31 | INFO | fairseq_cli.train | end of epoch 237 (average epoch stats below)
2022-08-17 04:03:31 | INFO | train | epoch 237 | loss 3.41 | nll_loss 0.369 | ppl 1.29 | wps 4357.2 | ups 0.79 | wpb 5523.2 | bsz 358 | num_updates 19197 | lr 0.000456471 | gnorm 0.321 | train_wall 63 | gb_free 10.2 | wall 21107
2022-08-17 04:03:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:03:31 | INFO | fairseq.trainer | begin training epoch 238
2022-08-17 04:03:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:03:34 | INFO | train_inner | epoch 238:      3 / 81 loss=3.41, nll_loss=0.369, ppl=1.29, wps=3529.9, ups=0.64, wpb=5498, bsz=357.1, num_updates=19200, lr=0.000456435, gnorm=0.318, train_wall=79, gb_free=10, wall=21110
2022-08-17 04:04:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:04:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:04:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:04:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:04:54 | INFO | valid | epoch 238 | valid on 'valid' subset | loss 5.055 | nll_loss 2.397 | ppl 5.27 | bleu 56.24 | wps 1254.2 | wpb 933.5 | bsz 59.6 | num_updates 19278 | best_bleu 57.08
2022-08-17 04:04:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 238 @ 19278 updates
2022-08-17 04:04:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint238.pt
2022-08-17 04:04:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint238.pt
2022-08-17 04:05:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint238.pt (epoch 238 @ 19278 updates, score 56.24) (writing took 24.78703996166587 seconds)
2022-08-17 04:05:19 | INFO | fairseq_cli.train | end of epoch 238 (average epoch stats below)
2022-08-17 04:05:19 | INFO | train | epoch 238 | loss 3.409 | nll_loss 0.367 | ppl 1.29 | wps 4137.9 | ups 0.75 | wpb 5523.2 | bsz 358 | num_updates 19278 | lr 0.000455511 | gnorm 0.302 | train_wall 68 | gb_free 10.2 | wall 21215
2022-08-17 04:05:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:05:19 | INFO | fairseq.trainer | begin training epoch 239
2022-08-17 04:05:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:05:39 | INFO | train_inner | epoch 239:     22 / 81 loss=3.409, nll_loss=0.367, ppl=1.29, wps=4424.6, ups=0.8, wpb=5527.9, bsz=357.8, num_updates=19300, lr=0.000455251, gnorm=0.306, train_wall=85, gb_free=10, wall=21235
2022-08-17 04:06:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:06:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:06:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:06:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:06:38 | INFO | valid | epoch 239 | valid on 'valid' subset | loss 5.049 | nll_loss 2.389 | ppl 5.24 | bleu 57.14 | wps 1647.8 | wpb 933.5 | bsz 59.6 | num_updates 19359 | best_bleu 57.14
2022-08-17 04:06:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 239 @ 19359 updates
2022-08-17 04:06:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint239.pt
2022-08-17 04:06:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint239.pt
2022-08-17 04:07:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint239.pt (epoch 239 @ 19359 updates, score 57.14) (writing took 42.81223912164569 seconds)
2022-08-17 04:07:21 | INFO | fairseq_cli.train | end of epoch 239 (average epoch stats below)
2022-08-17 04:07:21 | INFO | train | epoch 239 | loss 3.409 | nll_loss 0.367 | ppl 1.29 | wps 3652.5 | ups 0.66 | wpb 5523.2 | bsz 358 | num_updates 19359 | lr 0.000454557 | gnorm 0.322 | train_wall 68 | gb_free 10.1 | wall 21337
2022-08-17 04:07:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:07:21 | INFO | fairseq.trainer | begin training epoch 240
2022-08-17 04:07:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:07:50 | INFO | train_inner | epoch 240:     41 / 81 loss=3.408, nll_loss=0.367, ppl=1.29, wps=4190.4, ups=0.76, wpb=5517.5, bsz=360.3, num_updates=19400, lr=0.000454077, gnorm=0.339, train_wall=77, gb_free=10, wall=21366
2022-08-17 04:08:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:08:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:08:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:08:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:08:36 | INFO | valid | epoch 240 | valid on 'valid' subset | loss 5.065 | nll_loss 2.409 | ppl 5.31 | bleu 56.56 | wps 1220.6 | wpb 933.5 | bsz 59.6 | num_updates 19440 | best_bleu 57.14
2022-08-17 04:08:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 240 @ 19440 updates
2022-08-17 04:08:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint240.pt
2022-08-17 04:08:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint240.pt
2022-08-17 04:08:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint240.pt (epoch 240 @ 19440 updates, score 56.56) (writing took 20.66991001367569 seconds)
2022-08-17 04:08:57 | INFO | fairseq_cli.train | end of epoch 240 (average epoch stats below)
2022-08-17 04:08:57 | INFO | train | epoch 240 | loss 3.409 | nll_loss 0.368 | ppl 1.29 | wps 4679.4 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 19440 | lr 0.000453609 | gnorm 0.325 | train_wall 60 | gb_free 10.2 | wall 21433
2022-08-17 04:08:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:08:57 | INFO | fairseq.trainer | begin training epoch 241
2022-08-17 04:08:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:09:41 | INFO | train_inner | epoch 241:     60 / 81 loss=3.408, nll_loss=0.367, ppl=1.29, wps=4988.1, ups=0.9, wpb=5551.3, bsz=357.9, num_updates=19500, lr=0.000452911, gnorm=0.296, train_wall=75, gb_free=10.1, wall=21478
2022-08-17 04:09:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:10:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:10:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:10:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:10:14 | INFO | valid | epoch 241 | valid on 'valid' subset | loss 5.068 | nll_loss 2.413 | ppl 5.33 | bleu 56.11 | wps 1181.4 | wpb 933.5 | bsz 59.6 | num_updates 19521 | best_bleu 57.14
2022-08-17 04:10:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 241 @ 19521 updates
2022-08-17 04:10:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint241.pt
2022-08-17 04:10:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint241.pt
2022-08-17 04:10:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint241.pt (epoch 241 @ 19521 updates, score 56.11) (writing took 21.929282315075397 seconds)
2022-08-17 04:10:36 | INFO | fairseq_cli.train | end of epoch 241 (average epoch stats below)
2022-08-17 04:10:36 | INFO | train | epoch 241 | loss 3.408 | nll_loss 0.366 | ppl 1.29 | wps 4529.6 | ups 0.82 | wpb 5523.2 | bsz 358 | num_updates 19521 | lr 0.000452667 | gnorm 0.302 | train_wall 61 | gb_free 10.1 | wall 21532
2022-08-17 04:10:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:10:36 | INFO | fairseq.trainer | begin training epoch 242
2022-08-17 04:10:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:11:41 | INFO | train_inner | epoch 242:     79 / 81 loss=3.407, nll_loss=0.366, ppl=1.29, wps=4631, ups=0.84, wpb=5518.8, bsz=358.3, num_updates=19600, lr=0.000451754, gnorm=0.284, train_wall=81, gb_free=10.1, wall=21597
2022-08-17 04:11:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:11:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:11:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:11:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:11:56 | INFO | valid | epoch 242 | valid on 'valid' subset | loss 5.036 | nll_loss 2.378 | ppl 5.2 | bleu 56.62 | wps 1167.9 | wpb 933.5 | bsz 59.6 | num_updates 19602 | best_bleu 57.14
2022-08-17 04:11:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 242 @ 19602 updates
2022-08-17 04:11:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint242.pt
2022-08-17 04:11:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint242.pt
2022-08-17 04:12:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint242.pt (epoch 242 @ 19602 updates, score 56.62) (writing took 36.72083739563823 seconds)
2022-08-17 04:12:33 | INFO | fairseq_cli.train | end of epoch 242 (average epoch stats below)
2022-08-17 04:12:33 | INFO | train | epoch 242 | loss 3.407 | nll_loss 0.366 | ppl 1.29 | wps 3799.9 | ups 0.69 | wpb 5523.2 | bsz 358 | num_updates 19602 | lr 0.000451731 | gnorm 0.278 | train_wall 65 | gb_free 10.2 | wall 21650
2022-08-17 04:12:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:12:34 | INFO | fairseq.trainer | begin training epoch 243
2022-08-17 04:12:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:13:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:13:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:13:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:13:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:13:51 | INFO | valid | epoch 243 | valid on 'valid' subset | loss 5.03 | nll_loss 2.368 | ppl 5.16 | bleu 56.98 | wps 1335.9 | wpb 933.5 | bsz 59.6 | num_updates 19683 | best_bleu 57.14
2022-08-17 04:13:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 243 @ 19683 updates
2022-08-17 04:13:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint243.pt
2022-08-17 04:13:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint243.pt
2022-08-17 04:14:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint243.pt (epoch 243 @ 19683 updates, score 56.98) (writing took 20.129186894744635 seconds)
2022-08-17 04:14:12 | INFO | fairseq_cli.train | end of epoch 243 (average epoch stats below)
2022-08-17 04:14:12 | INFO | train | epoch 243 | loss 3.406 | nll_loss 0.365 | ppl 1.29 | wps 4556.7 | ups 0.83 | wpb 5523.2 | bsz 358 | num_updates 19683 | lr 0.0004508 | gnorm 0.29 | train_wall 64 | gb_free 10.1 | wall 21748
2022-08-17 04:14:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:14:12 | INFO | fairseq.trainer | begin training epoch 244
2022-08-17 04:14:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:14:27 | INFO | train_inner | epoch 244:     17 / 81 loss=3.406, nll_loss=0.365, ppl=1.29, wps=3316.2, ups=0.6, wpb=5502.7, bsz=358.2, num_updates=19700, lr=0.000450606, gnorm=0.289, train_wall=79, gb_free=10.1, wall=21763
2022-08-17 04:15:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:15:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:15:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:15:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:15:26 | INFO | valid | epoch 244 | valid on 'valid' subset | loss 5.029 | nll_loss 2.367 | ppl 5.16 | bleu 56.68 | wps 1219.8 | wpb 933.5 | bsz 59.6 | num_updates 19764 | best_bleu 57.14
2022-08-17 04:15:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 244 @ 19764 updates
2022-08-17 04:15:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint244.pt
2022-08-17 04:15:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint244.pt
2022-08-17 04:15:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint244.pt (epoch 244 @ 19764 updates, score 56.68) (writing took 21.86968081444502 seconds)
2022-08-17 04:15:48 | INFO | fairseq_cli.train | end of epoch 244 (average epoch stats below)
2022-08-17 04:15:48 | INFO | train | epoch 244 | loss 3.408 | nll_loss 0.367 | ppl 1.29 | wps 4633.1 | ups 0.84 | wpb 5523.2 | bsz 358 | num_updates 19764 | lr 0.000449876 | gnorm 0.306 | train_wall 59 | gb_free 10.3 | wall 21844
2022-08-17 04:15:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:15:48 | INFO | fairseq.trainer | begin training epoch 245
2022-08-17 04:15:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:16:20 | INFO | train_inner | epoch 245:     36 / 81 loss=3.408, nll_loss=0.367, ppl=1.29, wps=4872.6, ups=0.88, wpb=5542.1, bsz=353.8, num_updates=19800, lr=0.000449467, gnorm=0.303, train_wall=76, gb_free=10.1, wall=21877
2022-08-17 04:16:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:16:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:16:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:16:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:17:05 | INFO | valid | epoch 245 | valid on 'valid' subset | loss 5.043 | nll_loss 2.382 | ppl 5.21 | bleu 56.66 | wps 1214.9 | wpb 933.5 | bsz 59.6 | num_updates 19845 | best_bleu 57.14
2022-08-17 04:17:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 245 @ 19845 updates
2022-08-17 04:17:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint245.pt
2022-08-17 04:17:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint245.pt
2022-08-17 04:17:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint245.pt (epoch 245 @ 19845 updates, score 56.66) (writing took 29.63233982026577 seconds)
2022-08-17 04:17:35 | INFO | fairseq_cli.train | end of epoch 245 (average epoch stats below)
2022-08-17 04:17:35 | INFO | train | epoch 245 | loss 3.408 | nll_loss 0.367 | ppl 1.29 | wps 4174.9 | ups 0.76 | wpb 5523.2 | bsz 358 | num_updates 19845 | lr 0.000448957 | gnorm 0.309 | train_wall 62 | gb_free 10.1 | wall 21952
2022-08-17 04:17:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:17:35 | INFO | fairseq.trainer | begin training epoch 246
2022-08-17 04:17:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:18:17 | INFO | train_inner | epoch 246:     55 / 81 loss=3.407, nll_loss=0.366, ppl=1.29, wps=4734.1, ups=0.86, wpb=5506, bsz=359.4, num_updates=19900, lr=0.000448336, gnorm=0.339, train_wall=71, gb_free=10.1, wall=21993
2022-08-17 04:18:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:18:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:18:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:18:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:18:53 | INFO | valid | epoch 246 | valid on 'valid' subset | loss 5.039 | nll_loss 2.38 | ppl 5.21 | bleu 56.7 | wps 1183.7 | wpb 933.5 | bsz 59.6 | num_updates 19926 | best_bleu 57.14
2022-08-17 04:18:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 246 @ 19926 updates
2022-08-17 04:18:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint246.pt
2022-08-17 04:18:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint246.pt
2022-08-17 04:19:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint246.pt (epoch 246 @ 19926 updates, score 56.7) (writing took 25.154800046235323 seconds)
2022-08-17 04:19:19 | INFO | fairseq_cli.train | end of epoch 246 (average epoch stats below)
2022-08-17 04:19:19 | INFO | train | epoch 246 | loss 3.407 | nll_loss 0.366 | ppl 1.29 | wps 4331.9 | ups 0.78 | wpb 5523.2 | bsz 358 | num_updates 19926 | lr 0.000448043 | gnorm 0.344 | train_wall 62 | gb_free 10.1 | wall 22055
2022-08-17 04:19:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:19:19 | INFO | fairseq.trainer | begin training epoch 247
2022-08-17 04:19:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:20:15 | INFO | train_inner | epoch 247:     74 / 81 loss=3.408, nll_loss=0.367, ppl=1.29, wps=4669.4, ups=0.84, wpb=5548.9, bsz=362.3, num_updates=20000, lr=0.000447214, gnorm=0.337, train_wall=78, gb_free=10.2, wall=22112
2022-08-17 04:20:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:20:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:20:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:20:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:20:27 | INFO | valid | epoch 247 | valid on 'valid' subset | loss 5.034 | nll_loss 2.381 | ppl 5.21 | bleu 57.2 | wps 2129.4 | wpb 933.5 | bsz 59.6 | num_updates 20007 | best_bleu 57.2
2022-08-17 04:20:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 247 @ 20007 updates
2022-08-17 04:20:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint247.pt
2022-08-17 04:20:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint247.pt
2022-08-17 04:21:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint247.pt (epoch 247 @ 20007 updates, score 57.2) (writing took 64.42329682782292 seconds)
2022-08-17 04:21:31 | INFO | fairseq_cli.train | end of epoch 247 (average epoch stats below)
2022-08-17 04:21:31 | INFO | train | epoch 247 | loss 3.408 | nll_loss 0.367 | ppl 1.29 | wps 3369.6 | ups 0.61 | wpb 5523.2 | bsz 358 | num_updates 20007 | lr 0.000447135 | gnorm 0.339 | train_wall 58 | gb_free 10.1 | wall 22188
2022-08-17 04:21:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:21:32 | INFO | fairseq.trainer | begin training epoch 248
2022-08-17 04:21:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:22:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:22:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:22:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:22:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:22:50 | INFO | valid | epoch 248 | valid on 'valid' subset | loss 5.04 | nll_loss 2.377 | ppl 5.2 | bleu 56.36 | wps 1180.8 | wpb 933.5 | bsz 59.6 | num_updates 20088 | best_bleu 57.2
2022-08-17 04:22:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 248 @ 20088 updates
2022-08-17 04:22:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint248.pt
2022-08-17 04:22:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint248.pt
2022-08-17 04:23:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint248.pt (epoch 248 @ 20088 updates, score 56.36) (writing took 13.931855667382479 seconds)
2022-08-17 04:23:04 | INFO | fairseq_cli.train | end of epoch 248 (average epoch stats below)
2022-08-17 04:23:04 | INFO | train | epoch 248 | loss 3.407 | nll_loss 0.366 | ppl 1.29 | wps 4821.8 | ups 0.87 | wpb 5523.2 | bsz 358 | num_updates 20088 | lr 0.000446233 | gnorm 0.322 | train_wall 62 | gb_free 10.2 | wall 22280
2022-08-17 04:23:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:23:04 | INFO | fairseq.trainer | begin training epoch 249
2022-08-17 04:23:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:23:11 | INFO | train_inner | epoch 249:     12 / 81 loss=3.407, nll_loss=0.366, ppl=1.29, wps=3137.2, ups=0.57, wpb=5515.3, bsz=355.6, num_updates=20100, lr=0.0004461, gnorm=0.319, train_wall=71, gb_free=10.1, wall=22288
2022-08-17 04:24:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:24:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:24:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:24:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:24:22 | INFO | valid | epoch 249 | valid on 'valid' subset | loss 5.021 | nll_loss 2.355 | ppl 5.12 | bleu 57.06 | wps 1178.8 | wpb 933.5 | bsz 59.6 | num_updates 20169 | best_bleu 57.2
2022-08-17 04:24:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 249 @ 20169 updates
2022-08-17 04:24:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint249.pt
2022-08-17 04:24:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint249.pt
2022-08-17 04:24:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint249.pt (epoch 249 @ 20169 updates, score 57.06) (writing took 17.80643965676427 seconds)
2022-08-17 04:24:40 | INFO | fairseq_cli.train | end of epoch 249 (average epoch stats below)
2022-08-17 04:24:40 | INFO | train | epoch 249 | loss 3.408 | nll_loss 0.367 | ppl 1.29 | wps 4646.6 | ups 0.84 | wpb 5523.2 | bsz 358 | num_updates 20169 | lr 0.000445336 | gnorm 0.301 | train_wall 63 | gb_free 10.1 | wall 22377
2022-08-17 04:24:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:24:41 | INFO | fairseq.trainer | begin training epoch 250
2022-08-17 04:24:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:25:06 | INFO | train_inner | epoch 250:     31 / 81 loss=3.408, nll_loss=0.367, ppl=1.29, wps=4810.2, ups=0.87, wpb=5503, bsz=355.8, num_updates=20200, lr=0.000444994, gnorm=0.315, train_wall=80, gb_free=10.1, wall=22402
2022-08-17 04:25:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:25:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:25:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:25:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:26:04 | INFO | valid | epoch 250 | valid on 'valid' subset | loss 5.054 | nll_loss 2.401 | ppl 5.28 | bleu 56.61 | wps 1209.5 | wpb 933.5 | bsz 59.6 | num_updates 20250 | best_bleu 57.2
2022-08-17 04:26:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 250 @ 20250 updates
2022-08-17 04:26:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint250.pt
2022-08-17 04:26:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint250.pt
2022-08-17 04:26:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint250.pt (epoch 250 @ 20250 updates, score 56.61) (writing took 17.40586433187127 seconds)
2022-08-17 04:26:22 | INFO | fairseq_cli.train | end of epoch 250 (average epoch stats below)
2022-08-17 04:26:22 | INFO | train | epoch 250 | loss 3.406 | nll_loss 0.365 | ppl 1.29 | wps 4415.6 | ups 0.8 | wpb 5523.2 | bsz 358 | num_updates 20250 | lr 0.000444444 | gnorm 0.303 | train_wall 68 | gb_free 10.2 | wall 22478
2022-08-17 04:26:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:26:22 | INFO | fairseq.trainer | begin training epoch 251
2022-08-17 04:26:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:27:07 | INFO | train_inner | epoch 251:     50 / 81 loss=3.405, nll_loss=0.365, ppl=1.29, wps=4554.7, ups=0.82, wpb=5531.4, bsz=361.9, num_updates=20300, lr=0.000443897, gnorm=0.326, train_wall=88, gb_free=10.1, wall=22523
2022-08-17 04:27:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:27:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:27:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:27:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:27:47 | INFO | valid | epoch 251 | valid on 'valid' subset | loss 5.054 | nll_loss 2.393 | ppl 5.25 | bleu 56.52 | wps 1312.5 | wpb 933.5 | bsz 59.6 | num_updates 20331 | best_bleu 57.2
2022-08-17 04:27:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 251 @ 20331 updates
2022-08-17 04:27:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint251.pt
2022-08-17 04:27:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint251.pt
2022-08-17 04:28:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint251.pt (epoch 251 @ 20331 updates, score 56.52) (writing took 33.42851686105132 seconds)
2022-08-17 04:28:21 | INFO | fairseq_cli.train | end of epoch 251 (average epoch stats below)
2022-08-17 04:28:21 | INFO | train | epoch 251 | loss 3.406 | nll_loss 0.366 | ppl 1.29 | wps 3761.1 | ups 0.68 | wpb 5523.2 | bsz 358 | num_updates 20331 | lr 0.000443558 | gnorm 0.345 | train_wall 71 | gb_free 10.3 | wall 22597
2022-08-17 04:28:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:28:21 | INFO | fairseq.trainer | begin training epoch 252
2022-08-17 04:28:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:29:17 | INFO | train_inner | epoch 252:     69 / 81 loss=3.405, nll_loss=0.365, ppl=1.29, wps=4258.6, ups=0.77, wpb=5525.8, bsz=359, num_updates=20400, lr=0.000442807, gnorm=0.292, train_wall=82, gb_free=10.2, wall=22653
2022-08-17 04:29:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:29:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:29:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:29:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:29:34 | INFO | valid | epoch 252 | valid on 'valid' subset | loss 5.063 | nll_loss 2.405 | ppl 5.3 | bleu 56.52 | wps 1497 | wpb 933.5 | bsz 59.6 | num_updates 20412 | best_bleu 57.2
2022-08-17 04:29:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 252 @ 20412 updates
2022-08-17 04:29:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint252.pt
2022-08-17 04:29:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint252.pt
2022-08-17 04:29:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint252.pt (epoch 252 @ 20412 updates, score 56.52) (writing took 18.511380951851606 seconds)
2022-08-17 04:29:52 | INFO | fairseq_cli.train | end of epoch 252 (average epoch stats below)
2022-08-17 04:29:52 | INFO | train | epoch 252 | loss 3.405 | nll_loss 0.364 | ppl 1.29 | wps 4877.5 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 20412 | lr 0.000442677 | gnorm 0.292 | train_wall 60 | gb_free 10.2 | wall 22689
2022-08-17 04:29:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:29:53 | INFO | fairseq.trainer | begin training epoch 253
2022-08-17 04:29:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:30:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:30:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:30:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:30:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:31:07 | INFO | valid | epoch 253 | valid on 'valid' subset | loss 5.053 | nll_loss 2.398 | ppl 5.27 | bleu 56.57 | wps 1347.2 | wpb 933.5 | bsz 59.6 | num_updates 20493 | best_bleu 57.2
2022-08-17 04:31:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 253 @ 20493 updates
2022-08-17 04:31:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint253.pt
2022-08-17 04:31:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint253.pt
2022-08-17 04:31:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint253.pt (epoch 253 @ 20493 updates, score 56.57) (writing took 15.488280139863491 seconds)
2022-08-17 04:31:23 | INFO | fairseq_cli.train | end of epoch 253 (average epoch stats below)
2022-08-17 04:31:23 | INFO | train | epoch 253 | loss 3.406 | nll_loss 0.365 | ppl 1.29 | wps 4953.6 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 20493 | lr 0.000441802 | gnorm 0.284 | train_wall 61 | gb_free 10.2 | wall 22779
2022-08-17 04:31:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:31:23 | INFO | fairseq.trainer | begin training epoch 254
2022-08-17 04:31:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:31:30 | INFO | train_inner | epoch 254:      7 / 81 loss=3.406, nll_loss=0.365, ppl=1.29, wps=4153.1, ups=0.75, wpb=5526, bsz=356.2, num_updates=20500, lr=0.000441726, gnorm=0.287, train_wall=72, gb_free=10.1, wall=22786
2022-08-17 04:32:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:32:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:32:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:32:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:32:36 | INFO | valid | epoch 254 | valid on 'valid' subset | loss 5.048 | nll_loss 2.395 | ppl 5.26 | bleu 57.08 | wps 1570.3 | wpb 933.5 | bsz 59.6 | num_updates 20574 | best_bleu 57.2
2022-08-17 04:32:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 254 @ 20574 updates
2022-08-17 04:32:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint254.pt
2022-08-17 04:32:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint254.pt
2022-08-17 04:32:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint254.pt (epoch 254 @ 20574 updates, score 57.08) (writing took 18.604754712432623 seconds)
2022-08-17 04:32:55 | INFO | fairseq_cli.train | end of epoch 254 (average epoch stats below)
2022-08-17 04:32:55 | INFO | train | epoch 254 | loss 3.406 | nll_loss 0.365 | ppl 1.29 | wps 4843.6 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 20574 | lr 0.000440931 | gnorm 0.528 | train_wall 61 | gb_free 10.2 | wall 22871
2022-08-17 04:32:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:32:55 | INFO | fairseq.trainer | begin training epoch 255
2022-08-17 04:32:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:33:19 | INFO | train_inner | epoch 255:     26 / 81 loss=3.406, nll_loss=0.366, ppl=1.29, wps=5076.7, ups=0.92, wpb=5516.6, bsz=355.3, num_updates=20600, lr=0.000440653, gnorm=0.483, train_wall=78, gb_free=10.1, wall=22895
2022-08-17 04:33:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:33:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:33:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:33:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:34:08 | INFO | valid | epoch 255 | valid on 'valid' subset | loss 5.052 | nll_loss 2.395 | ppl 5.26 | bleu 56.18 | wps 1529.8 | wpb 933.5 | bsz 59.6 | num_updates 20655 | best_bleu 57.2
2022-08-17 04:34:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 255 @ 20655 updates
2022-08-17 04:34:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint255.pt
2022-08-17 04:34:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint255.pt
2022-08-17 04:34:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint255.pt (epoch 255 @ 20655 updates, score 56.18) (writing took 15.559517044574022 seconds)
2022-08-17 04:34:24 | INFO | fairseq_cli.train | end of epoch 255 (average epoch stats below)
2022-08-17 04:34:24 | INFO | train | epoch 255 | loss 3.406 | nll_loss 0.365 | ppl 1.29 | wps 5027.8 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 20655 | lr 0.000440066 | gnorm 0.289 | train_wall 61 | gb_free 10.1 | wall 22960
2022-08-17 04:34:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:34:24 | INFO | fairseq.trainer | begin training epoch 256
2022-08-17 04:34:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:35:03 | INFO | train_inner | epoch 256:     45 / 81 loss=3.406, nll_loss=0.366, ppl=1.29, wps=5261.9, ups=0.95, wpb=5516.4, bsz=360, num_updates=20700, lr=0.000439587, gnorm=0.316, train_wall=76, gb_free=10.1, wall=23000
2022-08-17 04:35:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:35:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:35:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:35:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:35:37 | INFO | valid | epoch 256 | valid on 'valid' subset | loss 5.057 | nll_loss 2.404 | ppl 5.29 | bleu 56.33 | wps 1618.5 | wpb 933.5 | bsz 59.6 | num_updates 20736 | best_bleu 57.2
2022-08-17 04:35:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 256 @ 20736 updates
2022-08-17 04:35:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint256.pt
2022-08-17 04:35:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint256.pt
2022-08-17 04:35:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint256.pt (epoch 256 @ 20736 updates, score 56.33) (writing took 2.380729578435421 seconds)
2022-08-17 04:35:39 | INFO | fairseq_cli.train | end of epoch 256 (average epoch stats below)
2022-08-17 04:35:39 | INFO | train | epoch 256 | loss 3.406 | nll_loss 0.366 | ppl 1.29 | wps 5958.2 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 20736 | lr 0.000439205 | gnorm 0.322 | train_wall 61 | gb_free 10.1 | wall 23035
2022-08-17 04:35:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:35:39 | INFO | fairseq.trainer | begin training epoch 257
2022-08-17 04:35:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:36:38 | INFO | train_inner | epoch 257:     64 / 81 loss=3.404, nll_loss=0.364, ppl=1.29, wps=5881.4, ups=1.06, wpb=5554.8, bsz=361.4, num_updates=20800, lr=0.000438529, gnorm=0.28, train_wall=80, gb_free=10.1, wall=23094
2022-08-17 04:36:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:36:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:36:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:36:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:37:00 | INFO | valid | epoch 257 | valid on 'valid' subset | loss 5.05 | nll_loss 2.392 | ppl 5.25 | bleu 56.44 | wps 1919.3 | wpb 933.5 | bsz 59.6 | num_updates 20817 | best_bleu 57.2
2022-08-17 04:37:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 257 @ 20817 updates
2022-08-17 04:37:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint257.pt
2022-08-17 04:37:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint257.pt
2022-08-17 04:37:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint257.pt (epoch 257 @ 20817 updates, score 56.44) (writing took 13.813360299915075 seconds)
2022-08-17 04:37:14 | INFO | fairseq_cli.train | end of epoch 257 (average epoch stats below)
2022-08-17 04:37:14 | INFO | train | epoch 257 | loss 3.404 | nll_loss 0.364 | ppl 1.29 | wps 4714 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 20817 | lr 0.00043835 | gnorm 0.283 | train_wall 70 | gb_free 10.2 | wall 23130
2022-08-17 04:37:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:37:14 | INFO | fairseq.trainer | begin training epoch 258
2022-08-17 04:37:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:38:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:38:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:38:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:38:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:38:33 | INFO | valid | epoch 258 | valid on 'valid' subset | loss 5.059 | nll_loss 2.406 | ppl 5.3 | bleu 56.63 | wps 1918.3 | wpb 933.5 | bsz 59.6 | num_updates 20898 | best_bleu 57.2
2022-08-17 04:38:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 258 @ 20898 updates
2022-08-17 04:38:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint258.pt
2022-08-17 04:38:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint258.pt
2022-08-17 04:38:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint258.pt (epoch 258 @ 20898 updates, score 56.63) (writing took 17.968097254633904 seconds)
2022-08-17 04:38:52 | INFO | fairseq_cli.train | end of epoch 258 (average epoch stats below)
2022-08-17 04:38:52 | INFO | train | epoch 258 | loss 3.404 | nll_loss 0.364 | ppl 1.29 | wps 4585.6 | ups 0.83 | wpb 5523.2 | bsz 358 | num_updates 20898 | lr 0.0004375 | gnorm 0.306 | train_wall 69 | gb_free 10.1 | wall 23228
2022-08-17 04:38:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:38:52 | INFO | fairseq.trainer | begin training epoch 259
2022-08-17 04:38:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:38:55 | INFO | train_inner | epoch 259:      2 / 81 loss=3.405, nll_loss=0.364, ppl=1.29, wps=4021.2, ups=0.73, wpb=5495.8, bsz=354.1, num_updates=20900, lr=0.000437479, gnorm=0.309, train_wall=84, gb_free=10, wall=23231
2022-08-17 04:40:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:40:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:40:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:40:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:40:09 | INFO | valid | epoch 259 | valid on 'valid' subset | loss 5.07 | nll_loss 2.414 | ppl 5.33 | bleu 55.61 | wps 1997.8 | wpb 933.5 | bsz 59.6 | num_updates 20979 | best_bleu 57.2
2022-08-17 04:40:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 259 @ 20979 updates
2022-08-17 04:40:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint259.pt
2022-08-17 04:40:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint259.pt
2022-08-17 04:40:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint259.pt (epoch 259 @ 20979 updates, score 55.61) (writing took 2.3339206762611866 seconds)
2022-08-17 04:40:12 | INFO | fairseq_cli.train | end of epoch 259 (average epoch stats below)
2022-08-17 04:40:12 | INFO | train | epoch 259 | loss 3.405 | nll_loss 0.364 | ppl 1.29 | wps 5573.2 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 20979 | lr 0.000436654 | gnorm 0.458 | train_wall 68 | gb_free 10 | wall 23308
2022-08-17 04:40:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:40:12 | INFO | fairseq.trainer | begin training epoch 260
2022-08-17 04:40:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:40:31 | INFO | train_inner | epoch 260:     21 / 81 loss=3.405, nll_loss=0.364, ppl=1.29, wps=5732, ups=1.04, wpb=5534.5, bsz=356.6, num_updates=21000, lr=0.000436436, gnorm=0.423, train_wall=84, gb_free=10.1, wall=23327
2022-08-17 04:41:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:41:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:41:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:41:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:41:37 | INFO | valid | epoch 260 | valid on 'valid' subset | loss 5.078 | nll_loss 2.436 | ppl 5.41 | bleu 56.28 | wps 1252.4 | wpb 933.5 | bsz 59.6 | num_updates 21060 | best_bleu 57.2
2022-08-17 04:41:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 260 @ 21060 updates
2022-08-17 04:41:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint260.pt
2022-08-17 04:41:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint260.pt
2022-08-17 04:42:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint260.pt (epoch 260 @ 21060 updates, score 56.28) (writing took 33.49549490213394 seconds)
2022-08-17 04:42:10 | INFO | fairseq_cli.train | end of epoch 260 (average epoch stats below)
2022-08-17 04:42:10 | INFO | train | epoch 260 | loss 3.404 | nll_loss 0.363 | ppl 1.29 | wps 3782.1 | ups 0.68 | wpb 5523.2 | bsz 358 | num_updates 21060 | lr 0.000435814 | gnorm 0.287 | train_wall 70 | gb_free 10.2 | wall 23426
2022-08-17 04:42:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:42:10 | INFO | fairseq.trainer | begin training epoch 261
2022-08-17 04:42:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:42:47 | INFO | train_inner | epoch 261:     40 / 81 loss=3.403, nll_loss=0.363, ppl=1.29, wps=4077.6, ups=0.74, wpb=5529.8, bsz=359, num_updates=21100, lr=0.0004354, gnorm=0.288, train_wall=87, gb_free=10.1, wall=23463
2022-08-17 04:43:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:43:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:43:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:43:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:43:25 | INFO | valid | epoch 261 | valid on 'valid' subset | loss 5.068 | nll_loss 2.416 | ppl 5.34 | bleu 56.38 | wps 1675.1 | wpb 933.5 | bsz 59.6 | num_updates 21141 | best_bleu 57.2
2022-08-17 04:43:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 261 @ 21141 updates
2022-08-17 04:43:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint261.pt
2022-08-17 04:43:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint261.pt
2022-08-17 04:43:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint261.pt (epoch 261 @ 21141 updates, score 56.38) (writing took 16.66639680042863 seconds)
2022-08-17 04:43:42 | INFO | fairseq_cli.train | end of epoch 261 (average epoch stats below)
2022-08-17 04:43:42 | INFO | train | epoch 261 | loss 3.404 | nll_loss 0.364 | ppl 1.29 | wps 4884.9 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 21141 | lr 0.000434978 | gnorm 0.33 | train_wall 63 | gb_free 10.2 | wall 23518
2022-08-17 04:43:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:43:42 | INFO | fairseq.trainer | begin training epoch 262
2022-08-17 04:43:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:44:34 | INFO | train_inner | epoch 262:     59 / 81 loss=3.405, nll_loss=0.365, ppl=1.29, wps=5157, ups=0.94, wpb=5514.4, bsz=360.9, num_updates=21200, lr=0.000434372, gnorm=0.325, train_wall=79, gb_free=10.1, wall=23570
2022-08-17 04:44:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:44:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:44:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:44:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:44:57 | INFO | valid | epoch 262 | valid on 'valid' subset | loss 5.071 | nll_loss 2.42 | ppl 5.35 | bleu 56.57 | wps 1731.8 | wpb 933.5 | bsz 59.6 | num_updates 21222 | best_bleu 57.2
2022-08-17 04:44:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 262 @ 21222 updates
2022-08-17 04:44:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint262.pt
2022-08-17 04:44:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint262.pt
2022-08-17 04:45:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint262.pt (epoch 262 @ 21222 updates, score 56.57) (writing took 32.829344149678946 seconds)
2022-08-17 04:45:30 | INFO | fairseq_cli.train | end of epoch 262 (average epoch stats below)
2022-08-17 04:45:30 | INFO | train | epoch 262 | loss 3.405 | nll_loss 0.365 | ppl 1.29 | wps 4138.7 | ups 0.75 | wpb 5523.2 | bsz 358 | num_updates 21222 | lr 0.000434147 | gnorm 0.309 | train_wall 64 | gb_free 10.3 | wall 23626
2022-08-17 04:45:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:45:30 | INFO | fairseq.trainer | begin training epoch 263
2022-08-17 04:45:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:46:28 | INFO | train_inner | epoch 263:     78 / 81 loss=3.405, nll_loss=0.365, ppl=1.29, wps=4845.4, ups=0.88, wpb=5534.2, bsz=356.5, num_updates=21300, lr=0.000433351, gnorm=0.321, train_wall=68, gb_free=10, wall=23684
2022-08-17 04:46:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:46:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:46:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:46:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:46:44 | INFO | valid | epoch 263 | valid on 'valid' subset | loss 5.052 | nll_loss 2.393 | ppl 5.25 | bleu 56.46 | wps 1210.9 | wpb 933.5 | bsz 59.6 | num_updates 21303 | best_bleu 57.2
2022-08-17 04:46:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 263 @ 21303 updates
2022-08-17 04:46:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint263.pt
2022-08-17 04:46:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint263.pt
2022-08-17 04:47:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint263.pt (epoch 263 @ 21303 updates, score 56.46) (writing took 17.595881164073944 seconds)
2022-08-17 04:47:02 | INFO | fairseq_cli.train | end of epoch 263 (average epoch stats below)
2022-08-17 04:47:02 | INFO | train | epoch 263 | loss 3.404 | nll_loss 0.364 | ppl 1.29 | wps 4885.1 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 21303 | lr 0.000433321 | gnorm 0.321 | train_wall 57 | gb_free 10.1 | wall 23718
2022-08-17 04:47:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:47:02 | INFO | fairseq.trainer | begin training epoch 264
2022-08-17 04:47:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:48:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:48:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:48:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:48:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:48:14 | INFO | valid | epoch 264 | valid on 'valid' subset | loss 5.077 | nll_loss 2.43 | ppl 5.39 | bleu 56.46 | wps 1205 | wpb 933.5 | bsz 59.6 | num_updates 21384 | best_bleu 57.2
2022-08-17 04:48:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 264 @ 21384 updates
2022-08-17 04:48:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint264.pt
2022-08-17 04:48:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint264.pt
2022-08-17 04:48:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint264.pt (epoch 264 @ 21384 updates, score 56.46) (writing took 13.372716296464205 seconds)
2022-08-17 04:48:28 | INFO | fairseq_cli.train | end of epoch 264 (average epoch stats below)
2022-08-17 04:48:28 | INFO | train | epoch 264 | loss 3.406 | nll_loss 0.367 | ppl 1.29 | wps 5195.6 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 21384 | lr 0.000432499 | gnorm 0.337 | train_wall 57 | gb_free 10.1 | wall 23804
2022-08-17 04:48:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:48:28 | INFO | fairseq.trainer | begin training epoch 265
2022-08-17 04:48:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:48:42 | INFO | train_inner | epoch 265:     16 / 81 loss=3.406, nll_loss=0.367, ppl=1.29, wps=4124.2, ups=0.75, wpb=5526.6, bsz=357, num_updates=21400, lr=0.000432338, gnorm=0.342, train_wall=72, gb_free=10.1, wall=23818
2022-08-17 04:49:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:49:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:49:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:49:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:49:42 | INFO | valid | epoch 265 | valid on 'valid' subset | loss 5.075 | nll_loss 2.428 | ppl 5.38 | bleu 56.43 | wps 1212.9 | wpb 933.5 | bsz 59.6 | num_updates 21465 | best_bleu 57.2
2022-08-17 04:49:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 265 @ 21465 updates
2022-08-17 04:49:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint265.pt
2022-08-17 04:49:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint265.pt
2022-08-17 04:50:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint265.pt (epoch 265 @ 21465 updates, score 56.43) (writing took 18.06978454440832 seconds)
2022-08-17 04:50:01 | INFO | fairseq_cli.train | end of epoch 265 (average epoch stats below)
2022-08-17 04:50:01 | INFO | train | epoch 265 | loss 3.406 | nll_loss 0.366 | ppl 1.29 | wps 4815.9 | ups 0.87 | wpb 5523.2 | bsz 358 | num_updates 21465 | lr 0.000431683 | gnorm 0.329 | train_wall 59 | gb_free 10.2 | wall 23897
2022-08-17 04:50:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:50:01 | INFO | fairseq.trainer | begin training epoch 266
2022-08-17 04:50:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:50:32 | INFO | train_inner | epoch 266:     35 / 81 loss=3.405, nll_loss=0.365, ppl=1.29, wps=5002.8, ups=0.91, wpb=5493.4, bsz=357, num_updates=21500, lr=0.000431331, gnorm=0.313, train_wall=76, gb_free=10.1, wall=23928
2022-08-17 04:51:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:51:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:51:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:51:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:51:14 | INFO | valid | epoch 266 | valid on 'valid' subset | loss 5.086 | nll_loss 2.442 | ppl 5.43 | bleu 56.06 | wps 1196.9 | wpb 933.5 | bsz 59.6 | num_updates 21546 | best_bleu 57.2
2022-08-17 04:51:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 266 @ 21546 updates
2022-08-17 04:51:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint266.pt
2022-08-17 04:51:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint266.pt
2022-08-17 04:51:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint266.pt (epoch 266 @ 21546 updates, score 56.06) (writing took 14.520755935460329 seconds)
2022-08-17 04:51:29 | INFO | fairseq_cli.train | end of epoch 266 (average epoch stats below)
2022-08-17 04:51:29 | INFO | train | epoch 266 | loss 3.404 | nll_loss 0.364 | ppl 1.29 | wps 5072.3 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 21546 | lr 0.00043087 | gnorm 0.278 | train_wall 58 | gb_free 10.2 | wall 23985
2022-08-17 04:51:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:51:29 | INFO | fairseq.trainer | begin training epoch 267
2022-08-17 04:51:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:52:18 | INFO | train_inner | epoch 267:     54 / 81 loss=3.403, nll_loss=0.363, ppl=1.29, wps=5233.4, ups=0.94, wpb=5540.4, bsz=359.6, num_updates=21600, lr=0.000430331, gnorm=0.287, train_wall=76, gb_free=10.1, wall=24034
2022-08-17 04:52:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:52:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:52:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:52:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:52:46 | INFO | valid | epoch 267 | valid on 'valid' subset | loss 5.081 | nll_loss 2.436 | ppl 5.41 | bleu 56.06 | wps 1192.2 | wpb 933.5 | bsz 59.6 | num_updates 21627 | best_bleu 57.2
2022-08-17 04:52:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 267 @ 21627 updates
2022-08-17 04:52:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint267.pt
2022-08-17 04:52:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint267.pt
2022-08-17 04:52:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint267.pt (epoch 267 @ 21627 updates, score 56.06) (writing took 2.3544069193303585 seconds)
2022-08-17 04:52:49 | INFO | fairseq_cli.train | end of epoch 267 (average epoch stats below)
2022-08-17 04:52:49 | INFO | train | epoch 267 | loss 3.403 | nll_loss 0.363 | ppl 1.29 | wps 5582.3 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 21627 | lr 0.000430063 | gnorm 0.298 | train_wall 62 | gb_free 10.1 | wall 24065
2022-08-17 04:52:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:52:49 | INFO | fairseq.trainer | begin training epoch 268
2022-08-17 04:52:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:53:52 | INFO | train_inner | epoch 268:     73 / 81 loss=3.403, nll_loss=0.363, ppl=1.29, wps=5861, ups=1.06, wpb=5525.2, bsz=355.7, num_updates=21700, lr=0.000429339, gnorm=0.295, train_wall=76, gb_free=10.2, wall=24128
2022-08-17 04:53:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:53:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:53:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:53:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:54:05 | INFO | valid | epoch 268 | valid on 'valid' subset | loss 5.057 | nll_loss 2.407 | ppl 5.3 | bleu 56.64 | wps 1829.3 | wpb 933.5 | bsz 59.6 | num_updates 21708 | best_bleu 57.2
2022-08-17 04:54:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 268 @ 21708 updates
2022-08-17 04:54:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint268.pt
2022-08-17 04:54:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint268.pt
2022-08-17 04:54:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint268.pt (epoch 268 @ 21708 updates, score 56.64) (writing took 28.41310052201152 seconds)
2022-08-17 04:54:34 | INFO | fairseq_cli.train | end of epoch 268 (average epoch stats below)
2022-08-17 04:54:34 | INFO | train | epoch 268 | loss 3.403 | nll_loss 0.363 | ppl 1.29 | wps 4266.5 | ups 0.77 | wpb 5523.2 | bsz 358 | num_updates 21708 | lr 0.00042926 | gnorm 0.293 | train_wall 66 | gb_free 10.1 | wall 24170
2022-08-17 04:54:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:54:34 | INFO | fairseq.trainer | begin training epoch 269
2022-08-17 04:54:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:55:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:55:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:55:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:55:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:55:47 | INFO | valid | epoch 269 | valid on 'valid' subset | loss 5.088 | nll_loss 2.443 | ppl 5.44 | bleu 56.4 | wps 1253.2 | wpb 933.5 | bsz 59.6 | num_updates 21789 | best_bleu 57.2
2022-08-17 04:55:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 269 @ 21789 updates
2022-08-17 04:55:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint269.pt
2022-08-17 04:55:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint269.pt
2022-08-17 04:56:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint269.pt (epoch 269 @ 21789 updates, score 56.4) (writing took 17.32483321428299 seconds)
2022-08-17 04:56:04 | INFO | fairseq_cli.train | end of epoch 269 (average epoch stats below)
2022-08-17 04:56:04 | INFO | train | epoch 269 | loss 3.403 | nll_loss 0.363 | ppl 1.29 | wps 4938.5 | ups 0.89 | wpb 5523.2 | bsz 358 | num_updates 21789 | lr 0.000428461 | gnorm 0.298 | train_wall 59 | gb_free 10.1 | wall 24261
2022-08-17 04:56:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:56:05 | INFO | fairseq.trainer | begin training epoch 270
2022-08-17 04:56:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:56:15 | INFO | train_inner | epoch 270:     11 / 81 loss=3.403, nll_loss=0.363, ppl=1.29, wps=3835.8, ups=0.7, wpb=5499.4, bsz=358.5, num_updates=21800, lr=0.000428353, gnorm=0.298, train_wall=72, gb_free=10.1, wall=24272
2022-08-17 04:57:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:57:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:57:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:57:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:57:20 | INFO | valid | epoch 270 | valid on 'valid' subset | loss 5.068 | nll_loss 2.414 | ppl 5.33 | bleu 56.78 | wps 1171.4 | wpb 933.5 | bsz 59.6 | num_updates 21870 | best_bleu 57.2
2022-08-17 04:57:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 270 @ 21870 updates
2022-08-17 04:57:20 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint270.pt
2022-08-17 04:57:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint270.pt
2022-08-17 04:57:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint270.pt (epoch 270 @ 21870 updates, score 56.78) (writing took 20.968252174556255 seconds)
2022-08-17 04:57:41 | INFO | fairseq_cli.train | end of epoch 270 (average epoch stats below)
2022-08-17 04:57:41 | INFO | train | epoch 270 | loss 3.403 | nll_loss 0.363 | ppl 1.29 | wps 4626.3 | ups 0.84 | wpb 5523.2 | bsz 358 | num_updates 21870 | lr 0.000427667 | gnorm 0.361 | train_wall 60 | gb_free 10.1 | wall 24357
2022-08-17 04:57:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:57:41 | INFO | fairseq.trainer | begin training epoch 271
2022-08-17 04:57:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 04:58:09 | INFO | train_inner | epoch 271:     30 / 81 loss=3.403, nll_loss=0.363, ppl=1.29, wps=4877.4, ups=0.88, wpb=5524.5, bsz=356.6, num_updates=21900, lr=0.000427374, gnorm=0.351, train_wall=77, gb_free=10.1, wall=24385
2022-08-17 04:58:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 04:58:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 04:58:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 04:58:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 04:58:54 | INFO | valid | epoch 271 | valid on 'valid' subset | loss 5.042 | nll_loss 2.384 | ppl 5.22 | bleu 56.6 | wps 1193.2 | wpb 933.5 | bsz 59.6 | num_updates 21951 | best_bleu 57.2
2022-08-17 04:58:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 271 @ 21951 updates
2022-08-17 04:58:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint271.pt
2022-08-17 04:58:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint271.pt
2022-08-17 04:59:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint271.pt (epoch 271 @ 21951 updates, score 56.6) (writing took 35.71902298927307 seconds)
2022-08-17 04:59:30 | INFO | fairseq_cli.train | end of epoch 271 (average epoch stats below)
2022-08-17 04:59:30 | INFO | train | epoch 271 | loss 3.403 | nll_loss 0.363 | ppl 1.29 | wps 4113 | ups 0.74 | wpb 5523.2 | bsz 358 | num_updates 21951 | lr 0.000426877 | gnorm 0.312 | train_wall 57 | gb_free 10.2 | wall 24466
2022-08-17 04:59:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 04:59:30 | INFO | fairseq.trainer | begin training epoch 272
2022-08-17 04:59:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:00:03 | INFO | train_inner | epoch 272:     49 / 81 loss=3.403, nll_loss=0.363, ppl=1.29, wps=4847.2, ups=0.87, wpb=5544.9, bsz=359.5, num_updates=22000, lr=0.000426401, gnorm=0.306, train_wall=63, gb_free=10.1, wall=24499
2022-08-17 05:00:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:00:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:00:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:00:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:00:45 | INFO | valid | epoch 272 | valid on 'valid' subset | loss 5.056 | nll_loss 2.401 | ppl 5.28 | bleu 55.72 | wps 1194.8 | wpb 933.5 | bsz 59.6 | num_updates 22032 | best_bleu 57.2
2022-08-17 05:00:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 272 @ 22032 updates
2022-08-17 05:00:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint272.pt
2022-08-17 05:00:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint272.pt
2022-08-17 05:01:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint272.pt (epoch 272 @ 22032 updates, score 55.72) (writing took 15.764510735869408 seconds)
2022-08-17 05:01:01 | INFO | fairseq_cli.train | end of epoch 272 (average epoch stats below)
2022-08-17 05:01:01 | INFO | train | epoch 272 | loss 3.403 | nll_loss 0.364 | ppl 1.29 | wps 4922.6 | ups 0.89 | wpb 5523.2 | bsz 358 | num_updates 22032 | lr 0.000426092 | gnorm 0.292 | train_wall 59 | gb_free 10.1 | wall 24557
2022-08-17 05:01:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:01:01 | INFO | fairseq.trainer | begin training epoch 273
2022-08-17 05:01:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:01:46 | INFO | train_inner | epoch 273:     68 / 81 loss=3.404, nll_loss=0.365, ppl=1.29, wps=5333.2, ups=0.97, wpb=5512.5, bsz=356.7, num_updates=22100, lr=0.000425436, gnorm=0.303, train_wall=72, gb_free=10.1, wall=24603
2022-08-17 05:01:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:01:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:01:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:01:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:02:11 | INFO | valid | epoch 273 | valid on 'valid' subset | loss 5.06 | nll_loss 2.408 | ppl 5.31 | bleu 56.09 | wps 1216.9 | wpb 933.5 | bsz 59.6 | num_updates 22113 | best_bleu 57.2
2022-08-17 05:02:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 273 @ 22113 updates
2022-08-17 05:02:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint273.pt
2022-08-17 05:02:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint273.pt
2022-08-17 05:02:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint273.pt (epoch 273 @ 22113 updates, score 56.09) (writing took 34.918846763670444 seconds)
2022-08-17 05:02:46 | INFO | fairseq_cli.train | end of epoch 273 (average epoch stats below)
2022-08-17 05:02:46 | INFO | train | epoch 273 | loss 3.404 | nll_loss 0.364 | ppl 1.29 | wps 4235.3 | ups 0.77 | wpb 5523.2 | bsz 358 | num_updates 22113 | lr 0.000425311 | gnorm 0.302 | train_wall 55 | gb_free 10.1 | wall 24663
2022-08-17 05:02:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:02:47 | INFO | fairseq.trainer | begin training epoch 274
2022-08-17 05:02:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:03:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:03:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:03:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:03:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:04:03 | INFO | valid | epoch 274 | valid on 'valid' subset | loss 5.058 | nll_loss 2.407 | ppl 5.3 | bleu 56.68 | wps 1152.8 | wpb 933.5 | bsz 59.6 | num_updates 22194 | best_bleu 57.2
2022-08-17 05:04:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 274 @ 22194 updates
2022-08-17 05:04:03 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint274.pt
2022-08-17 05:04:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint274.pt
2022-08-17 05:04:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint274.pt (epoch 274 @ 22194 updates, score 56.68) (writing took 14.979219488799572 seconds)
2022-08-17 05:04:18 | INFO | fairseq_cli.train | end of epoch 274 (average epoch stats below)
2022-08-17 05:04:18 | INFO | train | epoch 274 | loss 3.402 | nll_loss 0.362 | ppl 1.29 | wps 4899.4 | ups 0.89 | wpb 5523.2 | bsz 358 | num_updates 22194 | lr 0.000424534 | gnorm 0.301 | train_wall 59 | gb_free 10.2 | wall 24754
2022-08-17 05:04:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:04:18 | INFO | fairseq.trainer | begin training epoch 275
2022-08-17 05:04:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:04:22 | INFO | train_inner | epoch 275:      6 / 81 loss=3.402, nll_loss=0.363, ppl=1.29, wps=3550.3, ups=0.64, wpb=5527.2, bsz=359.2, num_updates=22200, lr=0.000424476, gnorm=0.296, train_wall=74, gb_free=10.1, wall=24758
2022-08-17 05:05:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:05:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:05:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:05:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:05:32 | INFO | valid | epoch 275 | valid on 'valid' subset | loss 5.069 | nll_loss 2.419 | ppl 5.35 | bleu 56.88 | wps 1163.4 | wpb 933.5 | bsz 59.6 | num_updates 22275 | best_bleu 57.2
2022-08-17 05:05:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 275 @ 22275 updates
2022-08-17 05:05:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint275.pt
2022-08-17 05:05:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint275.pt
2022-08-17 05:05:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint275.pt (epoch 275 @ 22275 updates, score 56.88) (writing took 22.14580838009715 seconds)
2022-08-17 05:05:55 | INFO | fairseq_cli.train | end of epoch 275 (average epoch stats below)
2022-08-17 05:05:55 | INFO | train | epoch 275 | loss 3.402 | nll_loss 0.362 | ppl 1.29 | wps 4613.1 | ups 0.84 | wpb 5523.2 | bsz 358 | num_updates 22275 | lr 0.000423761 | gnorm 0.358 | train_wall 59 | gb_free 10.1 | wall 24851
2022-08-17 05:05:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:05:55 | INFO | fairseq.trainer | begin training epoch 276
2022-08-17 05:05:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:06:13 | INFO | train_inner | epoch 276:     25 / 81 loss=3.402, nll_loss=0.362, ppl=1.29, wps=4985.2, ups=0.9, wpb=5510.9, bsz=361.4, num_updates=22300, lr=0.000423524, gnorm=0.339, train_wall=72, gb_free=10.1, wall=24869
2022-08-17 05:07:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:07:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:07:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:07:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:07:14 | INFO | valid | epoch 276 | valid on 'valid' subset | loss 5.061 | nll_loss 2.413 | ppl 5.32 | bleu 56.84 | wps 1245.1 | wpb 933.5 | bsz 59.6 | num_updates 22356 | best_bleu 57.2
2022-08-17 05:07:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 276 @ 22356 updates
2022-08-17 05:07:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint276.pt
2022-08-17 05:07:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint276.pt
2022-08-17 05:07:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint276.pt (epoch 276 @ 22356 updates, score 56.84) (writing took 15.977737609297037 seconds)
2022-08-17 05:07:31 | INFO | fairseq_cli.train | end of epoch 276 (average epoch stats below)
2022-08-17 05:07:31 | INFO | train | epoch 276 | loss 3.402 | nll_loss 0.362 | ppl 1.29 | wps 4664 | ups 0.84 | wpb 5523.2 | bsz 358 | num_updates 22356 | lr 0.000422993 | gnorm 0.293 | train_wall 64 | gb_free 10.1 | wall 24947
2022-08-17 05:07:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:07:31 | INFO | fairseq.trainer | begin training epoch 277
2022-08-17 05:07:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:08:05 | INFO | train_inner | epoch 277:     44 / 81 loss=3.401, nll_loss=0.361, ppl=1.28, wps=4942.7, ups=0.89, wpb=5550.7, bsz=363.5, num_updates=22400, lr=0.000422577, gnorm=0.3, train_wall=81, gb_free=10.1, wall=24981
2022-08-17 05:08:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:08:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:08:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:08:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:08:51 | INFO | valid | epoch 277 | valid on 'valid' subset | loss 5.063 | nll_loss 2.412 | ppl 5.32 | bleu 57.11 | wps 1187.7 | wpb 933.5 | bsz 59.6 | num_updates 22437 | best_bleu 57.2
2022-08-17 05:08:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 277 @ 22437 updates
2022-08-17 05:08:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint277.pt
2022-08-17 05:08:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint277.pt
2022-08-17 05:08:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint277.pt (epoch 277 @ 22437 updates, score 57.11) (writing took 2.3316706717014313 seconds)
2022-08-17 05:08:53 | INFO | fairseq_cli.train | end of epoch 277 (average epoch stats below)
2022-08-17 05:08:53 | INFO | train | epoch 277 | loss 3.401 | nll_loss 0.361 | ppl 1.28 | wps 5426.2 | ups 0.98 | wpb 5523.2 | bsz 358 | num_updates 22437 | lr 0.000422229 | gnorm 0.297 | train_wall 64 | gb_free 10.1 | wall 25029
2022-08-17 05:08:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:08:53 | INFO | fairseq.trainer | begin training epoch 278
2022-08-17 05:08:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:09:40 | INFO | train_inner | epoch 278:     63 / 81 loss=3.402, nll_loss=0.363, ppl=1.29, wps=5808.6, ups=1.05, wpb=5519.1, bsz=351.8, num_updates=22500, lr=0.000421637, gnorm=0.285, train_wall=74, gb_free=10.1, wall=25076
2022-08-17 05:09:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:09:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:09:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:09:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:10:09 | INFO | valid | epoch 278 | valid on 'valid' subset | loss 5.074 | nll_loss 2.429 | ppl 5.38 | bleu 56.41 | wps 1235.2 | wpb 933.5 | bsz 59.6 | num_updates 22518 | best_bleu 57.2
2022-08-17 05:10:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 278 @ 22518 updates
2022-08-17 05:10:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint278.pt
2022-08-17 05:10:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint278.pt
2022-08-17 05:10:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint278.pt (epoch 278 @ 22518 updates, score 56.41) (writing took 18.73157001286745 seconds)
2022-08-17 05:10:28 | INFO | fairseq_cli.train | end of epoch 278 (average epoch stats below)
2022-08-17 05:10:28 | INFO | train | epoch 278 | loss 3.402 | nll_loss 0.363 | ppl 1.29 | wps 4716 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 22518 | lr 0.000421468 | gnorm 0.279 | train_wall 58 | gb_free 10 | wall 25124
2022-08-17 05:10:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:10:28 | INFO | fairseq.trainer | begin training epoch 279
2022-08-17 05:10:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:11:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:11:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:11:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:11:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:11:43 | INFO | valid | epoch 279 | valid on 'valid' subset | loss 5.064 | nll_loss 2.415 | ppl 5.33 | bleu 56.9 | wps 1209.4 | wpb 933.5 | bsz 59.6 | num_updates 22599 | best_bleu 57.2
2022-08-17 05:11:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 279 @ 22599 updates
2022-08-17 05:11:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint279.pt
2022-08-17 05:11:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint279.pt
2022-08-17 05:11:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint279.pt (epoch 279 @ 22599 updates, score 56.9) (writing took 16.245633851736784 seconds)
2022-08-17 05:11:59 | INFO | fairseq_cli.train | end of epoch 279 (average epoch stats below)
2022-08-17 05:11:59 | INFO | train | epoch 279 | loss 3.403 | nll_loss 0.364 | ppl 1.29 | wps 4906.7 | ups 0.89 | wpb 5523.2 | bsz 358 | num_updates 22599 | lr 0.000420712 | gnorm 0.406 | train_wall 59 | gb_free 10.2 | wall 25215
2022-08-17 05:11:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:11:59 | INFO | fairseq.trainer | begin training epoch 280
2022-08-17 05:11:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:12:01 | INFO | train_inner | epoch 280:      1 / 81 loss=3.403, nll_loss=0.364, ppl=1.29, wps=3898, ups=0.71, wpb=5503.4, bsz=356.4, num_updates=22600, lr=0.000420703, gnorm=0.383, train_wall=75, gb_free=10.1, wall=25217
2022-08-17 05:12:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:12:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:12:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:12:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:13:11 | INFO | valid | epoch 280 | valid on 'valid' subset | loss 5.056 | nll_loss 2.407 | ppl 5.3 | bleu 56.8 | wps 1191.1 | wpb 933.5 | bsz 59.6 | num_updates 22680 | best_bleu 57.2
2022-08-17 05:13:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 280 @ 22680 updates
2022-08-17 05:13:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint280.pt
2022-08-17 05:13:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint280.pt
2022-08-17 05:13:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint280.pt (epoch 280 @ 22680 updates, score 56.8) (writing took 2.4104450158774853 seconds)
2022-08-17 05:13:14 | INFO | fairseq_cli.train | end of epoch 280 (average epoch stats below)
2022-08-17 05:13:14 | INFO | train | epoch 280 | loss 3.401 | nll_loss 0.362 | ppl 1.28 | wps 5968.1 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 22680 | lr 0.000419961 | gnorm 0.309 | train_wall 57 | gb_free 10.1 | wall 25290
2022-08-17 05:13:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:13:14 | INFO | fairseq.trainer | begin training epoch 281
2022-08-17 05:13:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:13:33 | INFO | train_inner | epoch 281:     20 / 81 loss=3.401, nll_loss=0.361, ppl=1.28, wps=6051.9, ups=1.09, wpb=5531.7, bsz=356.8, num_updates=22700, lr=0.000419775, gnorm=0.308, train_wall=73, gb_free=10, wall=25309
2022-08-17 05:14:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:14:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:14:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:14:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:14:26 | INFO | valid | epoch 281 | valid on 'valid' subset | loss 5.066 | nll_loss 2.42 | ppl 5.35 | bleu 56.5 | wps 1237.3 | wpb 933.5 | bsz 59.6 | num_updates 22761 | best_bleu 57.2
2022-08-17 05:14:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 281 @ 22761 updates
2022-08-17 05:14:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint281.pt
2022-08-17 05:14:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint281.pt
2022-08-17 05:14:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint281.pt (epoch 281 @ 22761 updates, score 56.5) (writing took 15.239214055240154 seconds)
2022-08-17 05:14:41 | INFO | fairseq_cli.train | end of epoch 281 (average epoch stats below)
2022-08-17 05:14:41 | INFO | train | epoch 281 | loss 3.402 | nll_loss 0.362 | ppl 1.29 | wps 5120.7 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 22761 | lr 0.000419213 | gnorm 0.287 | train_wall 57 | gb_free 10.2 | wall 25378
2022-08-17 05:14:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:14:42 | INFO | fairseq.trainer | begin training epoch 282
2022-08-17 05:14:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:15:15 | INFO | train_inner | epoch 282:     39 / 81 loss=3.401, nll_loss=0.362, ppl=1.29, wps=5422.6, ups=0.98, wpb=5539.2, bsz=364, num_updates=22800, lr=0.000418854, gnorm=0.288, train_wall=71, gb_free=10.1, wall=25411
2022-08-17 05:15:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:15:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:15:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:15:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:15:55 | INFO | valid | epoch 282 | valid on 'valid' subset | loss 5.058 | nll_loss 2.41 | ppl 5.32 | bleu 56.97 | wps 1127.2 | wpb 933.5 | bsz 59.6 | num_updates 22842 | best_bleu 57.2
2022-08-17 05:15:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 282 @ 22842 updates
2022-08-17 05:15:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint282.pt
2022-08-17 05:15:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint282.pt
2022-08-17 05:16:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint282.pt (epoch 282 @ 22842 updates, score 56.97) (writing took 19.539934907108545 seconds)
2022-08-17 05:16:15 | INFO | fairseq_cli.train | end of epoch 282 (average epoch stats below)
2022-08-17 05:16:15 | INFO | train | epoch 282 | loss 3.402 | nll_loss 0.362 | ppl 1.29 | wps 4801.5 | ups 0.87 | wpb 5523.2 | bsz 358 | num_updates 22842 | lr 0.000418469 | gnorm 0.308 | train_wall 57 | gb_free 10.1 | wall 25471
2022-08-17 05:16:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:16:15 | INFO | fairseq.trainer | begin training epoch 283
2022-08-17 05:16:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:16:57 | INFO | train_inner | epoch 283:     58 / 81 loss=3.401, nll_loss=0.361, ppl=1.28, wps=5430.5, ups=0.98, wpb=5537.1, bsz=357.7, num_updates=22900, lr=0.000417938, gnorm=0.323, train_wall=66, gb_free=10, wall=25513
2022-08-17 05:17:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:17:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:17:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:17:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:17:27 | INFO | valid | epoch 283 | valid on 'valid' subset | loss 5.062 | nll_loss 2.411 | ppl 5.32 | bleu 56.66 | wps 1197.1 | wpb 933.5 | bsz 59.6 | num_updates 22923 | best_bleu 57.2
2022-08-17 05:17:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 283 @ 22923 updates
2022-08-17 05:17:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint283.pt
2022-08-17 05:17:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint283.pt
2022-08-17 05:17:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint283.pt (epoch 283 @ 22923 updates, score 56.66) (writing took 15.66203610226512 seconds)
2022-08-17 05:17:43 | INFO | fairseq_cli.train | end of epoch 283 (average epoch stats below)
2022-08-17 05:17:43 | INFO | train | epoch 283 | loss 3.4 | nll_loss 0.361 | ppl 1.28 | wps 5060.5 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 22923 | lr 0.000417729 | gnorm 0.317 | train_wall 57 | gb_free 10.1 | wall 25559
2022-08-17 05:17:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:17:43 | INFO | fairseq.trainer | begin training epoch 284
2022-08-17 05:17:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:18:39 | INFO | train_inner | epoch 284:     77 / 81 loss=3.402, nll_loss=0.363, ppl=1.29, wps=5383.1, ups=0.98, wpb=5518.9, bsz=354.5, num_updates=23000, lr=0.000417029, gnorm=0.321, train_wall=71, gb_free=10.2, wall=25615
2022-08-17 05:18:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:18:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:18:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:18:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:18:57 | INFO | valid | epoch 284 | valid on 'valid' subset | loss 5.068 | nll_loss 2.417 | ppl 5.34 | bleu 56.9 | wps 1170 | wpb 933.5 | bsz 59.6 | num_updates 23004 | best_bleu 57.2
2022-08-17 05:18:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 284 @ 23004 updates
2022-08-17 05:18:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint284.pt
2022-08-17 05:18:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint284.pt
2022-08-17 05:19:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint284.pt (epoch 284 @ 23004 updates, score 56.9) (writing took 23.302852995693684 seconds)
2022-08-17 05:19:20 | INFO | fairseq_cli.train | end of epoch 284 (average epoch stats below)
2022-08-17 05:19:20 | INFO | train | epoch 284 | loss 3.401 | nll_loss 0.362 | ppl 1.29 | wps 4600.6 | ups 0.83 | wpb 5523.2 | bsz 358 | num_updates 23004 | lr 0.000416993 | gnorm 0.324 | train_wall 57 | gb_free 10.1 | wall 25656
2022-08-17 05:19:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:19:20 | INFO | fairseq.trainer | begin training epoch 285
2022-08-17 05:19:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:20:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:20:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:20:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:20:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:20:33 | INFO | valid | epoch 285 | valid on 'valid' subset | loss 5.056 | nll_loss 2.402 | ppl 5.28 | bleu 57.23 | wps 1187 | wpb 933.5 | bsz 59.6 | num_updates 23085 | best_bleu 57.23
2022-08-17 05:20:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 285 @ 23085 updates
2022-08-17 05:20:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint285.pt
2022-08-17 05:20:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint285.pt
2022-08-17 05:21:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint285.pt (epoch 285 @ 23085 updates, score 57.23) (writing took 43.28719434514642 seconds)
2022-08-17 05:21:16 | INFO | fairseq_cli.train | end of epoch 285 (average epoch stats below)
2022-08-17 05:21:16 | INFO | train | epoch 285 | loss 3.4 | nll_loss 0.36 | ppl 1.28 | wps 3860.7 | ups 0.7 | wpb 5523.2 | bsz 358 | num_updates 23085 | lr 0.00041626 | gnorm 0.415 | train_wall 57 | gb_free 10.2 | wall 25772
2022-08-17 05:21:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:21:16 | INFO | fairseq.trainer | begin training epoch 286
2022-08-17 05:21:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:21:27 | INFO | train_inner | epoch 286:     15 / 81 loss=3.4, nll_loss=0.36, ppl=1.28, wps=3277.3, ups=0.59, wpb=5511.8, bsz=357.5, num_updates=23100, lr=0.000416125, gnorm=0.384, train_wall=69, gb_free=10, wall=25784
2022-08-17 05:22:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:22:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:22:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:22:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:22:38 | INFO | valid | epoch 286 | valid on 'valid' subset | loss 5.073 | nll_loss 2.428 | ppl 5.38 | bleu 56.27 | wps 1129.3 | wpb 933.5 | bsz 59.6 | num_updates 23166 | best_bleu 57.23
2022-08-17 05:22:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 286 @ 23166 updates
2022-08-17 05:22:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint286.pt
2022-08-17 05:22:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint286.pt
2022-08-17 05:22:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint286.pt (epoch 286 @ 23166 updates, score 56.27) (writing took 21.03438002243638 seconds)
2022-08-17 05:23:00 | INFO | fairseq_cli.train | end of epoch 286 (average epoch stats below)
2022-08-17 05:23:00 | INFO | train | epoch 286 | loss 3.402 | nll_loss 0.363 | ppl 1.29 | wps 4327.3 | ups 0.78 | wpb 5523.2 | bsz 358 | num_updates 23166 | lr 0.000415532 | gnorm 0.521 | train_wall 64 | gb_free 10.1 | wall 25876
2022-08-17 05:23:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:23:00 | INFO | fairseq.trainer | begin training epoch 287
2022-08-17 05:23:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:23:30 | INFO | train_inner | epoch 287:     34 / 81 loss=3.402, nll_loss=0.363, ppl=1.29, wps=4494.3, ups=0.82, wpb=5501.2, bsz=351.2, num_updates=23200, lr=0.000415227, gnorm=0.522, train_wall=85, gb_free=10.2, wall=25906
2022-08-17 05:24:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:24:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:24:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:24:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:24:23 | INFO | valid | epoch 287 | valid on 'valid' subset | loss 5.044 | nll_loss 2.395 | ppl 5.26 | bleu 56.86 | wps 1437.9 | wpb 933.5 | bsz 59.6 | num_updates 23247 | best_bleu 57.23
2022-08-17 05:24:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 287 @ 23247 updates
2022-08-17 05:24:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint287.pt
2022-08-17 05:24:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint287.pt
2022-08-17 05:24:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint287.pt (epoch 287 @ 23247 updates, score 56.86) (writing took 21.847225084900856 seconds)
2022-08-17 05:24:45 | INFO | fairseq_cli.train | end of epoch 287 (average epoch stats below)
2022-08-17 05:24:45 | INFO | train | epoch 287 | loss 3.402 | nll_loss 0.362 | ppl 1.29 | wps 4248.1 | ups 0.77 | wpb 5523.2 | bsz 358 | num_updates 23247 | lr 0.000414807 | gnorm 0.378 | train_wall 70 | gb_free 10.1 | wall 25981
2022-08-17 05:24:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:24:45 | INFO | fairseq.trainer | begin training epoch 288
2022-08-17 05:24:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:25:31 | INFO | train_inner | epoch 288:     53 / 81 loss=3.401, nll_loss=0.362, ppl=1.29, wps=4567.6, ups=0.82, wpb=5549, bsz=365.1, num_updates=23300, lr=0.000414335, gnorm=0.361, train_wall=86, gb_free=10.1, wall=26027
2022-08-17 05:25:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:25:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:25:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:25:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:26:03 | INFO | valid | epoch 288 | valid on 'valid' subset | loss 5.069 | nll_loss 2.414 | ppl 5.33 | bleu 56.42 | wps 1758.6 | wpb 933.5 | bsz 59.6 | num_updates 23328 | best_bleu 57.23
2022-08-17 05:26:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 288 @ 23328 updates
2022-08-17 05:26:03 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint288.pt
2022-08-17 05:26:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint288.pt
2022-08-17 05:26:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint288.pt (epoch 288 @ 23328 updates, score 56.42) (writing took 14.555140819400549 seconds)
2022-08-17 05:26:18 | INFO | fairseq_cli.train | end of epoch 288 (average epoch stats below)
2022-08-17 05:26:18 | INFO | train | epoch 288 | loss 3.401 | nll_loss 0.362 | ppl 1.28 | wps 4815 | ups 0.87 | wpb 5523.2 | bsz 358 | num_updates 23328 | lr 0.000414087 | gnorm 0.34 | train_wall 67 | gb_free 10.1 | wall 26074
2022-08-17 05:26:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:26:18 | INFO | fairseq.trainer | begin training epoch 289
2022-08-17 05:26:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:27:24 | INFO | train_inner | epoch 289:     72 / 81 loss=3.401, nll_loss=0.363, ppl=1.29, wps=4912.2, ups=0.89, wpb=5519.6, bsz=358.9, num_updates=23400, lr=0.000413449, gnorm=0.342, train_wall=87, gb_free=10.1, wall=26140
2022-08-17 05:27:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:27:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:27:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:27:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:27:39 | INFO | valid | epoch 289 | valid on 'valid' subset | loss 5.063 | nll_loss 2.409 | ppl 5.31 | bleu 56.97 | wps 1568.1 | wpb 933.5 | bsz 59.6 | num_updates 23409 | best_bleu 57.23
2022-08-17 05:27:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 289 @ 23409 updates
2022-08-17 05:27:39 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint289.pt
2022-08-17 05:27:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint289.pt
2022-08-17 05:28:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint289.pt (epoch 289 @ 23409 updates, score 56.97) (writing took 25.98309440538287 seconds)
2022-08-17 05:28:05 | INFO | fairseq_cli.train | end of epoch 289 (average epoch stats below)
2022-08-17 05:28:05 | INFO | train | epoch 289 | loss 3.402 | nll_loss 0.363 | ppl 1.29 | wps 4176.3 | ups 0.76 | wpb 5523.2 | bsz 358 | num_updates 23409 | lr 0.00041337 | gnorm 0.353 | train_wall 69 | gb_free 10.2 | wall 26181
2022-08-17 05:28:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:28:05 | INFO | fairseq.trainer | begin training epoch 290
2022-08-17 05:28:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:29:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:29:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:29:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:29:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:29:18 | INFO | valid | epoch 290 | valid on 'valid' subset | loss 5.067 | nll_loss 2.422 | ppl 5.36 | bleu 56.47 | wps 1216.9 | wpb 933.5 | bsz 59.6 | num_updates 23490 | best_bleu 57.23
2022-08-17 05:29:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 290 @ 23490 updates
2022-08-17 05:29:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint290.pt
2022-08-17 05:29:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint290.pt
2022-08-17 05:29:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint290.pt (epoch 290 @ 23490 updates, score 56.47) (writing took 2.3317030742764473 seconds)
2022-08-17 05:29:21 | INFO | fairseq_cli.train | end of epoch 290 (average epoch stats below)
2022-08-17 05:29:21 | INFO | train | epoch 290 | loss 3.399 | nll_loss 0.36 | ppl 1.28 | wps 5887.5 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 23490 | lr 0.000412656 | gnorm 0.298 | train_wall 58 | gb_free 10.1 | wall 26257
2022-08-17 05:29:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:29:21 | INFO | fairseq.trainer | begin training epoch 291
2022-08-17 05:29:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:29:31 | INFO | train_inner | epoch 291:     10 / 81 loss=3.4, nll_loss=0.361, ppl=1.28, wps=4329.4, ups=0.79, wpb=5501.6, bsz=357.7, num_updates=23500, lr=0.000412568, gnorm=0.325, train_wall=71, gb_free=10, wall=26267
2022-08-17 05:30:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:30:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:30:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:30:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:30:38 | INFO | valid | epoch 291 | valid on 'valid' subset | loss 5.077 | nll_loss 2.435 | ppl 5.41 | bleu 56.38 | wps 1904.7 | wpb 933.5 | bsz 59.6 | num_updates 23571 | best_bleu 57.23
2022-08-17 05:30:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 291 @ 23571 updates
2022-08-17 05:30:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint291.pt
2022-08-17 05:30:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint291.pt
2022-08-17 05:31:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint291.pt (epoch 291 @ 23571 updates, score 56.38) (writing took 32.93554009497166 seconds)
2022-08-17 05:31:12 | INFO | fairseq_cli.train | end of epoch 291 (average epoch stats below)
2022-08-17 05:31:12 | INFO | train | epoch 291 | loss 3.401 | nll_loss 0.362 | ppl 1.29 | wps 4044.3 | ups 0.73 | wpb 5523.2 | bsz 358 | num_updates 23571 | lr 0.000411947 | gnorm 0.386 | train_wall 67 | gb_free 10.1 | wall 26368
2022-08-17 05:31:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:31:12 | INFO | fairseq.trainer | begin training epoch 292
2022-08-17 05:31:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:31:38 | INFO | train_inner | epoch 292:     29 / 81 loss=3.4, nll_loss=0.361, ppl=1.28, wps=4365.7, ups=0.79, wpb=5536.1, bsz=355, num_updates=23600, lr=0.000411693, gnorm=0.341, train_wall=83, gb_free=10.1, wall=26394
2022-08-17 05:32:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:32:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:32:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:32:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:32:24 | INFO | valid | epoch 292 | valid on 'valid' subset | loss 5.062 | nll_loss 2.411 | ppl 5.32 | bleu 56.19 | wps 1234.1 | wpb 933.5 | bsz 59.6 | num_updates 23652 | best_bleu 57.23
2022-08-17 05:32:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 292 @ 23652 updates
2022-08-17 05:32:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint292.pt
2022-08-17 05:32:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint292.pt
2022-08-17 05:32:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint292.pt (epoch 292 @ 23652 updates, score 56.19) (writing took 17.964605312794447 seconds)
2022-08-17 05:32:42 | INFO | fairseq_cli.train | end of epoch 292 (average epoch stats below)
2022-08-17 05:32:42 | INFO | train | epoch 292 | loss 3.399 | nll_loss 0.359 | ppl 1.28 | wps 4943.6 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 23652 | lr 0.000411241 | gnorm 0.298 | train_wall 58 | gb_free 10.1 | wall 26458
2022-08-17 05:32:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:32:42 | INFO | fairseq.trainer | begin training epoch 293
2022-08-17 05:32:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:33:24 | INFO | train_inner | epoch 293:     48 / 81 loss=3.4, nll_loss=0.36, ppl=1.28, wps=5178.6, ups=0.94, wpb=5513.3, bsz=357.7, num_updates=23700, lr=0.000410824, gnorm=0.294, train_wall=74, gb_free=10.1, wall=26500
2022-08-17 05:33:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:33:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:33:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:33:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:33:55 | INFO | valid | epoch 293 | valid on 'valid' subset | loss 5.072 | nll_loss 2.423 | ppl 5.36 | bleu 56.45 | wps 1266.3 | wpb 933.5 | bsz 59.6 | num_updates 23733 | best_bleu 57.23
2022-08-17 05:33:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 293 @ 23733 updates
2022-08-17 05:33:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint293.pt
2022-08-17 05:33:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint293.pt
2022-08-17 05:34:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint293.pt (epoch 293 @ 23733 updates, score 56.45) (writing took 28.77767091244459 seconds)
2022-08-17 05:34:24 | INFO | fairseq_cli.train | end of epoch 293 (average epoch stats below)
2022-08-17 05:34:24 | INFO | train | epoch 293 | loss 3.399 | nll_loss 0.36 | ppl 1.28 | wps 4376.1 | ups 0.79 | wpb 5523.2 | bsz 358 | num_updates 23733 | lr 0.000410538 | gnorm 0.323 | train_wall 58 | gb_free 10.2 | wall 26560
2022-08-17 05:34:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:34:24 | INFO | fairseq.trainer | begin training epoch 294
2022-08-17 05:34:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:35:11 | INFO | train_inner | epoch 294:     67 / 81 loss=3.399, nll_loss=0.36, ppl=1.28, wps=5211.3, ups=0.94, wpb=5565.8, bsz=362.7, num_updates=23800, lr=0.00040996, gnorm=0.347, train_wall=62, gb_free=10.1, wall=26607
2022-08-17 05:35:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:35:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:35:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:35:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:35:36 | INFO | valid | epoch 294 | valid on 'valid' subset | loss 5.071 | nll_loss 2.425 | ppl 5.37 | bleu 56.35 | wps 1182.1 | wpb 933.5 | bsz 59.6 | num_updates 23814 | best_bleu 57.23
2022-08-17 05:35:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 294 @ 23814 updates
2022-08-17 05:35:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint294.pt
2022-08-17 05:35:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint294.pt
2022-08-17 05:35:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint294.pt (epoch 294 @ 23814 updates, score 56.35) (writing took 16.657798647880554 seconds)
2022-08-17 05:35:53 | INFO | fairseq_cli.train | end of epoch 294 (average epoch stats below)
2022-08-17 05:35:53 | INFO | train | epoch 294 | loss 3.399 | nll_loss 0.36 | ppl 1.28 | wps 5064.3 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 23814 | lr 0.00040984 | gnorm 0.326 | train_wall 55 | gb_free 10.2 | wall 26649
2022-08-17 05:35:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:35:53 | INFO | fairseq.trainer | begin training epoch 295
2022-08-17 05:35:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:36:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:36:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:36:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:36:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:37:05 | INFO | valid | epoch 295 | valid on 'valid' subset | loss 5.056 | nll_loss 2.41 | ppl 5.31 | bleu 56.38 | wps 1242.7 | wpb 933.5 | bsz 59.6 | num_updates 23895 | best_bleu 57.23
2022-08-17 05:37:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 295 @ 23895 updates
2022-08-17 05:37:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint295.pt
2022-08-17 05:37:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint295.pt
2022-08-17 05:37:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint295.pt (epoch 295 @ 23895 updates, score 56.38) (writing took 15.305277448147535 seconds)
2022-08-17 05:37:21 | INFO | fairseq_cli.train | end of epoch 295 (average epoch stats below)
2022-08-17 05:37:21 | INFO | train | epoch 295 | loss 3.4 | nll_loss 0.361 | ppl 1.28 | wps 5067.2 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 23895 | lr 0.000409144 | gnorm 0.321 | train_wall 58 | gb_free 10.1 | wall 26737
2022-08-17 05:37:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:37:21 | INFO | fairseq.trainer | begin training epoch 296
2022-08-17 05:37:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:37:26 | INFO | train_inner | epoch 296:      5 / 81 loss=3.4, nll_loss=0.361, ppl=1.28, wps=4033.3, ups=0.74, wpb=5472.9, bsz=355.8, num_updates=23900, lr=0.000409101, gnorm=0.313, train_wall=73, gb_free=10, wall=26743
2022-08-17 05:38:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:38:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:38:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:38:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:38:27 | INFO | valid | epoch 296 | valid on 'valid' subset | loss 5.07 | nll_loss 2.427 | ppl 5.38 | bleu 56.94 | wps 1998.6 | wpb 933.5 | bsz 59.6 | num_updates 23976 | best_bleu 57.23
2022-08-17 05:38:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 296 @ 23976 updates
2022-08-17 05:38:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint296.pt
2022-08-17 05:38:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint296.pt
2022-08-17 05:38:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint296.pt (epoch 296 @ 23976 updates, score 56.94) (writing took 16.50208818167448 seconds)
2022-08-17 05:38:44 | INFO | fairseq_cli.train | end of epoch 296 (average epoch stats below)
2022-08-17 05:38:44 | INFO | train | epoch 296 | loss 3.399 | nll_loss 0.36 | ppl 1.28 | wps 5378.2 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 23976 | lr 0.000408453 | gnorm 0.358 | train_wall 52 | gb_free 10.1 | wall 26820
2022-08-17 05:38:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:38:44 | INFO | fairseq.trainer | begin training epoch 297
2022-08-17 05:38:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:38:57 | INFO | train_inner | epoch 297:     24 / 81 loss=3.399, nll_loss=0.36, ppl=1.28, wps=6072.1, ups=1.1, wpb=5525.5, bsz=355.8, num_updates=24000, lr=0.000408248, gnorm=0.353, train_wall=59, gb_free=10.1, wall=26834
2022-08-17 05:39:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:39:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:39:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:39:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:39:36 | INFO | valid | epoch 297 | valid on 'valid' subset | loss 5.082 | nll_loss 2.442 | ppl 5.43 | bleu 56.46 | wps 1977.2 | wpb 933.5 | bsz 59.6 | num_updates 24057 | best_bleu 57.23
2022-08-17 05:39:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 297 @ 24057 updates
2022-08-17 05:39:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint297.pt
2022-08-17 05:39:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint297.pt
2022-08-17 05:39:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint297.pt (epoch 297 @ 24057 updates, score 56.46) (writing took 13.057333026081324 seconds)
2022-08-17 05:39:49 | INFO | fairseq_cli.train | end of epoch 297 (average epoch stats below)
2022-08-17 05:39:49 | INFO | train | epoch 297 | loss 3.398 | nll_loss 0.359 | ppl 1.28 | wps 6850 | ups 1.24 | wpb 5523.2 | bsz 358 | num_updates 24057 | lr 0.000407764 | gnorm 0.289 | train_wall 37 | gb_free 10.2 | wall 26886
2022-08-17 05:39:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:39:50 | INFO | fairseq.trainer | begin training epoch 298
2022-08-17 05:39:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:40:12 | INFO | train_inner | epoch 298:     43 / 81 loss=3.398, nll_loss=0.358, ppl=1.28, wps=7396.9, ups=1.33, wpb=5548.2, bsz=365, num_updates=24100, lr=0.0004074, gnorm=0.287, train_wall=48, gb_free=10.1, wall=26909
2022-08-17 05:40:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:40:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:40:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:40:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:40:40 | INFO | valid | epoch 298 | valid on 'valid' subset | loss 5.089 | nll_loss 2.447 | ppl 5.45 | bleu 56.29 | wps 2000.6 | wpb 933.5 | bsz 59.6 | num_updates 24138 | best_bleu 57.23
2022-08-17 05:40:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 298 @ 24138 updates
2022-08-17 05:40:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint298.pt
2022-08-17 05:40:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint298.pt
2022-08-17 05:40:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint298.pt (epoch 298 @ 24138 updates, score 56.29) (writing took 17.030149210244417 seconds)
2022-08-17 05:40:57 | INFO | fairseq_cli.train | end of epoch 298 (average epoch stats below)
2022-08-17 05:40:57 | INFO | train | epoch 298 | loss 3.399 | nll_loss 0.36 | ppl 1.28 | wps 6587.1 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 24138 | lr 0.00040708 | gnorm 0.368 | train_wall 39 | gb_free 10.1 | wall 26954
2022-08-17 05:40:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:40:57 | INFO | fairseq.trainer | begin training epoch 299
2022-08-17 05:40:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:41:29 | INFO | train_inner | epoch 299:     62 / 81 loss=3.399, nll_loss=0.36, ppl=1.28, wps=7237.1, ups=1.31, wpb=5517.8, bsz=353.4, num_updates=24200, lr=0.000406558, gnorm=0.346, train_wall=47, gb_free=10.1, wall=26985
2022-08-17 05:41:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:41:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:41:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:41:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:41:47 | INFO | valid | epoch 299 | valid on 'valid' subset | loss 5.076 | nll_loss 2.432 | ppl 5.4 | bleu 56.27 | wps 1790.8 | wpb 933.5 | bsz 59.6 | num_updates 24219 | best_bleu 57.23
2022-08-17 05:41:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 299 @ 24219 updates
2022-08-17 05:41:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint299.pt
2022-08-17 05:41:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint299.pt
2022-08-17 05:42:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint299.pt (epoch 299 @ 24219 updates, score 56.27) (writing took 17.532884560525417 seconds)
2022-08-17 05:42:05 | INFO | fairseq_cli.train | end of epoch 299 (average epoch stats below)
2022-08-17 05:42:05 | INFO | train | epoch 299 | loss 3.398 | nll_loss 0.359 | ppl 1.28 | wps 6598.4 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 24219 | lr 0.000406398 | gnorm 0.278 | train_wall 39 | gb_free 10.3 | wall 27021
2022-08-17 05:42:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:42:05 | INFO | fairseq.trainer | begin training epoch 300
2022-08-17 05:42:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:42:51 | INFO | train_inner | epoch 300:     81 / 81 loss=3.399, nll_loss=0.36, ppl=1.28, wps=6714.1, ups=1.22, wpb=5505.1, bsz=357.6, num_updates=24300, lr=0.00040572, gnorm=0.281, train_wall=48, gb_free=10.2, wall=27067
2022-08-17 05:42:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:42:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:42:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:42:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:42:59 | INFO | valid | epoch 300 | valid on 'valid' subset | loss 5.082 | nll_loss 2.443 | ppl 5.44 | bleu 56.15 | wps 2048.4 | wpb 933.5 | bsz 59.6 | num_updates 24300 | best_bleu 57.23
2022-08-17 05:42:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 300 @ 24300 updates
2022-08-17 05:42:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint300.pt
2022-08-17 05:43:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint300.pt
2022-08-17 05:43:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint300.pt (epoch 300 @ 24300 updates, score 56.15) (writing took 14.82268774881959 seconds)
2022-08-17 05:43:14 | INFO | fairseq_cli.train | end of epoch 300 (average epoch stats below)
2022-08-17 05:43:14 | INFO | train | epoch 300 | loss 3.399 | nll_loss 0.36 | ppl 1.28 | wps 6472.9 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 24300 | lr 0.00040572 | gnorm 0.285 | train_wall 39 | gb_free 10.2 | wall 27090
2022-08-17 05:43:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:43:14 | INFO | fairseq.trainer | begin training epoch 301
2022-08-17 05:43:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:43:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:43:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:43:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:43:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:44:05 | INFO | valid | epoch 301 | valid on 'valid' subset | loss 5.078 | nll_loss 2.43 | ppl 5.39 | bleu 56.38 | wps 2044.3 | wpb 933.5 | bsz 59.6 | num_updates 24381 | best_bleu 57.23
2022-08-17 05:44:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 301 @ 24381 updates
2022-08-17 05:44:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint301.pt
2022-08-17 05:44:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint301.pt
2022-08-17 05:44:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint301.pt (epoch 301 @ 24381 updates, score 56.38) (writing took 17.949489302933216 seconds)
2022-08-17 05:44:23 | INFO | fairseq_cli.train | end of epoch 301 (average epoch stats below)
2022-08-17 05:44:23 | INFO | train | epoch 301 | loss 3.398 | nll_loss 0.36 | ppl 1.28 | wps 6523.5 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 24381 | lr 0.000405046 | gnorm 0.296 | train_wall 39 | gb_free 10 | wall 27159
2022-08-17 05:44:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:44:23 | INFO | fairseq.trainer | begin training epoch 302
2022-08-17 05:44:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:44:34 | INFO | train_inner | epoch 302:     19 / 81 loss=3.399, nll_loss=0.36, ppl=1.28, wps=5357, ups=0.97, wpb=5524.6, bsz=357, num_updates=24400, lr=0.000404888, gnorm=0.29, train_wall=48, gb_free=10.1, wall=27170
2022-08-17 05:45:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:45:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:45:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:45:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:45:15 | INFO | valid | epoch 302 | valid on 'valid' subset | loss 5.066 | nll_loss 2.423 | ppl 5.36 | bleu 56.53 | wps 1984.4 | wpb 933.5 | bsz 59.6 | num_updates 24462 | best_bleu 57.23
2022-08-17 05:45:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 302 @ 24462 updates
2022-08-17 05:45:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint302.pt
2022-08-17 05:45:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint302.pt
2022-08-17 05:45:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint302.pt (epoch 302 @ 24462 updates, score 56.53) (writing took 32.26858210936189 seconds)
2022-08-17 05:45:47 | INFO | fairseq_cli.train | end of epoch 302 (average epoch stats below)
2022-08-17 05:45:47 | INFO | train | epoch 302 | loss 3.399 | nll_loss 0.36 | ppl 1.28 | wps 5310.2 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 24462 | lr 0.000404375 | gnorm 0.295 | train_wall 39 | gb_free 10.2 | wall 27243
2022-08-17 05:45:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:45:47 | INFO | fairseq.trainer | begin training epoch 303
2022-08-17 05:45:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:46:08 | INFO | train_inner | epoch 303:     38 / 81 loss=3.398, nll_loss=0.359, ppl=1.28, wps=5911.4, ups=1.06, wpb=5554.8, bsz=360.4, num_updates=24500, lr=0.000404061, gnorm=0.29, train_wall=48, gb_free=10.1, wall=27264
2022-08-17 05:46:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:46:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:46:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:46:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:46:38 | INFO | valid | epoch 303 | valid on 'valid' subset | loss 5.054 | nll_loss 2.406 | ppl 5.3 | bleu 57.06 | wps 1800.4 | wpb 933.5 | bsz 59.6 | num_updates 24543 | best_bleu 57.23
2022-08-17 05:46:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 303 @ 24543 updates
2022-08-17 05:46:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint303.pt
2022-08-17 05:46:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint303.pt
2022-08-17 05:46:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint303.pt (epoch 303 @ 24543 updates, score 57.06) (writing took 2.3744863495230675 seconds)
2022-08-17 05:46:41 | INFO | fairseq_cli.train | end of epoch 303 (average epoch stats below)
2022-08-17 05:46:41 | INFO | train | epoch 303 | loss 3.398 | nll_loss 0.359 | ppl 1.28 | wps 8319.7 | ups 1.51 | wpb 5523.2 | bsz 358 | num_updates 24543 | lr 0.000403707 | gnorm 0.283 | train_wall 38 | gb_free 10.3 | wall 27297
2022-08-17 05:46:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:46:41 | INFO | fairseq.trainer | begin training epoch 304
2022-08-17 05:46:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:47:18 | INFO | train_inner | epoch 304:     57 / 81 loss=3.399, nll_loss=0.361, ppl=1.28, wps=7898.2, ups=1.43, wpb=5508.7, bsz=355.2, num_updates=24600, lr=0.000403239, gnorm=0.306, train_wall=48, gb_free=10.1, wall=27334
2022-08-17 05:47:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:47:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:47:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:47:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:47:41 | INFO | valid | epoch 304 | valid on 'valid' subset | loss 5.078 | nll_loss 2.437 | ppl 5.42 | bleu 56.48 | wps 1797 | wpb 933.5 | bsz 59.6 | num_updates 24624 | best_bleu 57.23
2022-08-17 05:47:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 304 @ 24624 updates
2022-08-17 05:47:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint304.pt
2022-08-17 05:47:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint304.pt
2022-08-17 05:47:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint304.pt (epoch 304 @ 24624 updates, score 56.48) (writing took 2.306594744324684 seconds)
2022-08-17 05:47:44 | INFO | fairseq_cli.train | end of epoch 304 (average epoch stats below)
2022-08-17 05:47:44 | INFO | train | epoch 304 | loss 3.399 | nll_loss 0.361 | ppl 1.28 | wps 7139.3 | ups 1.29 | wpb 5523.2 | bsz 358 | num_updates 24624 | lr 0.000403042 | gnorm 0.301 | train_wall 38 | gb_free 10.1 | wall 27360
2022-08-17 05:47:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:47:44 | INFO | fairseq.trainer | begin training epoch 305
2022-08-17 05:47:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:48:24 | INFO | train_inner | epoch 305:     76 / 81 loss=3.399, nll_loss=0.361, ppl=1.28, wps=8348.4, ups=1.51, wpb=5532.3, bsz=361.3, num_updates=24700, lr=0.000402422, gnorm=0.293, train_wall=47, gb_free=10.1, wall=27400
2022-08-17 05:48:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:48:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:48:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:48:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:48:35 | INFO | valid | epoch 305 | valid on 'valid' subset | loss 5.073 | nll_loss 2.427 | ppl 5.38 | bleu 56.44 | wps 2077.5 | wpb 933.5 | bsz 59.6 | num_updates 24705 | best_bleu 57.23
2022-08-17 05:48:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 305 @ 24705 updates
2022-08-17 05:48:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint305.pt
2022-08-17 05:48:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint305.pt
2022-08-17 05:49:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint305.pt (epoch 305 @ 24705 updates, score 56.44) (writing took 29.765604369342327 seconds)
2022-08-17 05:49:05 | INFO | fairseq_cli.train | end of epoch 305 (average epoch stats below)
2022-08-17 05:49:05 | INFO | train | epoch 305 | loss 3.399 | nll_loss 0.36 | ppl 1.28 | wps 5513.1 | ups 1 | wpb 5523.2 | bsz 358 | num_updates 24705 | lr 0.000402381 | gnorm 0.299 | train_wall 38 | gb_free 10.2 | wall 27441
2022-08-17 05:49:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:49:05 | INFO | fairseq.trainer | begin training epoch 306
2022-08-17 05:49:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:49:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:49:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:49:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:49:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:49:55 | INFO | valid | epoch 306 | valid on 'valid' subset | loss 5.07 | nll_loss 2.423 | ppl 5.36 | bleu 56.98 | wps 2013.3 | wpb 933.5 | bsz 59.6 | num_updates 24786 | best_bleu 57.23
2022-08-17 05:49:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 306 @ 24786 updates
2022-08-17 05:49:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint306.pt
2022-08-17 05:49:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint306.pt
2022-08-17 05:50:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint306.pt (epoch 306 @ 24786 updates, score 56.98) (writing took 15.309761837124825 seconds)
2022-08-17 05:50:11 | INFO | fairseq_cli.train | end of epoch 306 (average epoch stats below)
2022-08-17 05:50:11 | INFO | train | epoch 306 | loss 3.398 | nll_loss 0.359 | ppl 1.28 | wps 6763.5 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 24786 | lr 0.000401723 | gnorm 0.516 | train_wall 38 | gb_free 10.2 | wall 27507
2022-08-17 05:50:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:50:11 | INFO | fairseq.trainer | begin training epoch 307
2022-08-17 05:50:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:50:19 | INFO | train_inner | epoch 307:     14 / 81 loss=3.398, nll_loss=0.359, ppl=1.28, wps=4767.3, ups=0.87, wpb=5508, bsz=358.6, num_updates=24800, lr=0.00040161, gnorm=0.481, train_wall=47, gb_free=10, wall=27516
2022-08-17 05:50:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:50:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:50:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:50:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:51:02 | INFO | valid | epoch 307 | valid on 'valid' subset | loss 5.058 | nll_loss 2.413 | ppl 5.33 | bleu 57.16 | wps 1933.7 | wpb 933.5 | bsz 59.6 | num_updates 24867 | best_bleu 57.23
2022-08-17 05:51:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 307 @ 24867 updates
2022-08-17 05:51:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint307.pt
2022-08-17 05:51:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint307.pt
2022-08-17 05:51:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint307.pt (epoch 307 @ 24867 updates, score 57.16) (writing took 39.66925636678934 seconds)
2022-08-17 05:51:42 | INFO | fairseq_cli.train | end of epoch 307 (average epoch stats below)
2022-08-17 05:51:42 | INFO | train | epoch 307 | loss 3.398 | nll_loss 0.36 | ppl 1.28 | wps 4898.1 | ups 0.89 | wpb 5523.2 | bsz 358 | num_updates 24867 | lr 0.000401068 | gnorm 0.352 | train_wall 38 | gb_free 10.1 | wall 27598
2022-08-17 05:51:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:51:42 | INFO | fairseq.trainer | begin training epoch 308
2022-08-17 05:51:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:52:01 | INFO | train_inner | epoch 308:     33 / 81 loss=3.397, nll_loss=0.359, ppl=1.28, wps=5446.5, ups=0.99, wpb=5525.2, bsz=359.5, num_updates=24900, lr=0.000400802, gnorm=0.329, train_wall=48, gb_free=10.1, wall=27617
2022-08-17 05:52:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:52:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:52:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:52:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:52:34 | INFO | valid | epoch 308 | valid on 'valid' subset | loss 5.053 | nll_loss 2.408 | ppl 5.31 | bleu 56.92 | wps 2041.7 | wpb 933.5 | bsz 59.6 | num_updates 24948 | best_bleu 57.23
2022-08-17 05:52:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 308 @ 24948 updates
2022-08-17 05:52:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint308.pt
2022-08-17 05:52:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint308.pt
2022-08-17 05:52:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint308.pt (epoch 308 @ 24948 updates, score 56.92) (writing took 15.596896458417177 seconds)
2022-08-17 05:52:49 | INFO | fairseq_cli.train | end of epoch 308 (average epoch stats below)
2022-08-17 05:52:49 | INFO | train | epoch 308 | loss 3.398 | nll_loss 0.36 | ppl 1.28 | wps 6651.2 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 24948 | lr 0.000400417 | gnorm 0.301 | train_wall 38 | gb_free 10.2 | wall 27666
2022-08-17 05:52:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:52:50 | INFO | fairseq.trainer | begin training epoch 309
2022-08-17 05:52:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:53:17 | INFO | train_inner | epoch 309:     52 / 81 loss=3.399, nll_loss=0.36, ppl=1.28, wps=7265.6, ups=1.31, wpb=5547.8, bsz=359.4, num_updates=25000, lr=0.0004, gnorm=0.302, train_wall=48, gb_free=10, wall=27693
2022-08-17 05:53:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:53:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:53:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:53:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:53:40 | INFO | valid | epoch 309 | valid on 'valid' subset | loss 5.073 | nll_loss 2.43 | ppl 5.39 | bleu 56.45 | wps 2002.8 | wpb 933.5 | bsz 59.6 | num_updates 25029 | best_bleu 57.23
2022-08-17 05:53:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 309 @ 25029 updates
2022-08-17 05:53:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint309.pt
2022-08-17 05:53:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint309.pt
2022-08-17 05:53:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint309.pt (epoch 309 @ 25029 updates, score 56.45) (writing took 16.065969932824373 seconds)
2022-08-17 05:53:56 | INFO | fairseq_cli.train | end of epoch 309 (average epoch stats below)
2022-08-17 05:53:56 | INFO | train | epoch 309 | loss 3.397 | nll_loss 0.359 | ppl 1.28 | wps 6709.3 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 25029 | lr 0.000399768 | gnorm 0.301 | train_wall 39 | gb_free 10.4 | wall 27732
2022-08-17 05:53:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:53:56 | INFO | fairseq.trainer | begin training epoch 310
2022-08-17 05:53:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:54:35 | INFO | train_inner | epoch 310:     71 / 81 loss=3.397, nll_loss=0.359, ppl=1.28, wps=7112.9, ups=1.29, wpb=5509.8, bsz=353.1, num_updates=25100, lr=0.000399202, gnorm=0.305, train_wall=49, gb_free=10, wall=27771
2022-08-17 05:54:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:54:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:54:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:54:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:54:48 | INFO | valid | epoch 310 | valid on 'valid' subset | loss 5.079 | nll_loss 2.44 | ppl 5.43 | bleu 55.95 | wps 2030 | wpb 933.5 | bsz 59.6 | num_updates 25110 | best_bleu 57.23
2022-08-17 05:54:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 310 @ 25110 updates
2022-08-17 05:54:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint310.pt
2022-08-17 05:54:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint310.pt
2022-08-17 05:55:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint310.pt (epoch 310 @ 25110 updates, score 55.95) (writing took 16.47759325429797 seconds)
2022-08-17 05:55:05 | INFO | fairseq_cli.train | end of epoch 310 (average epoch stats below)
2022-08-17 05:55:05 | INFO | train | epoch 310 | loss 3.397 | nll_loss 0.359 | ppl 1.28 | wps 6486.6 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 25110 | lr 0.000399123 | gnorm 0.299 | train_wall 39 | gb_free 10.1 | wall 27801
2022-08-17 05:55:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:55:05 | INFO | fairseq.trainer | begin training epoch 311
2022-08-17 05:55:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:55:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:55:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:55:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:55:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:55:56 | INFO | valid | epoch 311 | valid on 'valid' subset | loss 5.063 | nll_loss 2.41 | ppl 5.32 | bleu 56.21 | wps 2023.1 | wpb 933.5 | bsz 59.6 | num_updates 25191 | best_bleu 57.23
2022-08-17 05:55:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 311 @ 25191 updates
2022-08-17 05:55:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint311.pt
2022-08-17 05:55:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint311.pt
2022-08-17 05:56:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint311.pt (epoch 311 @ 25191 updates, score 56.21) (writing took 27.17669876292348 seconds)
2022-08-17 05:56:24 | INFO | fairseq_cli.train | end of epoch 311 (average epoch stats below)
2022-08-17 05:56:24 | INFO | train | epoch 311 | loss 3.399 | nll_loss 0.361 | ppl 1.28 | wps 5686.1 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 25191 | lr 0.000398481 | gnorm 0.305 | train_wall 38 | gb_free 10.1 | wall 27880
2022-08-17 05:56:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:56:24 | INFO | fairseq.trainer | begin training epoch 312
2022-08-17 05:56:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:56:29 | INFO | train_inner | epoch 312:      9 / 81 loss=3.399, nll_loss=0.361, ppl=1.28, wps=4792.2, ups=0.87, wpb=5487.7, bsz=356.6, num_updates=25200, lr=0.00039841, gnorm=0.299, train_wall=46, gb_free=10.2, wall=27885
2022-08-17 05:57:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:57:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:57:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:57:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:57:13 | INFO | valid | epoch 312 | valid on 'valid' subset | loss 5.059 | nll_loss 2.416 | ppl 5.34 | bleu 56.74 | wps 2057.1 | wpb 933.5 | bsz 59.6 | num_updates 25272 | best_bleu 57.23
2022-08-17 05:57:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 312 @ 25272 updates
2022-08-17 05:57:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint312.pt
2022-08-17 05:57:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint312.pt
2022-08-17 05:57:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint312.pt (epoch 312 @ 25272 updates, score 56.74) (writing took 16.264126412570477 seconds)
2022-08-17 05:57:30 | INFO | fairseq_cli.train | end of epoch 312 (average epoch stats below)
2022-08-17 05:57:30 | INFO | train | epoch 312 | loss 3.397 | nll_loss 0.358 | ppl 1.28 | wps 6792.9 | ups 1.23 | wpb 5523.2 | bsz 358 | num_updates 25272 | lr 0.000397842 | gnorm 0.287 | train_wall 37 | gb_free 10.1 | wall 27946
2022-08-17 05:57:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:57:30 | INFO | fairseq.trainer | begin training epoch 313
2022-08-17 05:57:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:57:46 | INFO | train_inner | epoch 313:     28 / 81 loss=3.396, nll_loss=0.358, ppl=1.28, wps=7244, ups=1.3, wpb=5552.5, bsz=360.1, num_updates=25300, lr=0.000397621, gnorm=0.284, train_wall=47, gb_free=10.1, wall=27962
2022-08-17 05:58:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:58:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:58:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:58:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:58:22 | INFO | valid | epoch 313 | valid on 'valid' subset | loss 5.084 | nll_loss 2.445 | ppl 5.44 | bleu 56.65 | wps 2023.1 | wpb 933.5 | bsz 59.6 | num_updates 25353 | best_bleu 57.23
2022-08-17 05:58:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 313 @ 25353 updates
2022-08-17 05:58:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint313.pt
2022-08-17 05:58:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint313.pt
2022-08-17 05:58:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint313.pt (epoch 313 @ 25353 updates, score 56.65) (writing took 2.349822912365198 seconds)
2022-08-17 05:58:24 | INFO | fairseq_cli.train | end of epoch 313 (average epoch stats below)
2022-08-17 05:58:24 | INFO | train | epoch 313 | loss 3.396 | nll_loss 0.358 | ppl 1.28 | wps 8215.9 | ups 1.49 | wpb 5523.2 | bsz 358 | num_updates 25353 | lr 0.000397206 | gnorm 0.273 | train_wall 39 | gb_free 10.1 | wall 28000
2022-08-17 05:58:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:58:24 | INFO | fairseq.trainer | begin training epoch 314
2022-08-17 05:58:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 05:58:51 | INFO | train_inner | epoch 314:     47 / 81 loss=3.396, nll_loss=0.358, ppl=1.28, wps=8462.7, ups=1.53, wpb=5533.3, bsz=362.2, num_updates=25400, lr=0.000396838, gnorm=0.282, train_wall=48, gb_free=10.2, wall=28027
2022-08-17 05:59:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 05:59:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 05:59:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 05:59:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 05:59:17 | INFO | valid | epoch 314 | valid on 'valid' subset | loss 5.076 | nll_loss 2.429 | ppl 5.39 | bleu 56.51 | wps 2020.6 | wpb 933.5 | bsz 59.6 | num_updates 25434 | best_bleu 57.23
2022-08-17 05:59:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 314 @ 25434 updates
2022-08-17 05:59:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint314.pt
2022-08-17 05:59:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint314.pt
2022-08-17 05:59:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint314.pt (epoch 314 @ 25434 updates, score 56.51) (writing took 21.37191117182374 seconds)
2022-08-17 05:59:39 | INFO | fairseq_cli.train | end of epoch 314 (average epoch stats below)
2022-08-17 05:59:39 | INFO | train | epoch 314 | loss 3.397 | nll_loss 0.359 | ppl 1.28 | wps 5986.2 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 25434 | lr 0.000396573 | gnorm 0.298 | train_wall 39 | gb_free 10.1 | wall 28075
2022-08-17 05:59:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 05:59:39 | INFO | fairseq.trainer | begin training epoch 315
2022-08-17 05:59:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:00:13 | INFO | train_inner | epoch 315:     66 / 81 loss=3.397, nll_loss=0.359, ppl=1.28, wps=6780.4, ups=1.23, wpb=5515, bsz=355.3, num_updates=25500, lr=0.000396059, gnorm=0.3, train_wall=47, gb_free=10.1, wall=28109
2022-08-17 06:00:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:00:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:00:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:00:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:00:28 | INFO | valid | epoch 315 | valid on 'valid' subset | loss 5.066 | nll_loss 2.417 | ppl 5.34 | bleu 57.27 | wps 1986.5 | wpb 933.5 | bsz 59.6 | num_updates 25515 | best_bleu 57.27
2022-08-17 06:00:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 315 @ 25515 updates
2022-08-17 06:00:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint315.pt
2022-08-17 06:00:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint315.pt
2022-08-17 06:00:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint315.pt (epoch 315 @ 25515 updates, score 57.27) (writing took 28.050379686057568 seconds)
2022-08-17 06:00:56 | INFO | fairseq_cli.train | end of epoch 315 (average epoch stats below)
2022-08-17 06:00:56 | INFO | train | epoch 315 | loss 3.397 | nll_loss 0.359 | ppl 1.28 | wps 5768.4 | ups 1.04 | wpb 5523.2 | bsz 358 | num_updates 25515 | lr 0.000395943 | gnorm 0.298 | train_wall 38 | gb_free 10.2 | wall 28153
2022-08-17 06:00:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:00:57 | INFO | fairseq.trainer | begin training epoch 316
2022-08-17 06:00:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:01:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:01:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:01:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:01:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:01:50 | INFO | valid | epoch 316 | valid on 'valid' subset | loss 5.07 | nll_loss 2.429 | ppl 5.39 | bleu 56.46 | wps 1840.8 | wpb 933.5 | bsz 59.6 | num_updates 25596 | best_bleu 57.27
2022-08-17 06:01:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 316 @ 25596 updates
2022-08-17 06:01:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint316.pt
2022-08-17 06:01:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint316.pt
2022-08-17 06:02:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint316.pt (epoch 316 @ 25596 updates, score 56.46) (writing took 24.31989810988307 seconds)
2022-08-17 06:02:14 | INFO | fairseq_cli.train | end of epoch 316 (average epoch stats below)
2022-08-17 06:02:14 | INFO | train | epoch 316 | loss 3.397 | nll_loss 0.359 | ppl 1.28 | wps 5754.2 | ups 1.04 | wpb 5523.2 | bsz 358 | num_updates 25596 | lr 0.000395316 | gnorm 0.33 | train_wall 38 | gb_free 10.2 | wall 28230
2022-08-17 06:02:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:02:14 | INFO | fairseq.trainer | begin training epoch 317
2022-08-17 06:02:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:02:17 | INFO | train_inner | epoch 317:      4 / 81 loss=3.397, nll_loss=0.359, ppl=1.28, wps=4404.5, ups=0.8, wpb=5493.4, bsz=355.8, num_updates=25600, lr=0.000395285, gnorm=0.326, train_wall=47, gb_free=10.1, wall=28234
2022-08-17 06:02:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:02:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:02:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:02:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:03:06 | INFO | valid | epoch 317 | valid on 'valid' subset | loss 5.084 | nll_loss 2.451 | ppl 5.47 | bleu 56.48 | wps 1705.1 | wpb 933.5 | bsz 59.6 | num_updates 25677 | best_bleu 57.27
2022-08-17 06:03:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 317 @ 25677 updates
2022-08-17 06:03:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint317.pt
2022-08-17 06:03:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint317.pt
2022-08-17 06:03:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint317.pt (epoch 317 @ 25677 updates, score 56.48) (writing took 21.754389081150293 seconds)
2022-08-17 06:03:28 | INFO | fairseq_cli.train | end of epoch 317 (average epoch stats below)
2022-08-17 06:03:28 | INFO | train | epoch 317 | loss 3.397 | nll_loss 0.359 | ppl 1.28 | wps 6040.7 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 25677 | lr 0.000394692 | gnorm 0.311 | train_wall 38 | gb_free 10.1 | wall 28304
2022-08-17 06:03:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:03:28 | INFO | fairseq.trainer | begin training epoch 318
2022-08-17 06:03:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:03:41 | INFO | train_inner | epoch 318:     23 / 81 loss=3.397, nll_loss=0.359, ppl=1.28, wps=6621, ups=1.19, wpb=5543, bsz=359.4, num_updates=25700, lr=0.000394515, gnorm=0.325, train_wall=47, gb_free=10.1, wall=28317
2022-08-17 06:04:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:04:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:04:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:04:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:04:19 | INFO | valid | epoch 318 | valid on 'valid' subset | loss 5.072 | nll_loss 2.427 | ppl 5.38 | bleu 56.77 | wps 2050 | wpb 933.5 | bsz 59.6 | num_updates 25758 | best_bleu 57.27
2022-08-17 06:04:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 318 @ 25758 updates
2022-08-17 06:04:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint318.pt
2022-08-17 06:04:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint318.pt
2022-08-17 06:04:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint318.pt (epoch 318 @ 25758 updates, score 56.77) (writing took 2.5325609631836414 seconds)
2022-08-17 06:04:21 | INFO | fairseq_cli.train | end of epoch 318 (average epoch stats below)
2022-08-17 06:04:21 | INFO | train | epoch 318 | loss 3.396 | nll_loss 0.358 | ppl 1.28 | wps 8441.4 | ups 1.53 | wpb 5523.2 | bsz 358 | num_updates 25758 | lr 0.000394071 | gnorm 0.302 | train_wall 38 | gb_free 10.1 | wall 28357
2022-08-17 06:04:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:04:21 | INFO | fairseq.trainer | begin training epoch 319
2022-08-17 06:04:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:04:45 | INFO | train_inner | epoch 319:     42 / 81 loss=3.396, nll_loss=0.358, ppl=1.28, wps=8617.4, ups=1.55, wpb=5543.1, bsz=360.5, num_updates=25800, lr=0.00039375, gnorm=0.284, train_wall=47, gb_free=10.1, wall=28382
2022-08-17 06:05:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:05:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:05:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:05:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:05:15 | INFO | valid | epoch 319 | valid on 'valid' subset | loss 5.085 | nll_loss 2.446 | ppl 5.45 | bleu 56.59 | wps 1953.2 | wpb 933.5 | bsz 59.6 | num_updates 25839 | best_bleu 57.27
2022-08-17 06:05:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 319 @ 25839 updates
2022-08-17 06:05:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint319.pt
2022-08-17 06:05:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint319.pt
2022-08-17 06:05:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint319.pt (epoch 319 @ 25839 updates, score 56.59) (writing took 38.17782215774059 seconds)
2022-08-17 06:05:53 | INFO | fairseq_cli.train | end of epoch 319 (average epoch stats below)
2022-08-17 06:05:53 | INFO | train | epoch 319 | loss 3.395 | nll_loss 0.357 | ppl 1.28 | wps 4871.9 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 25839 | lr 0.000393452 | gnorm 0.376 | train_wall 38 | gb_free 10.1 | wall 28449
2022-08-17 06:05:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:05:53 | INFO | fairseq.trainer | begin training epoch 320
2022-08-17 06:05:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:06:27 | INFO | train_inner | epoch 320:     61 / 81 loss=3.395, nll_loss=0.356, ppl=1.28, wps=5474.1, ups=0.99, wpb=5556.6, bsz=358.4, num_updates=25900, lr=0.000392989, gnorm=0.328, train_wall=48, gb_free=10.1, wall=28483
2022-08-17 06:06:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:06:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:06:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:06:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:06:45 | INFO | valid | epoch 320 | valid on 'valid' subset | loss 5.081 | nll_loss 2.441 | ppl 5.43 | bleu 56.5 | wps 1979.5 | wpb 933.5 | bsz 59.6 | num_updates 25920 | best_bleu 57.27
2022-08-17 06:06:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 320 @ 25920 updates
2022-08-17 06:06:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint320.pt
2022-08-17 06:06:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint320.pt
2022-08-17 06:07:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint320.pt (epoch 320 @ 25920 updates, score 56.5) (writing took 19.91962595656514 seconds)
2022-08-17 06:07:05 | INFO | fairseq_cli.train | end of epoch 320 (average epoch stats below)
2022-08-17 06:07:05 | INFO | train | epoch 320 | loss 3.395 | nll_loss 0.356 | ppl 1.28 | wps 6180.7 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 25920 | lr 0.000392837 | gnorm 0.251 | train_wall 39 | gb_free 10.1 | wall 28522
2022-08-17 06:07:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:07:06 | INFO | fairseq.trainer | begin training epoch 321
2022-08-17 06:07:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:07:46 | INFO | train_inner | epoch 321:     80 / 81 loss=3.396, nll_loss=0.358, ppl=1.28, wps=6917.8, ups=1.26, wpb=5486.5, bsz=355.1, num_updates=26000, lr=0.000392232, gnorm=0.294, train_wall=47, gb_free=10.1, wall=28562
2022-08-17 06:07:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:07:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:07:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:07:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:07:55 | INFO | valid | epoch 321 | valid on 'valid' subset | loss 5.067 | nll_loss 2.42 | ppl 5.35 | bleu 56.75 | wps 1988.6 | wpb 933.5 | bsz 59.6 | num_updates 26001 | best_bleu 57.27
2022-08-17 06:07:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 321 @ 26001 updates
2022-08-17 06:07:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint321.pt
2022-08-17 06:07:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint321.pt
2022-08-17 06:08:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint321.pt (epoch 321 @ 26001 updates, score 56.75) (writing took 19.231594134122133 seconds)
2022-08-17 06:08:15 | INFO | fairseq_cli.train | end of epoch 321 (average epoch stats below)
2022-08-17 06:08:15 | INFO | train | epoch 321 | loss 3.396 | nll_loss 0.358 | ppl 1.28 | wps 6462 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 26001 | lr 0.000392225 | gnorm 0.297 | train_wall 38 | gb_free 10.1 | wall 28591
2022-08-17 06:08:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:08:15 | INFO | fairseq.trainer | begin training epoch 322
2022-08-17 06:08:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:08:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:08:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:08:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:08:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:09:07 | INFO | valid | epoch 322 | valid on 'valid' subset | loss 5.069 | nll_loss 2.421 | ppl 5.35 | bleu 56.37 | wps 1892.2 | wpb 933.5 | bsz 59.6 | num_updates 26082 | best_bleu 57.27
2022-08-17 06:09:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 322 @ 26082 updates
2022-08-17 06:09:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint322.pt
2022-08-17 06:09:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint322.pt
2022-08-17 06:09:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint322.pt (epoch 322 @ 26082 updates, score 56.37) (writing took 14.377507336437702 seconds)
2022-08-17 06:09:22 | INFO | fairseq_cli.train | end of epoch 322 (average epoch stats below)
2022-08-17 06:09:22 | INFO | train | epoch 322 | loss 3.396 | nll_loss 0.358 | ppl 1.28 | wps 6669.1 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 26082 | lr 0.000391615 | gnorm 0.306 | train_wall 37 | gb_free 10.3 | wall 28658
2022-08-17 06:09:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:09:22 | INFO | fairseq.trainer | begin training epoch 323
2022-08-17 06:09:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:09:32 | INFO | train_inner | epoch 323:     18 / 81 loss=3.395, nll_loss=0.357, ppl=1.28, wps=5192.9, ups=0.94, wpb=5512.4, bsz=358.8, num_updates=26100, lr=0.00039148, gnorm=0.293, train_wall=46, gb_free=10.1, wall=28669
2022-08-17 06:10:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:10:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:10:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:10:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:10:13 | INFO | valid | epoch 323 | valid on 'valid' subset | loss 5.071 | nll_loss 2.43 | ppl 5.39 | bleu 56.8 | wps 1984.9 | wpb 933.5 | bsz 59.6 | num_updates 26163 | best_bleu 57.27
2022-08-17 06:10:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 323 @ 26163 updates
2022-08-17 06:10:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint323.pt
2022-08-17 06:10:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint323.pt
2022-08-17 06:10:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint323.pt (epoch 323 @ 26163 updates, score 56.8) (writing took 16.45001647248864 seconds)
2022-08-17 06:10:29 | INFO | fairseq_cli.train | end of epoch 323 (average epoch stats below)
2022-08-17 06:10:29 | INFO | train | epoch 323 | loss 3.395 | nll_loss 0.357 | ppl 1.28 | wps 6624.9 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 26163 | lr 0.000391009 | gnorm 0.297 | train_wall 38 | gb_free 10.2 | wall 28726
2022-08-17 06:10:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:10:29 | INFO | fairseq.trainer | begin training epoch 324
2022-08-17 06:10:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:10:51 | INFO | train_inner | epoch 324:     37 / 81 loss=3.396, nll_loss=0.358, ppl=1.28, wps=7059.5, ups=1.28, wpb=5516.4, bsz=356.4, num_updates=26200, lr=0.000390732, gnorm=0.308, train_wall=48, gb_free=10, wall=28747
2022-08-17 06:11:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:11:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:11:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:11:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:11:22 | INFO | valid | epoch 324 | valid on 'valid' subset | loss 5.085 | nll_loss 2.448 | ppl 5.45 | bleu 56.75 | wps 1947 | wpb 933.5 | bsz 59.6 | num_updates 26244 | best_bleu 57.27
2022-08-17 06:11:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 324 @ 26244 updates
2022-08-17 06:11:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint324.pt
2022-08-17 06:11:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint324.pt
2022-08-17 06:11:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint324.pt (epoch 324 @ 26244 updates, score 56.75) (writing took 35.382900808006525 seconds)
2022-08-17 06:11:57 | INFO | fairseq_cli.train | end of epoch 324 (average epoch stats below)
2022-08-17 06:11:57 | INFO | train | epoch 324 | loss 3.397 | nll_loss 0.359 | ppl 1.28 | wps 5093.4 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 26244 | lr 0.000390405 | gnorm 0.304 | train_wall 38 | gb_free 10.1 | wall 28813
2022-08-17 06:11:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:11:57 | INFO | fairseq.trainer | begin training epoch 325
2022-08-17 06:11:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:12:29 | INFO | train_inner | epoch 325:     56 / 81 loss=3.396, nll_loss=0.358, ppl=1.28, wps=5605.6, ups=1.02, wpb=5518.4, bsz=357.4, num_updates=26300, lr=0.000389989, gnorm=0.282, train_wall=47, gb_free=10.1, wall=28845
2022-08-17 06:12:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:12:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:12:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:12:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:12:51 | INFO | valid | epoch 325 | valid on 'valid' subset | loss 5.092 | nll_loss 2.456 | ppl 5.49 | bleu 56.94 | wps 1964.3 | wpb 933.5 | bsz 59.6 | num_updates 26325 | best_bleu 57.27
2022-08-17 06:12:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 325 @ 26325 updates
2022-08-17 06:12:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint325.pt
2022-08-17 06:12:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint325.pt
2022-08-17 06:12:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint325.pt (epoch 325 @ 26325 updates, score 56.94) (writing took 2.615170679986477 seconds)
2022-08-17 06:12:54 | INFO | fairseq_cli.train | end of epoch 325 (average epoch stats below)
2022-08-17 06:12:54 | INFO | train | epoch 325 | loss 3.395 | nll_loss 0.356 | ppl 1.28 | wps 7845.4 | ups 1.42 | wpb 5523.2 | bsz 358 | num_updates 26325 | lr 0.000389804 | gnorm 0.276 | train_wall 39 | gb_free 10.2 | wall 28870
2022-08-17 06:12:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:12:54 | INFO | fairseq.trainer | begin training epoch 326
2022-08-17 06:12:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:13:34 | INFO | train_inner | epoch 326:     75 / 81 loss=3.396, nll_loss=0.358, ppl=1.28, wps=8549.5, ups=1.54, wpb=5553.4, bsz=362.2, num_updates=26400, lr=0.000389249, gnorm=0.441, train_wall=48, gb_free=10.1, wall=28910
2022-08-17 06:13:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:13:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:13:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:13:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:13:46 | INFO | valid | epoch 326 | valid on 'valid' subset | loss 5.09 | nll_loss 2.451 | ppl 5.47 | bleu 56.23 | wps 1939.8 | wpb 933.5 | bsz 59.6 | num_updates 26406 | best_bleu 57.27
2022-08-17 06:13:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 326 @ 26406 updates
2022-08-17 06:13:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint326.pt
2022-08-17 06:13:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint326.pt
2022-08-17 06:14:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint326.pt (epoch 326 @ 26406 updates, score 56.23) (writing took 19.157304652035236 seconds)
2022-08-17 06:14:05 | INFO | fairseq_cli.train | end of epoch 326 (average epoch stats below)
2022-08-17 06:14:05 | INFO | train | epoch 326 | loss 3.397 | nll_loss 0.359 | ppl 1.28 | wps 6328.4 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 26406 | lr 0.000389205 | gnorm 0.469 | train_wall 38 | gb_free 10.4 | wall 28941
2022-08-17 06:14:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:14:05 | INFO | fairseq.trainer | begin training epoch 327
2022-08-17 06:14:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:14:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:14:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:14:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:14:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:14:57 | INFO | valid | epoch 327 | valid on 'valid' subset | loss 5.091 | nll_loss 2.455 | ppl 5.48 | bleu 56.62 | wps 1918.7 | wpb 933.5 | bsz 59.6 | num_updates 26487 | best_bleu 57.27
2022-08-17 06:14:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 327 @ 26487 updates
2022-08-17 06:14:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint327.pt
2022-08-17 06:14:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint327.pt
2022-08-17 06:15:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint327.pt (epoch 327 @ 26487 updates, score 56.62) (writing took 15.050677366554737 seconds)
2022-08-17 06:15:12 | INFO | fairseq_cli.train | end of epoch 327 (average epoch stats below)
2022-08-17 06:15:12 | INFO | train | epoch 327 | loss 3.396 | nll_loss 0.357 | ppl 1.28 | wps 6632.5 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 26487 | lr 0.00038861 | gnorm 0.311 | train_wall 39 | gb_free 10.1 | wall 29009
2022-08-17 06:15:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:15:13 | INFO | fairseq.trainer | begin training epoch 328
2022-08-17 06:15:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:15:21 | INFO | train_inner | epoch 328:     13 / 81 loss=3.396, nll_loss=0.358, ppl=1.28, wps=5150.8, ups=0.94, wpb=5498.2, bsz=354.5, num_updates=26500, lr=0.000388514, gnorm=0.308, train_wall=48, gb_free=10, wall=29017
2022-08-17 06:15:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:15:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:15:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:15:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:16:05 | INFO | valid | epoch 328 | valid on 'valid' subset | loss 5.079 | nll_loss 2.439 | ppl 5.42 | bleu 57.03 | wps 2006.4 | wpb 933.5 | bsz 59.6 | num_updates 26568 | best_bleu 57.27
2022-08-17 06:16:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 328 @ 26568 updates
2022-08-17 06:16:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint328.pt
2022-08-17 06:16:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint328.pt
2022-08-17 06:16:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint328.pt (epoch 328 @ 26568 updates, score 57.03) (writing took 21.392404470592737 seconds)
2022-08-17 06:16:26 | INFO | fairseq_cli.train | end of epoch 328 (average epoch stats below)
2022-08-17 06:16:26 | INFO | train | epoch 328 | loss 3.396 | nll_loss 0.359 | ppl 1.28 | wps 6043.6 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 26568 | lr 0.000388017 | gnorm 0.339 | train_wall 39 | gb_free 10.1 | wall 29083
2022-08-17 06:16:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:16:27 | INFO | fairseq.trainer | begin training epoch 329
2022-08-17 06:16:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:16:43 | INFO | train_inner | epoch 329:     32 / 81 loss=3.396, nll_loss=0.359, ppl=1.28, wps=6660.7, ups=1.21, wpb=5501.8, bsz=356.9, num_updates=26600, lr=0.000387783, gnorm=0.328, train_wall=49, gb_free=10.1, wall=29099
2022-08-17 06:17:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:17:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:17:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:17:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:17:16 | INFO | valid | epoch 329 | valid on 'valid' subset | loss 5.087 | nll_loss 2.448 | ppl 5.46 | bleu 56.23 | wps 1836.7 | wpb 933.5 | bsz 59.6 | num_updates 26649 | best_bleu 57.27
2022-08-17 06:17:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 329 @ 26649 updates
2022-08-17 06:17:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint329.pt
2022-08-17 06:17:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint329.pt
2022-08-17 06:17:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint329.pt (epoch 329 @ 26649 updates, score 56.23) (writing took 20.94841769710183 seconds)
2022-08-17 06:17:37 | INFO | fairseq_cli.train | end of epoch 329 (average epoch stats below)
2022-08-17 06:17:37 | INFO | train | epoch 329 | loss 3.396 | nll_loss 0.358 | ppl 1.28 | wps 6322.4 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 26649 | lr 0.000387427 | gnorm 0.297 | train_wall 39 | gb_free 10.1 | wall 29153
2022-08-17 06:17:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:17:37 | INFO | fairseq.trainer | begin training epoch 330
2022-08-17 06:17:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:18:05 | INFO | train_inner | epoch 330:     51 / 81 loss=3.395, nll_loss=0.357, ppl=1.28, wps=6782.8, ups=1.23, wpb=5532.2, bsz=361.8, num_updates=26700, lr=0.000387056, gnorm=0.303, train_wall=47, gb_free=10.2, wall=29181
2022-08-17 06:18:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:18:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:18:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:18:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:18:28 | INFO | valid | epoch 330 | valid on 'valid' subset | loss 5.09 | nll_loss 2.457 | ppl 5.49 | bleu 56.87 | wps 1935.4 | wpb 933.5 | bsz 59.6 | num_updates 26730 | best_bleu 57.27
2022-08-17 06:18:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 330 @ 26730 updates
2022-08-17 06:18:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint330.pt
2022-08-17 06:18:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint330.pt
2022-08-17 06:18:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint330.pt (epoch 330 @ 26730 updates, score 56.87) (writing took 22.26687839254737 seconds)
2022-08-17 06:18:51 | INFO | fairseq_cli.train | end of epoch 330 (average epoch stats below)
2022-08-17 06:18:51 | INFO | train | epoch 330 | loss 3.395 | nll_loss 0.358 | ppl 1.28 | wps 6075.9 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 26730 | lr 0.000386839 | gnorm 0.297 | train_wall 38 | gb_free 10.2 | wall 29227
2022-08-17 06:18:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:18:51 | INFO | fairseq.trainer | begin training epoch 331
2022-08-17 06:18:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:19:27 | INFO | train_inner | epoch 331:     70 / 81 loss=3.396, nll_loss=0.358, ppl=1.28, wps=6737.2, ups=1.21, wpb=5558.1, bsz=354.9, num_updates=26800, lr=0.000386334, gnorm=0.294, train_wall=48, gb_free=10, wall=29264
2022-08-17 06:19:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:19:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:19:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:19:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:19:42 | INFO | valid | epoch 331 | valid on 'valid' subset | loss 5.097 | nll_loss 2.461 | ppl 5.51 | bleu 56.41 | wps 1911.8 | wpb 933.5 | bsz 59.6 | num_updates 26811 | best_bleu 57.27
2022-08-17 06:19:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 331 @ 26811 updates
2022-08-17 06:19:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint331.pt
2022-08-17 06:19:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint331.pt
2022-08-17 06:20:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint331.pt (epoch 331 @ 26811 updates, score 56.41) (writing took 42.75347213074565 seconds)
2022-08-17 06:20:25 | INFO | fairseq_cli.train | end of epoch 331 (average epoch stats below)
2022-08-17 06:20:25 | INFO | train | epoch 331 | loss 3.396 | nll_loss 0.358 | ppl 1.28 | wps 4764.4 | ups 0.86 | wpb 5523.2 | bsz 358 | num_updates 26811 | lr 0.000386254 | gnorm 0.296 | train_wall 39 | gb_free 10 | wall 29321
2022-08-17 06:20:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:20:25 | INFO | fairseq.trainer | begin training epoch 332
2022-08-17 06:20:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:21:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:21:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:21:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:21:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:21:18 | INFO | valid | epoch 332 | valid on 'valid' subset | loss 5.104 | nll_loss 2.472 | ppl 5.55 | bleu 56.48 | wps 1976.3 | wpb 933.5 | bsz 59.6 | num_updates 26892 | best_bleu 57.27
2022-08-17 06:21:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 332 @ 26892 updates
2022-08-17 06:21:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint332.pt
2022-08-17 06:21:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint332.pt
2022-08-17 06:21:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint332.pt (epoch 332 @ 26892 updates, score 56.48) (writing took 19.481250789016485 seconds)
2022-08-17 06:21:37 | INFO | fairseq_cli.train | end of epoch 332 (average epoch stats below)
2022-08-17 06:21:37 | INFO | train | epoch 332 | loss 3.393 | nll_loss 0.355 | ppl 1.28 | wps 6149.2 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 26892 | lr 0.000385672 | gnorm 0.277 | train_wall 38 | gb_free 10.1 | wall 29394
2022-08-17 06:21:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:21:38 | INFO | fairseq.trainer | begin training epoch 333
2022-08-17 06:21:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:21:43 | INFO | train_inner | epoch 333:      8 / 81 loss=3.393, nll_loss=0.355, ppl=1.28, wps=4048.1, ups=0.74, wpb=5495.6, bsz=358, num_updates=26900, lr=0.000385615, gnorm=0.279, train_wall=47, gb_free=10.1, wall=29399
2022-08-17 06:22:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:22:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:22:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:22:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:22:28 | INFO | valid | epoch 333 | valid on 'valid' subset | loss 5.088 | nll_loss 2.452 | ppl 5.47 | bleu 56.64 | wps 1989.3 | wpb 933.5 | bsz 59.6 | num_updates 26973 | best_bleu 57.27
2022-08-17 06:22:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 333 @ 26973 updates
2022-08-17 06:22:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint333.pt
2022-08-17 06:22:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint333.pt
2022-08-17 06:22:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint333.pt (epoch 333 @ 26973 updates, score 56.64) (writing took 21.63196286931634 seconds)
2022-08-17 06:22:50 | INFO | fairseq_cli.train | end of epoch 333 (average epoch stats below)
2022-08-17 06:22:50 | INFO | train | epoch 333 | loss 3.394 | nll_loss 0.357 | ppl 1.28 | wps 6189.3 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 26973 | lr 0.000385093 | gnorm 0.299 | train_wall 39 | gb_free 10.1 | wall 29466
2022-08-17 06:22:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:22:50 | INFO | fairseq.trainer | begin training epoch 334
2022-08-17 06:22:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:23:05 | INFO | train_inner | epoch 334:     27 / 81 loss=3.395, nll_loss=0.357, ppl=1.28, wps=6775.3, ups=1.22, wpb=5531.9, bsz=356.8, num_updates=27000, lr=0.0003849, gnorm=0.298, train_wall=47, gb_free=10.1, wall=29481
2022-08-17 06:23:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:23:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:23:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:23:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:23:42 | INFO | valid | epoch 334 | valid on 'valid' subset | loss 5.085 | nll_loss 2.446 | ppl 5.45 | bleu 56.86 | wps 1973.2 | wpb 933.5 | bsz 59.6 | num_updates 27054 | best_bleu 57.27
2022-08-17 06:23:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 334 @ 27054 updates
2022-08-17 06:23:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint334.pt
2022-08-17 06:23:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint334.pt
2022-08-17 06:23:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint334.pt (epoch 334 @ 27054 updates, score 56.86) (writing took 16.779523216187954 seconds)
2022-08-17 06:23:59 | INFO | fairseq_cli.train | end of epoch 334 (average epoch stats below)
2022-08-17 06:23:59 | INFO | train | epoch 334 | loss 3.395 | nll_loss 0.357 | ppl 1.28 | wps 6488.4 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 27054 | lr 0.000384516 | gnorm 0.326 | train_wall 38 | gb_free 10.2 | wall 29535
2022-08-17 06:23:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:23:59 | INFO | fairseq.trainer | begin training epoch 335
2022-08-17 06:23:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:24:23 | INFO | train_inner | epoch 335:     46 / 81 loss=3.395, nll_loss=0.357, ppl=1.28, wps=7077.7, ups=1.28, wpb=5535.6, bsz=359.2, num_updates=27100, lr=0.000384189, gnorm=0.324, train_wall=48, gb_free=10, wall=29559
2022-08-17 06:24:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:24:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:24:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:24:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:24:49 | INFO | valid | epoch 335 | valid on 'valid' subset | loss 5.082 | nll_loss 2.442 | ppl 5.43 | bleu 56.89 | wps 1954 | wpb 933.5 | bsz 59.6 | num_updates 27135 | best_bleu 57.27
2022-08-17 06:24:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 335 @ 27135 updates
2022-08-17 06:24:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint335.pt
2022-08-17 06:24:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint335.pt
2022-08-17 06:25:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint335.pt (epoch 335 @ 27135 updates, score 56.89) (writing took 18.800987128168344 seconds)
2022-08-17 06:25:08 | INFO | fairseq_cli.train | end of epoch 335 (average epoch stats below)
2022-08-17 06:25:08 | INFO | train | epoch 335 | loss 3.395 | nll_loss 0.358 | ppl 1.28 | wps 6433.8 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 27135 | lr 0.000383942 | gnorm 0.301 | train_wall 39 | gb_free 10.1 | wall 29604
2022-08-17 06:25:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:25:08 | INFO | fairseq.trainer | begin training epoch 336
2022-08-17 06:25:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:25:42 | INFO | train_inner | epoch 336:     65 / 81 loss=3.395, nll_loss=0.358, ppl=1.28, wps=6984.9, ups=1.27, wpb=5517.5, bsz=360.6, num_updates=27200, lr=0.000383482, gnorm=0.285, train_wall=49, gb_free=10, wall=29638
2022-08-17 06:25:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:25:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:25:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:25:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:25:59 | INFO | valid | epoch 336 | valid on 'valid' subset | loss 5.109 | nll_loss 2.475 | ppl 5.56 | bleu 55.8 | wps 1987.7 | wpb 933.5 | bsz 59.6 | num_updates 27216 | best_bleu 57.27
2022-08-17 06:25:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 336 @ 27216 updates
2022-08-17 06:25:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint336.pt
2022-08-17 06:26:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint336.pt
2022-08-17 06:26:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint336.pt (epoch 336 @ 27216 updates, score 55.8) (writing took 20.871353797614574 seconds)
2022-08-17 06:26:20 | INFO | fairseq_cli.train | end of epoch 336 (average epoch stats below)
2022-08-17 06:26:20 | INFO | train | epoch 336 | loss 3.394 | nll_loss 0.357 | ppl 1.28 | wps 6256.9 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 27216 | lr 0.00038337 | gnorm 0.276 | train_wall 39 | gb_free 10.2 | wall 29676
2022-08-17 06:26:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:26:20 | INFO | fairseq.trainer | begin training epoch 337
2022-08-17 06:26:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:27:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:27:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:27:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:27:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:27:12 | INFO | valid | epoch 337 | valid on 'valid' subset | loss 5.095 | nll_loss 2.459 | ppl 5.5 | bleu 56.39 | wps 2006.8 | wpb 933.5 | bsz 59.6 | num_updates 27297 | best_bleu 57.27
2022-08-17 06:27:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 337 @ 27297 updates
2022-08-17 06:27:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint337.pt
2022-08-17 06:27:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint337.pt
2022-08-17 06:27:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint337.pt (epoch 337 @ 27297 updates, score 56.39) (writing took 21.227771658450365 seconds)
2022-08-17 06:27:34 | INFO | fairseq_cli.train | end of epoch 337 (average epoch stats below)
2022-08-17 06:27:34 | INFO | train | epoch 337 | loss 3.393 | nll_loss 0.355 | ppl 1.28 | wps 6034.5 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 27297 | lr 0.000382801 | gnorm 0.456 | train_wall 39 | gb_free 10.2 | wall 29750
2022-08-17 06:27:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:27:34 | INFO | fairseq.trainer | begin training epoch 338
2022-08-17 06:27:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:27:37 | INFO | train_inner | epoch 338:      3 / 81 loss=3.393, nll_loss=0.355, ppl=1.28, wps=4800.8, ups=0.87, wpb=5506, bsz=354.6, num_updates=27300, lr=0.00038278, gnorm=0.427, train_wall=47, gb_free=10.2, wall=29753
2022-08-17 06:28:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:28:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:28:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:28:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:28:27 | INFO | valid | epoch 338 | valid on 'valid' subset | loss 5.108 | nll_loss 2.475 | ppl 5.56 | bleu 56.99 | wps 1982.4 | wpb 933.5 | bsz 59.6 | num_updates 27378 | best_bleu 57.27
2022-08-17 06:28:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 338 @ 27378 updates
2022-08-17 06:28:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint338.pt
2022-08-17 06:28:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint338.pt
2022-08-17 06:29:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint338.pt (epoch 338 @ 27378 updates, score 56.99) (writing took 39.87311998382211 seconds)
2022-08-17 06:29:07 | INFO | fairseq_cli.train | end of epoch 338 (average epoch stats below)
2022-08-17 06:29:07 | INFO | train | epoch 338 | loss 3.394 | nll_loss 0.357 | ppl 1.28 | wps 4780.3 | ups 0.87 | wpb 5523.2 | bsz 358 | num_updates 27378 | lr 0.000382234 | gnorm 0.304 | train_wall 39 | gb_free 10.1 | wall 29844
2022-08-17 06:29:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:29:08 | INFO | fairseq.trainer | begin training epoch 339
2022-08-17 06:29:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:29:20 | INFO | train_inner | epoch 339:     22 / 81 loss=3.395, nll_loss=0.357, ppl=1.28, wps=5357.4, ups=0.97, wpb=5522.4, bsz=358.5, num_updates=27400, lr=0.00038208, gnorm=0.295, train_wall=47, gb_free=10.1, wall=29856
2022-08-17 06:29:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:29:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:29:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:29:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:29:58 | INFO | valid | epoch 339 | valid on 'valid' subset | loss 5.083 | nll_loss 2.447 | ppl 5.45 | bleu 56.64 | wps 1976 | wpb 933.5 | bsz 59.6 | num_updates 27459 | best_bleu 57.27
2022-08-17 06:29:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 339 @ 27459 updates
2022-08-17 06:29:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint339.pt
2022-08-17 06:29:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint339.pt
2022-08-17 06:30:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint339.pt (epoch 339 @ 27459 updates, score 56.64) (writing took 22.490946501493454 seconds)
2022-08-17 06:30:20 | INFO | fairseq_cli.train | end of epoch 339 (average epoch stats below)
2022-08-17 06:30:20 | INFO | train | epoch 339 | loss 3.396 | nll_loss 0.358 | ppl 1.28 | wps 6127.5 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 27459 | lr 0.00038167 | gnorm 0.298 | train_wall 39 | gb_free 10.1 | wall 29917
2022-08-17 06:30:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:30:21 | INFO | fairseq.trainer | begin training epoch 340
2022-08-17 06:30:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:30:44 | INFO | train_inner | epoch 340:     41 / 81 loss=3.394, nll_loss=0.357, ppl=1.28, wps=6562.5, ups=1.19, wpb=5518.6, bsz=359.6, num_updates=27500, lr=0.000381385, gnorm=0.298, train_wall=48, gb_free=10.1, wall=29940
2022-08-17 06:31:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:31:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:31:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:31:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:31:13 | INFO | valid | epoch 340 | valid on 'valid' subset | loss 5.088 | nll_loss 2.452 | ppl 5.47 | bleu 56.36 | wps 1897.9 | wpb 933.5 | bsz 59.6 | num_updates 27540 | best_bleu 57.27
2022-08-17 06:31:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 340 @ 27540 updates
2022-08-17 06:31:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint340.pt
2022-08-17 06:31:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint340.pt
2022-08-17 06:32:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint340.pt (epoch 340 @ 27540 updates, score 56.36) (writing took 51.90025022998452 seconds)
2022-08-17 06:32:05 | INFO | fairseq_cli.train | end of epoch 340 (average epoch stats below)
2022-08-17 06:32:05 | INFO | train | epoch 340 | loss 3.394 | nll_loss 0.356 | ppl 1.28 | wps 4267.6 | ups 0.77 | wpb 5523.2 | bsz 358 | num_updates 27540 | lr 0.000381108 | gnorm 0.287 | train_wall 38 | gb_free 10.1 | wall 30022
2022-08-17 06:32:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:32:05 | INFO | fairseq.trainer | begin training epoch 341
2022-08-17 06:32:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:32:38 | INFO | train_inner | epoch 341:     60 / 81 loss=3.394, nll_loss=0.356, ppl=1.28, wps=4853.4, ups=0.88, wpb=5542.4, bsz=357.3, num_updates=27600, lr=0.000380693, gnorm=0.291, train_wall=48, gb_free=10.1, wall=30054
2022-08-17 06:32:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:32:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:32:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:32:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:32:56 | INFO | valid | epoch 341 | valid on 'valid' subset | loss 5.082 | nll_loss 2.44 | ppl 5.43 | bleu 56.46 | wps 2044.3 | wpb 933.5 | bsz 59.6 | num_updates 27621 | best_bleu 57.27
2022-08-17 06:32:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 341 @ 27621 updates
2022-08-17 06:32:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint341.pt
2022-08-17 06:32:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint341.pt
2022-08-17 06:33:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint341.pt (epoch 341 @ 27621 updates, score 56.46) (writing took 39.756581753492355 seconds)
2022-08-17 06:33:36 | INFO | fairseq_cli.train | end of epoch 341 (average epoch stats below)
2022-08-17 06:33:36 | INFO | train | epoch 341 | loss 3.394 | nll_loss 0.356 | ppl 1.28 | wps 4936.8 | ups 0.89 | wpb 5523.2 | bsz 358 | num_updates 27621 | lr 0.000380549 | gnorm 0.293 | train_wall 38 | gb_free 10.2 | wall 30112
2022-08-17 06:33:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:33:36 | INFO | fairseq.trainer | begin training epoch 342
2022-08-17 06:33:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:34:19 | INFO | train_inner | epoch 342:     79 / 81 loss=3.394, nll_loss=0.357, ppl=1.28, wps=5464.7, ups=0.99, wpb=5533.5, bsz=359, num_updates=27700, lr=0.000380006, gnorm=0.3, train_wall=46, gb_free=10.1, wall=30156
2022-08-17 06:34:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:34:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:34:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:34:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:34:29 | INFO | valid | epoch 342 | valid on 'valid' subset | loss 5.076 | nll_loss 2.435 | ppl 5.41 | bleu 56.82 | wps 1839.3 | wpb 933.5 | bsz 59.6 | num_updates 27702 | best_bleu 57.27
2022-08-17 06:34:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 342 @ 27702 updates
2022-08-17 06:34:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint342.pt
2022-08-17 06:34:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint342.pt
2022-08-17 06:35:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint342.pt (epoch 342 @ 27702 updates, score 56.82) (writing took 56.4153096228838 seconds)
2022-08-17 06:35:26 | INFO | fairseq_cli.train | end of epoch 342 (average epoch stats below)
2022-08-17 06:35:26 | INFO | train | epoch 342 | loss 3.394 | nll_loss 0.357 | ppl 1.28 | wps 4063.8 | ups 0.74 | wpb 5523.2 | bsz 358 | num_updates 27702 | lr 0.000379992 | gnorm 0.31 | train_wall 38 | gb_free 10.2 | wall 30222
2022-08-17 06:35:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:35:26 | INFO | fairseq.trainer | begin training epoch 343
2022-08-17 06:35:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:36:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:36:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:36:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:36:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:36:19 | INFO | valid | epoch 343 | valid on 'valid' subset | loss 5.086 | nll_loss 2.454 | ppl 5.48 | bleu 56.59 | wps 1926.6 | wpb 933.5 | bsz 59.6 | num_updates 27783 | best_bleu 57.27
2022-08-17 06:36:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 343 @ 27783 updates
2022-08-17 06:36:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint343.pt
2022-08-17 06:36:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint343.pt
2022-08-17 06:36:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint343.pt (epoch 343 @ 27783 updates, score 56.59) (writing took 38.98588743433356 seconds)
2022-08-17 06:36:58 | INFO | fairseq_cli.train | end of epoch 343 (average epoch stats below)
2022-08-17 06:36:58 | INFO | train | epoch 343 | loss 3.395 | nll_loss 0.358 | ppl 1.28 | wps 4875.2 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 27783 | lr 0.000379438 | gnorm 0.325 | train_wall 38 | gb_free 10.1 | wall 30314
2022-08-17 06:36:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:36:58 | INFO | fairseq.trainer | begin training epoch 344
2022-08-17 06:36:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:37:08 | INFO | train_inner | epoch 344:     17 / 81 loss=3.394, nll_loss=0.357, ppl=1.28, wps=3256.2, ups=0.59, wpb=5501, bsz=358.1, num_updates=27800, lr=0.000379322, gnorm=0.316, train_wall=47, gb_free=10.1, wall=30324
2022-08-17 06:37:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:37:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:37:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:37:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:37:48 | INFO | valid | epoch 344 | valid on 'valid' subset | loss 5.09 | nll_loss 2.453 | ppl 5.48 | bleu 56.43 | wps 1801.3 | wpb 933.5 | bsz 59.6 | num_updates 27864 | best_bleu 57.27
2022-08-17 06:37:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 344 @ 27864 updates
2022-08-17 06:37:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint344.pt
2022-08-17 06:37:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint344.pt
2022-08-17 06:38:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint344.pt (epoch 344 @ 27864 updates, score 56.43) (writing took 58.2995322458446 seconds)
2022-08-17 06:38:47 | INFO | fairseq_cli.train | end of epoch 344 (average epoch stats below)
2022-08-17 06:38:47 | INFO | train | epoch 344 | loss 3.393 | nll_loss 0.355 | ppl 1.28 | wps 4114.1 | ups 0.74 | wpb 5523.2 | bsz 358 | num_updates 27864 | lr 0.000378886 | gnorm 0.329 | train_wall 37 | gb_free 10 | wall 30423
2022-08-17 06:38:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:38:47 | INFO | fairseq.trainer | begin training epoch 345
2022-08-17 06:38:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:39:05 | INFO | train_inner | epoch 345:     36 / 81 loss=3.393, nll_loss=0.355, ppl=1.28, wps=4726.1, ups=0.85, wpb=5527.8, bsz=354.9, num_updates=27900, lr=0.000378641, gnorm=0.339, train_wall=45, gb_free=10.1, wall=30441
2022-08-17 06:39:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:39:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:39:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:39:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:39:37 | INFO | valid | epoch 345 | valid on 'valid' subset | loss 5.074 | nll_loss 2.437 | ppl 5.42 | bleu 57.26 | wps 1968.7 | wpb 933.5 | bsz 59.6 | num_updates 27945 | best_bleu 57.27
2022-08-17 06:39:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 345 @ 27945 updates
2022-08-17 06:39:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint345.pt
2022-08-17 06:39:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint345.pt
2022-08-17 06:40:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint345.pt (epoch 345 @ 27945 updates, score 57.26) (writing took 42.98632878437638 seconds)
2022-08-17 06:40:20 | INFO | fairseq_cli.train | end of epoch 345 (average epoch stats below)
2022-08-17 06:40:20 | INFO | train | epoch 345 | loss 3.394 | nll_loss 0.356 | ppl 1.28 | wps 4774.6 | ups 0.86 | wpb 5523.2 | bsz 358 | num_updates 27945 | lr 0.000378336 | gnorm 0.317 | train_wall 37 | gb_free 10.2 | wall 30516
2022-08-17 06:40:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:40:20 | INFO | fairseq.trainer | begin training epoch 346
2022-08-17 06:40:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:40:50 | INFO | train_inner | epoch 346:     55 / 81 loss=3.393, nll_loss=0.355, ppl=1.28, wps=5332.4, ups=0.96, wpb=5569.6, bsz=361, num_updates=28000, lr=0.000377964, gnorm=0.298, train_wall=48, gb_free=10.1, wall=30546
2022-08-17 06:41:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:41:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:41:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:41:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:41:11 | INFO | valid | epoch 346 | valid on 'valid' subset | loss 5.077 | nll_loss 2.44 | ppl 5.43 | bleu 56.25 | wps 1992.2 | wpb 933.5 | bsz 59.6 | num_updates 28026 | best_bleu 57.27
2022-08-17 06:41:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 346 @ 28026 updates
2022-08-17 06:41:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint346.pt
2022-08-17 06:41:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint346.pt
2022-08-17 06:42:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint346.pt (epoch 346 @ 28026 updates, score 56.25) (writing took 55.82238819077611 seconds)
2022-08-17 06:42:07 | INFO | fairseq_cli.train | end of epoch 346 (average epoch stats below)
2022-08-17 06:42:07 | INFO | train | epoch 346 | loss 3.393 | nll_loss 0.355 | ppl 1.28 | wps 4179.6 | ups 0.76 | wpb 5523.2 | bsz 358 | num_updates 28026 | lr 0.000377789 | gnorm 0.287 | train_wall 39 | gb_free 10.2 | wall 30624
2022-08-17 06:42:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:42:07 | INFO | fairseq.trainer | begin training epoch 347
2022-08-17 06:42:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:42:51 | INFO | train_inner | epoch 347:     74 / 81 loss=3.393, nll_loss=0.356, ppl=1.28, wps=4554.1, ups=0.83, wpb=5503.4, bsz=357.8, num_updates=28100, lr=0.000377291, gnorm=0.298, train_wall=46, gb_free=10.1, wall=30667
2022-08-17 06:42:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:42:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:42:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:42:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:43:03 | INFO | valid | epoch 347 | valid on 'valid' subset | loss 5.093 | nll_loss 2.459 | ppl 5.5 | bleu 56.77 | wps 2114.3 | wpb 933.5 | bsz 59.6 | num_updates 28107 | best_bleu 57.27
2022-08-17 06:43:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 347 @ 28107 updates
2022-08-17 06:43:03 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint347.pt
2022-08-17 06:43:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint347.pt
2022-08-17 06:43:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint347.pt (epoch 347 @ 28107 updates, score 56.77) (writing took 38.96929696947336 seconds)
2022-08-17 06:43:42 | INFO | fairseq_cli.train | end of epoch 347 (average epoch stats below)
2022-08-17 06:43:42 | INFO | train | epoch 347 | loss 3.393 | nll_loss 0.356 | ppl 1.28 | wps 4724.8 | ups 0.86 | wpb 5523.2 | bsz 358 | num_updates 28107 | lr 0.000377244 | gnorm 0.302 | train_wall 37 | gb_free 10.1 | wall 30718
2022-08-17 06:43:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:43:42 | INFO | fairseq.trainer | begin training epoch 348
2022-08-17 06:43:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:44:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:44:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:44:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:44:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:44:34 | INFO | valid | epoch 348 | valid on 'valid' subset | loss 5.108 | nll_loss 2.481 | ppl 5.58 | bleu 56.47 | wps 2057 | wpb 933.5 | bsz 59.6 | num_updates 28188 | best_bleu 57.27
2022-08-17 06:44:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 348 @ 28188 updates
2022-08-17 06:44:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint348.pt
2022-08-17 06:44:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint348.pt
2022-08-17 06:45:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint348.pt (epoch 348 @ 28188 updates, score 56.47) (writing took 50.964787632226944 seconds)
2022-08-17 06:45:25 | INFO | fairseq_cli.train | end of epoch 348 (average epoch stats below)
2022-08-17 06:45:25 | INFO | train | epoch 348 | loss 3.393 | nll_loss 0.356 | ppl 1.28 | wps 4348.4 | ups 0.79 | wpb 5523.2 | bsz 358 | num_updates 28188 | lr 0.000376702 | gnorm 0.3 | train_wall 37 | gb_free 10.1 | wall 30821
2022-08-17 06:45:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:45:25 | INFO | fairseq.trainer | begin training epoch 349
2022-08-17 06:45:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:45:33 | INFO | train_inner | epoch 349:     12 / 81 loss=3.393, nll_loss=0.355, ppl=1.28, wps=3392.5, ups=0.62, wpb=5497.8, bsz=359.8, num_updates=28200, lr=0.000376622, gnorm=0.293, train_wall=45, gb_free=10.1, wall=30829
2022-08-17 06:46:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:46:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:46:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:46:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:46:16 | INFO | valid | epoch 349 | valid on 'valid' subset | loss 5.088 | nll_loss 2.451 | ppl 5.47 | bleu 56.51 | wps 2099.2 | wpb 933.5 | bsz 59.6 | num_updates 28269 | best_bleu 57.27
2022-08-17 06:46:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 349 @ 28269 updates
2022-08-17 06:46:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint349.pt
2022-08-17 06:46:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint349.pt
2022-08-17 06:47:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint349.pt (epoch 349 @ 28269 updates, score 56.51) (writing took 43.78343207016587 seconds)
2022-08-17 06:47:00 | INFO | fairseq_cli.train | end of epoch 349 (average epoch stats below)
2022-08-17 06:47:00 | INFO | train | epoch 349 | loss 3.393 | nll_loss 0.356 | ppl 1.28 | wps 4708.2 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 28269 | lr 0.000376162 | gnorm 0.335 | train_wall 40 | gb_free 10.4 | wall 30916
2022-08-17 06:47:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:47:00 | INFO | fairseq.trainer | begin training epoch 350
2022-08-17 06:47:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:47:17 | INFO | train_inner | epoch 350:     31 / 81 loss=3.394, nll_loss=0.356, ppl=1.28, wps=5306.5, ups=0.96, wpb=5523.6, bsz=354.6, num_updates=28300, lr=0.000375956, gnorm=0.335, train_wall=50, gb_free=10.1, wall=30933
2022-08-17 06:47:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:47:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:47:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:47:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:47:50 | INFO | valid | epoch 350 | valid on 'valid' subset | loss 5.084 | nll_loss 2.443 | ppl 5.44 | bleu 56.4 | wps 2075.3 | wpb 933.5 | bsz 59.6 | num_updates 28350 | best_bleu 57.27
2022-08-17 06:47:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 350 @ 28350 updates
2022-08-17 06:47:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint350.pt
2022-08-17 06:47:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint350.pt
2022-08-17 06:48:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint350.pt (epoch 350 @ 28350 updates, score 56.4) (writing took 15.933258935809135 seconds)
2022-08-17 06:48:06 | INFO | fairseq_cli.train | end of epoch 350 (average epoch stats below)
2022-08-17 06:48:06 | INFO | train | epoch 350 | loss 3.394 | nll_loss 0.357 | ppl 1.28 | wps 6791.1 | ups 1.23 | wpb 5523.2 | bsz 358 | num_updates 28350 | lr 0.000375624 | gnorm 0.304 | train_wall 40 | gb_free 10.1 | wall 30982
2022-08-17 06:48:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:48:06 | INFO | fairseq.trainer | begin training epoch 351
2022-08-17 06:48:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:48:31 | INFO | train_inner | epoch 351:     50 / 81 loss=3.393, nll_loss=0.356, ppl=1.28, wps=7372.6, ups=1.34, wpb=5515.8, bsz=358.9, num_updates=28400, lr=0.000375293, gnorm=0.317, train_wall=48, gb_free=10.1, wall=31008
2022-08-17 06:48:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:48:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:48:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:48:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:48:54 | INFO | valid | epoch 351 | valid on 'valid' subset | loss 5.09 | nll_loss 2.448 | ppl 5.46 | bleu 56.96 | wps 2059.3 | wpb 933.5 | bsz 59.6 | num_updates 28431 | best_bleu 57.27
2022-08-17 06:48:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 351 @ 28431 updates
2022-08-17 06:48:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint351.pt
2022-08-17 06:48:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint351.pt
2022-08-17 06:49:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint351.pt (epoch 351 @ 28431 updates, score 56.96) (writing took 17.545785695314407 seconds)
2022-08-17 06:49:12 | INFO | fairseq_cli.train | end of epoch 351 (average epoch stats below)
2022-08-17 06:49:12 | INFO | train | epoch 351 | loss 3.393 | nll_loss 0.355 | ppl 1.28 | wps 6750.8 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 28431 | lr 0.000375089 | gnorm 0.32 | train_wall 38 | gb_free 10.1 | wall 31048
2022-08-17 06:49:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:49:12 | INFO | fairseq.trainer | begin training epoch 352
2022-08-17 06:49:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:49:49 | INFO | train_inner | epoch 352:     69 / 81 loss=3.393, nll_loss=0.356, ppl=1.28, wps=7139.6, ups=1.29, wpb=5546.2, bsz=359.2, num_updates=28500, lr=0.000374634, gnorm=0.302, train_wall=47, gb_free=10.1, wall=31085
2022-08-17 06:49:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:49:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:49:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:49:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:50:04 | INFO | valid | epoch 352 | valid on 'valid' subset | loss 5.084 | nll_loss 2.448 | ppl 5.46 | bleu 56.37 | wps 1979.1 | wpb 933.5 | bsz 59.6 | num_updates 28512 | best_bleu 57.27
2022-08-17 06:50:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 352 @ 28512 updates
2022-08-17 06:50:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint352.pt
2022-08-17 06:50:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint352.pt
2022-08-17 06:50:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint352.pt (epoch 352 @ 28512 updates, score 56.37) (writing took 17.115311585366726 seconds)
2022-08-17 06:50:21 | INFO | fairseq_cli.train | end of epoch 352 (average epoch stats below)
2022-08-17 06:50:21 | INFO | train | epoch 352 | loss 3.393 | nll_loss 0.356 | ppl 1.28 | wps 6461.1 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 28512 | lr 0.000374555 | gnorm 0.301 | train_wall 39 | gb_free 10.2 | wall 31118
2022-08-17 06:50:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:50:21 | INFO | fairseq.trainer | begin training epoch 353
2022-08-17 06:50:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:51:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:51:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:51:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:51:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:51:14 | INFO | valid | epoch 353 | valid on 'valid' subset | loss 5.074 | nll_loss 2.436 | ppl 5.41 | bleu 56.64 | wps 1991.7 | wpb 933.5 | bsz 59.6 | num_updates 28593 | best_bleu 57.27
2022-08-17 06:51:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 353 @ 28593 updates
2022-08-17 06:51:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint353.pt
2022-08-17 06:51:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint353.pt
2022-08-17 06:51:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint353.pt (epoch 353 @ 28593 updates, score 56.64) (writing took 17.149312186986208 seconds)
2022-08-17 06:51:31 | INFO | fairseq_cli.train | end of epoch 353 (average epoch stats below)
2022-08-17 06:51:31 | INFO | train | epoch 353 | loss 3.393 | nll_loss 0.355 | ppl 1.28 | wps 6425.4 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 28593 | lr 0.000374025 | gnorm 0.295 | train_wall 39 | gb_free 10.1 | wall 31187
2022-08-17 06:51:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:51:31 | INFO | fairseq.trainer | begin training epoch 354
2022-08-17 06:51:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:51:36 | INFO | train_inner | epoch 354:      7 / 81 loss=3.393, nll_loss=0.355, ppl=1.28, wps=5172.8, ups=0.94, wpb=5506.6, bsz=357.9, num_updates=28600, lr=0.000373979, gnorm=0.298, train_wall=48, gb_free=10.1, wall=31192
2022-08-17 06:52:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:52:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:52:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:52:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:52:22 | INFO | valid | epoch 354 | valid on 'valid' subset | loss 5.091 | nll_loss 2.459 | ppl 5.5 | bleu 56.7 | wps 1980.3 | wpb 933.5 | bsz 59.6 | num_updates 28674 | best_bleu 57.27
2022-08-17 06:52:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 354 @ 28674 updates
2022-08-17 06:52:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint354.pt
2022-08-17 06:52:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint354.pt
2022-08-17 06:52:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint354.pt (epoch 354 @ 28674 updates, score 56.7) (writing took 18.781256675720215 seconds)
2022-08-17 06:52:40 | INFO | fairseq_cli.train | end of epoch 354 (average epoch stats below)
2022-08-17 06:52:40 | INFO | train | epoch 354 | loss 3.393 | nll_loss 0.355 | ppl 1.28 | wps 6435.1 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 28674 | lr 0.000373496 | gnorm 0.292 | train_wall 37 | gb_free 10.1 | wall 31257
2022-08-17 06:52:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:52:41 | INFO | fairseq.trainer | begin training epoch 355
2022-08-17 06:52:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:52:55 | INFO | train_inner | epoch 355:     26 / 81 loss=3.392, nll_loss=0.355, ppl=1.28, wps=6997, ups=1.26, wpb=5544.2, bsz=359.1, num_updates=28700, lr=0.000373327, gnorm=0.281, train_wall=47, gb_free=10.1, wall=31271
2022-08-17 06:53:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:53:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:53:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:53:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:53:32 | INFO | valid | epoch 355 | valid on 'valid' subset | loss 5.092 | nll_loss 2.454 | ppl 5.48 | bleu 56.57 | wps 1997.1 | wpb 933.5 | bsz 59.6 | num_updates 28755 | best_bleu 57.27
2022-08-17 06:53:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 355 @ 28755 updates
2022-08-17 06:53:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint355.pt
2022-08-17 06:53:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint355.pt
2022-08-17 06:53:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint355.pt (epoch 355 @ 28755 updates, score 56.57) (writing took 19.476200196892023 seconds)
2022-08-17 06:53:52 | INFO | fairseq_cli.train | end of epoch 355 (average epoch stats below)
2022-08-17 06:53:52 | INFO | train | epoch 355 | loss 3.392 | nll_loss 0.354 | ppl 1.28 | wps 6260.3 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 28755 | lr 0.000372969 | gnorm 0.273 | train_wall 39 | gb_free 10.1 | wall 31328
2022-08-17 06:53:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:53:52 | INFO | fairseq.trainer | begin training epoch 356
2022-08-17 06:53:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:54:19 | INFO | train_inner | epoch 356:     45 / 81 loss=3.393, nll_loss=0.355, ppl=1.28, wps=6530.8, ups=1.19, wpb=5498, bsz=361.4, num_updates=28800, lr=0.000372678, gnorm=0.29, train_wall=48, gb_free=10.1, wall=31355
2022-08-17 06:54:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:54:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:54:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:54:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:54:47 | INFO | valid | epoch 356 | valid on 'valid' subset | loss 5.081 | nll_loss 2.447 | ppl 5.45 | bleu 56.56 | wps 1990.4 | wpb 933.5 | bsz 59.6 | num_updates 28836 | best_bleu 57.27
2022-08-17 06:54:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 356 @ 28836 updates
2022-08-17 06:54:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint356.pt
2022-08-17 06:54:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint356.pt
2022-08-17 06:55:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint356.pt (epoch 356 @ 28836 updates, score 56.56) (writing took 15.191654771566391 seconds)
2022-08-17 06:55:02 | INFO | fairseq_cli.train | end of epoch 356 (average epoch stats below)
2022-08-17 06:55:02 | INFO | train | epoch 356 | loss 3.393 | nll_loss 0.356 | ppl 1.28 | wps 6355.7 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 28836 | lr 0.000372445 | gnorm 0.297 | train_wall 39 | gb_free 10.1 | wall 31399
2022-08-17 06:55:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:55:02 | INFO | fairseq.trainer | begin training epoch 357
2022-08-17 06:55:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:55:36 | INFO | train_inner | epoch 357:     64 / 81 loss=3.393, nll_loss=0.356, ppl=1.28, wps=7215.8, ups=1.3, wpb=5542.1, bsz=353.4, num_updates=28900, lr=0.000372033, gnorm=0.281, train_wall=48, gb_free=10.1, wall=31432
2022-08-17 06:55:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:55:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:55:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:55:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:55:53 | INFO | valid | epoch 357 | valid on 'valid' subset | loss 5.08 | nll_loss 2.444 | ppl 5.44 | bleu 56.88 | wps 1844.7 | wpb 933.5 | bsz 59.6 | num_updates 28917 | best_bleu 57.27
2022-08-17 06:55:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 357 @ 28917 updates
2022-08-17 06:55:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint357.pt
2022-08-17 06:55:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint357.pt
2022-08-17 06:56:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint357.pt (epoch 357 @ 28917 updates, score 56.88) (writing took 25.82195470109582 seconds)
2022-08-17 06:56:19 | INFO | fairseq_cli.train | end of epoch 357 (average epoch stats below)
2022-08-17 06:56:19 | INFO | train | epoch 357 | loss 3.392 | nll_loss 0.355 | ppl 1.28 | wps 5805.4 | ups 1.05 | wpb 5523.2 | bsz 358 | num_updates 28917 | lr 0.000371923 | gnorm 0.267 | train_wall 39 | gb_free 10.2 | wall 31476
2022-08-17 06:56:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:56:20 | INFO | fairseq.trainer | begin training epoch 358
2022-08-17 06:56:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:57:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:57:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:57:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:57:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:57:11 | INFO | valid | epoch 358 | valid on 'valid' subset | loss 5.089 | nll_loss 2.454 | ppl 5.48 | bleu 56.79 | wps 1964.7 | wpb 933.5 | bsz 59.6 | num_updates 28998 | best_bleu 57.27
2022-08-17 06:57:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 358 @ 28998 updates
2022-08-17 06:57:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint358.pt
2022-08-17 06:57:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint358.pt
2022-08-17 06:57:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint358.pt (epoch 358 @ 28998 updates, score 56.79) (writing took 26.046719279140234 seconds)
2022-08-17 06:57:37 | INFO | fairseq_cli.train | end of epoch 358 (average epoch stats below)
2022-08-17 06:57:37 | INFO | train | epoch 358 | loss 3.392 | nll_loss 0.355 | ppl 1.28 | wps 5762.3 | ups 1.04 | wpb 5523.2 | bsz 358 | num_updates 28998 | lr 0.000371403 | gnorm 0.361 | train_wall 40 | gb_free 10.3 | wall 31553
2022-08-17 06:57:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:57:37 | INFO | fairseq.trainer | begin training epoch 359
2022-08-17 06:57:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:57:39 | INFO | train_inner | epoch 359:      2 / 81 loss=3.392, nll_loss=0.355, ppl=1.28, wps=4456.3, ups=0.81, wpb=5490.1, bsz=356.9, num_updates=29000, lr=0.000371391, gnorm=0.339, train_wall=49, gb_free=10.1, wall=31555
2022-08-17 06:58:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:58:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:58:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:58:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:58:30 | INFO | valid | epoch 359 | valid on 'valid' subset | loss 5.081 | nll_loss 2.443 | ppl 5.44 | bleu 56.95 | wps 1856.5 | wpb 933.5 | bsz 59.6 | num_updates 29079 | best_bleu 57.27
2022-08-17 06:58:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 359 @ 29079 updates
2022-08-17 06:58:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint359.pt
2022-08-17 06:58:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint359.pt
2022-08-17 06:58:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint359.pt (epoch 359 @ 29079 updates, score 56.95) (writing took 18.83048127964139 seconds)
2022-08-17 06:58:49 | INFO | fairseq_cli.train | end of epoch 359 (average epoch stats below)
2022-08-17 06:58:49 | INFO | train | epoch 359 | loss 3.392 | nll_loss 0.355 | ppl 1.28 | wps 6188.4 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 29079 | lr 0.000370886 | gnorm 0.301 | train_wall 41 | gb_free 10.1 | wall 31626
2022-08-17 06:58:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 06:58:50 | INFO | fairseq.trainer | begin training epoch 360
2022-08-17 06:58:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 06:59:03 | INFO | train_inner | epoch 360:     21 / 81 loss=3.392, nll_loss=0.355, ppl=1.28, wps=6592.3, ups=1.19, wpb=5525, bsz=357.2, num_updates=29100, lr=0.000370752, gnorm=0.349, train_wall=51, gb_free=10.1, wall=31639
2022-08-17 06:59:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 06:59:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 06:59:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 06:59:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 06:59:45 | INFO | valid | epoch 360 | valid on 'valid' subset | loss 5.081 | nll_loss 2.448 | ppl 5.46 | bleu 56.85 | wps 1921.3 | wpb 933.5 | bsz 59.6 | num_updates 29160 | best_bleu 57.27
2022-08-17 06:59:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 360 @ 29160 updates
2022-08-17 06:59:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint360.pt
2022-08-17 06:59:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint360.pt
2022-08-17 07:00:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint360.pt (epoch 360 @ 29160 updates, score 56.85) (writing took 35.717958599328995 seconds)
2022-08-17 07:00:21 | INFO | fairseq_cli.train | end of epoch 360 (average epoch stats below)
2022-08-17 07:00:21 | INFO | train | epoch 360 | loss 3.392 | nll_loss 0.355 | ppl 1.28 | wps 4896 | ups 0.89 | wpb 5523.2 | bsz 358 | num_updates 29160 | lr 0.00037037 | gnorm 0.368 | train_wall 41 | gb_free 10.1 | wall 31717
2022-08-17 07:00:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:00:21 | INFO | fairseq.trainer | begin training epoch 361
2022-08-17 07:00:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:00:45 | INFO | train_inner | epoch 361:     40 / 81 loss=3.392, nll_loss=0.354, ppl=1.28, wps=5409.9, ups=0.98, wpb=5543, bsz=361.9, num_updates=29200, lr=0.000370117, gnorm=0.295, train_wall=51, gb_free=10, wall=31742
2022-08-17 07:01:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:01:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:01:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:01:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:01:16 | INFO | valid | epoch 361 | valid on 'valid' subset | loss 5.095 | nll_loss 2.459 | ppl 5.5 | bleu 56.08 | wps 1788.5 | wpb 933.5 | bsz 59.6 | num_updates 29241 | best_bleu 57.27
2022-08-17 07:01:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 361 @ 29241 updates
2022-08-17 07:01:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint361.pt
2022-08-17 07:01:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint361.pt
2022-08-17 07:01:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint361.pt (epoch 361 @ 29241 updates, score 56.08) (writing took 28.02796520665288 seconds)
2022-08-17 07:01:44 | INFO | fairseq_cli.train | end of epoch 361 (average epoch stats below)
2022-08-17 07:01:44 | INFO | train | epoch 361 | loss 3.392 | nll_loss 0.355 | ppl 1.28 | wps 5380.1 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 29241 | lr 0.000369857 | gnorm 0.288 | train_wall 41 | gb_free 10.3 | wall 31800
2022-08-17 07:01:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:01:44 | INFO | fairseq.trainer | begin training epoch 362
2022-08-17 07:01:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:02:15 | INFO | train_inner | epoch 362:     59 / 81 loss=3.393, nll_loss=0.356, ppl=1.28, wps=6141.8, ups=1.11, wpb=5530.4, bsz=357, num_updates=29300, lr=0.000369484, gnorm=0.359, train_wall=51, gb_free=10.1, wall=31832
2022-08-17 07:02:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:02:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:02:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:02:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:02:36 | INFO | valid | epoch 362 | valid on 'valid' subset | loss 5.107 | nll_loss 2.475 | ppl 5.56 | bleu 55.91 | wps 1822.7 | wpb 933.5 | bsz 59.6 | num_updates 29322 | best_bleu 57.27
2022-08-17 07:02:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 362 @ 29322 updates
2022-08-17 07:02:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint362.pt
2022-08-17 07:02:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint362.pt
2022-08-17 07:02:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint362.pt (epoch 362 @ 29322 updates, score 55.91) (writing took 16.394226770848036 seconds)
2022-08-17 07:02:52 | INFO | fairseq_cli.train | end of epoch 362 (average epoch stats below)
2022-08-17 07:02:52 | INFO | train | epoch 362 | loss 3.393 | nll_loss 0.356 | ppl 1.28 | wps 6540.2 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 29322 | lr 0.000369346 | gnorm 0.387 | train_wall 41 | gb_free 10.2 | wall 31868
2022-08-17 07:02:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:02:52 | INFO | fairseq.trainer | begin training epoch 363
2022-08-17 07:02:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:03:34 | INFO | train_inner | epoch 363:     78 / 81 loss=3.393, nll_loss=0.356, ppl=1.28, wps=7013.3, ups=1.27, wpb=5522.2, bsz=356.7, num_updates=29400, lr=0.000368856, gnorm=0.41, train_wall=51, gb_free=10, wall=31910
2022-08-17 07:03:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:03:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:03:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:03:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:03:45 | INFO | valid | epoch 363 | valid on 'valid' subset | loss 5.092 | nll_loss 2.461 | ppl 5.51 | bleu 55.81 | wps 1920.2 | wpb 933.5 | bsz 59.6 | num_updates 29403 | best_bleu 57.27
2022-08-17 07:03:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 363 @ 29403 updates
2022-08-17 07:03:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint363.pt
2022-08-17 07:03:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint363.pt
2022-08-17 07:04:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint363.pt (epoch 363 @ 29403 updates, score 55.81) (writing took 38.561437882483006 seconds)
2022-08-17 07:04:23 | INFO | fairseq_cli.train | end of epoch 363 (average epoch stats below)
2022-08-17 07:04:23 | INFO | train | epoch 363 | loss 3.392 | nll_loss 0.355 | ppl 1.28 | wps 4912.7 | ups 0.89 | wpb 5523.2 | bsz 358 | num_updates 29403 | lr 0.000368837 | gnorm 0.416 | train_wall 41 | gb_free 10.1 | wall 31960
2022-08-17 07:04:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:04:24 | INFO | fairseq.trainer | begin training epoch 364
2022-08-17 07:04:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:05:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:05:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:05:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:05:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:05:15 | INFO | valid | epoch 364 | valid on 'valid' subset | loss 5.103 | nll_loss 2.469 | ppl 5.54 | bleu 56.49 | wps 1944.1 | wpb 933.5 | bsz 59.6 | num_updates 29484 | best_bleu 57.27
2022-08-17 07:05:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 364 @ 29484 updates
2022-08-17 07:05:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint364.pt
2022-08-17 07:05:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint364.pt
2022-08-17 07:05:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint364.pt (epoch 364 @ 29484 updates, score 56.49) (writing took 19.105411894619465 seconds)
2022-08-17 07:05:34 | INFO | fairseq_cli.train | end of epoch 364 (average epoch stats below)
2022-08-17 07:05:34 | INFO | train | epoch 364 | loss 3.392 | nll_loss 0.356 | ppl 1.28 | wps 6342.5 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 29484 | lr 0.00036833 | gnorm 0.468 | train_wall 41 | gb_free 10.1 | wall 32030
2022-08-17 07:05:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:05:34 | INFO | fairseq.trainer | begin training epoch 365
2022-08-17 07:05:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:05:43 | INFO | train_inner | epoch 365:     16 / 81 loss=3.392, nll_loss=0.355, ppl=1.28, wps=4282, ups=0.78, wpb=5517.9, bsz=358.6, num_updates=29500, lr=0.00036823, gnorm=0.429, train_wall=50, gb_free=10.2, wall=32039
2022-08-17 07:06:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:06:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:06:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:06:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:06:25 | INFO | valid | epoch 365 | valid on 'valid' subset | loss 5.077 | nll_loss 2.439 | ppl 5.42 | bleu 56.7 | wps 1848.5 | wpb 933.5 | bsz 59.6 | num_updates 29565 | best_bleu 57.27
2022-08-17 07:06:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 365 @ 29565 updates
2022-08-17 07:06:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint365.pt
2022-08-17 07:06:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint365.pt
2022-08-17 07:06:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint365.pt (epoch 365 @ 29565 updates, score 56.7) (writing took 26.73342813178897 seconds)
2022-08-17 07:06:52 | INFO | fairseq_cli.train | end of epoch 365 (average epoch stats below)
2022-08-17 07:06:52 | INFO | train | epoch 365 | loss 3.392 | nll_loss 0.355 | ppl 1.28 | wps 5739.9 | ups 1.04 | wpb 5523.2 | bsz 358 | num_updates 29565 | lr 0.000367825 | gnorm 0.298 | train_wall 40 | gb_free 10.2 | wall 32108
2022-08-17 07:06:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:06:52 | INFO | fairseq.trainer | begin training epoch 366
2022-08-17 07:06:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:07:12 | INFO | train_inner | epoch 366:     35 / 81 loss=3.393, nll_loss=0.356, ppl=1.28, wps=6226.8, ups=1.13, wpb=5509.1, bsz=354.6, num_updates=29600, lr=0.000367607, gnorm=0.315, train_wall=49, gb_free=10.1, wall=32128
2022-08-17 07:07:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:07:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:07:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:07:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:07:44 | INFO | valid | epoch 366 | valid on 'valid' subset | loss 5.108 | nll_loss 2.478 | ppl 5.57 | bleu 55.93 | wps 1913.4 | wpb 933.5 | bsz 59.6 | num_updates 29646 | best_bleu 57.27
2022-08-17 07:07:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 366 @ 29646 updates
2022-08-17 07:07:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint366.pt
2022-08-17 07:07:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint366.pt
2022-08-17 07:08:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint366.pt (epoch 366 @ 29646 updates, score 55.93) (writing took 18.367166735231876 seconds)
2022-08-17 07:08:03 | INFO | fairseq_cli.train | end of epoch 366 (average epoch stats below)
2022-08-17 07:08:03 | INFO | train | epoch 366 | loss 3.392 | nll_loss 0.356 | ppl 1.28 | wps 6327.2 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 29646 | lr 0.000367322 | gnorm 0.31 | train_wall 40 | gb_free 10.1 | wall 32179
2022-08-17 07:08:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:08:03 | INFO | fairseq.trainer | begin training epoch 367
2022-08-17 07:08:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:08:30 | INFO | train_inner | epoch 367:     54 / 81 loss=3.391, nll_loss=0.354, ppl=1.28, wps=7049.3, ups=1.27, wpb=5563.7, bsz=361.2, num_updates=29700, lr=0.000366988, gnorm=0.287, train_wall=50, gb_free=10, wall=32207
2022-08-17 07:08:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:08:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:08:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:08:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:08:53 | INFO | valid | epoch 367 | valid on 'valid' subset | loss 5.112 | nll_loss 2.483 | ppl 5.59 | bleu 56.2 | wps 1795.8 | wpb 933.5 | bsz 59.6 | num_updates 29727 | best_bleu 57.27
2022-08-17 07:08:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 367 @ 29727 updates
2022-08-17 07:08:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint367.pt
2022-08-17 07:08:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint367.pt
2022-08-17 07:09:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint367.pt (epoch 367 @ 29727 updates, score 56.2) (writing took 21.806357696652412 seconds)
2022-08-17 07:09:15 | INFO | fairseq_cli.train | end of epoch 367 (average epoch stats below)
2022-08-17 07:09:15 | INFO | train | epoch 367 | loss 3.391 | nll_loss 0.354 | ppl 1.28 | wps 6141.1 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 29727 | lr 0.000366821 | gnorm 0.287 | train_wall 40 | gb_free 10.1 | wall 32252
2022-08-17 07:09:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:09:16 | INFO | fairseq.trainer | begin training epoch 368
2022-08-17 07:09:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:10:01 | INFO | train_inner | epoch 368:     73 / 81 loss=3.393, nll_loss=0.356, ppl=1.28, wps=6101.6, ups=1.11, wpb=5503.8, bsz=359, num_updates=29800, lr=0.000366372, gnorm=0.36, train_wall=51, gb_free=10.2, wall=32297
2022-08-17 07:10:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:10:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:10:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:10:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:10:15 | INFO | valid | epoch 368 | valid on 'valid' subset | loss 5.104 | nll_loss 2.477 | ppl 5.57 | bleu 56.37 | wps 1772.2 | wpb 933.5 | bsz 59.6 | num_updates 29808 | best_bleu 57.27
2022-08-17 07:10:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 368 @ 29808 updates
2022-08-17 07:10:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint368.pt
2022-08-17 07:10:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint368.pt
2022-08-17 07:10:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint368.pt (epoch 368 @ 29808 updates, score 56.37) (writing took 17.00063243880868 seconds)
2022-08-17 07:10:32 | INFO | fairseq_cli.train | end of epoch 368 (average epoch stats below)
2022-08-17 07:10:32 | INFO | train | epoch 368 | loss 3.393 | nll_loss 0.356 | ppl 1.28 | wps 5829.8 | ups 1.06 | wpb 5523.2 | bsz 358 | num_updates 29808 | lr 0.000366322 | gnorm 0.374 | train_wall 42 | gb_free 10.1 | wall 32328
2022-08-17 07:10:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:10:32 | INFO | fairseq.trainer | begin training epoch 369
2022-08-17 07:10:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:11:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:11:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:11:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:11:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:11:23 | INFO | valid | epoch 369 | valid on 'valid' subset | loss 5.074 | nll_loss 2.441 | ppl 5.43 | bleu 56.46 | wps 2065.4 | wpb 933.5 | bsz 59.6 | num_updates 29889 | best_bleu 57.27
2022-08-17 07:11:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 369 @ 29889 updates
2022-08-17 07:11:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint369.pt
2022-08-17 07:11:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint369.pt
2022-08-17 07:12:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint369.pt (epoch 369 @ 29889 updates, score 56.46) (writing took 36.87334446981549 seconds)
2022-08-17 07:12:00 | INFO | fairseq_cli.train | end of epoch 369 (average epoch stats below)
2022-08-17 07:12:00 | INFO | train | epoch 369 | loss 3.391 | nll_loss 0.354 | ppl 1.28 | wps 5080 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 29889 | lr 0.000365826 | gnorm 0.29 | train_wall 40 | gb_free 10.2 | wall 32416
2022-08-17 07:12:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:12:00 | INFO | fairseq.trainer | begin training epoch 370
2022-08-17 07:12:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:12:08 | INFO | train_inner | epoch 370:     11 / 81 loss=3.391, nll_loss=0.354, ppl=1.28, wps=4318.8, ups=0.78, wpb=5517.3, bsz=360.2, num_updates=29900, lr=0.000365758, gnorm=0.286, train_wall=49, gb_free=10, wall=32425
2022-08-17 07:12:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:12:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:12:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:12:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:12:56 | INFO | valid | epoch 370 | valid on 'valid' subset | loss 5.064 | nll_loss 2.429 | ppl 5.38 | bleu 56.57 | wps 1795.6 | wpb 933.5 | bsz 59.6 | num_updates 29970 | best_bleu 57.27
2022-08-17 07:12:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 370 @ 29970 updates
2022-08-17 07:12:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint370.pt
2022-08-17 07:12:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint370.pt
2022-08-17 07:13:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint370.pt (epoch 370 @ 29970 updates, score 56.57) (writing took 26.549627285450697 seconds)
2022-08-17 07:13:23 | INFO | fairseq_cli.train | end of epoch 370 (average epoch stats below)
2022-08-17 07:13:23 | INFO | train | epoch 370 | loss 3.391 | nll_loss 0.354 | ppl 1.28 | wps 5415.7 | ups 0.98 | wpb 5523.2 | bsz 358 | num_updates 29970 | lr 0.000365331 | gnorm 0.285 | train_wall 41 | gb_free 10.1 | wall 32499
2022-08-17 07:13:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:13:23 | INFO | fairseq.trainer | begin training epoch 371
2022-08-17 07:13:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:13:40 | INFO | train_inner | epoch 371:     30 / 81 loss=3.392, nll_loss=0.355, ppl=1.28, wps=5992.4, ups=1.09, wpb=5512.5, bsz=353.3, num_updates=30000, lr=0.000365148, gnorm=0.283, train_wall=50, gb_free=10.1, wall=32517
2022-08-17 07:14:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:14:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:14:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:14:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:14:15 | INFO | valid | epoch 371 | valid on 'valid' subset | loss 5.084 | nll_loss 2.452 | ppl 5.47 | bleu 57.17 | wps 1855.6 | wpb 933.5 | bsz 59.6 | num_updates 30051 | best_bleu 57.27
2022-08-17 07:14:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 371 @ 30051 updates
2022-08-17 07:14:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint371.pt
2022-08-17 07:14:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint371.pt
2022-08-17 07:14:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint371.pt (epoch 371 @ 30051 updates, score 57.17) (writing took 16.6435243524611 seconds)
2022-08-17 07:14:32 | INFO | fairseq_cli.train | end of epoch 371 (average epoch stats below)
2022-08-17 07:14:32 | INFO | train | epoch 371 | loss 3.391 | nll_loss 0.355 | ppl 1.28 | wps 6447.7 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 30051 | lr 0.000364838 | gnorm 0.281 | train_wall 39 | gb_free 10.2 | wall 32568
2022-08-17 07:14:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:14:32 | INFO | fairseq.trainer | begin training epoch 372
2022-08-17 07:14:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:14:58 | INFO | train_inner | epoch 372:     49 / 81 loss=3.391, nll_loss=0.354, ppl=1.28, wps=7111.2, ups=1.28, wpb=5547.9, bsz=362.6, num_updates=30100, lr=0.000364541, gnorm=0.297, train_wall=49, gb_free=10.1, wall=32595
2022-08-17 07:15:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:15:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:15:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:15:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:15:23 | INFO | valid | epoch 372 | valid on 'valid' subset | loss 5.099 | nll_loss 2.468 | ppl 5.53 | bleu 56.62 | wps 1887.2 | wpb 933.5 | bsz 59.6 | num_updates 30132 | best_bleu 57.27
2022-08-17 07:15:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 372 @ 30132 updates
2022-08-17 07:15:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint372.pt
2022-08-17 07:15:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint372.pt
2022-08-17 07:15:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint372.pt (epoch 372 @ 30132 updates, score 56.62) (writing took 23.516052462160587 seconds)
2022-08-17 07:15:47 | INFO | fairseq_cli.train | end of epoch 372 (average epoch stats below)
2022-08-17 07:15:47 | INFO | train | epoch 372 | loss 3.39 | nll_loss 0.353 | ppl 1.28 | wps 5972.8 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 30132 | lr 0.000364348 | gnorm 0.293 | train_wall 40 | gb_free 10.1 | wall 32643
2022-08-17 07:15:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:15:47 | INFO | fairseq.trainer | begin training epoch 373
2022-08-17 07:15:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:16:22 | INFO | train_inner | epoch 373:     68 / 81 loss=3.392, nll_loss=0.355, ppl=1.28, wps=6589.6, ups=1.2, wpb=5506.3, bsz=351, num_updates=30200, lr=0.000363937, gnorm=0.295, train_wall=49, gb_free=10.1, wall=32678
2022-08-17 07:16:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:16:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:16:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:16:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:16:39 | INFO | valid | epoch 373 | valid on 'valid' subset | loss 5.096 | nll_loss 2.461 | ppl 5.51 | bleu 56.97 | wps 1741.1 | wpb 933.5 | bsz 59.6 | num_updates 30213 | best_bleu 57.27
2022-08-17 07:16:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 373 @ 30213 updates
2022-08-17 07:16:39 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint373.pt
2022-08-17 07:16:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint373.pt
2022-08-17 07:17:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint373.pt (epoch 373 @ 30213 updates, score 56.97) (writing took 37.75155778974295 seconds)
2022-08-17 07:17:17 | INFO | fairseq_cli.train | end of epoch 373 (average epoch stats below)
2022-08-17 07:17:17 | INFO | train | epoch 373 | loss 3.392 | nll_loss 0.355 | ppl 1.28 | wps 5000.7 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 30213 | lr 0.000363859 | gnorm 0.304 | train_wall 40 | gb_free 10.1 | wall 32733
2022-08-17 07:17:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:17:17 | INFO | fairseq.trainer | begin training epoch 374
2022-08-17 07:17:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:18:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:18:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:18:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:18:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:18:10 | INFO | valid | epoch 374 | valid on 'valid' subset | loss 5.095 | nll_loss 2.461 | ppl 5.51 | bleu 57.07 | wps 1623.1 | wpb 933.5 | bsz 59.6 | num_updates 30294 | best_bleu 57.27
2022-08-17 07:18:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 374 @ 30294 updates
2022-08-17 07:18:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint374.pt
2022-08-17 07:18:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint374.pt
2022-08-17 07:18:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint374.pt (epoch 374 @ 30294 updates, score 57.07) (writing took 20.647390000522137 seconds)
2022-08-17 07:18:31 | INFO | fairseq_cli.train | end of epoch 374 (average epoch stats below)
2022-08-17 07:18:31 | INFO | train | epoch 374 | loss 3.393 | nll_loss 0.357 | ppl 1.28 | wps 6024.1 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 30294 | lr 0.000363372 | gnorm 0.393 | train_wall 40 | gb_free 10.1 | wall 32807
2022-08-17 07:18:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:18:31 | INFO | fairseq.trainer | begin training epoch 375
2022-08-17 07:18:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:18:35 | INFO | train_inner | epoch 375:      6 / 81 loss=3.392, nll_loss=0.356, ppl=1.28, wps=4145, ups=0.75, wpb=5524.1, bsz=361.8, num_updates=30300, lr=0.000363336, gnorm=0.368, train_wall=50, gb_free=10, wall=32811
2022-08-17 07:19:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:19:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:19:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:19:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:19:22 | INFO | valid | epoch 375 | valid on 'valid' subset | loss 5.097 | nll_loss 2.463 | ppl 5.51 | bleu 56.53 | wps 1881.5 | wpb 933.5 | bsz 59.6 | num_updates 30375 | best_bleu 57.27
2022-08-17 07:19:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 375 @ 30375 updates
2022-08-17 07:19:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint375.pt
2022-08-17 07:19:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint375.pt
2022-08-17 07:19:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint375.pt (epoch 375 @ 30375 updates, score 56.53) (writing took 23.906115867197514 seconds)
2022-08-17 07:19:46 | INFO | fairseq_cli.train | end of epoch 375 (average epoch stats below)
2022-08-17 07:19:46 | INFO | train | epoch 375 | loss 3.39 | nll_loss 0.353 | ppl 1.28 | wps 5938.2 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 30375 | lr 0.000362887 | gnorm 0.365 | train_wall 40 | gb_free 10.4 | wall 32882
2022-08-17 07:19:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:19:46 | INFO | fairseq.trainer | begin training epoch 376
2022-08-17 07:19:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:20:00 | INFO | train_inner | epoch 376:     25 / 81 loss=3.391, nll_loss=0.353, ppl=1.28, wps=6509.7, ups=1.18, wpb=5515.5, bsz=359, num_updates=30400, lr=0.000362738, gnorm=0.363, train_wall=49, gb_free=10.2, wall=32896
2022-08-17 07:20:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:20:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:20:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:20:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:20:37 | INFO | valid | epoch 376 | valid on 'valid' subset | loss 5.102 | nll_loss 2.471 | ppl 5.54 | bleu 56.7 | wps 1765 | wpb 933.5 | bsz 59.6 | num_updates 30456 | best_bleu 57.27
2022-08-17 07:20:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 376 @ 30456 updates
2022-08-17 07:20:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint376.pt
2022-08-17 07:20:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint376.pt
2022-08-17 07:21:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint376.pt (epoch 376 @ 30456 updates, score 56.7) (writing took 31.02421072125435 seconds)
2022-08-17 07:21:08 | INFO | fairseq_cli.train | end of epoch 376 (average epoch stats below)
2022-08-17 07:21:08 | INFO | train | epoch 376 | loss 3.392 | nll_loss 0.356 | ppl 1.28 | wps 5446.6 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 30456 | lr 0.000362404 | gnorm 0.305 | train_wall 39 | gb_free 10.1 | wall 32965
2022-08-17 07:21:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:21:09 | INFO | fairseq.trainer | begin training epoch 377
2022-08-17 07:21:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:21:32 | INFO | train_inner | epoch 377:     44 / 81 loss=3.392, nll_loss=0.355, ppl=1.28, wps=6013.5, ups=1.09, wpb=5532.2, bsz=353.4, num_updates=30500, lr=0.000362143, gnorm=0.306, train_wall=49, gb_free=10.1, wall=32988
2022-08-17 07:21:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:21:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:21:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:21:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:21:59 | INFO | valid | epoch 377 | valid on 'valid' subset | loss 5.096 | nll_loss 2.467 | ppl 5.53 | bleu 57.16 | wps 1945.6 | wpb 933.5 | bsz 59.6 | num_updates 30537 | best_bleu 57.27
2022-08-17 07:21:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 377 @ 30537 updates
2022-08-17 07:21:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint377.pt
2022-08-17 07:22:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint377.pt
2022-08-17 07:22:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint377.pt (epoch 377 @ 30537 updates, score 57.16) (writing took 22.095322780311108 seconds)
2022-08-17 07:22:22 | INFO | fairseq_cli.train | end of epoch 377 (average epoch stats below)
2022-08-17 07:22:22 | INFO | train | epoch 377 | loss 3.391 | nll_loss 0.354 | ppl 1.28 | wps 6114 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 30537 | lr 0.000361924 | gnorm 0.316 | train_wall 39 | gb_free 10.1 | wall 33038
2022-08-17 07:22:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:22:22 | INFO | fairseq.trainer | begin training epoch 378
2022-08-17 07:22:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:22:58 | INFO | train_inner | epoch 378:     63 / 81 loss=3.391, nll_loss=0.355, ppl=1.28, wps=6487.4, ups=1.17, wpb=5551.3, bsz=362.9, num_updates=30600, lr=0.000361551, gnorm=0.311, train_wall=50, gb_free=10.1, wall=33074
2022-08-17 07:23:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:23:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:23:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:23:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:23:15 | INFO | valid | epoch 378 | valid on 'valid' subset | loss 5.088 | nll_loss 2.458 | ppl 5.49 | bleu 56.74 | wps 1977.8 | wpb 933.5 | bsz 59.6 | num_updates 30618 | best_bleu 57.27
2022-08-17 07:23:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 378 @ 30618 updates
2022-08-17 07:23:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint378.pt
2022-08-17 07:23:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint378.pt
2022-08-17 07:23:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint378.pt (epoch 378 @ 30618 updates, score 56.74) (writing took 17.404399298131466 seconds)
2022-08-17 07:23:33 | INFO | fairseq_cli.train | end of epoch 378 (average epoch stats below)
2022-08-17 07:23:33 | INFO | train | epoch 378 | loss 3.391 | nll_loss 0.354 | ppl 1.28 | wps 6283.7 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 30618 | lr 0.000361444 | gnorm 0.301 | train_wall 41 | gb_free 10.1 | wall 33109
2022-08-17 07:23:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:23:33 | INFO | fairseq.trainer | begin training epoch 379
2022-08-17 07:23:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:24:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:24:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:24:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:24:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:24:24 | INFO | valid | epoch 379 | valid on 'valid' subset | loss 5.078 | nll_loss 2.448 | ppl 5.46 | bleu 56.27 | wps 1889.3 | wpb 933.5 | bsz 59.6 | num_updates 30699 | best_bleu 57.27
2022-08-17 07:24:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 379 @ 30699 updates
2022-08-17 07:24:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint379.pt
2022-08-17 07:24:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint379.pt
2022-08-17 07:24:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint379.pt (epoch 379 @ 30699 updates, score 56.27) (writing took 32.03997318446636 seconds)
2022-08-17 07:24:56 | INFO | fairseq_cli.train | end of epoch 379 (average epoch stats below)
2022-08-17 07:24:56 | INFO | train | epoch 379 | loss 3.391 | nll_loss 0.354 | ppl 1.28 | wps 5348.3 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 30699 | lr 0.000360967 | gnorm 0.283 | train_wall 41 | gb_free 10.2 | wall 33193
2022-08-17 07:24:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:24:57 | INFO | fairseq.trainer | begin training epoch 380
2022-08-17 07:24:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:24:58 | INFO | train_inner | epoch 380:      1 / 81 loss=3.391, nll_loss=0.354, ppl=1.28, wps=4549.1, ups=0.83, wpb=5485.4, bsz=357, num_updates=30700, lr=0.000360961, gnorm=0.28, train_wall=50, gb_free=10, wall=33194
2022-08-17 07:25:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:25:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:25:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:25:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:25:56 | INFO | valid | epoch 380 | valid on 'valid' subset | loss 5.079 | nll_loss 2.446 | ppl 5.45 | bleu 57.17 | wps 1787.2 | wpb 933.5 | bsz 59.6 | num_updates 30780 | best_bleu 57.27
2022-08-17 07:25:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 380 @ 30780 updates
2022-08-17 07:25:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint380.pt
2022-08-17 07:25:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint380.pt
2022-08-17 07:26:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint380.pt (epoch 380 @ 30780 updates, score 57.17) (writing took 38.25309581682086 seconds)
2022-08-17 07:26:34 | INFO | fairseq_cli.train | end of epoch 380 (average epoch stats below)
2022-08-17 07:26:34 | INFO | train | epoch 380 | loss 3.39 | nll_loss 0.354 | ppl 1.28 | wps 4559.8 | ups 0.83 | wpb 5523.2 | bsz 358 | num_updates 30780 | lr 0.000360492 | gnorm 0.386 | train_wall 40 | gb_free 10.1 | wall 33291
2022-08-17 07:26:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:26:35 | INFO | fairseq.trainer | begin training epoch 381
2022-08-17 07:26:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:26:47 | INFO | train_inner | epoch 381:     20 / 81 loss=3.39, nll_loss=0.353, ppl=1.28, wps=5105, ups=0.92, wpb=5534.4, bsz=357.1, num_updates=30800, lr=0.000360375, gnorm=0.381, train_wall=49, gb_free=10.1, wall=33303
2022-08-17 07:27:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:27:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:27:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:27:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:27:27 | INFO | valid | epoch 381 | valid on 'valid' subset | loss 5.085 | nll_loss 2.454 | ppl 5.48 | bleu 57.04 | wps 1862.3 | wpb 933.5 | bsz 59.6 | num_updates 30861 | best_bleu 57.27
2022-08-17 07:27:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 381 @ 30861 updates
2022-08-17 07:27:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint381.pt
2022-08-17 07:27:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint381.pt
2022-08-17 07:27:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint381.pt (epoch 381 @ 30861 updates, score 57.04) (writing took 2.288924168795347 seconds)
2022-08-17 07:27:29 | INFO | fairseq_cli.train | end of epoch 381 (average epoch stats below)
2022-08-17 07:27:29 | INFO | train | epoch 381 | loss 3.391 | nll_loss 0.354 | ppl 1.28 | wps 8187.4 | ups 1.48 | wpb 5523.2 | bsz 358 | num_updates 30861 | lr 0.000360019 | gnorm 0.348 | train_wall 41 | gb_free 10 | wall 33345
2022-08-17 07:27:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:27:29 | INFO | fairseq.trainer | begin training epoch 382
2022-08-17 07:27:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:27:51 | INFO | train_inner | epoch 382:     39 / 81 loss=3.391, nll_loss=0.354, ppl=1.28, wps=8514.5, ups=1.55, wpb=5510.5, bsz=358.2, num_updates=30900, lr=0.000359791, gnorm=0.335, train_wall=51, gb_free=10.1, wall=33367
2022-08-17 07:28:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:28:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:28:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:28:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:28:22 | INFO | valid | epoch 382 | valid on 'valid' subset | loss 5.088 | nll_loss 2.455 | ppl 5.48 | bleu 57.03 | wps 1740.7 | wpb 933.5 | bsz 59.6 | num_updates 30942 | best_bleu 57.27
2022-08-17 07:28:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 382 @ 30942 updates
2022-08-17 07:28:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint382.pt
2022-08-17 07:28:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint382.pt
2022-08-17 07:28:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint382.pt (epoch 382 @ 30942 updates, score 57.03) (writing took 13.449906680732965 seconds)
2022-08-17 07:28:36 | INFO | fairseq_cli.train | end of epoch 382 (average epoch stats below)
2022-08-17 07:28:36 | INFO | train | epoch 382 | loss 3.391 | nll_loss 0.355 | ppl 1.28 | wps 6682.4 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 30942 | lr 0.000359547 | gnorm 0.327 | train_wall 42 | gb_free 10.1 | wall 33412
2022-08-17 07:28:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:28:36 | INFO | fairseq.trainer | begin training epoch 383
2022-08-17 07:28:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:29:08 | INFO | train_inner | epoch 383:     58 / 81 loss=3.391, nll_loss=0.355, ppl=1.28, wps=7156.3, ups=1.3, wpb=5517.7, bsz=359.5, num_updates=31000, lr=0.000359211, gnorm=0.317, train_wall=51, gb_free=10.1, wall=33445
2022-08-17 07:29:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:29:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:29:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:29:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:29:30 | INFO | valid | epoch 383 | valid on 'valid' subset | loss 5.089 | nll_loss 2.455 | ppl 5.48 | bleu 56.64 | wps 1811.7 | wpb 933.5 | bsz 59.6 | num_updates 31023 | best_bleu 57.27
2022-08-17 07:29:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 383 @ 31023 updates
2022-08-17 07:29:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint383.pt
2022-08-17 07:29:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint383.pt
2022-08-17 07:29:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint383.pt (epoch 383 @ 31023 updates, score 56.64) (writing took 2.3189618438482285 seconds)
2022-08-17 07:29:33 | INFO | fairseq_cli.train | end of epoch 383 (average epoch stats below)
2022-08-17 07:29:33 | INFO | train | epoch 383 | loss 3.391 | nll_loss 0.354 | ppl 1.28 | wps 7927.4 | ups 1.44 | wpb 5523.2 | bsz 358 | num_updates 31023 | lr 0.000359077 | gnorm 0.3 | train_wall 41 | gb_free 10.3 | wall 33469
2022-08-17 07:29:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:29:33 | INFO | fairseq.trainer | begin training epoch 384
2022-08-17 07:29:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:30:15 | INFO | train_inner | epoch 384:     77 / 81 loss=3.391, nll_loss=0.355, ppl=1.28, wps=8353.1, ups=1.5, wpb=5553.7, bsz=358.1, num_updates=31100, lr=0.000358633, gnorm=0.305, train_wall=52, gb_free=10, wall=33511
2022-08-17 07:30:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:30:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:30:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:30:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:30:26 | INFO | valid | epoch 384 | valid on 'valid' subset | loss 5.098 | nll_loss 2.466 | ppl 5.53 | bleu 56.92 | wps 1842.7 | wpb 933.5 | bsz 59.6 | num_updates 31104 | best_bleu 57.27
2022-08-17 07:30:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 384 @ 31104 updates
2022-08-17 07:30:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint384.pt
2022-08-17 07:30:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint384.pt
2022-08-17 07:30:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint384.pt (epoch 384 @ 31104 updates, score 56.92) (writing took 25.60165387019515 seconds)
2022-08-17 07:30:52 | INFO | fairseq_cli.train | end of epoch 384 (average epoch stats below)
2022-08-17 07:30:52 | INFO | train | epoch 384 | loss 3.391 | nll_loss 0.355 | ppl 1.28 | wps 5640 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 31104 | lr 0.00035861 | gnorm 0.307 | train_wall 42 | gb_free 10.2 | wall 33548
2022-08-17 07:30:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:30:52 | INFO | fairseq.trainer | begin training epoch 385
2022-08-17 07:30:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:31:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:31:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:31:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:31:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:31:44 | INFO | valid | epoch 385 | valid on 'valid' subset | loss 5.1 | nll_loss 2.466 | ppl 5.53 | bleu 56.98 | wps 1819.8 | wpb 933.5 | bsz 59.6 | num_updates 31185 | best_bleu 57.27
2022-08-17 07:31:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 385 @ 31185 updates
2022-08-17 07:31:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint385.pt
2022-08-17 07:31:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint385.pt
2022-08-17 07:32:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint385.pt (epoch 385 @ 31185 updates, score 56.98) (writing took 40.029203202575445 seconds)
2022-08-17 07:32:25 | INFO | fairseq_cli.train | end of epoch 385 (average epoch stats below)
2022-08-17 07:32:25 | INFO | train | epoch 385 | loss 3.39 | nll_loss 0.353 | ppl 1.28 | wps 4821.8 | ups 0.87 | wpb 5523.2 | bsz 358 | num_updates 31185 | lr 0.000358144 | gnorm 0.283 | train_wall 41 | gb_free 10.1 | wall 33641
2022-08-17 07:32:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:32:25 | INFO | fairseq.trainer | begin training epoch 386
2022-08-17 07:32:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:32:34 | INFO | train_inner | epoch 386:     15 / 81 loss=3.39, nll_loss=0.353, ppl=1.28, wps=3940.3, ups=0.72, wpb=5469.9, bsz=352.8, num_updates=31200, lr=0.000358057, gnorm=0.288, train_wall=51, gb_free=10.1, wall=33650
2022-08-17 07:33:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:33:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:33:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:33:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:33:17 | INFO | valid | epoch 386 | valid on 'valid' subset | loss 5.082 | nll_loss 2.448 | ppl 5.46 | bleu 56.69 | wps 1814.4 | wpb 933.5 | bsz 59.6 | num_updates 31266 | best_bleu 57.27
2022-08-17 07:33:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 386 @ 31266 updates
2022-08-17 07:33:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint386.pt
2022-08-17 07:33:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint386.pt
2022-08-17 07:33:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint386.pt (epoch 386 @ 31266 updates, score 56.69) (writing took 17.7772536277771 seconds)
2022-08-17 07:33:35 | INFO | fairseq_cli.train | end of epoch 386 (average epoch stats below)
2022-08-17 07:33:35 | INFO | train | epoch 386 | loss 3.39 | nll_loss 0.354 | ppl 1.28 | wps 6351.3 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 31266 | lr 0.000357679 | gnorm 0.279 | train_wall 41 | gb_free 10.3 | wall 33711
2022-08-17 07:33:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:33:35 | INFO | fairseq.trainer | begin training epoch 387
2022-08-17 07:33:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:33:54 | INFO | train_inner | epoch 387:     34 / 81 loss=3.39, nll_loss=0.353, ppl=1.28, wps=6960.2, ups=1.25, wpb=5566.5, bsz=362.6, num_updates=31300, lr=0.000357485, gnorm=0.284, train_wall=51, gb_free=10.1, wall=33730
2022-08-17 07:34:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:34:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:34:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:34:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:34:27 | INFO | valid | epoch 387 | valid on 'valid' subset | loss 5.088 | nll_loss 2.463 | ppl 5.52 | bleu 57.18 | wps 1831.4 | wpb 933.5 | bsz 59.6 | num_updates 31347 | best_bleu 57.27
2022-08-17 07:34:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 387 @ 31347 updates
2022-08-17 07:34:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint387.pt
2022-08-17 07:34:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint387.pt
2022-08-17 07:34:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint387.pt (epoch 387 @ 31347 updates, score 57.18) (writing took 31.088948726654053 seconds)
2022-08-17 07:34:59 | INFO | fairseq_cli.train | end of epoch 387 (average epoch stats below)
2022-08-17 07:34:59 | INFO | train | epoch 387 | loss 3.39 | nll_loss 0.354 | ppl 1.28 | wps 5361.6 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 31347 | lr 0.000357217 | gnorm 0.312 | train_wall 41 | gb_free 10.1 | wall 33795
2022-08-17 07:34:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:34:59 | INFO | fairseq.trainer | begin training epoch 388
2022-08-17 07:34:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:35:27 | INFO | train_inner | epoch 388:     53 / 81 loss=3.39, nll_loss=0.354, ppl=1.28, wps=5881.9, ups=1.07, wpb=5510.1, bsz=359, num_updates=31400, lr=0.000356915, gnorm=0.311, train_wall=50, gb_free=10.1, wall=33824
2022-08-17 07:35:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:35:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:35:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:35:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:35:51 | INFO | valid | epoch 388 | valid on 'valid' subset | loss 5.083 | nll_loss 2.449 | ppl 5.46 | bleu 56.97 | wps 1819.7 | wpb 933.5 | bsz 59.6 | num_updates 31428 | best_bleu 57.27
2022-08-17 07:35:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 388 @ 31428 updates
2022-08-17 07:35:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint388.pt
2022-08-17 07:35:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint388.pt
2022-08-17 07:36:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint388.pt (epoch 388 @ 31428 updates, score 56.97) (writing took 32.508484080433846 seconds)
2022-08-17 07:36:24 | INFO | fairseq_cli.train | end of epoch 388 (average epoch stats below)
2022-08-17 07:36:24 | INFO | train | epoch 388 | loss 3.39 | nll_loss 0.353 | ppl 1.28 | wps 5248.6 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 31428 | lr 0.000356756 | gnorm 0.292 | train_wall 41 | gb_free 10.1 | wall 33880
2022-08-17 07:36:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:36:24 | INFO | fairseq.trainer | begin training epoch 389
2022-08-17 07:36:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:37:02 | INFO | train_inner | epoch 389:     72 / 81 loss=3.39, nll_loss=0.353, ppl=1.28, wps=5882.7, ups=1.06, wpb=5537.7, bsz=355.6, num_updates=31500, lr=0.000356348, gnorm=0.28, train_wall=50, gb_free=10.2, wall=33918
2022-08-17 07:37:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:37:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:37:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:37:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:37:15 | INFO | valid | epoch 389 | valid on 'valid' subset | loss 5.078 | nll_loss 2.444 | ppl 5.44 | bleu 56.94 | wps 1815.3 | wpb 933.5 | bsz 59.6 | num_updates 31509 | best_bleu 57.27
2022-08-17 07:37:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 389 @ 31509 updates
2022-08-17 07:37:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint389.pt
2022-08-17 07:37:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint389.pt
2022-08-17 07:37:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint389.pt (epoch 389 @ 31509 updates, score 56.94) (writing took 33.27046275883913 seconds)
2022-08-17 07:37:49 | INFO | fairseq_cli.train | end of epoch 389 (average epoch stats below)
2022-08-17 07:37:49 | INFO | train | epoch 389 | loss 3.39 | nll_loss 0.353 | ppl 1.28 | wps 5279.1 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 31509 | lr 0.000356297 | gnorm 0.291 | train_wall 40 | gb_free 10.2 | wall 33965
2022-08-17 07:37:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:37:49 | INFO | fairseq.trainer | begin training epoch 390
2022-08-17 07:37:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:38:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:38:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:38:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:38:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:38:41 | INFO | valid | epoch 390 | valid on 'valid' subset | loss 5.09 | nll_loss 2.458 | ppl 5.49 | bleu 56.32 | wps 1890 | wpb 933.5 | bsz 59.6 | num_updates 31590 | best_bleu 57.27
2022-08-17 07:38:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 390 @ 31590 updates
2022-08-17 07:38:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint390.pt
2022-08-17 07:38:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint390.pt
2022-08-17 07:39:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint390.pt (epoch 390 @ 31590 updates, score 56.32) (writing took 25.19287719205022 seconds)
2022-08-17 07:39:06 | INFO | fairseq_cli.train | end of epoch 390 (average epoch stats below)
2022-08-17 07:39:06 | INFO | train | epoch 390 | loss 3.389 | nll_loss 0.352 | ppl 1.28 | wps 5761 | ups 1.04 | wpb 5523.2 | bsz 358 | num_updates 31590 | lr 0.00035584 | gnorm 0.272 | train_wall 41 | gb_free 10.1 | wall 34042
2022-08-17 07:39:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:39:06 | INFO | fairseq.trainer | begin training epoch 391
2022-08-17 07:39:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:39:13 | INFO | train_inner | epoch 391:     10 / 81 loss=3.389, nll_loss=0.352, ppl=1.28, wps=4190.3, ups=0.76, wpb=5514.9, bsz=360.4, num_updates=31600, lr=0.000355784, gnorm=0.275, train_wall=50, gb_free=10.2, wall=34049
2022-08-17 07:39:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:39:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:39:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:39:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:40:03 | INFO | valid | epoch 391 | valid on 'valid' subset | loss 5.087 | nll_loss 2.454 | ppl 5.48 | bleu 56.62 | wps 1875.7 | wpb 933.5 | bsz 59.6 | num_updates 31671 | best_bleu 57.27
2022-08-17 07:40:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 391 @ 31671 updates
2022-08-17 07:40:03 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint391.pt
2022-08-17 07:40:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint391.pt
2022-08-17 07:40:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint391.pt (epoch 391 @ 31671 updates, score 56.62) (writing took 17.48866882175207 seconds)
2022-08-17 07:40:21 | INFO | fairseq_cli.train | end of epoch 391 (average epoch stats below)
2022-08-17 07:40:21 | INFO | train | epoch 391 | loss 3.39 | nll_loss 0.353 | ppl 1.28 | wps 6014.2 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 31671 | lr 0.000355385 | gnorm 0.295 | train_wall 41 | gb_free 10.1 | wall 34117
2022-08-17 07:40:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:40:21 | INFO | fairseq.trainer | begin training epoch 392
2022-08-17 07:40:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:40:37 | INFO | train_inner | epoch 392:     29 / 81 loss=3.39, nll_loss=0.353, ppl=1.28, wps=6607.7, ups=1.19, wpb=5538.6, bsz=357.6, num_updates=31700, lr=0.000355222, gnorm=0.294, train_wall=51, gb_free=10, wall=34133
2022-08-17 07:41:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:41:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:41:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:41:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:41:12 | INFO | valid | epoch 392 | valid on 'valid' subset | loss 5.085 | nll_loss 2.448 | ppl 5.46 | bleu 56.2 | wps 1866.5 | wpb 933.5 | bsz 59.6 | num_updates 31752 | best_bleu 57.27
2022-08-17 07:41:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 392 @ 31752 updates
2022-08-17 07:41:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint392.pt
2022-08-17 07:41:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint392.pt
2022-08-17 07:41:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint392.pt (epoch 392 @ 31752 updates, score 56.2) (writing took 19.61794413253665 seconds)
2022-08-17 07:41:32 | INFO | fairseq_cli.train | end of epoch 392 (average epoch stats below)
2022-08-17 07:41:32 | INFO | train | epoch 392 | loss 3.39 | nll_loss 0.354 | ppl 1.28 | wps 6247.4 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 31752 | lr 0.000354931 | gnorm 0.288 | train_wall 40 | gb_free 10.3 | wall 34188
2022-08-17 07:41:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:41:32 | INFO | fairseq.trainer | begin training epoch 393
2022-08-17 07:41:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:41:58 | INFO | train_inner | epoch 393:     48 / 81 loss=3.39, nll_loss=0.354, ppl=1.28, wps=6810.2, ups=1.24, wpb=5512.7, bsz=356.7, num_updates=31800, lr=0.000354663, gnorm=0.278, train_wall=50, gb_free=10.1, wall=34214
2022-08-17 07:42:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:42:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:42:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:42:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:42:25 | INFO | valid | epoch 393 | valid on 'valid' subset | loss 5.088 | nll_loss 2.456 | ppl 5.49 | bleu 56.59 | wps 1764.1 | wpb 933.5 | bsz 59.6 | num_updates 31833 | best_bleu 57.27
2022-08-17 07:42:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 393 @ 31833 updates
2022-08-17 07:42:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint393.pt
2022-08-17 07:42:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint393.pt
2022-08-17 07:42:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint393.pt (epoch 393 @ 31833 updates, score 56.59) (writing took 2.4239832907915115 seconds)
2022-08-17 07:42:27 | INFO | fairseq_cli.train | end of epoch 393 (average epoch stats below)
2022-08-17 07:42:27 | INFO | train | epoch 393 | loss 3.39 | nll_loss 0.354 | ppl 1.28 | wps 8089.8 | ups 1.46 | wpb 5523.2 | bsz 358 | num_updates 31833 | lr 0.00035448 | gnorm 0.272 | train_wall 41 | gb_free 10.2 | wall 34244
2022-08-17 07:42:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:42:28 | INFO | fairseq.trainer | begin training epoch 394
2022-08-17 07:42:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:43:03 | INFO | train_inner | epoch 394:     67 / 81 loss=3.391, nll_loss=0.355, ppl=1.28, wps=8512.5, ups=1.54, wpb=5522.2, bsz=356.6, num_updates=31900, lr=0.000354107, gnorm=0.298, train_wall=51, gb_free=10.1, wall=34279
2022-08-17 07:43:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:43:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:43:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:43:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:43:20 | INFO | valid | epoch 394 | valid on 'valid' subset | loss 5.089 | nll_loss 2.459 | ppl 5.5 | bleu 56.93 | wps 1818.9 | wpb 933.5 | bsz 59.6 | num_updates 31914 | best_bleu 57.27
2022-08-17 07:43:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 394 @ 31914 updates
2022-08-17 07:43:20 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint394.pt
2022-08-17 07:43:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint394.pt
2022-08-17 07:43:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint394.pt (epoch 394 @ 31914 updates, score 56.93) (writing took 25.595693714916706 seconds)
2022-08-17 07:43:46 | INFO | fairseq_cli.train | end of epoch 394 (average epoch stats below)
2022-08-17 07:43:46 | INFO | train | epoch 394 | loss 3.391 | nll_loss 0.355 | ppl 1.28 | wps 5726.2 | ups 1.04 | wpb 5523.2 | bsz 358 | num_updates 31914 | lr 0.000354029 | gnorm 0.301 | train_wall 40 | gb_free 10.1 | wall 34322
2022-08-17 07:43:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:43:46 | INFO | fairseq.trainer | begin training epoch 395
2022-08-17 07:43:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:44:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:44:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:44:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:44:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:44:38 | INFO | valid | epoch 395 | valid on 'valid' subset | loss 5.092 | nll_loss 2.462 | ppl 5.51 | bleu 57.31 | wps 1861 | wpb 933.5 | bsz 59.6 | num_updates 31995 | best_bleu 57.31
2022-08-17 07:44:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 395 @ 31995 updates
2022-08-17 07:44:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint395.pt
2022-08-17 07:44:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint395.pt
2022-08-17 07:45:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint395.pt (epoch 395 @ 31995 updates, score 57.31) (writing took 78.72657014802098 seconds)
2022-08-17 07:45:57 | INFO | fairseq_cli.train | end of epoch 395 (average epoch stats below)
2022-08-17 07:45:57 | INFO | train | epoch 395 | loss 3.392 | nll_loss 0.356 | ppl 1.28 | wps 3411.9 | ups 0.62 | wpb 5523.2 | bsz 358 | num_updates 31995 | lr 0.000353581 | gnorm 0.375 | train_wall 41 | gb_free 10.1 | wall 34453
2022-08-17 07:45:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:45:57 | INFO | fairseq.trainer | begin training epoch 396
2022-08-17 07:45:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:46:01 | INFO | train_inner | epoch 396:      5 / 81 loss=3.391, nll_loss=0.356, ppl=1.28, wps=3095.9, ups=0.56, wpb=5510.6, bsz=359.9, num_updates=32000, lr=0.000353553, gnorm=0.355, train_wall=50, gb_free=10.1, wall=34457
2022-08-17 07:46:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:46:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:46:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:46:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:46:56 | INFO | valid | epoch 396 | valid on 'valid' subset | loss 5.105 | nll_loss 2.477 | ppl 5.57 | bleu 56.97 | wps 1833.1 | wpb 933.5 | bsz 59.6 | num_updates 32076 | best_bleu 57.31
2022-08-17 07:46:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 396 @ 32076 updates
2022-08-17 07:46:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint396.pt
2022-08-17 07:46:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint396.pt
2022-08-17 07:47:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint396.pt (epoch 396 @ 32076 updates, score 56.97) (writing took 30.77697667479515 seconds)
2022-08-17 07:47:27 | INFO | fairseq_cli.train | end of epoch 396 (average epoch stats below)
2022-08-17 07:47:27 | INFO | train | epoch 396 | loss 3.389 | nll_loss 0.352 | ppl 1.28 | wps 4945.9 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 32076 | lr 0.000353134 | gnorm 0.285 | train_wall 40 | gb_free 10 | wall 34543
2022-08-17 07:47:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:47:27 | INFO | fairseq.trainer | begin training epoch 397
2022-08-17 07:47:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:47:41 | INFO | train_inner | epoch 397:     24 / 81 loss=3.389, nll_loss=0.352, ppl=1.28, wps=5501.7, ups=1, wpb=5523.6, bsz=357.4, num_updates=32100, lr=0.000353002, gnorm=0.311, train_wall=50, gb_free=10.1, wall=34557
2022-08-17 07:48:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:48:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:48:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:48:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:48:20 | INFO | valid | epoch 397 | valid on 'valid' subset | loss 5.085 | nll_loss 2.451 | ppl 5.47 | bleu 56.99 | wps 1734.9 | wpb 933.5 | bsz 59.6 | num_updates 32157 | best_bleu 57.31
2022-08-17 07:48:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 397 @ 32157 updates
2022-08-17 07:48:20 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint397.pt
2022-08-17 07:48:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint397.pt
2022-08-17 07:48:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint397.pt (epoch 397 @ 32157 updates, score 56.99) (writing took 29.142190121114254 seconds)
2022-08-17 07:48:50 | INFO | fairseq_cli.train | end of epoch 397 (average epoch stats below)
2022-08-17 07:48:50 | INFO | train | epoch 397 | loss 3.389 | nll_loss 0.353 | ppl 1.28 | wps 5417.4 | ups 0.98 | wpb 5523.2 | bsz 358 | num_updates 32157 | lr 0.000352689 | gnorm 0.318 | train_wall 41 | gb_free 10.2 | wall 34626
2022-08-17 07:48:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:48:50 | INFO | fairseq.trainer | begin training epoch 398
2022-08-17 07:48:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:49:14 | INFO | train_inner | epoch 398:     43 / 81 loss=3.39, nll_loss=0.354, ppl=1.28, wps=5985, ups=1.08, wpb=5552.6, bsz=362.3, num_updates=32200, lr=0.000352454, gnorm=0.341, train_wall=49, gb_free=10.1, wall=34650
2022-08-17 07:49:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:49:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:49:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:49:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:49:42 | INFO | valid | epoch 398 | valid on 'valid' subset | loss 5.091 | nll_loss 2.462 | ppl 5.51 | bleu 56.92 | wps 1916.1 | wpb 933.5 | bsz 59.6 | num_updates 32238 | best_bleu 57.31
2022-08-17 07:49:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 398 @ 32238 updates
2022-08-17 07:49:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint398.pt
2022-08-17 07:49:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint398.pt
2022-08-17 07:49:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint398.pt (epoch 398 @ 32238 updates, score 56.92) (writing took 16.272850215435028 seconds)
2022-08-17 07:49:59 | INFO | fairseq_cli.train | end of epoch 398 (average epoch stats below)
2022-08-17 07:49:59 | INFO | train | epoch 398 | loss 3.391 | nll_loss 0.356 | ppl 1.28 | wps 6473.7 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 32238 | lr 0.000352246 | gnorm 0.37 | train_wall 40 | gb_free 10.1 | wall 34695
2022-08-17 07:49:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:49:59 | INFO | fairseq.trainer | begin training epoch 399
2022-08-17 07:49:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:50:32 | INFO | train_inner | epoch 399:     62 / 81 loss=3.39, nll_loss=0.354, ppl=1.28, wps=7083.1, ups=1.29, wpb=5497.5, bsz=355.8, num_updates=32300, lr=0.000351908, gnorm=0.32, train_wall=49, gb_free=10.1, wall=34728
2022-08-17 07:50:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:50:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:50:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:50:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:50:51 | INFO | valid | epoch 399 | valid on 'valid' subset | loss 5.091 | nll_loss 2.463 | ppl 5.51 | bleu 57.33 | wps 1886.6 | wpb 933.5 | bsz 59.6 | num_updates 32319 | best_bleu 57.33
2022-08-17 07:50:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 399 @ 32319 updates
2022-08-17 07:50:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint399.pt
2022-08-17 07:50:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint399.pt
2022-08-17 07:51:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint399.pt (epoch 399 @ 32319 updates, score 57.33) (writing took 53.966632291674614 seconds)
2022-08-17 07:51:45 | INFO | fairseq_cli.train | end of epoch 399 (average epoch stats below)
2022-08-17 07:51:45 | INFO | train | epoch 399 | loss 3.389 | nll_loss 0.353 | ppl 1.28 | wps 4221.5 | ups 0.76 | wpb 5523.2 | bsz 358 | num_updates 32319 | lr 0.000351804 | gnorm 0.303 | train_wall 40 | gb_free 10.3 | wall 34801
2022-08-17 07:51:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:51:45 | INFO | fairseq.trainer | begin training epoch 400
2022-08-17 07:51:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:52:28 | INFO | train_inner | epoch 400:     81 / 81 loss=3.389, nll_loss=0.353, ppl=1.28, wps=4732.1, ups=0.86, wpb=5519.2, bsz=355.6, num_updates=32400, lr=0.000351364, gnorm=0.274, train_wall=51, gb_free=10.2, wall=34844
2022-08-17 07:52:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:52:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:52:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:52:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:52:39 | INFO | valid | epoch 400 | valid on 'valid' subset | loss 5.095 | nll_loss 2.466 | ppl 5.52 | bleu 57.01 | wps 1665.7 | wpb 933.5 | bsz 59.6 | num_updates 32400 | best_bleu 57.33
2022-08-17 07:52:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 400 @ 32400 updates
2022-08-17 07:52:39 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint400.pt
2022-08-17 07:52:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint400.pt
2022-08-17 07:53:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint400.pt (epoch 400 @ 32400 updates, score 57.01) (writing took 22.081117659807205 seconds)
2022-08-17 07:53:01 | INFO | fairseq_cli.train | end of epoch 400 (average epoch stats below)
2022-08-17 07:53:01 | INFO | train | epoch 400 | loss 3.389 | nll_loss 0.353 | ppl 1.28 | wps 5894.7 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 32400 | lr 0.000351364 | gnorm 0.278 | train_wall 42 | gb_free 10.2 | wall 34877
2022-08-17 07:53:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:53:01 | INFO | fairseq.trainer | begin training epoch 401
2022-08-17 07:53:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:53:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:53:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:53:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:53:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:54:02 | INFO | valid | epoch 401 | valid on 'valid' subset | loss 5.093 | nll_loss 2.464 | ppl 5.52 | bleu 57.22 | wps 1755.7 | wpb 933.5 | bsz 59.6 | num_updates 32481 | best_bleu 57.33
2022-08-17 07:54:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 401 @ 32481 updates
2022-08-17 07:54:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint401.pt
2022-08-17 07:54:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint401.pt
2022-08-17 07:54:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint401.pt (epoch 401 @ 32481 updates, score 57.22) (writing took 41.40151359140873 seconds)
2022-08-17 07:54:44 | INFO | fairseq_cli.train | end of epoch 401 (average epoch stats below)
2022-08-17 07:54:44 | INFO | train | epoch 401 | loss 3.389 | nll_loss 0.352 | ppl 1.28 | wps 4332 | ups 0.78 | wpb 5523.2 | bsz 358 | num_updates 32481 | lr 0.000350926 | gnorm 0.302 | train_wall 42 | gb_free 10.1 | wall 34980
2022-08-17 07:54:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:54:44 | INFO | fairseq.trainer | begin training epoch 402
2022-08-17 07:54:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:54:56 | INFO | train_inner | epoch 402:     19 / 81 loss=3.389, nll_loss=0.353, ppl=1.28, wps=3733.7, ups=0.68, wpb=5503.1, bsz=352.2, num_updates=32500, lr=0.000350823, gnorm=0.305, train_wall=52, gb_free=10.1, wall=34992
2022-08-17 07:55:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:55:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:55:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:55:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:55:37 | INFO | valid | epoch 402 | valid on 'valid' subset | loss 5.107 | nll_loss 2.483 | ppl 5.59 | bleu 57.09 | wps 1833.3 | wpb 933.5 | bsz 59.6 | num_updates 32562 | best_bleu 57.33
2022-08-17 07:55:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 402 @ 32562 updates
2022-08-17 07:55:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint402.pt
2022-08-17 07:55:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint402.pt
2022-08-17 07:55:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint402.pt (epoch 402 @ 32562 updates, score 57.09) (writing took 17.293882127851248 seconds)
2022-08-17 07:55:55 | INFO | fairseq_cli.train | end of epoch 402 (average epoch stats below)
2022-08-17 07:55:55 | INFO | train | epoch 402 | loss 3.39 | nll_loss 0.354 | ppl 1.28 | wps 6327.4 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 32562 | lr 0.000350489 | gnorm 0.63 | train_wall 42 | gb_free 10.2 | wall 35051
2022-08-17 07:55:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:55:55 | INFO | fairseq.trainer | begin training epoch 403
2022-08-17 07:55:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:56:15 | INFO | train_inner | epoch 403:     38 / 81 loss=3.389, nll_loss=0.352, ppl=1.28, wps=7003.3, ups=1.26, wpb=5552.9, bsz=367.2, num_updates=32600, lr=0.000350285, gnorm=0.547, train_wall=50, gb_free=10.1, wall=35071
2022-08-17 07:56:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:56:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:56:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:56:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:56:46 | INFO | valid | epoch 403 | valid on 'valid' subset | loss 5.107 | nll_loss 2.481 | ppl 5.58 | bleu 56.65 | wps 1851.3 | wpb 933.5 | bsz 59.6 | num_updates 32643 | best_bleu 57.33
2022-08-17 07:56:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 403 @ 32643 updates
2022-08-17 07:56:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint403.pt
2022-08-17 07:56:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint403.pt
2022-08-17 07:57:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint403.pt (epoch 403 @ 32643 updates, score 56.65) (writing took 19.91364384442568 seconds)
2022-08-17 07:57:06 | INFO | fairseq_cli.train | end of epoch 403 (average epoch stats below)
2022-08-17 07:57:06 | INFO | train | epoch 403 | loss 3.388 | nll_loss 0.351 | ppl 1.28 | wps 6281.6 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 32643 | lr 0.000350054 | gnorm 0.288 | train_wall 39 | gb_free 10.3 | wall 35122
2022-08-17 07:57:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:57:06 | INFO | fairseq.trainer | begin training epoch 404
2022-08-17 07:57:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:57:37 | INFO | train_inner | epoch 404:     57 / 81 loss=3.389, nll_loss=0.353, ppl=1.28, wps=6770.8, ups=1.22, wpb=5535.1, bsz=353.9, num_updates=32700, lr=0.000349749, gnorm=0.304, train_wall=49, gb_free=10.2, wall=35153
2022-08-17 07:57:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:57:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:57:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:57:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:57:59 | INFO | valid | epoch 404 | valid on 'valid' subset | loss 5.1 | nll_loss 2.47 | ppl 5.54 | bleu 56.46 | wps 1892.5 | wpb 933.5 | bsz 59.6 | num_updates 32724 | best_bleu 57.33
2022-08-17 07:57:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 404 @ 32724 updates
2022-08-17 07:57:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint404.pt
2022-08-17 07:58:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint404.pt
2022-08-17 07:58:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint404.pt (epoch 404 @ 32724 updates, score 56.46) (writing took 30.4487168751657 seconds)
2022-08-17 07:58:29 | INFO | fairseq_cli.train | end of epoch 404 (average epoch stats below)
2022-08-17 07:58:29 | INFO | train | epoch 404 | loss 3.389 | nll_loss 0.353 | ppl 1.28 | wps 5368.7 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 32724 | lr 0.00034962 | gnorm 0.285 | train_wall 40 | gb_free 10.1 | wall 35206
2022-08-17 07:58:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 07:58:29 | INFO | fairseq.trainer | begin training epoch 405
2022-08-17 07:58:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 07:59:11 | INFO | train_inner | epoch 405:     76 / 81 loss=3.388, nll_loss=0.352, ppl=1.28, wps=5863.6, ups=1.06, wpb=5529.9, bsz=361.2, num_updates=32800, lr=0.000349215, gnorm=0.291, train_wall=50, gb_free=10.1, wall=35247
2022-08-17 07:59:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 07:59:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 07:59:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 07:59:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 07:59:23 | INFO | valid | epoch 405 | valid on 'valid' subset | loss 5.097 | nll_loss 2.467 | ppl 5.53 | bleu 57.17 | wps 1776.7 | wpb 933.5 | bsz 59.6 | num_updates 32805 | best_bleu 57.33
2022-08-17 07:59:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 405 @ 32805 updates
2022-08-17 07:59:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint405.pt
2022-08-17 07:59:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint405.pt
2022-08-17 08:00:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint405.pt (epoch 405 @ 32805 updates, score 57.17) (writing took 40.985776249319315 seconds)
2022-08-17 08:00:04 | INFO | fairseq_cli.train | end of epoch 405 (average epoch stats below)
2022-08-17 08:00:04 | INFO | train | epoch 405 | loss 3.388 | nll_loss 0.352 | ppl 1.28 | wps 4726.3 | ups 0.86 | wpb 5523.2 | bsz 358 | num_updates 32805 | lr 0.000349189 | gnorm 0.302 | train_wall 41 | gb_free 10.2 | wall 35300
2022-08-17 08:00:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:00:04 | INFO | fairseq.trainer | begin training epoch 406
2022-08-17 08:00:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:00:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:00:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:00:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:00:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:00:55 | INFO | valid | epoch 406 | valid on 'valid' subset | loss 5.094 | nll_loss 2.465 | ppl 5.52 | bleu 56.53 | wps 1963.4 | wpb 933.5 | bsz 59.6 | num_updates 32886 | best_bleu 57.33
2022-08-17 08:00:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 406 @ 32886 updates
2022-08-17 08:00:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint406.pt
2022-08-17 08:00:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint406.pt
2022-08-17 08:01:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint406.pt (epoch 406 @ 32886 updates, score 56.53) (writing took 17.95824171230197 seconds)
2022-08-17 08:01:13 | INFO | fairseq_cli.train | end of epoch 406 (average epoch stats below)
2022-08-17 08:01:13 | INFO | train | epoch 406 | loss 3.389 | nll_loss 0.353 | ppl 1.28 | wps 6500.3 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 32886 | lr 0.000348758 | gnorm 0.34 | train_wall 40 | gb_free 10.2 | wall 35369
2022-08-17 08:01:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:01:13 | INFO | fairseq.trainer | begin training epoch 407
2022-08-17 08:01:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:01:21 | INFO | train_inner | epoch 407:     14 / 81 loss=3.389, nll_loss=0.353, ppl=1.28, wps=4241.9, ups=0.77, wpb=5509.9, bsz=356.2, num_updates=32900, lr=0.000348684, gnorm=0.345, train_wall=49, gb_free=10.1, wall=35377
2022-08-17 08:01:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:01:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:01:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:01:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:02:05 | INFO | valid | epoch 407 | valid on 'valid' subset | loss 5.088 | nll_loss 2.455 | ppl 5.48 | bleu 56.49 | wps 1894.3 | wpb 933.5 | bsz 59.6 | num_updates 32967 | best_bleu 57.33
2022-08-17 08:02:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 407 @ 32967 updates
2022-08-17 08:02:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint407.pt
2022-08-17 08:02:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint407.pt
2022-08-17 08:02:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint407.pt (epoch 407 @ 32967 updates, score 56.49) (writing took 20.62929232046008 seconds)
2022-08-17 08:02:26 | INFO | fairseq_cli.train | end of epoch 407 (average epoch stats below)
2022-08-17 08:02:26 | INFO | train | epoch 407 | loss 3.389 | nll_loss 0.352 | ppl 1.28 | wps 6154.4 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 32967 | lr 0.00034833 | gnorm 0.294 | train_wall 41 | gb_free 10.2 | wall 35442
2022-08-17 08:02:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:02:26 | INFO | fairseq.trainer | begin training epoch 408
2022-08-17 08:02:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:02:44 | INFO | train_inner | epoch 408:     33 / 81 loss=3.388, nll_loss=0.351, ppl=1.28, wps=6629.6, ups=1.2, wpb=5536.4, bsz=358.3, num_updates=33000, lr=0.000348155, gnorm=0.276, train_wall=51, gb_free=10.1, wall=35461
2022-08-17 08:03:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:03:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:03:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:03:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:03:18 | INFO | valid | epoch 408 | valid on 'valid' subset | loss 5.104 | nll_loss 2.475 | ppl 5.56 | bleu 56.5 | wps 1927.9 | wpb 933.5 | bsz 59.6 | num_updates 33048 | best_bleu 57.33
2022-08-17 08:03:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 408 @ 33048 updates
2022-08-17 08:03:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint408.pt
2022-08-17 08:03:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint408.pt
2022-08-17 08:03:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint408.pt (epoch 408 @ 33048 updates, score 56.5) (writing took 34.563661094754934 seconds)
2022-08-17 08:03:52 | INFO | fairseq_cli.train | end of epoch 408 (average epoch stats below)
2022-08-17 08:03:52 | INFO | train | epoch 408 | loss 3.387 | nll_loss 0.351 | ppl 1.28 | wps 5145.1 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 33048 | lr 0.000347902 | gnorm 0.32 | train_wall 41 | gb_free 10.2 | wall 35529
2022-08-17 08:03:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:03:53 | INFO | fairseq.trainer | begin training epoch 409
2022-08-17 08:03:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:04:22 | INFO | train_inner | epoch 409:     52 / 81 loss=3.389, nll_loss=0.353, ppl=1.28, wps=5631.5, ups=1.03, wpb=5491.6, bsz=355.9, num_updates=33100, lr=0.000347629, gnorm=0.381, train_wall=50, gb_free=10.1, wall=35558
2022-08-17 08:04:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:04:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:04:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:04:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:04:46 | INFO | valid | epoch 409 | valid on 'valid' subset | loss 5.092 | nll_loss 2.467 | ppl 5.53 | bleu 56.42 | wps 1840.6 | wpb 933.5 | bsz 59.6 | num_updates 33129 | best_bleu 57.33
2022-08-17 08:04:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 409 @ 33129 updates
2022-08-17 08:04:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint409.pt
2022-08-17 08:04:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint409.pt
2022-08-17 08:05:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint409.pt (epoch 409 @ 33129 updates, score 56.42) (writing took 25.742729496210814 seconds)
2022-08-17 08:05:12 | INFO | fairseq_cli.train | end of epoch 409 (average epoch stats below)
2022-08-17 08:05:12 | INFO | train | epoch 409 | loss 3.389 | nll_loss 0.353 | ppl 1.28 | wps 5610.2 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 33129 | lr 0.000347477 | gnorm 0.366 | train_wall 41 | gb_free 10.1 | wall 35608
2022-08-17 08:05:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:05:12 | INFO | fairseq.trainer | begin training epoch 410
2022-08-17 08:05:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:05:51 | INFO | train_inner | epoch 410:     71 / 81 loss=3.388, nll_loss=0.352, ppl=1.28, wps=6221.4, ups=1.12, wpb=5571.6, bsz=362.9, num_updates=33200, lr=0.000347105, gnorm=0.293, train_wall=50, gb_free=10.1, wall=35648
2022-08-17 08:05:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:05:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:05:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:05:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:06:05 | INFO | valid | epoch 410 | valid on 'valid' subset | loss 5.114 | nll_loss 2.49 | ppl 5.62 | bleu 56.23 | wps 2020.9 | wpb 933.5 | bsz 59.6 | num_updates 33210 | best_bleu 57.33
2022-08-17 08:06:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 410 @ 33210 updates
2022-08-17 08:06:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint410.pt
2022-08-17 08:06:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint410.pt
2022-08-17 08:06:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint410.pt (epoch 410 @ 33210 updates, score 56.23) (writing took 16.91440474614501 seconds)
2022-08-17 08:06:23 | INFO | fairseq_cli.train | end of epoch 410 (average epoch stats below)
2022-08-17 08:06:23 | INFO | train | epoch 410 | loss 3.388 | nll_loss 0.352 | ppl 1.28 | wps 6353.3 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 33210 | lr 0.000347053 | gnorm 0.291 | train_wall 40 | gb_free 10.1 | wall 35679
2022-08-17 08:06:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:06:23 | INFO | fairseq.trainer | begin training epoch 411
2022-08-17 08:06:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:07:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:07:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:07:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:07:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:07:14 | INFO | valid | epoch 411 | valid on 'valid' subset | loss 5.114 | nll_loss 2.487 | ppl 5.61 | bleu 56.39 | wps 1932.2 | wpb 933.5 | bsz 59.6 | num_updates 33291 | best_bleu 57.33
2022-08-17 08:07:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 411 @ 33291 updates
2022-08-17 08:07:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint411.pt
2022-08-17 08:07:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint411.pt
2022-08-17 08:07:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint411.pt (epoch 411 @ 33291 updates, score 56.39) (writing took 33.73490106314421 seconds)
2022-08-17 08:07:48 | INFO | fairseq_cli.train | end of epoch 411 (average epoch stats below)
2022-08-17 08:07:48 | INFO | train | epoch 411 | loss 3.388 | nll_loss 0.353 | ppl 1.28 | wps 5223.2 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 33291 | lr 0.00034663 | gnorm 0.286 | train_wall 41 | gb_free 10.1 | wall 35765
2022-08-17 08:07:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:07:48 | INFO | fairseq.trainer | begin training epoch 412
2022-08-17 08:07:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:07:54 | INFO | train_inner | epoch 412:      9 / 81 loss=3.388, nll_loss=0.352, ppl=1.28, wps=4472.1, ups=0.82, wpb=5479.8, bsz=355, num_updates=33300, lr=0.000346583, gnorm=0.284, train_wall=50, gb_free=10.1, wall=35770
2022-08-17 08:08:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:08:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:08:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:08:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:08:40 | INFO | valid | epoch 412 | valid on 'valid' subset | loss 5.091 | nll_loss 2.461 | ppl 5.51 | bleu 56.86 | wps 1930.5 | wpb 933.5 | bsz 59.6 | num_updates 33372 | best_bleu 57.33
2022-08-17 08:08:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 412 @ 33372 updates
2022-08-17 08:08:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint412.pt
2022-08-17 08:08:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint412.pt
2022-08-17 08:09:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint412.pt (epoch 412 @ 33372 updates, score 56.86) (writing took 32.72297136858106 seconds)
2022-08-17 08:09:13 | INFO | fairseq_cli.train | end of epoch 412 (average epoch stats below)
2022-08-17 08:09:13 | INFO | train | epoch 412 | loss 3.389 | nll_loss 0.353 | ppl 1.28 | wps 5297.3 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 33372 | lr 0.000346209 | gnorm 0.41 | train_wall 40 | gb_free 10 | wall 35849
2022-08-17 08:09:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:09:13 | INFO | fairseq.trainer | begin training epoch 413
2022-08-17 08:09:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:09:28 | INFO | train_inner | epoch 413:     28 / 81 loss=3.389, nll_loss=0.353, ppl=1.28, wps=5884.8, ups=1.06, wpb=5551, bsz=359.4, num_updates=33400, lr=0.000346064, gnorm=0.489, train_wall=49, gb_free=10.1, wall=35865
2022-08-17 08:09:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:09:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:09:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:09:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:10:04 | INFO | valid | epoch 413 | valid on 'valid' subset | loss 5.083 | nll_loss 2.451 | ppl 5.47 | bleu 56.81 | wps 1906.6 | wpb 933.5 | bsz 59.6 | num_updates 33453 | best_bleu 57.33
2022-08-17 08:10:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 413 @ 33453 updates
2022-08-17 08:10:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint413.pt
2022-08-17 08:10:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint413.pt
2022-08-17 08:10:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint413.pt (epoch 413 @ 33453 updates, score 56.81) (writing took 23.62433246523142 seconds)
2022-08-17 08:10:28 | INFO | fairseq_cli.train | end of epoch 413 (average epoch stats below)
2022-08-17 08:10:28 | INFO | train | epoch 413 | loss 3.388 | nll_loss 0.352 | ppl 1.28 | wps 5936.7 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 33453 | lr 0.00034579 | gnorm 0.413 | train_wall 41 | gb_free 10.2 | wall 35924
2022-08-17 08:10:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:10:28 | INFO | fairseq.trainer | begin training epoch 414
2022-08-17 08:10:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:10:59 | INFO | train_inner | epoch 414:     47 / 81 loss=3.388, nll_loss=0.352, ppl=1.28, wps=6070.3, ups=1.11, wpb=5486.9, bsz=352.7, num_updates=33500, lr=0.000345547, gnorm=0.291, train_wall=50, gb_free=10.2, wall=35955
2022-08-17 08:11:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:11:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:11:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:11:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:11:29 | INFO | valid | epoch 414 | valid on 'valid' subset | loss 5.106 | nll_loss 2.476 | ppl 5.56 | bleu 56.13 | wps 1922.3 | wpb 933.5 | bsz 59.6 | num_updates 33534 | best_bleu 57.33
2022-08-17 08:11:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 414 @ 33534 updates
2022-08-17 08:11:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint414.pt
2022-08-17 08:11:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint414.pt
2022-08-17 08:11:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint414.pt (epoch 414 @ 33534 updates, score 56.13) (writing took 17.002458170056343 seconds)
2022-08-17 08:11:46 | INFO | fairseq_cli.train | end of epoch 414 (average epoch stats below)
2022-08-17 08:11:46 | INFO | train | epoch 414 | loss 3.388 | nll_loss 0.351 | ppl 1.28 | wps 5741.9 | ups 1.04 | wpb 5523.2 | bsz 358 | num_updates 33534 | lr 0.000345372 | gnorm 0.276 | train_wall 41 | gb_free 10.1 | wall 36002
2022-08-17 08:11:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:11:46 | INFO | fairseq.trainer | begin training epoch 415
2022-08-17 08:11:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:12:21 | INFO | train_inner | epoch 415:     66 / 81 loss=3.388, nll_loss=0.352, ppl=1.28, wps=6744.1, ups=1.22, wpb=5542.3, bsz=362.5, num_updates=33600, lr=0.000345033, gnorm=0.272, train_wall=51, gb_free=10.1, wall=36037
2022-08-17 08:12:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:12:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:12:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:12:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:12:38 | INFO | valid | epoch 415 | valid on 'valid' subset | loss 5.096 | nll_loss 2.466 | ppl 5.53 | bleu 56.62 | wps 1879.6 | wpb 933.5 | bsz 59.6 | num_updates 33615 | best_bleu 57.33
2022-08-17 08:12:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 415 @ 33615 updates
2022-08-17 08:12:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint415.pt
2022-08-17 08:12:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint415.pt
2022-08-17 08:13:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint415.pt (epoch 415 @ 33615 updates, score 56.62) (writing took 25.6742347702384 seconds)
2022-08-17 08:13:03 | INFO | fairseq_cli.train | end of epoch 415 (average epoch stats below)
2022-08-17 08:13:03 | INFO | train | epoch 415 | loss 3.388 | nll_loss 0.352 | ppl 1.28 | wps 5784 | ups 1.05 | wpb 5523.2 | bsz 358 | num_updates 33615 | lr 0.000344956 | gnorm 0.286 | train_wall 41 | gb_free 10.2 | wall 36080
2022-08-17 08:13:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:13:04 | INFO | fairseq.trainer | begin training epoch 416
2022-08-17 08:13:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:13:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:13:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:13:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:13:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:14:00 | INFO | valid | epoch 416 | valid on 'valid' subset | loss 5.111 | nll_loss 2.489 | ppl 5.61 | bleu 56.54 | wps 1795.2 | wpb 933.5 | bsz 59.6 | num_updates 33696 | best_bleu 57.33
2022-08-17 08:14:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 416 @ 33696 updates
2022-08-17 08:14:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint416.pt
2022-08-17 08:14:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint416.pt
2022-08-17 08:14:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint416.pt (epoch 416 @ 33696 updates, score 56.54) (writing took 41.53540838882327 seconds)
2022-08-17 08:14:41 | INFO | fairseq_cli.train | end of epoch 416 (average epoch stats below)
2022-08-17 08:14:41 | INFO | train | epoch 416 | loss 3.388 | nll_loss 0.352 | ppl 1.28 | wps 4568.9 | ups 0.83 | wpb 5523.2 | bsz 358 | num_updates 33696 | lr 0.000344541 | gnorm 0.286 | train_wall 41 | gb_free 10.1 | wall 36178
2022-08-17 08:14:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:14:42 | INFO | fairseq.trainer | begin training epoch 417
2022-08-17 08:14:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:14:45 | INFO | train_inner | epoch 417:      4 / 81 loss=3.388, nll_loss=0.352, ppl=1.28, wps=3839.2, ups=0.7, wpb=5514.5, bsz=357.7, num_updates=33700, lr=0.00034452, gnorm=0.288, train_wall=50, gb_free=10, wall=36181
2022-08-17 08:15:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:15:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:15:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:15:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:15:33 | INFO | valid | epoch 417 | valid on 'valid' subset | loss 5.096 | nll_loss 2.464 | ppl 5.52 | bleu 56 | wps 1871.6 | wpb 933.5 | bsz 59.6 | num_updates 33777 | best_bleu 57.33
2022-08-17 08:15:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 417 @ 33777 updates
2022-08-17 08:15:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint417.pt
2022-08-17 08:15:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint417.pt
2022-08-17 08:15:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint417.pt (epoch 417 @ 33777 updates, score 56.0) (writing took 2.3345759622752666 seconds)
2022-08-17 08:15:35 | INFO | fairseq_cli.train | end of epoch 417 (average epoch stats below)
2022-08-17 08:15:35 | INFO | train | epoch 417 | loss 3.388 | nll_loss 0.352 | ppl 1.28 | wps 8323.8 | ups 1.51 | wpb 5523.2 | bsz 358 | num_updates 33777 | lr 0.000344128 | gnorm 0.304 | train_wall 40 | gb_free 10.2 | wall 36231
2022-08-17 08:15:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:15:35 | INFO | fairseq.trainer | begin training epoch 418
2022-08-17 08:15:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:15:49 | INFO | train_inner | epoch 418:     23 / 81 loss=3.387, nll_loss=0.351, ppl=1.28, wps=8550.6, ups=1.54, wpb=5537.1, bsz=362.3, num_updates=33800, lr=0.00034401, gnorm=0.307, train_wall=50, gb_free=10.1, wall=36246
2022-08-17 08:16:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:16:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:16:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:16:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:16:29 | INFO | valid | epoch 418 | valid on 'valid' subset | loss 5.096 | nll_loss 2.47 | ppl 5.54 | bleu 56.32 | wps 1811.9 | wpb 933.5 | bsz 59.6 | num_updates 33858 | best_bleu 57.33
2022-08-17 08:16:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 418 @ 33858 updates
2022-08-17 08:16:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint418.pt
2022-08-17 08:16:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint418.pt
2022-08-17 08:16:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint418.pt (epoch 418 @ 33858 updates, score 56.32) (writing took 21.623413622379303 seconds)
2022-08-17 08:16:50 | INFO | fairseq_cli.train | end of epoch 418 (average epoch stats below)
2022-08-17 08:16:50 | INFO | train | epoch 418 | loss 3.387 | nll_loss 0.351 | ppl 1.28 | wps 5930.9 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 33858 | lr 0.000343716 | gnorm 0.289 | train_wall 41 | gb_free 10.1 | wall 36307
2022-08-17 08:16:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:16:51 | INFO | fairseq.trainer | begin training epoch 419
2022-08-17 08:16:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:17:13 | INFO | train_inner | epoch 419:     42 / 81 loss=3.388, nll_loss=0.352, ppl=1.28, wps=6544.1, ups=1.19, wpb=5497, bsz=347.3, num_updates=33900, lr=0.000343503, gnorm=0.287, train_wall=50, gb_free=10, wall=36330
2022-08-17 08:17:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:17:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:17:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:17:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:17:44 | INFO | valid | epoch 419 | valid on 'valid' subset | loss 5.111 | nll_loss 2.485 | ppl 5.6 | bleu 56.94 | wps 1758.9 | wpb 933.5 | bsz 59.6 | num_updates 33939 | best_bleu 57.33
2022-08-17 08:17:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 419 @ 33939 updates
2022-08-17 08:17:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint419.pt
2022-08-17 08:17:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint419.pt
2022-08-17 08:18:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint419.pt (epoch 419 @ 33939 updates, score 56.94) (writing took 33.53892530873418 seconds)
2022-08-17 08:18:18 | INFO | fairseq_cli.train | end of epoch 419 (average epoch stats below)
2022-08-17 08:18:18 | INFO | train | epoch 419 | loss 3.388 | nll_loss 0.351 | ppl 1.28 | wps 5138.8 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 33939 | lr 0.000343305 | gnorm 0.289 | train_wall 41 | gb_free 10 | wall 36394
2022-08-17 08:18:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:18:18 | INFO | fairseq.trainer | begin training epoch 420
2022-08-17 08:18:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:18:51 | INFO | train_inner | epoch 420:     61 / 81 loss=3.388, nll_loss=0.352, ppl=1.28, wps=5714.6, ups=1.03, wpb=5556.2, bsz=368.5, num_updates=34000, lr=0.000342997, gnorm=0.351, train_wall=51, gb_free=10, wall=36427
2022-08-17 08:19:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:19:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:19:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:19:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:19:10 | INFO | valid | epoch 420 | valid on 'valid' subset | loss 5.1 | nll_loss 2.474 | ppl 5.56 | bleu 57.24 | wps 1708.5 | wpb 933.5 | bsz 59.6 | num_updates 34020 | best_bleu 57.33
2022-08-17 08:19:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 420 @ 34020 updates
2022-08-17 08:19:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint420.pt
2022-08-17 08:19:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint420.pt
2022-08-17 08:19:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint420.pt (epoch 420 @ 34020 updates, score 57.24) (writing took 18.64487835392356 seconds)
2022-08-17 08:19:29 | INFO | fairseq_cli.train | end of epoch 420 (average epoch stats below)
2022-08-17 08:19:29 | INFO | train | epoch 420 | loss 3.389 | nll_loss 0.354 | ppl 1.28 | wps 6241.2 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 34020 | lr 0.000342896 | gnorm 0.395 | train_wall 41 | gb_free 10.1 | wall 36465
2022-08-17 08:19:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:19:29 | INFO | fairseq.trainer | begin training epoch 421
2022-08-17 08:19:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:20:12 | INFO | train_inner | epoch 421:     80 / 81 loss=3.388, nll_loss=0.352, ppl=1.28, wps=6772.5, ups=1.23, wpb=5527, bsz=355, num_updates=34100, lr=0.000342494, gnorm=0.306, train_wall=51, gb_free=10.1, wall=36508
2022-08-17 08:20:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:20:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:20:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:20:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:20:22 | INFO | valid | epoch 421 | valid on 'valid' subset | loss 5.121 | nll_loss 2.5 | ppl 5.66 | bleu 56.59 | wps 1789.7 | wpb 933.5 | bsz 59.6 | num_updates 34101 | best_bleu 57.33
2022-08-17 08:20:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 421 @ 34101 updates
2022-08-17 08:20:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint421.pt
2022-08-17 08:20:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint421.pt
2022-08-17 08:20:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint421.pt (epoch 421 @ 34101 updates, score 56.59) (writing took 16.664058472961187 seconds)
2022-08-17 08:20:39 | INFO | fairseq_cli.train | end of epoch 421 (average epoch stats below)
2022-08-17 08:20:39 | INFO | train | epoch 421 | loss 3.387 | nll_loss 0.351 | ppl 1.28 | wps 6414.6 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 34101 | lr 0.000342489 | gnorm 0.285 | train_wall 41 | gb_free 10.1 | wall 36535
2022-08-17 08:20:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:20:39 | INFO | fairseq.trainer | begin training epoch 422
2022-08-17 08:20:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:21:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:21:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:21:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:21:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:21:32 | INFO | valid | epoch 422 | valid on 'valid' subset | loss 5.119 | nll_loss 2.499 | ppl 5.65 | bleu 56.58 | wps 1789.9 | wpb 933.5 | bsz 59.6 | num_updates 34182 | best_bleu 57.33
2022-08-17 08:21:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 422 @ 34182 updates
2022-08-17 08:21:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint422.pt
2022-08-17 08:21:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint422.pt
2022-08-17 08:22:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint422.pt (epoch 422 @ 34182 updates, score 56.58) (writing took 38.48607637733221 seconds)
2022-08-17 08:22:10 | INFO | fairseq_cli.train | end of epoch 422 (average epoch stats below)
2022-08-17 08:22:10 | INFO | train | epoch 422 | loss 3.388 | nll_loss 0.352 | ppl 1.28 | wps 4906.3 | ups 0.89 | wpb 5523.2 | bsz 358 | num_updates 34182 | lr 0.000342083 | gnorm 0.278 | train_wall 41 | gb_free 10.1 | wall 36626
2022-08-17 08:22:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:22:10 | INFO | fairseq.trainer | begin training epoch 423
2022-08-17 08:22:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:22:20 | INFO | train_inner | epoch 423:     18 / 81 loss=3.387, nll_loss=0.351, ppl=1.28, wps=4275.7, ups=0.78, wpb=5485.5, bsz=353.8, num_updates=34200, lr=0.000341993, gnorm=0.278, train_wall=50, gb_free=10, wall=36637
2022-08-17 08:22:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:22:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:22:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:22:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:23:03 | INFO | valid | epoch 423 | valid on 'valid' subset | loss 5.124 | nll_loss 2.503 | ppl 5.67 | bleu 55.66 | wps 1876.9 | wpb 933.5 | bsz 59.6 | num_updates 34263 | best_bleu 57.33
2022-08-17 08:23:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 423 @ 34263 updates
2022-08-17 08:23:03 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint423.pt
2022-08-17 08:23:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint423.pt
2022-08-17 08:23:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint423.pt (epoch 423 @ 34263 updates, score 55.66) (writing took 17.67218467593193 seconds)
2022-08-17 08:23:21 | INFO | fairseq_cli.train | end of epoch 423 (average epoch stats below)
2022-08-17 08:23:21 | INFO | train | epoch 423 | loss 3.388 | nll_loss 0.352 | ppl 1.28 | wps 6319.5 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 34263 | lr 0.000341678 | gnorm 0.367 | train_wall 41 | gb_free 10.1 | wall 36697
2022-08-17 08:23:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:23:21 | INFO | fairseq.trainer | begin training epoch 424
2022-08-17 08:23:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:23:42 | INFO | train_inner | epoch 424:     37 / 81 loss=3.388, nll_loss=0.352, ppl=1.28, wps=6814.1, ups=1.23, wpb=5550, bsz=360, num_updates=34300, lr=0.000341494, gnorm=0.353, train_wall=51, gb_free=10.1, wall=36718
2022-08-17 08:24:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:24:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:24:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:24:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:24:15 | INFO | valid | epoch 424 | valid on 'valid' subset | loss 5.112 | nll_loss 2.487 | ppl 5.6 | bleu 56.77 | wps 1902.1 | wpb 933.5 | bsz 59.6 | num_updates 34344 | best_bleu 57.33
2022-08-17 08:24:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 424 @ 34344 updates
2022-08-17 08:24:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint424.pt
2022-08-17 08:24:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint424.pt
2022-08-17 08:24:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint424.pt (epoch 424 @ 34344 updates, score 56.77) (writing took 26.48529192060232 seconds)
2022-08-17 08:24:41 | INFO | fairseq_cli.train | end of epoch 424 (average epoch stats below)
2022-08-17 08:24:41 | INFO | train | epoch 424 | loss 3.389 | nll_loss 0.353 | ppl 1.28 | wps 5556.6 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 34344 | lr 0.000341275 | gnorm 0.596 | train_wall 43 | gb_free 10.1 | wall 36778
2022-08-17 08:24:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:24:42 | INFO | fairseq.trainer | begin training epoch 425
2022-08-17 08:24:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:25:12 | INFO | train_inner | epoch 425:     56 / 81 loss=3.39, nll_loss=0.355, ppl=1.28, wps=6142.8, ups=1.11, wpb=5535.9, bsz=359.6, num_updates=34400, lr=0.000340997, gnorm=0.568, train_wall=52, gb_free=10.1, wall=36808
2022-08-17 08:25:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:25:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:25:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:25:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:25:33 | INFO | valid | epoch 425 | valid on 'valid' subset | loss 5.101 | nll_loss 2.477 | ppl 5.57 | bleu 56.75 | wps 1967.8 | wpb 933.5 | bsz 59.6 | num_updates 34425 | best_bleu 57.33
2022-08-17 08:25:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 425 @ 34425 updates
2022-08-17 08:25:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint425.pt
2022-08-17 08:25:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint425.pt
2022-08-17 08:26:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint425.pt (epoch 425 @ 34425 updates, score 56.75) (writing took 32.77912801876664 seconds)
2022-08-17 08:26:06 | INFO | fairseq_cli.train | end of epoch 425 (average epoch stats below)
2022-08-17 08:26:06 | INFO | train | epoch 425 | loss 3.39 | nll_loss 0.355 | ppl 1.28 | wps 5277.7 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 34425 | lr 0.000340873 | gnorm 0.358 | train_wall 41 | gb_free 10.1 | wall 36862
2022-08-17 08:26:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:26:06 | INFO | fairseq.trainer | begin training epoch 426
2022-08-17 08:26:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:26:47 | INFO | train_inner | epoch 426:     75 / 81 loss=3.389, nll_loss=0.353, ppl=1.28, wps=5808.9, ups=1.05, wpb=5517.1, bsz=357.1, num_updates=34500, lr=0.000340503, gnorm=0.387, train_wall=50, gb_free=10.2, wall=36903
2022-08-17 08:26:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:26:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:26:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:26:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:26:59 | INFO | valid | epoch 426 | valid on 'valid' subset | loss 5.111 | nll_loss 2.49 | ppl 5.62 | bleu 56.63 | wps 1907 | wpb 933.5 | bsz 59.6 | num_updates 34506 | best_bleu 57.33
2022-08-17 08:26:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 426 @ 34506 updates
2022-08-17 08:26:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint426.pt
2022-08-17 08:27:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint426.pt
2022-08-17 08:27:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint426.pt (epoch 426 @ 34506 updates, score 56.63) (writing took 28.163870632648468 seconds)
2022-08-17 08:27:27 | INFO | fairseq_cli.train | end of epoch 426 (average epoch stats below)
2022-08-17 08:27:27 | INFO | train | epoch 426 | loss 3.388 | nll_loss 0.352 | ppl 1.28 | wps 5528.4 | ups 1 | wpb 5523.2 | bsz 358 | num_updates 34506 | lr 0.000340473 | gnorm 0.378 | train_wall 41 | gb_free 10.1 | wall 36943
2022-08-17 08:27:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:27:27 | INFO | fairseq.trainer | begin training epoch 427
2022-08-17 08:27:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:28:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:28:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:28:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:28:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:28:19 | INFO | valid | epoch 427 | valid on 'valid' subset | loss 5.101 | nll_loss 2.476 | ppl 5.56 | bleu 57.15 | wps 1939.7 | wpb 933.5 | bsz 59.6 | num_updates 34587 | best_bleu 57.33
2022-08-17 08:28:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 427 @ 34587 updates
2022-08-17 08:28:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint427.pt
2022-08-17 08:28:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint427.pt
2022-08-17 08:28:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint427.pt (epoch 427 @ 34587 updates, score 57.15) (writing took 16.973666422069073 seconds)
2022-08-17 08:28:37 | INFO | fairseq_cli.train | end of epoch 427 (average epoch stats below)
2022-08-17 08:28:37 | INFO | train | epoch 427 | loss 3.388 | nll_loss 0.352 | ppl 1.28 | wps 6448.4 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 34587 | lr 0.000340074 | gnorm 0.322 | train_wall 41 | gb_free 10.2 | wall 37013
2022-08-17 08:28:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:28:37 | INFO | fairseq.trainer | begin training epoch 428
2022-08-17 08:28:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:28:45 | INFO | train_inner | epoch 428:     13 / 81 loss=3.387, nll_loss=0.351, ppl=1.28, wps=4660.7, ups=0.85, wpb=5514.9, bsz=360.6, num_updates=34600, lr=0.00034001, gnorm=0.314, train_wall=51, gb_free=10.1, wall=37022
2022-08-17 08:29:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:29:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:29:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:29:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:29:30 | INFO | valid | epoch 428 | valid on 'valid' subset | loss 5.097 | nll_loss 2.474 | ppl 5.56 | bleu 57.1 | wps 1884.3 | wpb 933.5 | bsz 59.6 | num_updates 34668 | best_bleu 57.33
2022-08-17 08:29:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 428 @ 34668 updates
2022-08-17 08:29:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint428.pt
2022-08-17 08:29:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint428.pt
2022-08-17 08:30:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint428.pt (epoch 428 @ 34668 updates, score 57.1) (writing took 39.81024618074298 seconds)
2022-08-17 08:30:10 | INFO | fairseq_cli.train | end of epoch 428 (average epoch stats below)
2022-08-17 08:30:10 | INFO | train | epoch 428 | loss 3.387 | nll_loss 0.35 | ppl 1.27 | wps 4812 | ups 0.87 | wpb 5523.2 | bsz 358 | num_updates 34668 | lr 0.000339677 | gnorm 0.265 | train_wall 41 | gb_free 10.2 | wall 37106
2022-08-17 08:30:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:30:10 | INFO | fairseq.trainer | begin training epoch 429
2022-08-17 08:30:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:30:30 | INFO | train_inner | epoch 429:     32 / 81 loss=3.387, nll_loss=0.35, ppl=1.27, wps=5283.2, ups=0.96, wpb=5529.2, bsz=359.1, num_updates=34700, lr=0.00033952, gnorm=0.268, train_wall=51, gb_free=10.1, wall=37126
2022-08-17 08:30:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:31:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:31:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:31:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:31:08 | INFO | valid | epoch 429 | valid on 'valid' subset | loss 5.119 | nll_loss 2.499 | ppl 5.65 | bleu 56.44 | wps 1774.4 | wpb 933.5 | bsz 59.6 | num_updates 34749 | best_bleu 57.33
2022-08-17 08:31:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 429 @ 34749 updates
2022-08-17 08:31:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint429.pt
2022-08-17 08:31:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint429.pt
2022-08-17 08:31:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint429.pt (epoch 429 @ 34749 updates, score 56.44) (writing took 50.32366729155183 seconds)
2022-08-17 08:31:58 | INFO | fairseq_cli.train | end of epoch 429 (average epoch stats below)
2022-08-17 08:31:58 | INFO | train | epoch 429 | loss 3.387 | nll_loss 0.351 | ppl 1.28 | wps 4107.9 | ups 0.74 | wpb 5523.2 | bsz 358 | num_updates 34749 | lr 0.00033928 | gnorm 0.291 | train_wall 42 | gb_free 10.1 | wall 37215
2022-08-17 08:31:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:31:59 | INFO | fairseq.trainer | begin training epoch 430
2022-08-17 08:31:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:32:27 | INFO | train_inner | epoch 430:     51 / 81 loss=3.387, nll_loss=0.351, ppl=1.28, wps=4710.2, ups=0.85, wpb=5517.4, bsz=357, num_updates=34800, lr=0.000339032, gnorm=0.289, train_wall=51, gb_free=10.1, wall=37243
2022-08-17 08:32:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:32:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:32:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:32:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:32:52 | INFO | valid | epoch 430 | valid on 'valid' subset | loss 5.118 | nll_loss 2.5 | ppl 5.65 | bleu 56.98 | wps 1890.4 | wpb 933.5 | bsz 59.6 | num_updates 34830 | best_bleu 57.33
2022-08-17 08:32:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 430 @ 34830 updates
2022-08-17 08:32:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint430.pt
2022-08-17 08:32:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint430.pt
2022-08-17 08:33:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint430.pt (epoch 430 @ 34830 updates, score 56.98) (writing took 21.882757045328617 seconds)
2022-08-17 08:33:14 | INFO | fairseq_cli.train | end of epoch 430 (average epoch stats below)
2022-08-17 08:33:14 | INFO | train | epoch 430 | loss 3.387 | nll_loss 0.351 | ppl 1.28 | wps 5955.7 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 34830 | lr 0.000338886 | gnorm 1.325 | train_wall 41 | gb_free 10.1 | wall 37290
2022-08-17 08:33:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:33:14 | INFO | fairseq.trainer | begin training epoch 431
2022-08-17 08:33:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:33:53 | INFO | train_inner | epoch 431:     70 / 81 loss=3.389, nll_loss=0.354, ppl=1.28, wps=6466.1, ups=1.17, wpb=5524.7, bsz=355.8, num_updates=34900, lr=0.000338546, gnorm=1.193, train_wall=50, gb_free=10.1, wall=37329
2022-08-17 08:33:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:34:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:34:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:34:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:34:08 | INFO | valid | epoch 431 | valid on 'valid' subset | loss 5.106 | nll_loss 2.482 | ppl 5.59 | bleu 56.56 | wps 1892 | wpb 933.5 | bsz 59.6 | num_updates 34911 | best_bleu 57.33
2022-08-17 08:34:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 431 @ 34911 updates
2022-08-17 08:34:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint431.pt
2022-08-17 08:34:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint431.pt
2022-08-17 08:34:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint431.pt (epoch 431 @ 34911 updates, score 56.56) (writing took 17.316996965557337 seconds)
2022-08-17 08:34:25 | INFO | fairseq_cli.train | end of epoch 431 (average epoch stats below)
2022-08-17 08:34:25 | INFO | train | epoch 431 | loss 3.389 | nll_loss 0.354 | ppl 1.28 | wps 6235.9 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 34911 | lr 0.000338492 | gnorm 0.361 | train_wall 41 | gb_free 10.1 | wall 37362
2022-08-17 08:34:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:34:25 | INFO | fairseq.trainer | begin training epoch 432
2022-08-17 08:34:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:35:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:35:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:35:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:35:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:35:18 | INFO | valid | epoch 432 | valid on 'valid' subset | loss 5.098 | nll_loss 2.467 | ppl 5.53 | bleu 57.1 | wps 1752 | wpb 933.5 | bsz 59.6 | num_updates 34992 | best_bleu 57.33
2022-08-17 08:35:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 432 @ 34992 updates
2022-08-17 08:35:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint432.pt
2022-08-17 08:35:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint432.pt
2022-08-17 08:35:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint432.pt (epoch 432 @ 34992 updates, score 57.1) (writing took 16.92144401744008 seconds)
2022-08-17 08:35:35 | INFO | fairseq_cli.train | end of epoch 432 (average epoch stats below)
2022-08-17 08:35:35 | INFO | train | epoch 432 | loss 3.388 | nll_loss 0.352 | ppl 1.28 | wps 6420.8 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 34992 | lr 0.0003381 | gnorm 0.323 | train_wall 41 | gb_free 10.1 | wall 37431
2022-08-17 08:35:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:35:35 | INFO | fairseq.trainer | begin training epoch 433
2022-08-17 08:35:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:35:41 | INFO | train_inner | epoch 433:      8 / 81 loss=3.388, nll_loss=0.352, ppl=1.28, wps=5083.7, ups=0.92, wpb=5505.1, bsz=357.8, num_updates=35000, lr=0.000338062, gnorm=0.318, train_wall=51, gb_free=10, wall=37437
2022-08-17 08:36:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:36:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:36:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:36:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:36:28 | INFO | valid | epoch 433 | valid on 'valid' subset | loss 5.116 | nll_loss 2.496 | ppl 5.64 | bleu 56.67 | wps 1881.4 | wpb 933.5 | bsz 59.6 | num_updates 35073 | best_bleu 57.33
2022-08-17 08:36:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 433 @ 35073 updates
2022-08-17 08:36:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint433.pt
2022-08-17 08:36:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint433.pt
2022-08-17 08:36:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint433.pt (epoch 433 @ 35073 updates, score 56.67) (writing took 2.543365750461817 seconds)
2022-08-17 08:36:31 | INFO | fairseq_cli.train | end of epoch 433 (average epoch stats below)
2022-08-17 08:36:31 | INFO | train | epoch 433 | loss 3.387 | nll_loss 0.351 | ppl 1.28 | wps 8037.9 | ups 1.46 | wpb 5523.2 | bsz 358 | num_updates 35073 | lr 0.00033771 | gnorm 0.278 | train_wall 41 | gb_free 10.2 | wall 37487
2022-08-17 08:36:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:36:31 | INFO | fairseq.trainer | begin training epoch 434
2022-08-17 08:36:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:36:46 | INFO | train_inner | epoch 434:     27 / 81 loss=3.387, nll_loss=0.351, ppl=1.28, wps=8435, ups=1.53, wpb=5530.2, bsz=361.4, num_updates=35100, lr=0.00033758, gnorm=0.305, train_wall=51, gb_free=10.1, wall=37503
2022-08-17 08:37:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:37:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:37:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:37:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:37:24 | INFO | valid | epoch 434 | valid on 'valid' subset | loss 5.117 | nll_loss 2.499 | ppl 5.65 | bleu 56.53 | wps 1993.1 | wpb 933.5 | bsz 59.6 | num_updates 35154 | best_bleu 57.33
2022-08-17 08:37:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 434 @ 35154 updates
2022-08-17 08:37:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint434.pt
2022-08-17 08:37:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint434.pt
2022-08-17 08:37:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint434.pt (epoch 434 @ 35154 updates, score 56.53) (writing took 22.212880309671164 seconds)
2022-08-17 08:37:46 | INFO | fairseq_cli.train | end of epoch 434 (average epoch stats below)
2022-08-17 08:37:46 | INFO | train | epoch 434 | loss 3.387 | nll_loss 0.351 | ppl 1.28 | wps 5915.1 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 35154 | lr 0.00033732 | gnorm 0.319 | train_wall 41 | gb_free 10.1 | wall 37563
2022-08-17 08:37:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:37:46 | INFO | fairseq.trainer | begin training epoch 435
2022-08-17 08:37:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:38:14 | INFO | train_inner | epoch 435:     46 / 81 loss=3.386, nll_loss=0.35, ppl=1.27, wps=6325.8, ups=1.14, wpb=5542.6, bsz=354.3, num_updates=35200, lr=0.0003371, gnorm=0.308, train_wall=51, gb_free=10.1, wall=37590
2022-08-17 08:38:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:38:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:38:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:38:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:38:41 | INFO | valid | epoch 435 | valid on 'valid' subset | loss 5.093 | nll_loss 2.464 | ppl 5.52 | bleu 57.28 | wps 1910.7 | wpb 933.5 | bsz 59.6 | num_updates 35235 | best_bleu 57.33
2022-08-17 08:38:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 435 @ 35235 updates
2022-08-17 08:38:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint435.pt
2022-08-17 08:38:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint435.pt
2022-08-17 08:39:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint435.pt (epoch 435 @ 35235 updates, score 57.28) (writing took 36.562033139169216 seconds)
2022-08-17 08:39:18 | INFO | fairseq_cli.train | end of epoch 435 (average epoch stats below)
2022-08-17 08:39:18 | INFO | train | epoch 435 | loss 3.386 | nll_loss 0.351 | ppl 1.28 | wps 4873.4 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 35235 | lr 0.000336932 | gnorm 0.317 | train_wall 41 | gb_free 10.1 | wall 37654
2022-08-17 08:39:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:39:18 | INFO | fairseq.trainer | begin training epoch 436
2022-08-17 08:39:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:39:53 | INFO | train_inner | epoch 436:     65 / 81 loss=3.387, nll_loss=0.352, ppl=1.28, wps=5599.8, ups=1.01, wpb=5526.7, bsz=356.5, num_updates=35300, lr=0.000336622, gnorm=0.297, train_wall=50, gb_free=10.1, wall=37689
2022-08-17 08:40:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:40:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:40:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:40:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:40:11 | INFO | valid | epoch 436 | valid on 'valid' subset | loss 5.088 | nll_loss 2.46 | ppl 5.5 | bleu 56.53 | wps 1812.6 | wpb 933.5 | bsz 59.6 | num_updates 35316 | best_bleu 57.33
2022-08-17 08:40:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 436 @ 35316 updates
2022-08-17 08:40:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint436.pt
2022-08-17 08:40:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint436.pt
2022-08-17 08:40:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint436.pt (epoch 436 @ 35316 updates, score 56.53) (writing took 29.16230207681656 seconds)
2022-08-17 08:40:40 | INFO | fairseq_cli.train | end of epoch 436 (average epoch stats below)
2022-08-17 08:40:40 | INFO | train | epoch 436 | loss 3.387 | nll_loss 0.351 | ppl 1.28 | wps 5467.9 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 35316 | lr 0.000336546 | gnorm 0.295 | train_wall 41 | gb_free 10.1 | wall 37736
2022-08-17 08:40:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:40:40 | INFO | fairseq.trainer | begin training epoch 437
2022-08-17 08:40:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:41:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:41:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:41:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:41:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:41:34 | INFO | valid | epoch 437 | valid on 'valid' subset | loss 5.103 | nll_loss 2.478 | ppl 5.57 | bleu 56.9 | wps 1851.5 | wpb 933.5 | bsz 59.6 | num_updates 35397 | best_bleu 57.33
2022-08-17 08:41:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 437 @ 35397 updates
2022-08-17 08:41:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint437.pt
2022-08-17 08:41:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint437.pt
2022-08-17 08:41:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint437.pt (epoch 437 @ 35397 updates, score 56.9) (writing took 19.17010924592614 seconds)
2022-08-17 08:41:53 | INFO | fairseq_cli.train | end of epoch 437 (average epoch stats below)
2022-08-17 08:41:53 | INFO | train | epoch 437 | loss 3.388 | nll_loss 0.353 | ppl 1.28 | wps 6088.4 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 35397 | lr 0.000336161 | gnorm 0.549 | train_wall 42 | gb_free 10.2 | wall 37810
2022-08-17 08:41:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:41:54 | INFO | fairseq.trainer | begin training epoch 438
2022-08-17 08:41:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:41:56 | INFO | train_inner | epoch 438:      3 / 81 loss=3.387, nll_loss=0.352, ppl=1.28, wps=4454.7, ups=0.81, wpb=5495.3, bsz=360.2, num_updates=35400, lr=0.000336146, gnorm=0.511, train_wall=52, gb_free=10.1, wall=37812
2022-08-17 08:42:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:42:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:42:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:42:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:42:46 | INFO | valid | epoch 438 | valid on 'valid' subset | loss 5.099 | nll_loss 2.475 | ppl 5.56 | bleu 57.2 | wps 1822.9 | wpb 933.5 | bsz 59.6 | num_updates 35478 | best_bleu 57.33
2022-08-17 08:42:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 438 @ 35478 updates
2022-08-17 08:42:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint438.pt
2022-08-17 08:42:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint438.pt
2022-08-17 08:43:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint438.pt (epoch 438 @ 35478 updates, score 57.2) (writing took 17.39764567092061 seconds)
2022-08-17 08:43:03 | INFO | fairseq_cli.train | end of epoch 438 (average epoch stats below)
2022-08-17 08:43:03 | INFO | train | epoch 438 | loss 3.387 | nll_loss 0.351 | ppl 1.28 | wps 6414.3 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 35478 | lr 0.000335777 | gnorm 0.309 | train_wall 41 | gb_free 10.1 | wall 37879
2022-08-17 08:43:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:43:03 | INFO | fairseq.trainer | begin training epoch 439
2022-08-17 08:43:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:43:17 | INFO | train_inner | epoch 439:     22 / 81 loss=3.386, nll_loss=0.351, ppl=1.28, wps=6900.4, ups=1.24, wpb=5557, bsz=361.8, num_updates=35500, lr=0.000335673, gnorm=0.29, train_wall=51, gb_free=10.1, wall=37893
2022-08-17 08:43:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:43:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:43:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:43:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:43:57 | INFO | valid | epoch 439 | valid on 'valid' subset | loss 5.098 | nll_loss 2.472 | ppl 5.55 | bleu 57.21 | wps 1883.1 | wpb 933.5 | bsz 59.6 | num_updates 35559 | best_bleu 57.33
2022-08-17 08:43:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 439 @ 35559 updates
2022-08-17 08:43:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint439.pt
2022-08-17 08:43:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint439.pt
2022-08-17 08:44:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint439.pt (epoch 439 @ 35559 updates, score 57.21) (writing took 2.413907505571842 seconds)
2022-08-17 08:44:00 | INFO | fairseq_cli.train | end of epoch 439 (average epoch stats below)
2022-08-17 08:44:00 | INFO | train | epoch 439 | loss 3.388 | nll_loss 0.352 | ppl 1.28 | wps 7916.3 | ups 1.43 | wpb 5523.2 | bsz 358 | num_updates 35559 | lr 0.000335394 | gnorm 0.296 | train_wall 43 | gb_free 10.1 | wall 37936
2022-08-17 08:44:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:44:00 | INFO | fairseq.trainer | begin training epoch 440
2022-08-17 08:44:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:44:23 | INFO | train_inner | epoch 440:     41 / 81 loss=3.388, nll_loss=0.353, ppl=1.28, wps=8322.8, ups=1.51, wpb=5504, bsz=353.9, num_updates=35600, lr=0.000335201, gnorm=0.314, train_wall=52, gb_free=10.1, wall=37959
2022-08-17 08:44:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:44:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:44:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:44:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:44:54 | INFO | valid | epoch 440 | valid on 'valid' subset | loss 5.095 | nll_loss 2.471 | ppl 5.54 | bleu 57.18 | wps 1775.3 | wpb 933.5 | bsz 59.6 | num_updates 35640 | best_bleu 57.33
2022-08-17 08:44:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 440 @ 35640 updates
2022-08-17 08:44:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint440.pt
2022-08-17 08:44:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint440.pt
2022-08-17 08:45:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint440.pt (epoch 440 @ 35640 updates, score 57.18) (writing took 24.144858803600073 seconds)
2022-08-17 08:45:18 | INFO | fairseq_cli.train | end of epoch 440 (average epoch stats below)
2022-08-17 08:45:18 | INFO | train | epoch 440 | loss 3.388 | nll_loss 0.352 | ppl 1.28 | wps 5692.1 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 35640 | lr 0.000335013 | gnorm 0.324 | train_wall 41 | gb_free 10.1 | wall 38015
2022-08-17 08:45:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:45:18 | INFO | fairseq.trainer | begin training epoch 441
2022-08-17 08:45:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:45:51 | INFO | train_inner | epoch 441:     60 / 81 loss=3.389, nll_loss=0.354, ppl=1.28, wps=6272.6, ups=1.13, wpb=5542.7, bsz=357.1, num_updates=35700, lr=0.000334731, gnorm=0.351, train_wall=50, gb_free=10, wall=38047
2022-08-17 08:46:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:46:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:46:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:46:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:46:11 | INFO | valid | epoch 441 | valid on 'valid' subset | loss 5.104 | nll_loss 2.48 | ppl 5.58 | bleu 56.72 | wps 1830.4 | wpb 933.5 | bsz 59.6 | num_updates 35721 | best_bleu 57.33
2022-08-17 08:46:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 441 @ 35721 updates
2022-08-17 08:46:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint441.pt
2022-08-17 08:46:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint441.pt
2022-08-17 08:46:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint441.pt (epoch 441 @ 35721 updates, score 56.72) (writing took 30.18930320441723 seconds)
2022-08-17 08:46:41 | INFO | fairseq_cli.train | end of epoch 441 (average epoch stats below)
2022-08-17 08:46:41 | INFO | train | epoch 441 | loss 3.389 | nll_loss 0.354 | ppl 1.28 | wps 5391.3 | ups 0.98 | wpb 5523.2 | bsz 358 | num_updates 35721 | lr 0.000334633 | gnorm 0.353 | train_wall 41 | gb_free 10.1 | wall 38097
2022-08-17 08:46:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:46:41 | INFO | fairseq.trainer | begin training epoch 442
2022-08-17 08:46:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:47:25 | INFO | train_inner | epoch 442:     79 / 81 loss=3.387, nll_loss=0.351, ppl=1.28, wps=5888.3, ups=1.07, wpb=5512.4, bsz=360.9, num_updates=35800, lr=0.000334263, gnorm=0.286, train_wall=51, gb_free=10.1, wall=38141
2022-08-17 08:47:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:47:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:47:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:47:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:47:35 | INFO | valid | epoch 442 | valid on 'valid' subset | loss 5.12 | nll_loss 2.505 | ppl 5.68 | bleu 57.08 | wps 1924.1 | wpb 933.5 | bsz 59.6 | num_updates 35802 | best_bleu 57.33
2022-08-17 08:47:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 442 @ 35802 updates
2022-08-17 08:47:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint442.pt
2022-08-17 08:47:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint442.pt
2022-08-17 08:47:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint442.pt (epoch 442 @ 35802 updates, score 57.08) (writing took 19.46122755482793 seconds)
2022-08-17 08:47:54 | INFO | fairseq_cli.train | end of epoch 442 (average epoch stats below)
2022-08-17 08:47:54 | INFO | train | epoch 442 | loss 3.387 | nll_loss 0.351 | ppl 1.28 | wps 6131 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 35802 | lr 0.000334254 | gnorm 0.287 | train_wall 41 | gb_free 10.2 | wall 38170
2022-08-17 08:47:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:47:54 | INFO | fairseq.trainer | begin training epoch 443
2022-08-17 08:47:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:48:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:48:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:48:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:48:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:48:48 | INFO | valid | epoch 443 | valid on 'valid' subset | loss 5.107 | nll_loss 2.485 | ppl 5.6 | bleu 56.5 | wps 1935.9 | wpb 933.5 | bsz 59.6 | num_updates 35883 | best_bleu 57.33
2022-08-17 08:48:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 443 @ 35883 updates
2022-08-17 08:48:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint443.pt
2022-08-17 08:48:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint443.pt
2022-08-17 08:49:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint443.pt (epoch 443 @ 35883 updates, score 56.5) (writing took 19.161050457507372 seconds)
2022-08-17 08:49:07 | INFO | fairseq_cli.train | end of epoch 443 (average epoch stats below)
2022-08-17 08:49:07 | INFO | train | epoch 443 | loss 3.386 | nll_loss 0.35 | ppl 1.27 | wps 6152.7 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 35883 | lr 0.000333876 | gnorm 0.29 | train_wall 40 | gb_free 10.2 | wall 38243
2022-08-17 08:49:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:49:07 | INFO | fairseq.trainer | begin training epoch 444
2022-08-17 08:49:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:49:17 | INFO | train_inner | epoch 444:     17 / 81 loss=3.386, nll_loss=0.35, ppl=1.27, wps=4895.8, ups=0.89, wpb=5494.6, bsz=355, num_updates=35900, lr=0.000333797, gnorm=0.302, train_wall=49, gb_free=10.1, wall=38253
2022-08-17 08:49:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:49:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:49:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:49:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:50:00 | INFO | valid | epoch 444 | valid on 'valid' subset | loss 5.12 | nll_loss 2.501 | ppl 5.66 | bleu 56.59 | wps 1932.5 | wpb 933.5 | bsz 59.6 | num_updates 35964 | best_bleu 57.33
2022-08-17 08:50:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 444 @ 35964 updates
2022-08-17 08:50:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint444.pt
2022-08-17 08:50:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint444.pt
2022-08-17 08:50:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint444.pt (epoch 444 @ 35964 updates, score 56.59) (writing took 41.885185182094574 seconds)
2022-08-17 08:50:42 | INFO | fairseq_cli.train | end of epoch 444 (average epoch stats below)
2022-08-17 08:50:42 | INFO | train | epoch 444 | loss 3.386 | nll_loss 0.351 | ppl 1.28 | wps 4701.4 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 35964 | lr 0.0003335 | gnorm 0.333 | train_wall 41 | gb_free 10.1 | wall 38338
2022-08-17 08:50:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:50:42 | INFO | fairseq.trainer | begin training epoch 445
2022-08-17 08:50:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:51:05 | INFO | train_inner | epoch 445:     36 / 81 loss=3.387, nll_loss=0.351, ppl=1.28, wps=5133, ups=0.93, wpb=5538.6, bsz=360.4, num_updates=36000, lr=0.000333333, gnorm=0.317, train_wall=50, gb_free=10.1, wall=38361
2022-08-17 08:51:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:51:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:51:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:51:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:51:38 | INFO | valid | epoch 445 | valid on 'valid' subset | loss 5.101 | nll_loss 2.476 | ppl 5.56 | bleu 56.86 | wps 1988.4 | wpb 933.5 | bsz 59.6 | num_updates 36045 | best_bleu 57.33
2022-08-17 08:51:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 445 @ 36045 updates
2022-08-17 08:51:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint445.pt
2022-08-17 08:51:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint445.pt
2022-08-17 08:51:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint445.pt (epoch 445 @ 36045 updates, score 56.86) (writing took 2.3624219447374344 seconds)
2022-08-17 08:51:40 | INFO | fairseq_cli.train | end of epoch 445 (average epoch stats below)
2022-08-17 08:51:40 | INFO | train | epoch 445 | loss 3.387 | nll_loss 0.351 | ppl 1.28 | wps 7666.4 | ups 1.39 | wpb 5523.2 | bsz 358 | num_updates 36045 | lr 0.000333125 | gnorm 0.305 | train_wall 40 | gb_free 10.1 | wall 38397
2022-08-17 08:51:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:51:41 | INFO | fairseq.trainer | begin training epoch 446
2022-08-17 08:51:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:52:11 | INFO | train_inner | epoch 446:     55 / 81 loss=3.387, nll_loss=0.351, ppl=1.28, wps=8372.4, ups=1.51, wpb=5529.1, bsz=360.6, num_updates=36100, lr=0.000332871, gnorm=0.315, train_wall=51, gb_free=10.1, wall=38427
2022-08-17 08:52:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:52:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:52:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:52:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:52:33 | INFO | valid | epoch 446 | valid on 'valid' subset | loss 5.112 | nll_loss 2.491 | ppl 5.62 | bleu 56.65 | wps 1916.5 | wpb 933.5 | bsz 59.6 | num_updates 36126 | best_bleu 57.33
2022-08-17 08:52:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 446 @ 36126 updates
2022-08-17 08:52:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint446.pt
2022-08-17 08:52:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint446.pt
2022-08-17 08:53:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint446.pt (epoch 446 @ 36126 updates, score 56.65) (writing took 32.756386410444975 seconds)
2022-08-17 08:53:06 | INFO | fairseq_cli.train | end of epoch 446 (average epoch stats below)
2022-08-17 08:53:06 | INFO | train | epoch 446 | loss 3.387 | nll_loss 0.351 | ppl 1.28 | wps 5257.4 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 36126 | lr 0.000332752 | gnorm 0.297 | train_wall 41 | gb_free 10.1 | wall 38482
2022-08-17 08:53:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:53:06 | INFO | fairseq.trainer | begin training epoch 447
2022-08-17 08:53:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:53:45 | INFO | train_inner | epoch 447:     74 / 81 loss=3.388, nll_loss=0.353, ppl=1.28, wps=5871.1, ups=1.06, wpb=5529.4, bsz=357.4, num_updates=36200, lr=0.000332411, gnorm=0.382, train_wall=50, gb_free=10.1, wall=38521
2022-08-17 08:53:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:53:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:53:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:53:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:53:58 | INFO | valid | epoch 447 | valid on 'valid' subset | loss 5.106 | nll_loss 2.481 | ppl 5.58 | bleu 57.41 | wps 1793.8 | wpb 933.5 | bsz 59.6 | num_updates 36207 | best_bleu 57.41
2022-08-17 08:53:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 447 @ 36207 updates
2022-08-17 08:53:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint447.pt
2022-08-17 08:53:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint447.pt
2022-08-17 08:55:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint447.pt (epoch 447 @ 36207 updates, score 57.41) (writing took 65.26183104515076 seconds)
2022-08-17 08:55:03 | INFO | fairseq_cli.train | end of epoch 447 (average epoch stats below)
2022-08-17 08:55:03 | INFO | train | epoch 447 | loss 3.387 | nll_loss 0.352 | ppl 1.28 | wps 3806.3 | ups 0.69 | wpb 5523.2 | bsz 358 | num_updates 36207 | lr 0.000332379 | gnorm 0.419 | train_wall 41 | gb_free 10.1 | wall 38599
2022-08-17 08:55:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:55:03 | INFO | fairseq.trainer | begin training epoch 448
2022-08-17 08:55:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:55:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:55:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:55:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:55:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:55:56 | INFO | valid | epoch 448 | valid on 'valid' subset | loss 5.113 | nll_loss 2.492 | ppl 5.62 | bleu 56.52 | wps 1923.4 | wpb 933.5 | bsz 59.6 | num_updates 36288 | best_bleu 57.41
2022-08-17 08:55:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 448 @ 36288 updates
2022-08-17 08:55:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint448.pt
2022-08-17 08:55:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint448.pt
2022-08-17 08:56:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint448.pt (epoch 448 @ 36288 updates, score 56.52) (writing took 19.825752921402454 seconds)
2022-08-17 08:56:16 | INFO | fairseq_cli.train | end of epoch 448 (average epoch stats below)
2022-08-17 08:56:16 | INFO | train | epoch 448 | loss 3.387 | nll_loss 0.351 | ppl 1.28 | wps 6158.7 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 36288 | lr 0.000332008 | gnorm 0.313 | train_wall 41 | gb_free 10.1 | wall 38672
2022-08-17 08:56:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:56:16 | INFO | fairseq.trainer | begin training epoch 449
2022-08-17 08:56:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:56:23 | INFO | train_inner | epoch 449:     12 / 81 loss=3.387, nll_loss=0.351, ppl=1.28, wps=3472.2, ups=0.63, wpb=5486, bsz=349.9, num_updates=36300, lr=0.000331953, gnorm=0.322, train_wall=50, gb_free=10.1, wall=38679
2022-08-17 08:56:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:56:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:56:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:56:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:57:07 | INFO | valid | epoch 449 | valid on 'valid' subset | loss 5.112 | nll_loss 2.492 | ppl 5.63 | bleu 56.87 | wps 1869.5 | wpb 933.5 | bsz 59.6 | num_updates 36369 | best_bleu 57.41
2022-08-17 08:57:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 449 @ 36369 updates
2022-08-17 08:57:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint449.pt
2022-08-17 08:57:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint449.pt
2022-08-17 08:57:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint449.pt (epoch 449 @ 36369 updates, score 56.87) (writing took 26.893778543919325 seconds)
2022-08-17 08:57:34 | INFO | fairseq_cli.train | end of epoch 449 (average epoch stats below)
2022-08-17 08:57:34 | INFO | train | epoch 449 | loss 3.388 | nll_loss 0.353 | ppl 1.28 | wps 5684.2 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 36369 | lr 0.000331638 | gnorm 0.312 | train_wall 40 | gb_free 10.1 | wall 38751
2022-08-17 08:57:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:57:35 | INFO | fairseq.trainer | begin training epoch 450
2022-08-17 08:57:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:57:52 | INFO | train_inner | epoch 450:     31 / 81 loss=3.387, nll_loss=0.352, ppl=1.28, wps=6236.7, ups=1.12, wpb=5552.5, bsz=365.1, num_updates=36400, lr=0.000331497, gnorm=0.306, train_wall=50, gb_free=10.1, wall=38768
2022-08-17 08:58:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:58:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:58:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:58:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:58:27 | INFO | valid | epoch 450 | valid on 'valid' subset | loss 5.111 | nll_loss 2.49 | ppl 5.62 | bleu 57.1 | wps 1965.1 | wpb 933.5 | bsz 59.6 | num_updates 36450 | best_bleu 57.41
2022-08-17 08:58:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 450 @ 36450 updates
2022-08-17 08:58:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint450.pt
2022-08-17 08:58:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint450.pt
2022-08-17 08:59:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint450.pt (epoch 450 @ 36450 updates, score 57.1) (writing took 35.2433544062078 seconds)
2022-08-17 08:59:02 | INFO | fairseq_cli.train | end of epoch 450 (average epoch stats below)
2022-08-17 08:59:02 | INFO | train | epoch 450 | loss 3.386 | nll_loss 0.35 | ppl 1.27 | wps 5102.2 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 36450 | lr 0.000331269 | gnorm 0.298 | train_wall 41 | gb_free 10.1 | wall 38838
2022-08-17 08:59:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 08:59:02 | INFO | fairseq.trainer | begin training epoch 451
2022-08-17 08:59:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 08:59:29 | INFO | train_inner | epoch 451:     50 / 81 loss=3.388, nll_loss=0.352, ppl=1.28, wps=5688, ups=1.03, wpb=5499.9, bsz=353, num_updates=36500, lr=0.000331042, gnorm=0.496, train_wall=50, gb_free=10.1, wall=38865
2022-08-17 08:59:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 08:59:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 08:59:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 08:59:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 08:59:55 | INFO | valid | epoch 451 | valid on 'valid' subset | loss 5.112 | nll_loss 2.489 | ppl 5.61 | bleu 56.93 | wps 1886.3 | wpb 933.5 | bsz 59.6 | num_updates 36531 | best_bleu 57.41
2022-08-17 08:59:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 451 @ 36531 updates
2022-08-17 08:59:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint451.pt
2022-08-17 08:59:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint451.pt
2022-08-17 09:00:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint451.pt (epoch 451 @ 36531 updates, score 56.93) (writing took 20.68703768774867 seconds)
2022-08-17 09:00:16 | INFO | fairseq_cli.train | end of epoch 451 (average epoch stats below)
2022-08-17 09:00:16 | INFO | train | epoch 451 | loss 3.387 | nll_loss 0.352 | ppl 1.28 | wps 6067.8 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 36531 | lr 0.000330902 | gnorm 0.567 | train_wall 40 | gb_free 10.1 | wall 38912
2022-08-17 09:00:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:00:16 | INFO | fairseq.trainer | begin training epoch 452
2022-08-17 09:00:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:00:55 | INFO | train_inner | epoch 452:     69 / 81 loss=3.387, nll_loss=0.352, ppl=1.28, wps=6490.4, ups=1.17, wpb=5562.4, bsz=366.6, num_updates=36600, lr=0.00033059, gnorm=0.553, train_wall=50, gb_free=10.1, wall=38951
2022-08-17 09:01:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:01:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:01:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:01:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:01:10 | INFO | valid | epoch 452 | valid on 'valid' subset | loss 5.106 | nll_loss 2.478 | ppl 5.57 | bleu 56.85 | wps 1924.8 | wpb 933.5 | bsz 59.6 | num_updates 36612 | best_bleu 57.41
2022-08-17 09:01:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 452 @ 36612 updates
2022-08-17 09:01:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint452.pt
2022-08-17 09:01:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint452.pt
2022-08-17 09:01:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint452.pt (epoch 452 @ 36612 updates, score 56.85) (writing took 14.786220882087946 seconds)
2022-08-17 09:01:25 | INFO | fairseq_cli.train | end of epoch 452 (average epoch stats below)
2022-08-17 09:01:25 | INFO | train | epoch 452 | loss 3.387 | nll_loss 0.352 | ppl 1.28 | wps 6511.8 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 36612 | lr 0.000330536 | gnorm 0.605 | train_wall 40 | gb_free 10.2 | wall 38981
2022-08-17 09:01:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:01:25 | INFO | fairseq.trainer | begin training epoch 453
2022-08-17 09:01:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:02:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:02:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:02:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:02:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:02:15 | INFO | valid | epoch 453 | valid on 'valid' subset | loss 5.093 | nll_loss 2.468 | ppl 5.53 | bleu 57.37 | wps 2032.6 | wpb 933.5 | bsz 59.6 | num_updates 36693 | best_bleu 57.41
2022-08-17 09:02:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 453 @ 36693 updates
2022-08-17 09:02:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint453.pt
2022-08-17 09:02:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint453.pt
2022-08-17 09:02:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint453.pt (epoch 453 @ 36693 updates, score 57.37) (writing took 33.49260004609823 seconds)
2022-08-17 09:02:49 | INFO | fairseq_cli.train | end of epoch 453 (average epoch stats below)
2022-08-17 09:02:49 | INFO | train | epoch 453 | loss 3.387 | nll_loss 0.351 | ppl 1.28 | wps 5305.3 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 36693 | lr 0.000330171 | gnorm 0.32 | train_wall 40 | gb_free 10.1 | wall 39065
2022-08-17 09:02:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:02:49 | INFO | fairseq.trainer | begin training epoch 454
2022-08-17 09:02:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:02:54 | INFO | train_inner | epoch 454:      7 / 81 loss=3.387, nll_loss=0.352, ppl=1.28, wps=4613.2, ups=0.84, wpb=5501.3, bsz=353.4, num_updates=36700, lr=0.000330139, gnorm=0.322, train_wall=49, gb_free=10.1, wall=39070
2022-08-17 09:03:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:03:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:03:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:03:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:03:41 | INFO | valid | epoch 454 | valid on 'valid' subset | loss 5.112 | nll_loss 2.49 | ppl 5.62 | bleu 56.67 | wps 1957.1 | wpb 933.5 | bsz 59.6 | num_updates 36774 | best_bleu 57.41
2022-08-17 09:03:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 454 @ 36774 updates
2022-08-17 09:03:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint454.pt
2022-08-17 09:03:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint454.pt
2022-08-17 09:03:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint454.pt (epoch 454 @ 36774 updates, score 56.67) (writing took 2.2763504311442375 seconds)
2022-08-17 09:03:43 | INFO | fairseq_cli.train | end of epoch 454 (average epoch stats below)
2022-08-17 09:03:43 | INFO | train | epoch 454 | loss 3.386 | nll_loss 0.351 | ppl 1.28 | wps 8244.9 | ups 1.49 | wpb 5523.2 | bsz 358 | num_updates 36774 | lr 0.000329807 | gnorm 0.325 | train_wall 40 | gb_free 10.2 | wall 39119
2022-08-17 09:03:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:03:43 | INFO | fairseq.trainer | begin training epoch 455
2022-08-17 09:03:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:03:58 | INFO | train_inner | epoch 455:     26 / 81 loss=3.386, nll_loss=0.351, ppl=1.28, wps=8615.8, ups=1.56, wpb=5529.9, bsz=358, num_updates=36800, lr=0.00032969, gnorm=0.328, train_wall=50, gb_free=10.1, wall=39134
2022-08-17 09:04:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:04:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:04:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:04:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:04:36 | INFO | valid | epoch 455 | valid on 'valid' subset | loss 5.105 | nll_loss 2.481 | ppl 5.58 | bleu 57.03 | wps 1828.3 | wpb 933.5 | bsz 59.6 | num_updates 36855 | best_bleu 57.41
2022-08-17 09:04:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 455 @ 36855 updates
2022-08-17 09:04:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint455.pt
2022-08-17 09:04:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint455.pt
2022-08-17 09:05:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint455.pt (epoch 455 @ 36855 updates, score 57.03) (writing took 29.64676573127508 seconds)
2022-08-17 09:05:06 | INFO | fairseq_cli.train | end of epoch 455 (average epoch stats below)
2022-08-17 09:05:06 | INFO | train | epoch 455 | loss 3.387 | nll_loss 0.351 | ppl 1.28 | wps 5417.7 | ups 0.98 | wpb 5523.2 | bsz 358 | num_updates 36855 | lr 0.000329444 | gnorm 1.37 | train_wall 41 | gb_free 10.1 | wall 39202
2022-08-17 09:05:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:05:06 | INFO | fairseq.trainer | begin training epoch 456
2022-08-17 09:05:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:05:32 | INFO | train_inner | epoch 456:     45 / 81 loss=3.386, nll_loss=0.35, ppl=1.27, wps=5889.7, ups=1.06, wpb=5550.2, bsz=360.6, num_updates=36900, lr=0.000329243, gnorm=1.173, train_wall=51, gb_free=10.1, wall=39229
2022-08-17 09:05:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:05:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:05:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:05:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:06:01 | INFO | valid | epoch 456 | valid on 'valid' subset | loss 5.115 | nll_loss 2.495 | ppl 5.64 | bleu 57.21 | wps 1813.6 | wpb 933.5 | bsz 59.6 | num_updates 36936 | best_bleu 57.41
2022-08-17 09:06:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 456 @ 36936 updates
2022-08-17 09:06:01 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint456.pt
2022-08-17 09:06:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint456.pt
2022-08-17 09:06:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint456.pt (epoch 456 @ 36936 updates, score 57.21) (writing took 35.17545160651207 seconds)
2022-08-17 09:06:36 | INFO | fairseq_cli.train | end of epoch 456 (average epoch stats below)
2022-08-17 09:06:36 | INFO | train | epoch 456 | loss 3.385 | nll_loss 0.35 | ppl 1.27 | wps 4942.4 | ups 0.89 | wpb 5523.2 | bsz 358 | num_updates 36936 | lr 0.000329083 | gnorm 0.316 | train_wall 40 | gb_free 10.1 | wall 39293
2022-08-17 09:06:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:06:37 | INFO | fairseq.trainer | begin training epoch 457
2022-08-17 09:06:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:07:11 | INFO | train_inner | epoch 457:     64 / 81 loss=3.385, nll_loss=0.35, ppl=1.27, wps=5602, ups=1.02, wpb=5514.2, bsz=354.6, num_updates=37000, lr=0.000328798, gnorm=0.321, train_wall=50, gb_free=10.1, wall=39327
2022-08-17 09:07:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:07:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:07:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:07:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:07:29 | INFO | valid | epoch 457 | valid on 'valid' subset | loss 5.113 | nll_loss 2.488 | ppl 5.61 | bleu 56.84 | wps 1755.3 | wpb 933.5 | bsz 59.6 | num_updates 37017 | best_bleu 57.41
2022-08-17 09:07:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 457 @ 37017 updates
2022-08-17 09:07:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint457.pt
2022-08-17 09:07:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint457.pt
2022-08-17 09:07:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint457.pt (epoch 457 @ 37017 updates, score 56.84) (writing took 16.725923478603363 seconds)
2022-08-17 09:07:46 | INFO | fairseq_cli.train | end of epoch 457 (average epoch stats below)
2022-08-17 09:07:46 | INFO | train | epoch 457 | loss 3.385 | nll_loss 0.349 | ppl 1.27 | wps 6445.2 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 37017 | lr 0.000328722 | gnorm 0.329 | train_wall 41 | gb_free 10.1 | wall 39362
2022-08-17 09:07:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:07:46 | INFO | fairseq.trainer | begin training epoch 458
2022-08-17 09:07:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:08:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:08:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:08:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:08:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:08:40 | INFO | valid | epoch 458 | valid on 'valid' subset | loss 5.117 | nll_loss 2.496 | ppl 5.64 | bleu 57.08 | wps 1621.2 | wpb 933.5 | bsz 59.6 | num_updates 37098 | best_bleu 57.41
2022-08-17 09:08:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 458 @ 37098 updates
2022-08-17 09:08:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint458.pt
2022-08-17 09:08:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint458.pt
2022-08-17 09:08:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint458.pt (epoch 458 @ 37098 updates, score 57.08) (writing took 15.589672673493624 seconds)
2022-08-17 09:08:56 | INFO | fairseq_cli.train | end of epoch 458 (average epoch stats below)
2022-08-17 09:08:56 | INFO | train | epoch 458 | loss 3.386 | nll_loss 0.351 | ppl 1.28 | wps 6409.2 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 37098 | lr 0.000328363 | gnorm 0.287 | train_wall 41 | gb_free 10.3 | wall 39432
2022-08-17 09:08:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:08:56 | INFO | fairseq.trainer | begin training epoch 459
2022-08-17 09:08:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:08:58 | INFO | train_inner | epoch 459:      2 / 81 loss=3.386, nll_loss=0.351, ppl=1.28, wps=5127.4, ups=0.93, wpb=5491.1, bsz=357.1, num_updates=37100, lr=0.000328355, gnorm=0.295, train_wall=51, gb_free=10.2, wall=39434
2022-08-17 09:09:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:09:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:09:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:09:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:09:50 | INFO | valid | epoch 459 | valid on 'valid' subset | loss 5.117 | nll_loss 2.498 | ppl 5.65 | bleu 56.68 | wps 1653.8 | wpb 933.5 | bsz 59.6 | num_updates 37179 | best_bleu 57.41
2022-08-17 09:09:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 459 @ 37179 updates
2022-08-17 09:09:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint459.pt
2022-08-17 09:09:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint459.pt
2022-08-17 09:09:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint459.pt (epoch 459 @ 37179 updates, score 56.68) (writing took 2.517552189528942 seconds)
2022-08-17 09:09:52 | INFO | fairseq_cli.train | end of epoch 459 (average epoch stats below)
2022-08-17 09:09:52 | INFO | train | epoch 459 | loss 3.386 | nll_loss 0.351 | ppl 1.28 | wps 7877.3 | ups 1.43 | wpb 5523.2 | bsz 358 | num_updates 37179 | lr 0.000328006 | gnorm 0.558 | train_wall 42 | gb_free 10.1 | wall 39489
2022-08-17 09:09:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:09:53 | INFO | fairseq.trainer | begin training epoch 460
2022-08-17 09:09:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:10:05 | INFO | train_inner | epoch 460:     21 / 81 loss=3.386, nll_loss=0.351, ppl=1.28, wps=8190.3, ups=1.48, wpb=5530, bsz=359.2, num_updates=37200, lr=0.000327913, gnorm=0.505, train_wall=52, gb_free=10.1, wall=39502
2022-08-17 09:10:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:10:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:10:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:10:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:10:45 | INFO | valid | epoch 460 | valid on 'valid' subset | loss 5.124 | nll_loss 2.504 | ppl 5.67 | bleu 56.82 | wps 1999.1 | wpb 933.5 | bsz 59.6 | num_updates 37260 | best_bleu 57.41
2022-08-17 09:10:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 460 @ 37260 updates
2022-08-17 09:10:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint460.pt
2022-08-17 09:10:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint460.pt
2022-08-17 09:11:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint460.pt (epoch 460 @ 37260 updates, score 56.82) (writing took 22.034424029290676 seconds)
2022-08-17 09:11:07 | INFO | fairseq_cli.train | end of epoch 460 (average epoch stats below)
2022-08-17 09:11:07 | INFO | train | epoch 460 | loss 3.386 | nll_loss 0.351 | ppl 1.28 | wps 6008.6 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 37260 | lr 0.000327649 | gnorm 0.297 | train_wall 41 | gb_free 10.1 | wall 39563
2022-08-17 09:11:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:11:07 | INFO | fairseq.trainer | begin training epoch 461
2022-08-17 09:11:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:11:29 | INFO | train_inner | epoch 461:     40 / 81 loss=3.386, nll_loss=0.35, ppl=1.27, wps=6662.1, ups=1.2, wpb=5553.3, bsz=360.1, num_updates=37300, lr=0.000327473, gnorm=0.294, train_wall=50, gb_free=10.1, wall=39585
2022-08-17 09:11:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:11:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:11:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:11:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:11:59 | INFO | valid | epoch 461 | valid on 'valid' subset | loss 5.106 | nll_loss 2.483 | ppl 5.59 | bleu 57.25 | wps 1764.5 | wpb 933.5 | bsz 59.6 | num_updates 37341 | best_bleu 57.41
2022-08-17 09:11:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 461 @ 37341 updates
2022-08-17 09:11:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint461.pt
2022-08-17 09:12:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint461.pt
2022-08-17 09:12:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint461.pt (epoch 461 @ 37341 updates, score 57.25) (writing took 39.4849866963923 seconds)
2022-08-17 09:12:39 | INFO | fairseq_cli.train | end of epoch 461 (average epoch stats below)
2022-08-17 09:12:39 | INFO | train | epoch 461 | loss 3.386 | nll_loss 0.351 | ppl 1.28 | wps 4875.5 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 37341 | lr 0.000327293 | gnorm 0.292 | train_wall 40 | gb_free 10.1 | wall 39655
2022-08-17 09:12:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:12:39 | INFO | fairseq.trainer | begin training epoch 462
2022-08-17 09:12:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:13:12 | INFO | train_inner | epoch 462:     59 / 81 loss=3.385, nll_loss=0.35, ppl=1.27, wps=5303.6, ups=0.97, wpb=5476.3, bsz=354.9, num_updates=37400, lr=0.000327035, gnorm=0.281, train_wall=51, gb_free=10.1, wall=39688
2022-08-17 09:13:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:13:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:13:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:13:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:13:32 | INFO | valid | epoch 462 | valid on 'valid' subset | loss 5.122 | nll_loss 2.504 | ppl 5.67 | bleu 57.11 | wps 1903.5 | wpb 933.5 | bsz 59.6 | num_updates 37422 | best_bleu 57.41
2022-08-17 09:13:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 462 @ 37422 updates
2022-08-17 09:13:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint462.pt
2022-08-17 09:13:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint462.pt
2022-08-17 09:13:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint462.pt (epoch 462 @ 37422 updates, score 57.11) (writing took 14.594608061015606 seconds)
2022-08-17 09:13:47 | INFO | fairseq_cli.train | end of epoch 462 (average epoch stats below)
2022-08-17 09:13:47 | INFO | train | epoch 462 | loss 3.385 | nll_loss 0.349 | ppl 1.27 | wps 6496.1 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 37422 | lr 0.000326939 | gnorm 0.269 | train_wall 42 | gb_free 10.2 | wall 39724
2022-08-17 09:13:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:13:48 | INFO | fairseq.trainer | begin training epoch 463
2022-08-17 09:13:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:14:31 | INFO | train_inner | epoch 463:     78 / 81 loss=3.385, nll_loss=0.35, ppl=1.27, wps=7016.4, ups=1.26, wpb=5558.6, bsz=361, num_updates=37500, lr=0.000326599, gnorm=0.269, train_wall=51, gb_free=10.1, wall=39767
2022-08-17 09:14:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:14:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:14:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:14:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:14:42 | INFO | valid | epoch 463 | valid on 'valid' subset | loss 5.125 | nll_loss 2.502 | ppl 5.66 | bleu 56.94 | wps 1833.3 | wpb 933.5 | bsz 59.6 | num_updates 37503 | best_bleu 57.41
2022-08-17 09:14:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 463 @ 37503 updates
2022-08-17 09:14:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint463.pt
2022-08-17 09:14:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint463.pt
2022-08-17 09:15:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint463.pt (epoch 463 @ 37503 updates, score 56.94) (writing took 29.80776895582676 seconds)
2022-08-17 09:15:12 | INFO | fairseq_cli.train | end of epoch 463 (average epoch stats below)
2022-08-17 09:15:12 | INFO | train | epoch 463 | loss 3.385 | nll_loss 0.35 | ppl 1.27 | wps 5303.2 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 37503 | lr 0.000326586 | gnorm 0.267 | train_wall 41 | gb_free 10.1 | wall 39808
2022-08-17 09:15:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:15:12 | INFO | fairseq.trainer | begin training epoch 464
2022-08-17 09:15:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:15:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:15:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:15:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:15:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:16:05 | INFO | valid | epoch 464 | valid on 'valid' subset | loss 5.116 | nll_loss 2.495 | ppl 5.64 | bleu 57 | wps 1973.9 | wpb 933.5 | bsz 59.6 | num_updates 37584 | best_bleu 57.41
2022-08-17 09:16:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 464 @ 37584 updates
2022-08-17 09:16:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint464.pt
2022-08-17 09:16:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint464.pt
2022-08-17 09:16:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint464.pt (epoch 464 @ 37584 updates, score 57.0) (writing took 31.72071048244834 seconds)
2022-08-17 09:16:37 | INFO | fairseq_cli.train | end of epoch 464 (average epoch stats below)
2022-08-17 09:16:37 | INFO | train | epoch 464 | loss 3.385 | nll_loss 0.35 | ppl 1.27 | wps 5271.8 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 37584 | lr 0.000326233 | gnorm 0.288 | train_wall 41 | gb_free 10.1 | wall 39893
2022-08-17 09:16:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:16:37 | INFO | fairseq.trainer | begin training epoch 465
2022-08-17 09:16:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:16:47 | INFO | train_inner | epoch 465:     16 / 81 loss=3.385, nll_loss=0.35, ppl=1.27, wps=4044.7, ups=0.74, wpb=5500.9, bsz=357.4, num_updates=37600, lr=0.000326164, gnorm=0.29, train_wall=50, gb_free=10, wall=39903
2022-08-17 09:17:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:17:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:17:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:17:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:17:32 | INFO | valid | epoch 465 | valid on 'valid' subset | loss 5.114 | nll_loss 2.488 | ppl 5.61 | bleu 57.14 | wps 1914.7 | wpb 933.5 | bsz 59.6 | num_updates 37665 | best_bleu 57.41
2022-08-17 09:17:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 465 @ 37665 updates
2022-08-17 09:17:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint465.pt
2022-08-17 09:17:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint465.pt
2022-08-17 09:17:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint465.pt (epoch 465 @ 37665 updates, score 57.14) (writing took 22.909257262945175 seconds)
2022-08-17 09:17:56 | INFO | fairseq_cli.train | end of epoch 465 (average epoch stats below)
2022-08-17 09:17:56 | INFO | train | epoch 465 | loss 3.386 | nll_loss 0.351 | ppl 1.28 | wps 5672.4 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 37665 | lr 0.000325882 | gnorm 0.293 | train_wall 41 | gb_free 10.1 | wall 39972
2022-08-17 09:17:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:17:56 | INFO | fairseq.trainer | begin training epoch 466
2022-08-17 09:17:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:18:15 | INFO | train_inner | epoch 466:     35 / 81 loss=3.386, nll_loss=0.351, ppl=1.28, wps=6240.7, ups=1.14, wpb=5490.5, bsz=350.6, num_updates=37700, lr=0.000325731, gnorm=0.343, train_wall=51, gb_free=10.1, wall=39991
2022-08-17 09:18:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:18:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:18:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:18:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:18:50 | INFO | valid | epoch 466 | valid on 'valid' subset | loss 5.108 | nll_loss 2.486 | ppl 5.6 | bleu 56.97 | wps 1900.7 | wpb 933.5 | bsz 59.6 | num_updates 37746 | best_bleu 57.41
2022-08-17 09:18:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 466 @ 37746 updates
2022-08-17 09:18:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint466.pt
2022-08-17 09:18:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint466.pt
2022-08-17 09:19:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint466.pt (epoch 466 @ 37746 updates, score 56.97) (writing took 15.335253980010748 seconds)
2022-08-17 09:19:05 | INFO | fairseq_cli.train | end of epoch 466 (average epoch stats below)
2022-08-17 09:19:05 | INFO | train | epoch 466 | loss 3.386 | nll_loss 0.35 | ppl 1.27 | wps 6424.8 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 37746 | lr 0.000325533 | gnorm 0.369 | train_wall 42 | gb_free 10.2 | wall 40041
2022-08-17 09:19:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:19:05 | INFO | fairseq.trainer | begin training epoch 467
2022-08-17 09:19:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:19:35 | INFO | train_inner | epoch 467:     54 / 81 loss=3.385, nll_loss=0.351, ppl=1.28, wps=6937.2, ups=1.25, wpb=5540.9, bsz=362.2, num_updates=37800, lr=0.0003253, gnorm=0.315, train_wall=53, gb_free=10.1, wall=40071
2022-08-17 09:19:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:19:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:19:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:19:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:19:59 | INFO | valid | epoch 467 | valid on 'valid' subset | loss 5.117 | nll_loss 2.495 | ppl 5.64 | bleu 56.39 | wps 1834.7 | wpb 933.5 | bsz 59.6 | num_updates 37827 | best_bleu 57.41
2022-08-17 09:19:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 467 @ 37827 updates
2022-08-17 09:19:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint467.pt
2022-08-17 09:20:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint467.pt
2022-08-17 09:20:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint467.pt (epoch 467 @ 37827 updates, score 56.39) (writing took 31.901963505893946 seconds)
2022-08-17 09:20:31 | INFO | fairseq_cli.train | end of epoch 467 (average epoch stats below)
2022-08-17 09:20:31 | INFO | train | epoch 467 | loss 3.386 | nll_loss 0.351 | ppl 1.28 | wps 5221.8 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 37827 | lr 0.000325184 | gnorm 0.305 | train_wall 42 | gb_free 10.2 | wall 40127
2022-08-17 09:20:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:20:31 | INFO | fairseq.trainer | begin training epoch 468
2022-08-17 09:20:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:21:11 | INFO | train_inner | epoch 468:     73 / 81 loss=3.385, nll_loss=0.35, ppl=1.27, wps=5787, ups=1.04, wpb=5566.1, bsz=361, num_updates=37900, lr=0.000324871, gnorm=0.312, train_wall=52, gb_free=10.1, wall=40167
2022-08-17 09:21:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:21:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:21:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:21:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:21:25 | INFO | valid | epoch 468 | valid on 'valid' subset | loss 5.12 | nll_loss 2.501 | ppl 5.66 | bleu 56.66 | wps 1779.9 | wpb 933.5 | bsz 59.6 | num_updates 37908 | best_bleu 57.41
2022-08-17 09:21:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 468 @ 37908 updates
2022-08-17 09:21:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint468.pt
2022-08-17 09:21:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint468.pt
2022-08-17 09:21:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint468.pt (epoch 468 @ 37908 updates, score 56.66) (writing took 21.158435478806496 seconds)
2022-08-17 09:21:46 | INFO | fairseq_cli.train | end of epoch 468 (average epoch stats below)
2022-08-17 09:21:46 | INFO | train | epoch 468 | loss 3.385 | nll_loss 0.35 | ppl 1.27 | wps 5926.4 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 37908 | lr 0.000324836 | gnorm 0.316 | train_wall 42 | gb_free 10.3 | wall 40203
2022-08-17 09:21:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:21:47 | INFO | fairseq.trainer | begin training epoch 469
2022-08-17 09:21:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:22:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:22:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:22:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:22:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:22:40 | INFO | valid | epoch 469 | valid on 'valid' subset | loss 5.123 | nll_loss 2.507 | ppl 5.69 | bleu 57 | wps 1933.2 | wpb 933.5 | bsz 59.6 | num_updates 37989 | best_bleu 57.41
2022-08-17 09:22:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 469 @ 37989 updates
2022-08-17 09:22:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint469.pt
2022-08-17 09:22:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint469.pt
2022-08-17 09:22:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint469.pt (epoch 469 @ 37989 updates, score 57.0) (writing took 18.209308922290802 seconds)
2022-08-17 09:22:58 | INFO | fairseq_cli.train | end of epoch 469 (average epoch stats below)
2022-08-17 09:22:58 | INFO | train | epoch 469 | loss 3.385 | nll_loss 0.35 | ppl 1.27 | wps 6203 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 37989 | lr 0.00032449 | gnorm 0.555 | train_wall 41 | gb_free 10.1 | wall 40275
2022-08-17 09:22:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:22:59 | INFO | fairseq.trainer | begin training epoch 470
2022-08-17 09:22:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:23:06 | INFO | train_inner | epoch 470:     11 / 81 loss=3.385, nll_loss=0.349, ppl=1.27, wps=4791.1, ups=0.87, wpb=5482.8, bsz=355.4, num_updates=38000, lr=0.000324443, gnorm=0.507, train_wall=51, gb_free=10.1, wall=40282
2022-08-17 09:23:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:23:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:23:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:23:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:23:56 | INFO | valid | epoch 470 | valid on 'valid' subset | loss 5.125 | nll_loss 2.509 | ppl 5.69 | bleu 56.93 | wps 1526.3 | wpb 933.5 | bsz 59.6 | num_updates 38070 | best_bleu 57.41
2022-08-17 09:23:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 470 @ 38070 updates
2022-08-17 09:23:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint470.pt
2022-08-17 09:23:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint470.pt
2022-08-17 09:24:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint470.pt (epoch 470 @ 38070 updates, score 56.93) (writing took 17.216901641339064 seconds)
2022-08-17 09:24:13 | INFO | fairseq_cli.train | end of epoch 470 (average epoch stats below)
2022-08-17 09:24:13 | INFO | train | epoch 470 | loss 3.385 | nll_loss 0.349 | ppl 1.27 | wps 6001 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 38070 | lr 0.000324144 | gnorm 0.293 | train_wall 41 | gb_free 10.1 | wall 40349
2022-08-17 09:24:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:24:13 | INFO | fairseq.trainer | begin training epoch 471
2022-08-17 09:24:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:24:30 | INFO | train_inner | epoch 471:     30 / 81 loss=3.385, nll_loss=0.349, ppl=1.27, wps=6566.2, ups=1.19, wpb=5540.2, bsz=356.2, num_updates=38100, lr=0.000324017, gnorm=0.29, train_wall=51, gb_free=10.1, wall=40366
2022-08-17 09:24:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:24:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:24:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:24:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:25:07 | INFO | valid | epoch 471 | valid on 'valid' subset | loss 5.117 | nll_loss 2.495 | ppl 5.64 | bleu 56.84 | wps 1976.9 | wpb 933.5 | bsz 59.6 | num_updates 38151 | best_bleu 57.41
2022-08-17 09:25:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 471 @ 38151 updates
2022-08-17 09:25:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint471.pt
2022-08-17 09:25:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint471.pt
2022-08-17 09:25:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint471.pt (epoch 471 @ 38151 updates, score 56.84) (writing took 23.47587325796485 seconds)
2022-08-17 09:25:30 | INFO | fairseq_cli.train | end of epoch 471 (average epoch stats below)
2022-08-17 09:25:30 | INFO | train | epoch 471 | loss 3.385 | nll_loss 0.349 | ppl 1.27 | wps 5784.1 | ups 1.05 | wpb 5523.2 | bsz 358 | num_updates 38151 | lr 0.0003238 | gnorm 0.317 | train_wall 43 | gb_free 10.2 | wall 40427
2022-08-17 09:25:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:25:31 | INFO | fairseq.trainer | begin training epoch 472
2022-08-17 09:25:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:25:59 | INFO | train_inner | epoch 472:     49 / 81 loss=3.384, nll_loss=0.349, ppl=1.27, wps=6224.5, ups=1.12, wpb=5543, bsz=362.5, num_updates=38200, lr=0.000323592, gnorm=0.316, train_wall=52, gb_free=10.1, wall=40455
2022-08-17 09:26:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:26:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:26:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:26:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:26:26 | INFO | valid | epoch 472 | valid on 'valid' subset | loss 5.123 | nll_loss 2.509 | ppl 5.69 | bleu 56.96 | wps 1905.4 | wpb 933.5 | bsz 59.6 | num_updates 38232 | best_bleu 57.41
2022-08-17 09:26:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 472 @ 38232 updates
2022-08-17 09:26:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint472.pt
2022-08-17 09:26:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint472.pt
2022-08-17 09:26:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint472.pt (epoch 472 @ 38232 updates, score 56.96) (writing took 16.206602949649096 seconds)
2022-08-17 09:26:42 | INFO | fairseq_cli.train | end of epoch 472 (average epoch stats below)
2022-08-17 09:26:42 | INFO | train | epoch 472 | loss 3.385 | nll_loss 0.35 | ppl 1.27 | wps 6220.4 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 38232 | lr 0.000323457 | gnorm 0.305 | train_wall 41 | gb_free 10.3 | wall 40499
2022-08-17 09:26:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:26:43 | INFO | fairseq.trainer | begin training epoch 473
2022-08-17 09:26:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:27:21 | INFO | train_inner | epoch 473:     68 / 81 loss=3.385, nll_loss=0.35, ppl=1.27, wps=6782.1, ups=1.22, wpb=5548.3, bsz=358.6, num_updates=38300, lr=0.00032317, gnorm=0.288, train_wall=52, gb_free=10.1, wall=40537
2022-08-17 09:27:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:27:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:27:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:27:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:27:37 | INFO | valid | epoch 473 | valid on 'valid' subset | loss 5.123 | nll_loss 2.504 | ppl 5.67 | bleu 56.68 | wps 1826.5 | wpb 933.5 | bsz 59.6 | num_updates 38313 | best_bleu 57.41
2022-08-17 09:27:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 473 @ 38313 updates
2022-08-17 09:27:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint473.pt
2022-08-17 09:27:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint473.pt
2022-08-17 09:28:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint473.pt (epoch 473 @ 38313 updates, score 56.68) (writing took 28.16711561009288 seconds)
2022-08-17 09:28:05 | INFO | fairseq_cli.train | end of epoch 473 (average epoch stats below)
2022-08-17 09:28:05 | INFO | train | epoch 473 | loss 3.384 | nll_loss 0.349 | ppl 1.27 | wps 5399.3 | ups 0.98 | wpb 5523.2 | bsz 358 | num_updates 38313 | lr 0.000323115 | gnorm 0.282 | train_wall 42 | gb_free 10.2 | wall 40581
2022-08-17 09:28:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:28:05 | INFO | fairseq.trainer | begin training epoch 474
2022-08-17 09:28:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:28:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:28:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:28:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:28:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:28:59 | INFO | valid | epoch 474 | valid on 'valid' subset | loss 5.123 | nll_loss 2.504 | ppl 5.67 | bleu 57.14 | wps 1924.6 | wpb 933.5 | bsz 59.6 | num_updates 38394 | best_bleu 57.41
2022-08-17 09:29:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 474 @ 38394 updates
2022-08-17 09:29:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint474.pt
2022-08-17 09:29:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint474.pt
2022-08-17 09:29:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint474.pt (epoch 474 @ 38394 updates, score 57.14) (writing took 22.543321184813976 seconds)
2022-08-17 09:29:22 | INFO | fairseq_cli.train | end of epoch 474 (average epoch stats below)
2022-08-17 09:29:22 | INFO | train | epoch 474 | loss 3.386 | nll_loss 0.352 | ppl 1.28 | wps 5806.4 | ups 1.05 | wpb 5523.2 | bsz 358 | num_updates 38394 | lr 0.000322774 | gnorm 0.46 | train_wall 41 | gb_free 10.1 | wall 40658
2022-08-17 09:29:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:29:22 | INFO | fairseq.trainer | begin training epoch 475
2022-08-17 09:29:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:29:27 | INFO | train_inner | epoch 475:      6 / 81 loss=3.386, nll_loss=0.351, ppl=1.28, wps=4319.7, ups=0.79, wpb=5458.9, bsz=354.8, num_updates=38400, lr=0.000322749, gnorm=0.431, train_wall=50, gb_free=10, wall=40664
2022-08-17 09:30:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:30:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:30:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:30:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:30:17 | INFO | valid | epoch 475 | valid on 'valid' subset | loss 5.107 | nll_loss 2.482 | ppl 5.59 | bleu 56.97 | wps 1888.1 | wpb 933.5 | bsz 59.6 | num_updates 38475 | best_bleu 57.41
2022-08-17 09:30:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 475 @ 38475 updates
2022-08-17 09:30:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint475.pt
2022-08-17 09:30:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint475.pt
2022-08-17 09:30:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint475.pt (epoch 475 @ 38475 updates, score 56.97) (writing took 15.652633629739285 seconds)
2022-08-17 09:30:33 | INFO | fairseq_cli.train | end of epoch 475 (average epoch stats below)
2022-08-17 09:30:33 | INFO | train | epoch 475 | loss 3.385 | nll_loss 0.349 | ppl 1.27 | wps 6357.4 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 38475 | lr 0.000322434 | gnorm 0.29 | train_wall 41 | gb_free 10.1 | wall 40729
2022-08-17 09:30:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:30:33 | INFO | fairseq.trainer | begin training epoch 476
2022-08-17 09:30:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:30:48 | INFO | train_inner | epoch 476:     25 / 81 loss=3.384, nll_loss=0.349, ppl=1.27, wps=6899.6, ups=1.25, wpb=5537.3, bsz=360.6, num_updates=38500, lr=0.000322329, gnorm=0.285, train_wall=50, gb_free=10.1, wall=40744
2022-08-17 09:31:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:31:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:31:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:31:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:31:25 | INFO | valid | epoch 476 | valid on 'valid' subset | loss 5.131 | nll_loss 2.517 | ppl 5.72 | bleu 56.69 | wps 2009.2 | wpb 933.5 | bsz 59.6 | num_updates 38556 | best_bleu 57.41
2022-08-17 09:31:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 476 @ 38556 updates
2022-08-17 09:31:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint476.pt
2022-08-17 09:31:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint476.pt
2022-08-17 09:32:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint476.pt (epoch 476 @ 38556 updates, score 56.69) (writing took 36.06536536663771 seconds)
2022-08-17 09:32:02 | INFO | fairseq_cli.train | end of epoch 476 (average epoch stats below)
2022-08-17 09:32:02 | INFO | train | epoch 476 | loss 3.384 | nll_loss 0.349 | ppl 1.27 | wps 5028.7 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 38556 | lr 0.000322095 | gnorm 0.288 | train_wall 40 | gb_free 10.1 | wall 40818
2022-08-17 09:32:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:32:02 | INFO | fairseq.trainer | begin training epoch 477
2022-08-17 09:32:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:32:25 | INFO | train_inner | epoch 477:     44 / 81 loss=3.384, nll_loss=0.349, ppl=1.27, wps=5680.2, ups=1.02, wpb=5545, bsz=356.3, num_updates=38600, lr=0.000321911, gnorm=0.291, train_wall=49, gb_free=10.1, wall=40841
2022-08-17 09:32:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:32:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:32:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:32:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:32:54 | INFO | valid | epoch 477 | valid on 'valid' subset | loss 5.112 | nll_loss 2.494 | ppl 5.63 | bleu 56.32 | wps 1901.8 | wpb 933.5 | bsz 59.6 | num_updates 38637 | best_bleu 57.41
2022-08-17 09:32:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 477 @ 38637 updates
2022-08-17 09:32:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint477.pt
2022-08-17 09:32:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint477.pt
2022-08-17 09:33:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint477.pt (epoch 477 @ 38637 updates, score 56.32) (writing took 26.492165476083755 seconds)
2022-08-17 09:33:21 | INFO | fairseq_cli.train | end of epoch 477 (average epoch stats below)
2022-08-17 09:33:21 | INFO | train | epoch 477 | loss 3.385 | nll_loss 0.35 | ppl 1.27 | wps 5655.7 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 38637 | lr 0.000321757 | gnorm 0.298 | train_wall 40 | gb_free 10.1 | wall 40897
2022-08-17 09:33:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:33:21 | INFO | fairseq.trainer | begin training epoch 478
2022-08-17 09:33:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:33:57 | INFO | train_inner | epoch 478:     63 / 81 loss=3.384, nll_loss=0.349, ppl=1.27, wps=6032, ups=1.09, wpb=5519.1, bsz=363, num_updates=38700, lr=0.000321495, gnorm=0.301, train_wall=50, gb_free=10.1, wall=40933
2022-08-17 09:34:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:34:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:34:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:34:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:34:16 | INFO | valid | epoch 478 | valid on 'valid' subset | loss 5.108 | nll_loss 2.491 | ppl 5.62 | bleu 56.59 | wps 1987.5 | wpb 933.5 | bsz 59.6 | num_updates 38718 | best_bleu 57.41
2022-08-17 09:34:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 478 @ 38718 updates
2022-08-17 09:34:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint478.pt
2022-08-17 09:34:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint478.pt
2022-08-17 09:34:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint478.pt (epoch 478 @ 38718 updates, score 56.59) (writing took 18.366004709154367 seconds)
2022-08-17 09:34:34 | INFO | fairseq_cli.train | end of epoch 478 (average epoch stats below)
2022-08-17 09:34:34 | INFO | train | epoch 478 | loss 3.384 | nll_loss 0.349 | ppl 1.27 | wps 6089 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 38718 | lr 0.00032142 | gnorm 0.301 | train_wall 41 | gb_free 10.2 | wall 40970
2022-08-17 09:34:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:34:34 | INFO | fairseq.trainer | begin training epoch 479
2022-08-17 09:34:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:35:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:35:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:35:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:35:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:35:27 | INFO | valid | epoch 479 | valid on 'valid' subset | loss 5.101 | nll_loss 2.482 | ppl 5.59 | bleu 56.66 | wps 1935 | wpb 933.5 | bsz 59.6 | num_updates 38799 | best_bleu 57.41
2022-08-17 09:35:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 479 @ 38799 updates
2022-08-17 09:35:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint479.pt
2022-08-17 09:35:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint479.pt
2022-08-17 09:35:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint479.pt (epoch 479 @ 38799 updates, score 56.66) (writing took 27.202925357967615 seconds)
2022-08-17 09:35:54 | INFO | fairseq_cli.train | end of epoch 479 (average epoch stats below)
2022-08-17 09:35:54 | INFO | train | epoch 479 | loss 3.385 | nll_loss 0.35 | ppl 1.27 | wps 5581.6 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 38799 | lr 0.000321085 | gnorm 0.33 | train_wall 42 | gb_free 10.1 | wall 41051
2022-08-17 09:35:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:35:55 | INFO | fairseq.trainer | begin training epoch 480
2022-08-17 09:35:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:35:56 | INFO | train_inner | epoch 480:      1 / 81 loss=3.385, nll_loss=0.35, ppl=1.27, wps=4611.2, ups=0.84, wpb=5504.6, bsz=352.6, num_updates=38800, lr=0.000321081, gnorm=0.327, train_wall=51, gb_free=10.2, wall=41052
2022-08-17 09:36:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:36:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:36:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:36:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:36:51 | INFO | valid | epoch 480 | valid on 'valid' subset | loss 5.093 | nll_loss 2.468 | ppl 5.53 | bleu 56.78 | wps 1722.9 | wpb 933.5 | bsz 59.6 | num_updates 38880 | best_bleu 57.41
2022-08-17 09:36:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 480 @ 38880 updates
2022-08-17 09:36:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint480.pt
2022-08-17 09:36:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint480.pt
2022-08-17 09:37:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint480.pt (epoch 480 @ 38880 updates, score 56.78) (writing took 33.98133397102356 seconds)
2022-08-17 09:37:25 | INFO | fairseq_cli.train | end of epoch 480 (average epoch stats below)
2022-08-17 09:37:25 | INFO | train | epoch 480 | loss 3.385 | nll_loss 0.35 | ppl 1.27 | wps 4921.1 | ups 0.89 | wpb 5523.2 | bsz 358 | num_updates 38880 | lr 0.00032075 | gnorm 0.311 | train_wall 41 | gb_free 10.2 | wall 41141
2022-08-17 09:37:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:37:25 | INFO | fairseq.trainer | begin training epoch 481
2022-08-17 09:37:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:37:38 | INFO | train_inner | epoch 481:     20 / 81 loss=3.384, nll_loss=0.349, ppl=1.27, wps=5391.5, ups=0.98, wpb=5508.8, bsz=360.7, num_updates=38900, lr=0.000320668, gnorm=0.298, train_wall=50, gb_free=10.1, wall=41154
2022-08-17 09:38:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:38:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:38:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:38:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:38:23 | INFO | valid | epoch 481 | valid on 'valid' subset | loss 5.1 | nll_loss 2.481 | ppl 5.58 | bleu 56.96 | wps 1992.8 | wpb 933.5 | bsz 59.6 | num_updates 38961 | best_bleu 57.41
2022-08-17 09:38:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 481 @ 38961 updates
2022-08-17 09:38:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint481.pt
2022-08-17 09:38:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint481.pt
2022-08-17 09:38:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint481.pt (epoch 481 @ 38961 updates, score 56.96) (writing took 2.2989052832126617 seconds)
2022-08-17 09:38:25 | INFO | fairseq_cli.train | end of epoch 481 (average epoch stats below)
2022-08-17 09:38:25 | INFO | train | epoch 481 | loss 3.385 | nll_loss 0.349 | ppl 1.27 | wps 7458.2 | ups 1.35 | wpb 5523.2 | bsz 358 | num_updates 38961 | lr 0.000320417 | gnorm 0.294 | train_wall 41 | gb_free 10.1 | wall 41201
2022-08-17 09:38:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:38:25 | INFO | fairseq.trainer | begin training epoch 482
2022-08-17 09:38:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:38:47 | INFO | train_inner | epoch 482:     39 / 81 loss=3.385, nll_loss=0.35, ppl=1.27, wps=8129.3, ups=1.46, wpb=5569.3, bsz=356.5, num_updates=39000, lr=0.000320256, gnorm=0.298, train_wall=51, gb_free=10.1, wall=41223
2022-08-17 09:39:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:39:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:39:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:39:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:39:16 | INFO | valid | epoch 482 | valid on 'valid' subset | loss 5.092 | nll_loss 2.465 | ppl 5.52 | bleu 57.08 | wps 1989 | wpb 933.5 | bsz 59.6 | num_updates 39042 | best_bleu 57.41
2022-08-17 09:39:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 482 @ 39042 updates
2022-08-17 09:39:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint482.pt
2022-08-17 09:39:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint482.pt
2022-08-17 09:39:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint482.pt (epoch 482 @ 39042 updates, score 57.08) (writing took 13.189843520522118 seconds)
2022-08-17 09:39:29 | INFO | fairseq_cli.train | end of epoch 482 (average epoch stats below)
2022-08-17 09:39:29 | INFO | train | epoch 482 | loss 3.385 | nll_loss 0.35 | ppl 1.27 | wps 6985.9 | ups 1.26 | wpb 5523.2 | bsz 358 | num_updates 39042 | lr 0.000320084 | gnorm 0.344 | train_wall 40 | gb_free 10 | wall 41265
2022-08-17 09:39:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:39:29 | INFO | fairseq.trainer | begin training epoch 483
2022-08-17 09:39:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:40:01 | INFO | train_inner | epoch 483:     58 / 81 loss=3.385, nll_loss=0.35, ppl=1.27, wps=7366, ups=1.34, wpb=5500.1, bsz=359.2, num_updates=39100, lr=0.000319847, gnorm=0.375, train_wall=50, gb_free=10.1, wall=41298
2022-08-17 09:40:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:40:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:40:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:40:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:40:22 | INFO | valid | epoch 483 | valid on 'valid' subset | loss 5.104 | nll_loss 2.482 | ppl 5.59 | bleu 57.01 | wps 1969.4 | wpb 933.5 | bsz 59.6 | num_updates 39123 | best_bleu 57.41
2022-08-17 09:40:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 483 @ 39123 updates
2022-08-17 09:40:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint483.pt
2022-08-17 09:40:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint483.pt
2022-08-17 09:40:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint483.pt (epoch 483 @ 39123 updates, score 57.01) (writing took 2.2842924632132053 seconds)
2022-08-17 09:40:25 | INFO | fairseq_cli.train | end of epoch 483 (average epoch stats below)
2022-08-17 09:40:25 | INFO | train | epoch 483 | loss 3.385 | nll_loss 0.351 | ppl 1.28 | wps 8057.9 | ups 1.46 | wpb 5523.2 | bsz 358 | num_updates 39123 | lr 0.000319752 | gnorm 0.346 | train_wall 41 | gb_free 10.1 | wall 41321
2022-08-17 09:40:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:40:25 | INFO | fairseq.trainer | begin training epoch 484
2022-08-17 09:40:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:41:06 | INFO | train_inner | epoch 484:     77 / 81 loss=3.386, nll_loss=0.352, ppl=1.28, wps=8598.9, ups=1.55, wpb=5541.5, bsz=357.8, num_updates=39200, lr=0.000319438, gnorm=0.463, train_wall=51, gb_free=10.1, wall=41362
2022-08-17 09:41:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:41:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:41:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:41:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:41:17 | INFO | valid | epoch 484 | valid on 'valid' subset | loss 5.112 | nll_loss 2.492 | ppl 5.62 | bleu 56.22 | wps 1917.2 | wpb 933.5 | bsz 59.6 | num_updates 39204 | best_bleu 57.41
2022-08-17 09:41:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 484 @ 39204 updates
2022-08-17 09:41:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint484.pt
2022-08-17 09:41:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint484.pt
2022-08-17 09:41:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint484.pt (epoch 484 @ 39204 updates, score 56.22) (writing took 22.26838944107294 seconds)
2022-08-17 09:41:39 | INFO | fairseq_cli.train | end of epoch 484 (average epoch stats below)
2022-08-17 09:41:39 | INFO | train | epoch 484 | loss 3.386 | nll_loss 0.351 | ppl 1.28 | wps 6006.7 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 39204 | lr 0.000319422 | gnorm 0.489 | train_wall 41 | gb_free 10.1 | wall 41395
2022-08-17 09:41:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:41:39 | INFO | fairseq.trainer | begin training epoch 485
2022-08-17 09:41:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:42:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:42:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:42:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:42:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:42:34 | INFO | valid | epoch 485 | valid on 'valid' subset | loss 5.1 | nll_loss 2.474 | ppl 5.55 | bleu 57.2 | wps 1688.7 | wpb 933.5 | bsz 59.6 | num_updates 39285 | best_bleu 57.41
2022-08-17 09:42:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 485 @ 39285 updates
2022-08-17 09:42:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint485.pt
2022-08-17 09:42:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint485.pt
2022-08-17 09:43:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint485.pt (epoch 485 @ 39285 updates, score 57.2) (writing took 37.30683336034417 seconds)
2022-08-17 09:43:11 | INFO | fairseq_cli.train | end of epoch 485 (average epoch stats below)
2022-08-17 09:43:11 | INFO | train | epoch 485 | loss 3.384 | nll_loss 0.349 | ppl 1.27 | wps 4864.8 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 39285 | lr 0.000319093 | gnorm 0.282 | train_wall 42 | gb_free 10.1 | wall 41487
2022-08-17 09:43:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:43:11 | INFO | fairseq.trainer | begin training epoch 486
2022-08-17 09:43:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:43:20 | INFO | train_inner | epoch 486:     15 / 81 loss=3.384, nll_loss=0.349, ppl=1.27, wps=4106.6, ups=0.74, wpb=5522.2, bsz=358.8, num_updates=39300, lr=0.000319032, gnorm=0.282, train_wall=51, gb_free=10, wall=41497
2022-08-17 09:43:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:43:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:43:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:43:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:44:05 | INFO | valid | epoch 486 | valid on 'valid' subset | loss 5.092 | nll_loss 2.469 | ppl 5.54 | bleu 57.22 | wps 1914.3 | wpb 933.5 | bsz 59.6 | num_updates 39366 | best_bleu 57.41
2022-08-17 09:44:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 486 @ 39366 updates
2022-08-17 09:44:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint486.pt
2022-08-17 09:44:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint486.pt
2022-08-17 09:44:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint486.pt (epoch 486 @ 39366 updates, score 57.22) (writing took 25.50427221879363 seconds)
2022-08-17 09:44:30 | INFO | fairseq_cli.train | end of epoch 486 (average epoch stats below)
2022-08-17 09:44:30 | INFO | train | epoch 486 | loss 3.384 | nll_loss 0.349 | ppl 1.27 | wps 5661.4 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 39366 | lr 0.000318764 | gnorm 0.289 | train_wall 41 | gb_free 10.2 | wall 41566
2022-08-17 09:44:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:44:30 | INFO | fairseq.trainer | begin training epoch 487
2022-08-17 09:44:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:44:49 | INFO | train_inner | epoch 487:     34 / 81 loss=3.384, nll_loss=0.349, ppl=1.27, wps=6220.8, ups=1.12, wpb=5531.7, bsz=356.2, num_updates=39400, lr=0.000318626, gnorm=0.297, train_wall=51, gb_free=10.1, wall=41586
2022-08-17 09:45:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:45:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:45:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:45:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:45:22 | INFO | valid | epoch 487 | valid on 'valid' subset | loss 5.104 | nll_loss 2.483 | ppl 5.59 | bleu 56.93 | wps 1993.8 | wpb 933.5 | bsz 59.6 | num_updates 39447 | best_bleu 57.41
2022-08-17 09:45:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 487 @ 39447 updates
2022-08-17 09:45:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint487.pt
2022-08-17 09:45:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint487.pt
2022-08-17 09:45:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint487.pt (epoch 487 @ 39447 updates, score 56.93) (writing took 14.423398539423943 seconds)
2022-08-17 09:45:37 | INFO | fairseq_cli.train | end of epoch 487 (average epoch stats below)
2022-08-17 09:45:37 | INFO | train | epoch 487 | loss 3.385 | nll_loss 0.35 | ppl 1.27 | wps 6701.1 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 39447 | lr 0.000318437 | gnorm 0.315 | train_wall 41 | gb_free 10.1 | wall 41633
2022-08-17 09:45:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:45:37 | INFO | fairseq.trainer | begin training epoch 488
2022-08-17 09:45:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:46:07 | INFO | train_inner | epoch 488:     53 / 81 loss=3.384, nll_loss=0.349, ppl=1.27, wps=7132.1, ups=1.29, wpb=5508.3, bsz=358.8, num_updates=39500, lr=0.000318223, gnorm=0.287, train_wall=50, gb_free=10.1, wall=41663
2022-08-17 09:46:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:46:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:46:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:46:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:46:30 | INFO | valid | epoch 488 | valid on 'valid' subset | loss 5.109 | nll_loss 2.483 | ppl 5.59 | bleu 57.28 | wps 1849.1 | wpb 933.5 | bsz 59.6 | num_updates 39528 | best_bleu 57.41
2022-08-17 09:46:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 488 @ 39528 updates
2022-08-17 09:46:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint488.pt
2022-08-17 09:46:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint488.pt
2022-08-17 09:46:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint488.pt (epoch 488 @ 39528 updates, score 57.28) (writing took 28.303743090480566 seconds)
2022-08-17 09:46:59 | INFO | fairseq_cli.train | end of epoch 488 (average epoch stats below)
2022-08-17 09:46:59 | INFO | train | epoch 488 | loss 3.384 | nll_loss 0.349 | ppl 1.27 | wps 5489 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 39528 | lr 0.00031811 | gnorm 0.281 | train_wall 41 | gb_free 10.1 | wall 41715
2022-08-17 09:46:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:46:59 | INFO | fairseq.trainer | begin training epoch 489
2022-08-17 09:46:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:47:37 | INFO | train_inner | epoch 489:     72 / 81 loss=3.385, nll_loss=0.35, ppl=1.27, wps=6120.5, ups=1.1, wpb=5543.6, bsz=361, num_updates=39600, lr=0.000317821, gnorm=0.286, train_wall=50, gb_free=10.1, wall=41753
2022-08-17 09:47:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:47:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:47:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:47:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:47:50 | INFO | valid | epoch 489 | valid on 'valid' subset | loss 5.093 | nll_loss 2.468 | ppl 5.53 | bleu 57.33 | wps 1996.8 | wpb 933.5 | bsz 59.6 | num_updates 39609 | best_bleu 57.41
2022-08-17 09:47:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 489 @ 39609 updates
2022-08-17 09:47:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint489.pt
2022-08-17 09:47:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint489.pt
2022-08-17 09:48:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint489.pt (epoch 489 @ 39609 updates, score 57.33) (writing took 20.86094857379794 seconds)
2022-08-17 09:48:11 | INFO | fairseq_cli.train | end of epoch 489 (average epoch stats below)
2022-08-17 09:48:11 | INFO | train | epoch 489 | loss 3.384 | nll_loss 0.349 | ppl 1.27 | wps 6143.2 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 39609 | lr 0.000317785 | gnorm 0.28 | train_wall 40 | gb_free 10.3 | wall 41788
2022-08-17 09:48:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:48:12 | INFO | fairseq.trainer | begin training epoch 490
2022-08-17 09:48:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:48:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:48:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:48:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:48:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:49:06 | INFO | valid | epoch 490 | valid on 'valid' subset | loss 5.107 | nll_loss 2.488 | ppl 5.61 | bleu 56.97 | wps 1835.6 | wpb 933.5 | bsz 59.6 | num_updates 39690 | best_bleu 57.41
2022-08-17 09:49:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 490 @ 39690 updates
2022-08-17 09:49:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint490.pt
2022-08-17 09:49:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint490.pt
2022-08-17 09:49:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint490.pt (epoch 490 @ 39690 updates, score 56.97) (writing took 14.180524580180645 seconds)
2022-08-17 09:49:20 | INFO | fairseq_cli.train | end of epoch 490 (average epoch stats below)
2022-08-17 09:49:20 | INFO | train | epoch 490 | loss 3.386 | nll_loss 0.352 | ppl 1.28 | wps 6516.7 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 39690 | lr 0.00031746 | gnorm 0.323 | train_wall 39 | gb_free 10.2 | wall 41856
2022-08-17 09:49:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:49:20 | INFO | fairseq.trainer | begin training epoch 491
2022-08-17 09:49:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:49:27 | INFO | train_inner | epoch 491:     10 / 81 loss=3.386, nll_loss=0.351, ppl=1.28, wps=5016.6, ups=0.91, wpb=5509.4, bsz=355.4, num_updates=39700, lr=0.00031742, gnorm=0.316, train_wall=48, gb_free=10.1, wall=41863
2022-08-17 09:50:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:50:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:50:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:50:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:50:13 | INFO | valid | epoch 491 | valid on 'valid' subset | loss 5.114 | nll_loss 2.493 | ppl 5.63 | bleu 56.9 | wps 1975 | wpb 933.5 | bsz 59.6 | num_updates 39771 | best_bleu 57.41
2022-08-17 09:50:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 491 @ 39771 updates
2022-08-17 09:50:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint491.pt
2022-08-17 09:50:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint491.pt
2022-08-17 09:50:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint491.pt (epoch 491 @ 39771 updates, score 56.9) (writing took 22.355572026222944 seconds)
2022-08-17 09:50:36 | INFO | fairseq_cli.train | end of epoch 491 (average epoch stats below)
2022-08-17 09:50:36 | INFO | train | epoch 491 | loss 3.385 | nll_loss 0.35 | ppl 1.27 | wps 5915.8 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 39771 | lr 0.000317137 | gnorm 0.293 | train_wall 42 | gb_free 10 | wall 41932
2022-08-17 09:50:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:50:36 | INFO | fairseq.trainer | begin training epoch 492
2022-08-17 09:50:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:50:52 | INFO | train_inner | epoch 492:     29 / 81 loss=3.384, nll_loss=0.349, ppl=1.27, wps=6484.2, ups=1.17, wpb=5526.6, bsz=358.2, num_updates=39800, lr=0.000317021, gnorm=0.297, train_wall=52, gb_free=10, wall=41948
2022-08-17 09:51:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:51:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:51:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:51:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:51:28 | INFO | valid | epoch 492 | valid on 'valid' subset | loss 5.127 | nll_loss 2.509 | ppl 5.69 | bleu 56.59 | wps 1866.4 | wpb 933.5 | bsz 59.6 | num_updates 39852 | best_bleu 57.41
2022-08-17 09:51:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 492 @ 39852 updates
2022-08-17 09:51:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint492.pt
2022-08-17 09:51:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint492.pt
2022-08-17 09:51:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint492.pt (epoch 492 @ 39852 updates, score 56.59) (writing took 17.32981476187706 seconds)
2022-08-17 09:51:45 | INFO | fairseq_cli.train | end of epoch 492 (average epoch stats below)
2022-08-17 09:51:45 | INFO | train | epoch 492 | loss 3.383 | nll_loss 0.348 | ppl 1.27 | wps 6407.3 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 39852 | lr 0.000316814 | gnorm 0.271 | train_wall 41 | gb_free 10.2 | wall 42002
2022-08-17 09:51:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:51:46 | INFO | fairseq.trainer | begin training epoch 493
2022-08-17 09:51:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:52:14 | INFO | train_inner | epoch 493:     48 / 81 loss=3.383, nll_loss=0.348, ppl=1.27, wps=6705.2, ups=1.22, wpb=5490.8, bsz=359.6, num_updates=39900, lr=0.000316624, gnorm=0.273, train_wall=52, gb_free=10.1, wall=42030
2022-08-17 09:52:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:52:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:52:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:52:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:52:40 | INFO | valid | epoch 493 | valid on 'valid' subset | loss 5.122 | nll_loss 2.505 | ppl 5.68 | bleu 57.21 | wps 1896.6 | wpb 933.5 | bsz 59.6 | num_updates 39933 | best_bleu 57.41
2022-08-17 09:52:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 493 @ 39933 updates
2022-08-17 09:52:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint493.pt
2022-08-17 09:52:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint493.pt
2022-08-17 09:52:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint493.pt (epoch 493 @ 39933 updates, score 57.21) (writing took 14.268751043826342 seconds)
2022-08-17 09:52:54 | INFO | fairseq_cli.train | end of epoch 493 (average epoch stats below)
2022-08-17 09:52:54 | INFO | train | epoch 493 | loss 3.383 | nll_loss 0.348 | ppl 1.27 | wps 6497.5 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 39933 | lr 0.000316493 | gnorm 0.285 | train_wall 42 | gb_free 10.1 | wall 42071
2022-08-17 09:52:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:52:55 | INFO | fairseq.trainer | begin training epoch 494
2022-08-17 09:52:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:53:31 | INFO | train_inner | epoch 494:     67 / 81 loss=3.384, nll_loss=0.349, ppl=1.27, wps=7149.5, ups=1.29, wpb=5532.8, bsz=353.4, num_updates=40000, lr=0.000316228, gnorm=0.266, train_wall=51, gb_free=10.2, wall=42108
2022-08-17 09:53:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:53:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:53:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:53:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:53:48 | INFO | valid | epoch 494 | valid on 'valid' subset | loss 5.103 | nll_loss 2.479 | ppl 5.57 | bleu 57.23 | wps 1940.1 | wpb 933.5 | bsz 59.6 | num_updates 40014 | best_bleu 57.41
2022-08-17 09:53:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 494 @ 40014 updates
2022-08-17 09:53:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint494.pt
2022-08-17 09:53:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint494.pt
2022-08-17 09:54:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint494.pt (epoch 494 @ 40014 updates, score 57.23) (writing took 34.924794767051935 seconds)
2022-08-17 09:54:23 | INFO | fairseq_cli.train | end of epoch 494 (average epoch stats below)
2022-08-17 09:54:23 | INFO | train | epoch 494 | loss 3.383 | nll_loss 0.348 | ppl 1.27 | wps 5055.1 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 40014 | lr 0.000316172 | gnorm 0.267 | train_wall 42 | gb_free 10.3 | wall 42159
2022-08-17 09:54:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:54:23 | INFO | fairseq.trainer | begin training epoch 495
2022-08-17 09:54:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:55:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:55:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:55:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:55:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:55:16 | INFO | valid | epoch 495 | valid on 'valid' subset | loss 5.127 | nll_loss 2.507 | ppl 5.69 | bleu 56.48 | wps 2001.3 | wpb 933.5 | bsz 59.6 | num_updates 40095 | best_bleu 57.41
2022-08-17 09:55:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 495 @ 40095 updates
2022-08-17 09:55:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint495.pt
2022-08-17 09:55:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint495.pt
2022-08-17 09:55:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint495.pt (epoch 495 @ 40095 updates, score 56.48) (writing took 19.339377116411924 seconds)
2022-08-17 09:55:35 | INFO | fairseq_cli.train | end of epoch 495 (average epoch stats below)
2022-08-17 09:55:35 | INFO | train | epoch 495 | loss 3.383 | nll_loss 0.348 | ppl 1.27 | wps 6191.2 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 40095 | lr 0.000315853 | gnorm 0.292 | train_wall 42 | gb_free 10.1 | wall 42231
2022-08-17 09:55:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:55:35 | INFO | fairseq.trainer | begin training epoch 496
2022-08-17 09:55:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:55:39 | INFO | train_inner | epoch 496:      5 / 81 loss=3.383, nll_loss=0.348, ppl=1.27, wps=4306.8, ups=0.78, wpb=5508.6, bsz=358.2, num_updates=40100, lr=0.000315833, gnorm=0.301, train_wall=51, gb_free=10.1, wall=42236
2022-08-17 09:56:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:56:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:56:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:56:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:56:30 | INFO | valid | epoch 496 | valid on 'valid' subset | loss 5.113 | nll_loss 2.492 | ppl 5.62 | bleu 56.81 | wps 1936.3 | wpb 933.5 | bsz 59.6 | num_updates 40176 | best_bleu 57.41
2022-08-17 09:56:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 496 @ 40176 updates
2022-08-17 09:56:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint496.pt
2022-08-17 09:56:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint496.pt
2022-08-17 09:56:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint496.pt (epoch 496 @ 40176 updates, score 56.81) (writing took 15.239157296717167 seconds)
2022-08-17 09:56:45 | INFO | fairseq_cli.train | end of epoch 496 (average epoch stats below)
2022-08-17 09:56:45 | INFO | train | epoch 496 | loss 3.383 | nll_loss 0.349 | ppl 1.27 | wps 6403.7 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 40176 | lr 0.000315534 | gnorm 0.379 | train_wall 42 | gb_free 10.1 | wall 42301
2022-08-17 09:56:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:56:45 | INFO | fairseq.trainer | begin training epoch 497
2022-08-17 09:56:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:57:00 | INFO | train_inner | epoch 497:     24 / 81 loss=3.384, nll_loss=0.349, ppl=1.27, wps=6923.9, ups=1.25, wpb=5556.5, bsz=365, num_updates=40200, lr=0.00031544, gnorm=0.373, train_wall=52, gb_free=10, wall=42316
2022-08-17 09:57:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:57:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:57:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:57:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:57:40 | INFO | valid | epoch 497 | valid on 'valid' subset | loss 5.123 | nll_loss 2.5 | ppl 5.66 | bleu 56.52 | wps 1805.3 | wpb 933.5 | bsz 59.6 | num_updates 40257 | best_bleu 57.41
2022-08-17 09:57:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 497 @ 40257 updates
2022-08-17 09:57:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint497.pt
2022-08-17 09:57:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint497.pt
2022-08-17 09:58:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint497.pt (epoch 497 @ 40257 updates, score 56.52) (writing took 35.217918284237385 seconds)
2022-08-17 09:58:15 | INFO | fairseq_cli.train | end of epoch 497 (average epoch stats below)
2022-08-17 09:58:15 | INFO | train | epoch 497 | loss 3.386 | nll_loss 0.351 | ppl 1.28 | wps 4954.1 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 40257 | lr 0.000315217 | gnorm 0.342 | train_wall 43 | gb_free 10.1 | wall 42392
2022-08-17 09:58:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:58:15 | INFO | fairseq.trainer | begin training epoch 498
2022-08-17 09:58:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 09:58:39 | INFO | train_inner | epoch 498:     43 / 81 loss=3.385, nll_loss=0.35, ppl=1.27, wps=5515.9, ups=1, wpb=5503.1, bsz=354.6, num_updates=40300, lr=0.000315049, gnorm=0.312, train_wall=52, gb_free=10, wall=42416
2022-08-17 09:58:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 09:59:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 09:59:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 09:59:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 09:59:08 | INFO | valid | epoch 498 | valid on 'valid' subset | loss 5.112 | nll_loss 2.493 | ppl 5.63 | bleu 56.68 | wps 1832.6 | wpb 933.5 | bsz 59.6 | num_updates 40338 | best_bleu 57.41
2022-08-17 09:59:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 498 @ 40338 updates
2022-08-17 09:59:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint498.pt
2022-08-17 09:59:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint498.pt
2022-08-17 09:59:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint498.pt (epoch 498 @ 40338 updates, score 56.68) (writing took 21.707641016691923 seconds)
2022-08-17 09:59:30 | INFO | fairseq_cli.train | end of epoch 498 (average epoch stats below)
2022-08-17 09:59:30 | INFO | train | epoch 498 | loss 3.383 | nll_loss 0.348 | ppl 1.27 | wps 5977.1 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 40338 | lr 0.0003149 | gnorm 0.296 | train_wall 40 | gb_free 10.2 | wall 42466
2022-08-17 09:59:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 09:59:30 | INFO | fairseq.trainer | begin training epoch 499
2022-08-17 09:59:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:00:11 | INFO | train_inner | epoch 499:     62 / 81 loss=3.383, nll_loss=0.348, ppl=1.27, wps=6074.2, ups=1.09, wpb=5556.5, bsz=354.7, num_updates=40400, lr=0.000314658, gnorm=0.337, train_wall=50, gb_free=10, wall=42507
2022-08-17 10:00:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:00:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:00:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:00:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:00:31 | INFO | valid | epoch 499 | valid on 'valid' subset | loss 5.122 | nll_loss 2.504 | ppl 5.67 | bleu 56.96 | wps 1994 | wpb 933.5 | bsz 59.6 | num_updates 40419 | best_bleu 57.41
2022-08-17 10:00:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 499 @ 40419 updates
2022-08-17 10:00:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint499.pt
2022-08-17 10:00:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint499.pt
2022-08-17 10:00:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint499.pt (epoch 499 @ 40419 updates, score 56.96) (writing took 2.352030638605356 seconds)
2022-08-17 10:00:34 | INFO | fairseq_cli.train | end of epoch 499 (average epoch stats below)
2022-08-17 10:00:34 | INFO | train | epoch 499 | loss 3.383 | nll_loss 0.348 | ppl 1.27 | wps 7052.7 | ups 1.28 | wpb 5523.2 | bsz 358 | num_updates 40419 | lr 0.000314584 | gnorm 0.341 | train_wall 41 | gb_free 10.4 | wall 42530
2022-08-17 10:00:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:00:34 | INFO | fairseq.trainer | begin training epoch 500
2022-08-17 10:00:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:01:22 | INFO | train_inner | epoch 500:     81 / 81 loss=3.384, nll_loss=0.349, ppl=1.27, wps=7678.9, ups=1.4, wpb=5485.1, bsz=358.4, num_updates=40500, lr=0.00031427, gnorm=0.316, train_wall=50, gb_free=10.2, wall=42579
2022-08-17 10:01:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:01:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:01:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:01:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:01:31 | INFO | valid | epoch 500 | valid on 'valid' subset | loss 5.097 | nll_loss 2.475 | ppl 5.56 | bleu 57.32 | wps 1973.5 | wpb 933.5 | bsz 59.6 | num_updates 40500 | best_bleu 57.41
2022-08-17 10:01:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 500 @ 40500 updates
2022-08-17 10:01:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint500.pt
2022-08-17 10:01:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint500.pt
2022-08-17 10:02:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint500.pt (epoch 500 @ 40500 updates, score 57.32) (writing took 34.0652078576386 seconds)
2022-08-17 10:02:06 | INFO | fairseq_cli.train | end of epoch 500 (average epoch stats below)
2022-08-17 10:02:06 | INFO | train | epoch 500 | loss 3.384 | nll_loss 0.349 | ppl 1.27 | wps 4860 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 40500 | lr 0.00031427 | gnorm 0.322 | train_wall 40 | gb_free 10.2 | wall 42622
2022-08-17 10:02:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:02:06 | INFO | fairseq.trainer | begin training epoch 501
2022-08-17 10:02:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:02:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:02:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:02:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:02:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:03:02 | INFO | valid | epoch 501 | valid on 'valid' subset | loss 5.135 | nll_loss 2.518 | ppl 5.73 | bleu 56.54 | wps 2065.8 | wpb 933.5 | bsz 59.6 | num_updates 40581 | best_bleu 57.41
2022-08-17 10:03:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 501 @ 40581 updates
2022-08-17 10:03:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint501.pt
2022-08-17 10:03:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint501.pt
2022-08-17 10:03:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint501.pt (epoch 501 @ 40581 updates, score 56.54) (writing took 18.948756117373705 seconds)
2022-08-17 10:03:21 | INFO | fairseq_cli.train | end of epoch 501 (average epoch stats below)
2022-08-17 10:03:21 | INFO | train | epoch 501 | loss 3.383 | nll_loss 0.348 | ppl 1.27 | wps 5957.7 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 40581 | lr 0.000313956 | gnorm 0.305 | train_wall 41 | gb_free 10.2 | wall 42697
2022-08-17 10:03:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:03:21 | INFO | fairseq.trainer | begin training epoch 502
2022-08-17 10:03:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:03:33 | INFO | train_inner | epoch 502:     19 / 81 loss=3.383, nll_loss=0.348, ppl=1.27, wps=4238.1, ups=0.77, wpb=5517.1, bsz=358.7, num_updates=40600, lr=0.000313882, gnorm=0.299, train_wall=52, gb_free=10.1, wall=42709
2022-08-17 10:04:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:04:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:04:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:04:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:04:14 | INFO | valid | epoch 502 | valid on 'valid' subset | loss 5.11 | nll_loss 2.49 | ppl 5.62 | bleu 56.65 | wps 1924.2 | wpb 933.5 | bsz 59.6 | num_updates 40662 | best_bleu 57.41
2022-08-17 10:04:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 502 @ 40662 updates
2022-08-17 10:04:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint502.pt
2022-08-17 10:04:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint502.pt
2022-08-17 10:04:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint502.pt (epoch 502 @ 40662 updates, score 56.65) (writing took 15.650955013930798 seconds)
2022-08-17 10:04:30 | INFO | fairseq_cli.train | end of epoch 502 (average epoch stats below)
2022-08-17 10:04:30 | INFO | train | epoch 502 | loss 3.383 | nll_loss 0.348 | ppl 1.27 | wps 6482.2 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 40662 | lr 0.000313643 | gnorm 0.281 | train_wall 42 | gb_free 10.2 | wall 42766
2022-08-17 10:04:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:04:30 | INFO | fairseq.trainer | begin training epoch 503
2022-08-17 10:04:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:04:52 | INFO | train_inner | epoch 503:     38 / 81 loss=3.383, nll_loss=0.348, ppl=1.27, wps=7017.3, ups=1.27, wpb=5543, bsz=356.6, num_updates=40700, lr=0.000313497, gnorm=0.283, train_wall=51, gb_free=10, wall=42788
2022-08-17 10:05:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:05:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:05:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:05:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:05:24 | INFO | valid | epoch 503 | valid on 'valid' subset | loss 5.112 | nll_loss 2.492 | ppl 5.63 | bleu 56.69 | wps 1739.8 | wpb 933.5 | bsz 59.6 | num_updates 40743 | best_bleu 57.41
2022-08-17 10:05:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 503 @ 40743 updates
2022-08-17 10:05:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint503.pt
2022-08-17 10:05:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint503.pt
2022-08-17 10:06:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint503.pt (epoch 503 @ 40743 updates, score 56.69) (writing took 36.571318838745356 seconds)
2022-08-17 10:06:00 | INFO | fairseq_cli.train | end of epoch 503 (average epoch stats below)
2022-08-17 10:06:00 | INFO | train | epoch 503 | loss 3.384 | nll_loss 0.35 | ppl 1.27 | wps 4934.7 | ups 0.89 | wpb 5523.2 | bsz 358 | num_updates 40743 | lr 0.000313331 | gnorm 0.318 | train_wall 40 | gb_free 10.1 | wall 42857
2022-08-17 10:06:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:06:01 | INFO | fairseq.trainer | begin training epoch 504
2022-08-17 10:06:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:06:38 | INFO | train_inner | epoch 504:     57 / 81 loss=3.384, nll_loss=0.349, ppl=1.27, wps=5210, ups=0.94, wpb=5539.5, bsz=364, num_updates=40800, lr=0.000313112, gnorm=0.323, train_wall=51, gb_free=10.1, wall=42894
2022-08-17 10:06:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:06:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:06:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:06:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:07:00 | INFO | valid | epoch 504 | valid on 'valid' subset | loss 5.111 | nll_loss 2.49 | ppl 5.62 | bleu 56.49 | wps 1963.7 | wpb 933.5 | bsz 59.6 | num_updates 40824 | best_bleu 57.41
2022-08-17 10:07:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 504 @ 40824 updates
2022-08-17 10:07:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint504.pt
2022-08-17 10:07:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint504.pt
2022-08-17 10:07:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint504.pt (epoch 504 @ 40824 updates, score 56.49) (writing took 20.671027697622776 seconds)
2022-08-17 10:07:21 | INFO | fairseq_cli.train | end of epoch 504 (average epoch stats below)
2022-08-17 10:07:21 | INFO | train | epoch 504 | loss 3.383 | nll_loss 0.348 | ppl 1.27 | wps 5560.7 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 40824 | lr 0.00031302 | gnorm 0.301 | train_wall 42 | gb_free 10.1 | wall 42937
2022-08-17 10:07:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:07:21 | INFO | fairseq.trainer | begin training epoch 505
2022-08-17 10:07:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:08:03 | INFO | train_inner | epoch 505:     76 / 81 loss=3.384, nll_loss=0.35, ppl=1.27, wps=6475.8, ups=1.17, wpb=5514.9, bsz=354.2, num_updates=40900, lr=0.000312729, gnorm=0.294, train_wall=52, gb_free=10.1, wall=42979
2022-08-17 10:08:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:08:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:08:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:08:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:08:15 | INFO | valid | epoch 505 | valid on 'valid' subset | loss 5.137 | nll_loss 2.522 | ppl 5.74 | bleu 56.4 | wps 1867 | wpb 933.5 | bsz 59.6 | num_updates 40905 | best_bleu 57.41
2022-08-17 10:08:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 505 @ 40905 updates
2022-08-17 10:08:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint505.pt
2022-08-17 10:08:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint505.pt
2022-08-17 10:08:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint505.pt (epoch 505 @ 40905 updates, score 56.4) (writing took 15.77167684957385 seconds)
2022-08-17 10:08:31 | INFO | fairseq_cli.train | end of epoch 505 (average epoch stats below)
2022-08-17 10:08:31 | INFO | train | epoch 505 | loss 3.384 | nll_loss 0.349 | ppl 1.27 | wps 6417.3 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 40905 | lr 0.00031271 | gnorm 0.289 | train_wall 42 | gb_free 10.1 | wall 43007
2022-08-17 10:08:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:08:31 | INFO | fairseq.trainer | begin training epoch 506
2022-08-17 10:08:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:09:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:09:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:09:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:09:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:09:22 | INFO | valid | epoch 506 | valid on 'valid' subset | loss 5.116 | nll_loss 2.492 | ppl 5.63 | bleu 56.8 | wps 1890.9 | wpb 933.5 | bsz 59.6 | num_updates 40986 | best_bleu 57.41
2022-08-17 10:09:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 506 @ 40986 updates
2022-08-17 10:09:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint506.pt
2022-08-17 10:09:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint506.pt
2022-08-17 10:09:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint506.pt (epoch 506 @ 40986 updates, score 56.8) (writing took 35.330462511628866 seconds)
2022-08-17 10:09:58 | INFO | fairseq_cli.train | end of epoch 506 (average epoch stats below)
2022-08-17 10:09:58 | INFO | train | epoch 506 | loss 3.384 | nll_loss 0.349 | ppl 1.27 | wps 5140.6 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 40986 | lr 0.000312401 | gnorm 0.36 | train_wall 40 | gb_free 10.1 | wall 43094
2022-08-17 10:09:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:09:58 | INFO | fairseq.trainer | begin training epoch 507
2022-08-17 10:09:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:10:06 | INFO | train_inner | epoch 507:     14 / 81 loss=3.384, nll_loss=0.349, ppl=1.27, wps=4471, ups=0.81, wpb=5499.8, bsz=354.2, num_updates=41000, lr=0.000312348, gnorm=0.346, train_wall=50, gb_free=10.1, wall=43102
2022-08-17 10:10:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:10:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:10:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:10:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:10:50 | INFO | valid | epoch 507 | valid on 'valid' subset | loss 5.128 | nll_loss 2.513 | ppl 5.71 | bleu 57.06 | wps 1888.2 | wpb 933.5 | bsz 59.6 | num_updates 41067 | best_bleu 57.41
2022-08-17 10:10:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 507 @ 41067 updates
2022-08-17 10:10:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint507.pt
2022-08-17 10:10:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint507.pt
2022-08-17 10:11:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint507.pt (epoch 507 @ 41067 updates, score 57.06) (writing took 25.21705761924386 seconds)
2022-08-17 10:11:16 | INFO | fairseq_cli.train | end of epoch 507 (average epoch stats below)
2022-08-17 10:11:16 | INFO | train | epoch 507 | loss 3.384 | nll_loss 0.35 | ppl 1.27 | wps 5734.3 | ups 1.04 | wpb 5523.2 | bsz 358 | num_updates 41067 | lr 0.000312093 | gnorm 0.329 | train_wall 42 | gb_free 10.1 | wall 43172
2022-08-17 10:11:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:11:16 | INFO | fairseq.trainer | begin training epoch 508
2022-08-17 10:11:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:11:34 | INFO | train_inner | epoch 508:     33 / 81 loss=3.384, nll_loss=0.349, ppl=1.27, wps=6325.8, ups=1.14, wpb=5548.2, bsz=358.2, num_updates=41100, lr=0.000311967, gnorm=0.325, train_wall=51, gb_free=10.1, wall=43190
2022-08-17 10:11:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:12:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:12:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:12:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:12:08 | INFO | valid | epoch 508 | valid on 'valid' subset | loss 5.116 | nll_loss 2.492 | ppl 5.62 | bleu 57.13 | wps 1917.2 | wpb 933.5 | bsz 59.6 | num_updates 41148 | best_bleu 57.41
2022-08-17 10:12:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 508 @ 41148 updates
2022-08-17 10:12:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint508.pt
2022-08-17 10:12:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint508.pt
2022-08-17 10:12:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint508.pt (epoch 508 @ 41148 updates, score 57.13) (writing took 14.737668838351965 seconds)
2022-08-17 10:12:23 | INFO | fairseq_cli.train | end of epoch 508 (average epoch stats below)
2022-08-17 10:12:23 | INFO | train | epoch 508 | loss 3.383 | nll_loss 0.348 | ppl 1.27 | wps 6686.9 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 41148 | lr 0.000311785 | gnorm 0.304 | train_wall 40 | gb_free 10.1 | wall 43239
2022-08-17 10:12:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:12:23 | INFO | fairseq.trainer | begin training epoch 509
2022-08-17 10:12:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:12:53 | INFO | train_inner | epoch 509:     52 / 81 loss=3.383, nll_loss=0.348, ppl=1.27, wps=6959.1, ups=1.27, wpb=5496.1, bsz=357.1, num_updates=41200, lr=0.000311588, gnorm=0.289, train_wall=51, gb_free=10.1, wall=43269
2022-08-17 10:13:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:13:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:13:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:13:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:13:18 | INFO | valid | epoch 509 | valid on 'valid' subset | loss 5.121 | nll_loss 2.502 | ppl 5.67 | bleu 57.24 | wps 1990.6 | wpb 933.5 | bsz 59.6 | num_updates 41229 | best_bleu 57.41
2022-08-17 10:13:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 509 @ 41229 updates
2022-08-17 10:13:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint509.pt
2022-08-17 10:13:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint509.pt
2022-08-17 10:13:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint509.pt (epoch 509 @ 41229 updates, score 57.24) (writing took 28.369810722768307 seconds)
2022-08-17 10:13:47 | INFO | fairseq_cli.train | end of epoch 509 (average epoch stats below)
2022-08-17 10:13:47 | INFO | train | epoch 509 | loss 3.383 | nll_loss 0.347 | ppl 1.27 | wps 5326.6 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 41229 | lr 0.000311479 | gnorm 0.273 | train_wall 40 | gb_free 10.1 | wall 43323
2022-08-17 10:13:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:13:47 | INFO | fairseq.trainer | begin training epoch 510
2022-08-17 10:13:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:14:26 | INFO | train_inner | epoch 510:     71 / 81 loss=3.383, nll_loss=0.349, ppl=1.27, wps=5929.4, ups=1.07, wpb=5541.6, bsz=363.1, num_updates=41300, lr=0.000311211, gnorm=0.372, train_wall=51, gb_free=10.1, wall=43362
2022-08-17 10:14:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:14:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:14:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:14:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:14:41 | INFO | valid | epoch 510 | valid on 'valid' subset | loss 5.114 | nll_loss 2.493 | ppl 5.63 | bleu 57.12 | wps 1648.9 | wpb 933.5 | bsz 59.6 | num_updates 41310 | best_bleu 57.41
2022-08-17 10:14:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 510 @ 41310 updates
2022-08-17 10:14:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint510.pt
2022-08-17 10:14:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint510.pt
2022-08-17 10:15:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint510.pt (epoch 510 @ 41310 updates, score 57.12) (writing took 41.314853988587856 seconds)
2022-08-17 10:15:23 | INFO | fairseq_cli.train | end of epoch 510 (average epoch stats below)
2022-08-17 10:15:23 | INFO | train | epoch 510 | loss 3.383 | nll_loss 0.349 | ppl 1.27 | wps 4648 | ups 0.84 | wpb 5523.2 | bsz 358 | num_updates 41310 | lr 0.000311173 | gnorm 0.406 | train_wall 41 | gb_free 10.1 | wall 43419
2022-08-17 10:15:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:15:23 | INFO | fairseq.trainer | begin training epoch 511
2022-08-17 10:15:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:16:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:16:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:16:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:16:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:16:17 | INFO | valid | epoch 511 | valid on 'valid' subset | loss 5.105 | nll_loss 2.485 | ppl 5.6 | bleu 57.19 | wps 1866 | wpb 933.5 | bsz 59.6 | num_updates 41391 | best_bleu 57.41
2022-08-17 10:16:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 511 @ 41391 updates
2022-08-17 10:16:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint511.pt
2022-08-17 10:16:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint511.pt
2022-08-17 10:16:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint511.pt (epoch 511 @ 41391 updates, score 57.19) (writing took 18.71723609045148 seconds)
2022-08-17 10:16:36 | INFO | fairseq_cli.train | end of epoch 511 (average epoch stats below)
2022-08-17 10:16:36 | INFO | train | epoch 511 | loss 3.384 | nll_loss 0.349 | ppl 1.27 | wps 6133.8 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 41391 | lr 0.000310869 | gnorm 0.333 | train_wall 43 | gb_free 10.3 | wall 43492
2022-08-17 10:16:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:16:36 | INFO | fairseq.trainer | begin training epoch 512
2022-08-17 10:16:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:16:42 | INFO | train_inner | epoch 512:      9 / 81 loss=3.384, nll_loss=0.349, ppl=1.27, wps=4061.8, ups=0.74, wpb=5513.2, bsz=357.7, num_updates=41400, lr=0.000310835, gnorm=0.336, train_wall=52, gb_free=10.1, wall=43498
2022-08-17 10:17:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:17:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:17:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:17:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:17:27 | INFO | valid | epoch 512 | valid on 'valid' subset | loss 5.115 | nll_loss 2.496 | ppl 5.64 | bleu 56.94 | wps 2015.7 | wpb 933.5 | bsz 59.6 | num_updates 41472 | best_bleu 57.41
2022-08-17 10:17:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 512 @ 41472 updates
2022-08-17 10:17:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint512.pt
2022-08-17 10:17:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint512.pt
2022-08-17 10:17:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint512.pt (epoch 512 @ 41472 updates, score 56.94) (writing took 24.64113125577569 seconds)
2022-08-17 10:17:52 | INFO | fairseq_cli.train | end of epoch 512 (average epoch stats below)
2022-08-17 10:17:52 | INFO | train | epoch 512 | loss 3.383 | nll_loss 0.349 | ppl 1.27 | wps 5855.9 | ups 1.06 | wpb 5523.2 | bsz 358 | num_updates 41472 | lr 0.000310565 | gnorm 0.308 | train_wall 41 | gb_free 10.1 | wall 43568
2022-08-17 10:17:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:17:52 | INFO | fairseq.trainer | begin training epoch 513
2022-08-17 10:17:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:18:08 | INFO | train_inner | epoch 513:     28 / 81 loss=3.383, nll_loss=0.348, ppl=1.27, wps=6435.4, ups=1.16, wpb=5527, bsz=357.2, num_updates=41500, lr=0.00031046, gnorm=0.302, train_wall=50, gb_free=10.1, wall=43584
2022-08-17 10:18:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:18:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:18:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:18:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:18:43 | INFO | valid | epoch 513 | valid on 'valid' subset | loss 5.116 | nll_loss 2.495 | ppl 5.64 | bleu 56.75 | wps 2025.3 | wpb 933.5 | bsz 59.6 | num_updates 41553 | best_bleu 57.41
2022-08-17 10:18:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 513 @ 41553 updates
2022-08-17 10:18:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint513.pt
2022-08-17 10:18:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint513.pt
2022-08-17 10:19:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint513.pt (epoch 513 @ 41553 updates, score 56.75) (writing took 37.72286620736122 seconds)
2022-08-17 10:19:20 | INFO | fairseq_cli.train | end of epoch 513 (average epoch stats below)
2022-08-17 10:19:20 | INFO | train | epoch 513 | loss 3.383 | nll_loss 0.348 | ppl 1.27 | wps 5069 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 41553 | lr 0.000310262 | gnorm 0.319 | train_wall 39 | gb_free 10.1 | wall 43657
2022-08-17 10:19:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:19:21 | INFO | fairseq.trainer | begin training epoch 514
2022-08-17 10:19:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:19:46 | INFO | train_inner | epoch 514:     47 / 81 loss=3.383, nll_loss=0.348, ppl=1.27, wps=5637.2, ups=1.02, wpb=5533.1, bsz=360.3, num_updates=41600, lr=0.000310087, gnorm=0.314, train_wall=48, gb_free=10.1, wall=43682
2022-08-17 10:20:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:20:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:20:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:20:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:20:12 | INFO | valid | epoch 514 | valid on 'valid' subset | loss 5.115 | nll_loss 2.493 | ppl 5.63 | bleu 56.93 | wps 1829.9 | wpb 933.5 | bsz 59.6 | num_updates 41634 | best_bleu 57.41
2022-08-17 10:20:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 514 @ 41634 updates
2022-08-17 10:20:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint514.pt
2022-08-17 10:20:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint514.pt
2022-08-17 10:20:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint514.pt (epoch 514 @ 41634 updates, score 56.93) (writing took 19.989579029381275 seconds)
2022-08-17 10:20:32 | INFO | fairseq_cli.train | end of epoch 514 (average epoch stats below)
2022-08-17 10:20:32 | INFO | train | epoch 514 | loss 3.383 | nll_loss 0.348 | ppl 1.27 | wps 6218.7 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 41634 | lr 0.00030996 | gnorm 0.305 | train_wall 38 | gb_free 10.2 | wall 43729
2022-08-17 10:20:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:20:33 | INFO | fairseq.trainer | begin training epoch 515
2022-08-17 10:20:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:21:08 | INFO | train_inner | epoch 515:     66 / 81 loss=3.383, nll_loss=0.349, ppl=1.27, wps=6713.7, ups=1.22, wpb=5522.1, bsz=355.8, num_updates=41700, lr=0.000309715, gnorm=0.335, train_wall=50, gb_free=10.1, wall=43764
2022-08-17 10:21:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:21:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:21:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:21:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:21:25 | INFO | valid | epoch 515 | valid on 'valid' subset | loss 5.109 | nll_loss 2.487 | ppl 5.61 | bleu 56.25 | wps 2029.9 | wpb 933.5 | bsz 59.6 | num_updates 41715 | best_bleu 57.41
2022-08-17 10:21:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 515 @ 41715 updates
2022-08-17 10:21:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint515.pt
2022-08-17 10:21:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint515.pt
2022-08-17 10:21:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint515.pt (epoch 515 @ 41715 updates, score 56.25) (writing took 22.288859825581312 seconds)
2022-08-17 10:21:47 | INFO | fairseq_cli.train | end of epoch 515 (average epoch stats below)
2022-08-17 10:21:47 | INFO | train | epoch 515 | loss 3.383 | nll_loss 0.348 | ppl 1.27 | wps 5972.3 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 41715 | lr 0.000309659 | gnorm 0.335 | train_wall 42 | gb_free 10.2 | wall 43803
2022-08-17 10:21:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:21:47 | INFO | fairseq.trainer | begin training epoch 516
2022-08-17 10:21:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:22:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:22:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:22:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:22:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:22:40 | INFO | valid | epoch 516 | valid on 'valid' subset | loss 5.124 | nll_loss 2.511 | ppl 5.7 | bleu 56.5 | wps 1797.3 | wpb 933.5 | bsz 59.6 | num_updates 41796 | best_bleu 57.41
2022-08-17 10:22:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 516 @ 41796 updates
2022-08-17 10:22:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint516.pt
2022-08-17 10:22:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint516.pt
2022-08-17 10:23:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint516.pt (epoch 516 @ 41796 updates, score 56.5) (writing took 33.700679790228605 seconds)
2022-08-17 10:23:14 | INFO | fairseq_cli.train | end of epoch 516 (average epoch stats below)
2022-08-17 10:23:14 | INFO | train | epoch 516 | loss 3.383 | nll_loss 0.348 | ppl 1.27 | wps 5153.7 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 41796 | lr 0.000309359 | gnorm 0.3 | train_wall 41 | gb_free 10.2 | wall 43890
2022-08-17 10:23:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:23:14 | INFO | fairseq.trainer | begin training epoch 517
2022-08-17 10:23:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:23:17 | INFO | train_inner | epoch 517:      4 / 81 loss=3.383, nll_loss=0.348, ppl=1.27, wps=4266.2, ups=0.78, wpb=5500.1, bsz=355.5, num_updates=41800, lr=0.000309344, gnorm=0.296, train_wall=51, gb_free=10.1, wall=43893
2022-08-17 10:23:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:23:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:23:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:23:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:24:06 | INFO | valid | epoch 517 | valid on 'valid' subset | loss 5.108 | nll_loss 2.489 | ppl 5.61 | bleu 56.61 | wps 2031.8 | wpb 933.5 | bsz 59.6 | num_updates 41877 | best_bleu 57.41
2022-08-17 10:24:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 517 @ 41877 updates
2022-08-17 10:24:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint517.pt
2022-08-17 10:24:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint517.pt
2022-08-17 10:24:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint517.pt (epoch 517 @ 41877 updates, score 56.61) (writing took 23.4954992569983 seconds)
2022-08-17 10:24:30 | INFO | fairseq_cli.train | end of epoch 517 (average epoch stats below)
2022-08-17 10:24:30 | INFO | train | epoch 517 | loss 3.383 | nll_loss 0.348 | ppl 1.27 | wps 5904.8 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 41877 | lr 0.00030906 | gnorm 0.314 | train_wall 41 | gb_free 10.1 | wall 43966
2022-08-17 10:24:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:24:30 | INFO | fairseq.trainer | begin training epoch 518
2022-08-17 10:24:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:24:44 | INFO | train_inner | epoch 518:     23 / 81 loss=3.383, nll_loss=0.348, ppl=1.27, wps=6394, ups=1.15, wpb=5555.3, bsz=363, num_updates=41900, lr=0.000308975, gnorm=0.354, train_wall=51, gb_free=10, wall=43980
2022-08-17 10:25:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:25:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:25:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:25:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:25:23 | INFO | valid | epoch 518 | valid on 'valid' subset | loss 5.13 | nll_loss 2.514 | ppl 5.71 | bleu 56.63 | wps 1970.1 | wpb 933.5 | bsz 59.6 | num_updates 41958 | best_bleu 57.41
2022-08-17 10:25:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 518 @ 41958 updates
2022-08-17 10:25:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint518.pt
2022-08-17 10:25:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint518.pt
2022-08-17 10:25:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint518.pt (epoch 518 @ 41958 updates, score 56.63) (writing took 17.224569655954838 seconds)
2022-08-17 10:25:40 | INFO | fairseq_cli.train | end of epoch 518 (average epoch stats below)
2022-08-17 10:25:40 | INFO | train | epoch 518 | loss 3.383 | nll_loss 0.349 | ppl 1.27 | wps 6339.6 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 41958 | lr 0.000308761 | gnorm 0.354 | train_wall 41 | gb_free 10 | wall 44037
2022-08-17 10:25:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:25:41 | INFO | fairseq.trainer | begin training epoch 519
2022-08-17 10:25:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:26:03 | INFO | train_inner | epoch 519:     42 / 81 loss=3.384, nll_loss=0.35, ppl=1.27, wps=6969.4, ups=1.26, wpb=5523.2, bsz=355.7, num_updates=42000, lr=0.000308607, gnorm=0.37, train_wall=51, gb_free=10, wall=44060
2022-08-17 10:26:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:26:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:26:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:26:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:26:32 | INFO | valid | epoch 519 | valid on 'valid' subset | loss 5.114 | nll_loss 2.495 | ppl 5.64 | bleu 56.61 | wps 1845.2 | wpb 933.5 | bsz 59.6 | num_updates 42039 | best_bleu 57.41
2022-08-17 10:26:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 519 @ 42039 updates
2022-08-17 10:26:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint519.pt
2022-08-17 10:26:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint519.pt
2022-08-17 10:27:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint519.pt (epoch 519 @ 42039 updates, score 56.61) (writing took 35.78250430896878 seconds)
2022-08-17 10:27:08 | INFO | fairseq_cli.train | end of epoch 519 (average epoch stats below)
2022-08-17 10:27:08 | INFO | train | epoch 519 | loss 3.384 | nll_loss 0.35 | ppl 1.27 | wps 5135.4 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 42039 | lr 0.000308464 | gnorm 0.395 | train_wall 40 | gb_free 10.1 | wall 44124
2022-08-17 10:27:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:27:08 | INFO | fairseq.trainer | begin training epoch 520
2022-08-17 10:27:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:27:40 | INFO | train_inner | epoch 520:     61 / 81 loss=3.383, nll_loss=0.349, ppl=1.27, wps=5686.3, ups=1.03, wpb=5505.5, bsz=359.6, num_updates=42100, lr=0.00030824, gnorm=0.372, train_wall=50, gb_free=10.1, wall=44156
2022-08-17 10:27:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:27:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:27:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:27:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:27:59 | INFO | valid | epoch 520 | valid on 'valid' subset | loss 5.13 | nll_loss 2.515 | ppl 5.72 | bleu 56.25 | wps 1948.3 | wpb 933.5 | bsz 59.6 | num_updates 42120 | best_bleu 57.41
2022-08-17 10:27:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 520 @ 42120 updates
2022-08-17 10:27:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint520.pt
2022-08-17 10:28:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint520.pt
2022-08-17 10:28:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint520.pt (epoch 520 @ 42120 updates, score 56.25) (writing took 20.2555680423975 seconds)
2022-08-17 10:28:20 | INFO | fairseq_cli.train | end of epoch 520 (average epoch stats below)
2022-08-17 10:28:20 | INFO | train | epoch 520 | loss 3.383 | nll_loss 0.348 | ppl 1.27 | wps 6205.2 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 42120 | lr 0.000308167 | gnorm 0.382 | train_wall 41 | gb_free 10.2 | wall 44196
2022-08-17 10:28:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:28:20 | INFO | fairseq.trainer | begin training epoch 521
2022-08-17 10:28:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:29:08 | INFO | train_inner | epoch 521:     80 / 81 loss=3.383, nll_loss=0.348, ppl=1.27, wps=6340.9, ups=1.14, wpb=5544.9, bsz=357.7, num_updates=42200, lr=0.000307875, gnorm=0.315, train_wall=52, gb_free=10.1, wall=44244
2022-08-17 10:29:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:29:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:29:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:29:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:29:17 | INFO | valid | epoch 521 | valid on 'valid' subset | loss 5.115 | nll_loss 2.497 | ppl 5.65 | bleu 57.05 | wps 1971.6 | wpb 933.5 | bsz 59.6 | num_updates 42201 | best_bleu 57.41
2022-08-17 10:29:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 521 @ 42201 updates
2022-08-17 10:29:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint521.pt
2022-08-17 10:29:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint521.pt
2022-08-17 10:29:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint521.pt (epoch 521 @ 42201 updates, score 57.05) (writing took 14.717946290969849 seconds)
2022-08-17 10:29:32 | INFO | fairseq_cli.train | end of epoch 521 (average epoch stats below)
2022-08-17 10:29:32 | INFO | train | epoch 521 | loss 3.383 | nll_loss 0.348 | ppl 1.27 | wps 6222.3 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 42201 | lr 0.000307871 | gnorm 0.312 | train_wall 42 | gb_free 10.2 | wall 44268
2022-08-17 10:29:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:29:32 | INFO | fairseq.trainer | begin training epoch 522
2022-08-17 10:29:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:30:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:30:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:30:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:30:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:30:23 | INFO | valid | epoch 522 | valid on 'valid' subset | loss 5.114 | nll_loss 2.496 | ppl 5.64 | bleu 56.52 | wps 1911.7 | wpb 933.5 | bsz 59.6 | num_updates 42282 | best_bleu 57.41
2022-08-17 10:30:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 522 @ 42282 updates
2022-08-17 10:30:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint522.pt
2022-08-17 10:30:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint522.pt
2022-08-17 10:31:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint522.pt (epoch 522 @ 42282 updates, score 56.52) (writing took 37.57858018577099 seconds)
2022-08-17 10:31:01 | INFO | fairseq_cli.train | end of epoch 522 (average epoch stats below)
2022-08-17 10:31:01 | INFO | train | epoch 522 | loss 3.383 | nll_loss 0.348 | ppl 1.27 | wps 4988.3 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 42282 | lr 0.000307576 | gnorm 0.311 | train_wall 41 | gb_free 10.1 | wall 44357
2022-08-17 10:31:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:31:01 | INFO | fairseq.trainer | begin training epoch 523
2022-08-17 10:31:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:31:12 | INFO | train_inner | epoch 523:     18 / 81 loss=3.383, nll_loss=0.348, ppl=1.27, wps=4432.2, ups=0.81, wpb=5498.9, bsz=354.6, num_updates=42300, lr=0.00030751, gnorm=0.331, train_wall=50, gb_free=10, wall=44368
2022-08-17 10:31:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:31:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:31:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:31:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:31:53 | INFO | valid | epoch 523 | valid on 'valid' subset | loss 5.125 | nll_loss 2.508 | ppl 5.69 | bleu 56.7 | wps 1943.7 | wpb 933.5 | bsz 59.6 | num_updates 42363 | best_bleu 57.41
2022-08-17 10:31:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 523 @ 42363 updates
2022-08-17 10:31:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint523.pt
2022-08-17 10:31:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint523.pt
2022-08-17 10:32:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint523.pt (epoch 523 @ 42363 updates, score 56.7) (writing took 20.70779364928603 seconds)
2022-08-17 10:32:14 | INFO | fairseq_cli.train | end of epoch 523 (average epoch stats below)
2022-08-17 10:32:14 | INFO | train | epoch 523 | loss 3.384 | nll_loss 0.35 | ppl 1.27 | wps 6149.3 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 42363 | lr 0.000307282 | gnorm 0.393 | train_wall 41 | gb_free 10.1 | wall 44430
2022-08-17 10:32:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:32:14 | INFO | fairseq.trainer | begin training epoch 524
2022-08-17 10:32:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:32:35 | INFO | train_inner | epoch 524:     37 / 81 loss=3.383, nll_loss=0.348, ppl=1.27, wps=6676.6, ups=1.21, wpb=5533.1, bsz=361.2, num_updates=42400, lr=0.000307148, gnorm=0.343, train_wall=51, gb_free=10, wall=44451
2022-08-17 10:32:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:32:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:32:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:32:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:33:06 | INFO | valid | epoch 524 | valid on 'valid' subset | loss 5.115 | nll_loss 2.495 | ppl 5.64 | bleu 56.92 | wps 1853.9 | wpb 933.5 | bsz 59.6 | num_updates 42444 | best_bleu 57.41
2022-08-17 10:33:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 524 @ 42444 updates
2022-08-17 10:33:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint524.pt
2022-08-17 10:33:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint524.pt
2022-08-17 10:33:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint524.pt (epoch 524 @ 42444 updates, score 56.92) (writing took 15.284759305417538 seconds)
2022-08-17 10:33:22 | INFO | fairseq_cli.train | end of epoch 524 (average epoch stats below)
2022-08-17 10:33:22 | INFO | train | epoch 524 | loss 3.383 | nll_loss 0.348 | ppl 1.27 | wps 6587 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 42444 | lr 0.000306988 | gnorm 0.321 | train_wall 41 | gb_free 10 | wall 44498
2022-08-17 10:33:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:33:22 | INFO | fairseq.trainer | begin training epoch 525
2022-08-17 10:33:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:33:53 | INFO | train_inner | epoch 525:     56 / 81 loss=3.383, nll_loss=0.349, ppl=1.27, wps=7048.1, ups=1.28, wpb=5498.5, bsz=354.5, num_updates=42500, lr=0.000306786, gnorm=0.619, train_wall=50, gb_free=10.1, wall=44529
2022-08-17 10:34:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:34:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:34:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:34:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:34:14 | INFO | valid | epoch 525 | valid on 'valid' subset | loss 5.111 | nll_loss 2.487 | ppl 5.61 | bleu 57.42 | wps 1852.4 | wpb 933.5 | bsz 59.6 | num_updates 42525 | best_bleu 57.42
2022-08-17 10:34:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 525 @ 42525 updates
2022-08-17 10:34:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint525.pt
2022-08-17 10:34:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint525.pt
2022-08-17 10:35:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint525.pt (epoch 525 @ 42525 updates, score 57.42) (writing took 67.4105636626482 seconds)
2022-08-17 10:35:22 | INFO | fairseq_cli.train | end of epoch 525 (average epoch stats below)
2022-08-17 10:35:22 | INFO | train | epoch 525 | loss 3.383 | nll_loss 0.348 | ppl 1.27 | wps 3737.6 | ups 0.68 | wpb 5523.2 | bsz 358 | num_updates 42525 | lr 0.000306696 | gnorm 0.649 | train_wall 40 | gb_free 10.1 | wall 44618
2022-08-17 10:35:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:35:22 | INFO | fairseq.trainer | begin training epoch 526
2022-08-17 10:35:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:36:05 | INFO | train_inner | epoch 526:     75 / 81 loss=3.382, nll_loss=0.348, ppl=1.27, wps=4175.7, ups=0.75, wpb=5548.8, bsz=362.7, num_updates=42600, lr=0.000306426, gnorm=0.33, train_wall=49, gb_free=10.1, wall=44662
2022-08-17 10:36:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:36:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:36:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:36:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:36:18 | INFO | valid | epoch 526 | valid on 'valid' subset | loss 5.114 | nll_loss 2.5 | ppl 5.66 | bleu 56.32 | wps 1802.3 | wpb 933.5 | bsz 59.6 | num_updates 42606 | best_bleu 57.42
2022-08-17 10:36:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 526 @ 42606 updates
2022-08-17 10:36:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint526.pt
2022-08-17 10:36:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint526.pt
2022-08-17 10:36:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint526.pt (epoch 526 @ 42606 updates, score 56.32) (writing took 32.815737538039684 seconds)
2022-08-17 10:36:51 | INFO | fairseq_cli.train | end of epoch 526 (average epoch stats below)
2022-08-17 10:36:51 | INFO | train | epoch 526 | loss 3.382 | nll_loss 0.348 | ppl 1.27 | wps 5012.3 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 42606 | lr 0.000306404 | gnorm 0.349 | train_wall 39 | gb_free 10.3 | wall 44707
2022-08-17 10:36:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:36:51 | INFO | fairseq.trainer | begin training epoch 527
2022-08-17 10:36:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:37:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:37:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:37:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:37:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:37:42 | INFO | valid | epoch 527 | valid on 'valid' subset | loss 5.11 | nll_loss 2.491 | ppl 5.62 | bleu 57.04 | wps 1873.5 | wpb 933.5 | bsz 59.6 | num_updates 42687 | best_bleu 57.42
2022-08-17 10:37:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 527 @ 42687 updates
2022-08-17 10:37:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint527.pt
2022-08-17 10:37:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint527.pt
2022-08-17 10:37:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint527.pt (epoch 527 @ 42687 updates, score 57.04) (writing took 2.525274060666561 seconds)
2022-08-17 10:37:45 | INFO | fairseq_cli.train | end of epoch 527 (average epoch stats below)
2022-08-17 10:37:45 | INFO | train | epoch 527 | loss 3.383 | nll_loss 0.348 | ppl 1.27 | wps 8280.7 | ups 1.5 | wpb 5523.2 | bsz 358 | num_updates 42687 | lr 0.000306113 | gnorm 0.343 | train_wall 40 | gb_free 10.1 | wall 44761
2022-08-17 10:37:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:37:45 | INFO | fairseq.trainer | begin training epoch 528
2022-08-17 10:37:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:37:53 | INFO | train_inner | epoch 528:     13 / 81 loss=3.383, nll_loss=0.348, ppl=1.27, wps=5139.7, ups=0.93, wpb=5505, bsz=353, num_updates=42700, lr=0.000306067, gnorm=0.336, train_wall=49, gb_free=10.1, wall=44769
2022-08-17 10:38:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:38:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:38:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:38:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:38:35 | INFO | valid | epoch 528 | valid on 'valid' subset | loss 5.104 | nll_loss 2.483 | ppl 5.59 | bleu 57.13 | wps 1891.2 | wpb 933.5 | bsz 59.6 | num_updates 42768 | best_bleu 57.42
2022-08-17 10:38:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 528 @ 42768 updates
2022-08-17 10:38:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint528.pt
2022-08-17 10:38:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint528.pt
2022-08-17 10:38:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint528.pt (epoch 528 @ 42768 updates, score 57.13) (writing took 16.00538118556142 seconds)
2022-08-17 10:38:51 | INFO | fairseq_cli.train | end of epoch 528 (average epoch stats below)
2022-08-17 10:38:51 | INFO | train | epoch 528 | loss 3.383 | nll_loss 0.348 | ppl 1.27 | wps 6750.7 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 42768 | lr 0.000305823 | gnorm 0.304 | train_wall 39 | gb_free 10.1 | wall 44827
2022-08-17 10:38:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:38:51 | INFO | fairseq.trainer | begin training epoch 529
2022-08-17 10:38:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:39:09 | INFO | train_inner | epoch 529:     32 / 81 loss=3.382, nll_loss=0.347, ppl=1.27, wps=7228.7, ups=1.31, wpb=5520.4, bsz=362.2, num_updates=42800, lr=0.000305709, gnorm=0.31, train_wall=48, gb_free=10, wall=44845
2022-08-17 10:39:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:39:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:39:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:39:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:39:43 | INFO | valid | epoch 529 | valid on 'valid' subset | loss 5.104 | nll_loss 2.484 | ppl 5.59 | bleu 56.6 | wps 1798 | wpb 933.5 | bsz 59.6 | num_updates 42849 | best_bleu 57.42
2022-08-17 10:39:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 529 @ 42849 updates
2022-08-17 10:39:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint529.pt
2022-08-17 10:39:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint529.pt
2022-08-17 10:40:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint529.pt (epoch 529 @ 42849 updates, score 56.6) (writing took 17.5873081125319 seconds)
2022-08-17 10:40:01 | INFO | fairseq_cli.train | end of epoch 529 (average epoch stats below)
2022-08-17 10:40:01 | INFO | train | epoch 529 | loss 3.382 | nll_loss 0.348 | ppl 1.27 | wps 6437.1 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 42849 | lr 0.000305534 | gnorm 0.306 | train_wall 39 | gb_free 10.4 | wall 44897
2022-08-17 10:40:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:40:01 | INFO | fairseq.trainer | begin training epoch 530
2022-08-17 10:40:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:40:29 | INFO | train_inner | epoch 530:     51 / 81 loss=3.382, nll_loss=0.348, ppl=1.27, wps=6900.4, ups=1.24, wpb=5543.1, bsz=359, num_updates=42900, lr=0.000305352, gnorm=0.296, train_wall=50, gb_free=10.1, wall=44925
2022-08-17 10:40:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:40:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:40:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:40:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:40:54 | INFO | valid | epoch 530 | valid on 'valid' subset | loss 5.115 | nll_loss 2.498 | ppl 5.65 | bleu 56.95 | wps 1702 | wpb 933.5 | bsz 59.6 | num_updates 42930 | best_bleu 57.42
2022-08-17 10:40:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 530 @ 42930 updates
2022-08-17 10:40:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint530.pt
2022-08-17 10:40:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint530.pt
2022-08-17 10:41:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint530.pt (epoch 530 @ 42930 updates, score 56.95) (writing took 19.036938667297363 seconds)
2022-08-17 10:41:13 | INFO | fairseq_cli.train | end of epoch 530 (average epoch stats below)
2022-08-17 10:41:13 | INFO | train | epoch 530 | loss 3.383 | nll_loss 0.348 | ppl 1.27 | wps 6167.2 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 42930 | lr 0.000305246 | gnorm 0.293 | train_wall 40 | gb_free 10.2 | wall 44969
2022-08-17 10:41:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:41:13 | INFO | fairseq.trainer | begin training epoch 531
2022-08-17 10:41:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:41:50 | INFO | train_inner | epoch 531:     70 / 81 loss=3.383, nll_loss=0.349, ppl=1.27, wps=6850.7, ups=1.24, wpb=5533.6, bsz=358, num_updates=43000, lr=0.000304997, gnorm=0.306, train_wall=50, gb_free=10.1, wall=45006
2022-08-17 10:41:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:41:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:41:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:41:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:42:05 | INFO | valid | epoch 531 | valid on 'valid' subset | loss 5.11 | nll_loss 2.49 | ppl 5.62 | bleu 56.76 | wps 1769.9 | wpb 933.5 | bsz 59.6 | num_updates 43011 | best_bleu 57.42
2022-08-17 10:42:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 531 @ 43011 updates
2022-08-17 10:42:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint531.pt
2022-08-17 10:42:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint531.pt
2022-08-17 10:42:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint531.pt (epoch 531 @ 43011 updates, score 56.76) (writing took 39.828032840043306 seconds)
2022-08-17 10:42:45 | INFO | fairseq_cli.train | end of epoch 531 (average epoch stats below)
2022-08-17 10:42:45 | INFO | train | epoch 531 | loss 3.383 | nll_loss 0.349 | ppl 1.27 | wps 4872.1 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 43011 | lr 0.000304958 | gnorm 0.338 | train_wall 40 | gb_free 10.1 | wall 45061
2022-08-17 10:42:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:42:45 | INFO | fairseq.trainer | begin training epoch 532
2022-08-17 10:42:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:43:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:43:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:43:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:43:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:43:36 | INFO | valid | epoch 532 | valid on 'valid' subset | loss 5.121 | nll_loss 2.509 | ppl 5.69 | bleu 57.31 | wps 1837.1 | wpb 933.5 | bsz 59.6 | num_updates 43092 | best_bleu 57.42
2022-08-17 10:43:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 532 @ 43092 updates
2022-08-17 10:43:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint532.pt
2022-08-17 10:43:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint532.pt
2022-08-17 10:43:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint532.pt (epoch 532 @ 43092 updates, score 57.31) (writing took 2.4740351885557175 seconds)
2022-08-17 10:43:39 | INFO | fairseq_cli.train | end of epoch 532 (average epoch stats below)
2022-08-17 10:43:39 | INFO | train | epoch 532 | loss 3.382 | nll_loss 0.348 | ppl 1.27 | wps 8328.3 | ups 1.51 | wpb 5523.2 | bsz 358 | num_updates 43092 | lr 0.000304671 | gnorm 0.322 | train_wall 40 | gb_free 10.3 | wall 45115
2022-08-17 10:43:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:43:39 | INFO | fairseq.trainer | begin training epoch 533
2022-08-17 10:43:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:43:44 | INFO | train_inner | epoch 533:      8 / 81 loss=3.383, nll_loss=0.348, ppl=1.27, wps=4821.7, ups=0.88, wpb=5498.4, bsz=355.9, num_updates=43100, lr=0.000304643, gnorm=0.339, train_wall=49, gb_free=10.1, wall=45120
2022-08-17 10:44:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:44:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:44:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:44:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:44:30 | INFO | valid | epoch 533 | valid on 'valid' subset | loss 5.122 | nll_loss 2.507 | ppl 5.68 | bleu 57.04 | wps 1864 | wpb 933.5 | bsz 59.6 | num_updates 43173 | best_bleu 57.42
2022-08-17 10:44:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 533 @ 43173 updates
2022-08-17 10:44:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint533.pt
2022-08-17 10:44:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint533.pt
2022-08-17 10:44:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint533.pt (epoch 533 @ 43173 updates, score 57.04) (writing took 19.606986321508884 seconds)
2022-08-17 10:44:49 | INFO | fairseq_cli.train | end of epoch 533 (average epoch stats below)
2022-08-17 10:44:49 | INFO | train | epoch 533 | loss 3.382 | nll_loss 0.348 | ppl 1.27 | wps 6337.2 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 43173 | lr 0.000304385 | gnorm 0.297 | train_wall 40 | gb_free 10.1 | wall 45186
2022-08-17 10:44:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:44:50 | INFO | fairseq.trainer | begin training epoch 534
2022-08-17 10:44:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:45:04 | INFO | train_inner | epoch 534:     27 / 81 loss=3.382, nll_loss=0.347, ppl=1.27, wps=6920, ups=1.25, wpb=5532.8, bsz=359.9, num_updates=43200, lr=0.00030429, gnorm=0.295, train_wall=49, gb_free=10.1, wall=45200
2022-08-17 10:45:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:45:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:45:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:45:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:45:40 | INFO | valid | epoch 534 | valid on 'valid' subset | loss 5.128 | nll_loss 2.512 | ppl 5.7 | bleu 56.73 | wps 1885.5 | wpb 933.5 | bsz 59.6 | num_updates 43254 | best_bleu 57.42
2022-08-17 10:45:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 534 @ 43254 updates
2022-08-17 10:45:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint534.pt
2022-08-17 10:45:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint534.pt
2022-08-17 10:46:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint534.pt (epoch 534 @ 43254 updates, score 56.73) (writing took 23.89255939796567 seconds)
2022-08-17 10:46:04 | INFO | fairseq_cli.train | end of epoch 534 (average epoch stats below)
2022-08-17 10:46:04 | INFO | train | epoch 534 | loss 3.382 | nll_loss 0.348 | ppl 1.27 | wps 5972.9 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 43254 | lr 0.0003041 | gnorm 0.293 | train_wall 40 | gb_free 10.2 | wall 45261
2022-08-17 10:46:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:46:05 | INFO | fairseq.trainer | begin training epoch 535
2022-08-17 10:46:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:46:29 | INFO | train_inner | epoch 535:     46 / 81 loss=3.382, nll_loss=0.348, ppl=1.27, wps=6472.7, ups=1.17, wpb=5525.4, bsz=359.4, num_updates=43300, lr=0.000303939, gnorm=0.293, train_wall=49, gb_free=10, wall=45286
2022-08-17 10:46:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:46:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:46:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:46:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:46:56 | INFO | valid | epoch 535 | valid on 'valid' subset | loss 5.109 | nll_loss 2.487 | ppl 5.61 | bleu 57.33 | wps 1837 | wpb 933.5 | bsz 59.6 | num_updates 43335 | best_bleu 57.42
2022-08-17 10:46:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 535 @ 43335 updates
2022-08-17 10:46:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint535.pt
2022-08-17 10:46:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint535.pt
2022-08-17 10:47:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint535.pt (epoch 535 @ 43335 updates, score 57.33) (writing took 17.27841017395258 seconds)
2022-08-17 10:47:13 | INFO | fairseq_cli.train | end of epoch 535 (average epoch stats below)
2022-08-17 10:47:13 | INFO | train | epoch 535 | loss 3.382 | nll_loss 0.348 | ppl 1.27 | wps 6477.6 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 43335 | lr 0.000303816 | gnorm 0.34 | train_wall 39 | gb_free 10.1 | wall 45330
2022-08-17 10:47:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:47:14 | INFO | fairseq.trainer | begin training epoch 536
2022-08-17 10:47:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:47:48 | INFO | train_inner | epoch 536:     65 / 81 loss=3.383, nll_loss=0.348, ppl=1.27, wps=7039.7, ups=1.28, wpb=5513.3, bsz=351.5, num_updates=43400, lr=0.000303588, gnorm=0.334, train_wall=49, gb_free=10.1, wall=45364
2022-08-17 10:47:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:47:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:47:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:47:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:48:05 | INFO | valid | epoch 536 | valid on 'valid' subset | loss 5.11 | nll_loss 2.486 | ppl 5.6 | bleu 57.02 | wps 1738.2 | wpb 933.5 | bsz 59.6 | num_updates 43416 | best_bleu 57.42
2022-08-17 10:48:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 536 @ 43416 updates
2022-08-17 10:48:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint536.pt
2022-08-17 10:48:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint536.pt
2022-08-17 10:48:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint536.pt (epoch 536 @ 43416 updates, score 57.02) (writing took 37.798130098730326 seconds)
2022-08-17 10:48:43 | INFO | fairseq_cli.train | end of epoch 536 (average epoch stats below)
2022-08-17 10:48:43 | INFO | train | epoch 536 | loss 3.382 | nll_loss 0.348 | ppl 1.27 | wps 4979.9 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 43416 | lr 0.000303532 | gnorm 0.296 | train_wall 40 | gb_free 10.1 | wall 45419
2022-08-17 10:48:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:48:43 | INFO | fairseq.trainer | begin training epoch 537
2022-08-17 10:48:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:49:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:49:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:49:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:49:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:49:37 | INFO | valid | epoch 537 | valid on 'valid' subset | loss 5.119 | nll_loss 2.504 | ppl 5.67 | bleu 56.55 | wps 1550.2 | wpb 933.5 | bsz 59.6 | num_updates 43497 | best_bleu 57.42
2022-08-17 10:49:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 537 @ 43497 updates
2022-08-17 10:49:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint537.pt
2022-08-17 10:49:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint537.pt
2022-08-17 10:49:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint537.pt (epoch 537 @ 43497 updates, score 56.55) (writing took 2.4904433377087116 seconds)
2022-08-17 10:49:39 | INFO | fairseq_cli.train | end of epoch 537 (average epoch stats below)
2022-08-17 10:49:39 | INFO | train | epoch 537 | loss 3.382 | nll_loss 0.348 | ppl 1.27 | wps 7999 | ups 1.45 | wpb 5523.2 | bsz 358 | num_updates 43497 | lr 0.00030325 | gnorm 0.341 | train_wall 40 | gb_free 10.3 | wall 45475
2022-08-17 10:49:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:49:39 | INFO | fairseq.trainer | begin training epoch 538
2022-08-17 10:49:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:49:42 | INFO | train_inner | epoch 538:      3 / 81 loss=3.382, nll_loss=0.348, ppl=1.27, wps=4831.7, ups=0.87, wpb=5522.8, bsz=359.5, num_updates=43500, lr=0.000303239, gnorm=0.333, train_wall=50, gb_free=10, wall=45478
2022-08-17 10:50:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:50:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:50:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:50:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:50:34 | INFO | valid | epoch 538 | valid on 'valid' subset | loss 5.114 | nll_loss 2.498 | ppl 5.65 | bleu 56.65 | wps 1760.2 | wpb 933.5 | bsz 59.6 | num_updates 43578 | best_bleu 57.42
2022-08-17 10:50:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 538 @ 43578 updates
2022-08-17 10:50:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint538.pt
2022-08-17 10:50:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint538.pt
2022-08-17 10:50:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint538.pt (epoch 538 @ 43578 updates, score 56.65) (writing took 2.3613839149475098 seconds)
2022-08-17 10:50:36 | INFO | fairseq_cli.train | end of epoch 538 (average epoch stats below)
2022-08-17 10:50:36 | INFO | train | epoch 538 | loss 3.383 | nll_loss 0.348 | ppl 1.27 | wps 7824 | ups 1.42 | wpb 5523.2 | bsz 358 | num_updates 43578 | lr 0.000302968 | gnorm 0.28 | train_wall 41 | gb_free 10.1 | wall 45533
2022-08-17 10:50:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:50:37 | INFO | fairseq.trainer | begin training epoch 539
2022-08-17 10:50:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:50:49 | INFO | train_inner | epoch 539:     22 / 81 loss=3.383, nll_loss=0.348, ppl=1.27, wps=8220.9, ups=1.49, wpb=5508.9, bsz=355, num_updates=43600, lr=0.000302891, gnorm=0.297, train_wall=50, gb_free=10.2, wall=45545
2022-08-17 10:51:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:51:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:51:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:51:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:51:28 | INFO | valid | epoch 539 | valid on 'valid' subset | loss 5.131 | nll_loss 2.517 | ppl 5.73 | bleu 56.59 | wps 1725.1 | wpb 933.5 | bsz 59.6 | num_updates 43659 | best_bleu 57.42
2022-08-17 10:51:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 539 @ 43659 updates
2022-08-17 10:51:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint539.pt
2022-08-17 10:51:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint539.pt
2022-08-17 10:52:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint539.pt (epoch 539 @ 43659 updates, score 56.59) (writing took 37.2445882409811 seconds)
2022-08-17 10:52:06 | INFO | fairseq_cli.train | end of epoch 539 (average epoch stats below)
2022-08-17 10:52:06 | INFO | train | epoch 539 | loss 3.382 | nll_loss 0.347 | ppl 1.27 | wps 4995.3 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 43659 | lr 0.000302687 | gnorm 0.31 | train_wall 41 | gb_free 10.1 | wall 45622
2022-08-17 10:52:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:52:06 | INFO | fairseq.trainer | begin training epoch 540
2022-08-17 10:52:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:52:27 | INFO | train_inner | epoch 540:     41 / 81 loss=3.382, nll_loss=0.347, ppl=1.27, wps=5626.8, ups=1.02, wpb=5531.2, bsz=362.8, num_updates=43700, lr=0.000302545, gnorm=0.278, train_wall=49, gb_free=10.1, wall=45644
2022-08-17 10:52:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:52:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:52:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:52:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:52:57 | INFO | valid | epoch 540 | valid on 'valid' subset | loss 5.128 | nll_loss 2.52 | ppl 5.73 | bleu 56.91 | wps 1801.5 | wpb 933.5 | bsz 59.6 | num_updates 43740 | best_bleu 57.42
2022-08-17 10:52:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 540 @ 43740 updates
2022-08-17 10:52:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint540.pt
2022-08-17 10:52:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint540.pt
2022-08-17 10:53:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint540.pt (epoch 540 @ 43740 updates, score 56.91) (writing took 40.92858066409826 seconds)
2022-08-17 10:53:38 | INFO | fairseq_cli.train | end of epoch 540 (average epoch stats below)
2022-08-17 10:53:38 | INFO | train | epoch 540 | loss 3.382 | nll_loss 0.347 | ppl 1.27 | wps 4860.1 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 43740 | lr 0.000302406 | gnorm 0.278 | train_wall 39 | gb_free 10.2 | wall 45714
2022-08-17 10:53:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:53:38 | INFO | fairseq.trainer | begin training epoch 541
2022-08-17 10:53:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:54:12 | INFO | train_inner | epoch 541:     60 / 81 loss=3.383, nll_loss=0.348, ppl=1.27, wps=5278.8, ups=0.95, wpb=5546.1, bsz=357.6, num_updates=43800, lr=0.000302199, gnorm=0.296, train_wall=50, gb_free=10.1, wall=45749
2022-08-17 10:54:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:54:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:54:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:54:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:54:33 | INFO | valid | epoch 541 | valid on 'valid' subset | loss 5.102 | nll_loss 2.482 | ppl 5.59 | bleu 57.38 | wps 1723.3 | wpb 933.5 | bsz 59.6 | num_updates 43821 | best_bleu 57.42
2022-08-17 10:54:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 541 @ 43821 updates
2022-08-17 10:54:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint541.pt
2022-08-17 10:54:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint541.pt
2022-08-17 10:55:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint541.pt (epoch 541 @ 43821 updates, score 57.38) (writing took 34.896059315651655 seconds)
2022-08-17 10:55:09 | INFO | fairseq_cli.train | end of epoch 541 (average epoch stats below)
2022-08-17 10:55:09 | INFO | train | epoch 541 | loss 3.383 | nll_loss 0.349 | ppl 1.27 | wps 4938.5 | ups 0.89 | wpb 5523.2 | bsz 358 | num_updates 43821 | lr 0.000302127 | gnorm 0.286 | train_wall 40 | gb_free 10.2 | wall 45805
2022-08-17 10:55:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:55:09 | INFO | fairseq.trainer | begin training epoch 542
2022-08-17 10:55:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:55:51 | INFO | train_inner | epoch 542:     79 / 81 loss=3.382, nll_loss=0.348, ppl=1.27, wps=5633.7, ups=1.02, wpb=5529.3, bsz=357.9, num_updates=43900, lr=0.000301855, gnorm=0.289, train_wall=51, gb_free=10.1, wall=45847
2022-08-17 10:55:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:55:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:55:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:55:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:56:01 | INFO | valid | epoch 542 | valid on 'valid' subset | loss 5.108 | nll_loss 2.488 | ppl 5.61 | bleu 57.45 | wps 1817.4 | wpb 933.5 | bsz 59.6 | num_updates 43902 | best_bleu 57.45
2022-08-17 10:56:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 542 @ 43902 updates
2022-08-17 10:56:01 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint542.pt
2022-08-17 10:56:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint542.pt
2022-08-17 10:56:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint542.pt (epoch 542 @ 43902 updates, score 57.45) (writing took 41.124062553048134 seconds)
2022-08-17 10:56:42 | INFO | fairseq_cli.train | end of epoch 542 (average epoch stats below)
2022-08-17 10:56:42 | INFO | train | epoch 542 | loss 3.382 | nll_loss 0.348 | ppl 1.27 | wps 4774 | ups 0.86 | wpb 5523.2 | bsz 358 | num_updates 43902 | lr 0.000301848 | gnorm 0.3 | train_wall 41 | gb_free 10.2 | wall 45898
2022-08-17 10:56:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:56:42 | INFO | fairseq.trainer | begin training epoch 543
2022-08-17 10:56:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:57:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:57:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:57:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:57:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:57:33 | INFO | valid | epoch 543 | valid on 'valid' subset | loss 5.113 | nll_loss 2.494 | ppl 5.63 | bleu 56.85 | wps 1831.1 | wpb 933.5 | bsz 59.6 | num_updates 43983 | best_bleu 57.45
2022-08-17 10:57:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 543 @ 43983 updates
2022-08-17 10:57:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint543.pt
2022-08-17 10:57:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint543.pt
2022-08-17 10:57:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint543.pt (epoch 543 @ 43983 updates, score 56.85) (writing took 2.365505088120699 seconds)
2022-08-17 10:57:36 | INFO | fairseq_cli.train | end of epoch 543 (average epoch stats below)
2022-08-17 10:57:36 | INFO | train | epoch 543 | loss 3.382 | nll_loss 0.348 | ppl 1.27 | wps 8328.1 | ups 1.51 | wpb 5523.2 | bsz 358 | num_updates 43983 | lr 0.00030157 | gnorm 0.531 | train_wall 39 | gb_free 10.1 | wall 45952
2022-08-17 10:57:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:57:36 | INFO | fairseq.trainer | begin training epoch 544
2022-08-17 10:57:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:57:46 | INFO | train_inner | epoch 544:     17 / 81 loss=3.382, nll_loss=0.347, ppl=1.27, wps=4767.8, ups=0.87, wpb=5494.1, bsz=360.4, num_updates=44000, lr=0.000301511, gnorm=0.504, train_wall=49, gb_free=10.2, wall=45962
2022-08-17 10:58:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:58:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:58:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:58:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:58:28 | INFO | valid | epoch 544 | valid on 'valid' subset | loss 5.112 | nll_loss 2.494 | ppl 5.63 | bleu 56.92 | wps 1719.2 | wpb 933.5 | bsz 59.6 | num_updates 44064 | best_bleu 57.45
2022-08-17 10:58:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 544 @ 44064 updates
2022-08-17 10:58:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint544.pt
2022-08-17 10:58:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint544.pt
2022-08-17 10:58:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint544.pt (epoch 544 @ 44064 updates, score 56.92) (writing took 22.78206243366003 seconds)
2022-08-17 10:58:51 | INFO | fairseq_cli.train | end of epoch 544 (average epoch stats below)
2022-08-17 10:58:51 | INFO | train | epoch 544 | loss 3.382 | nll_loss 0.348 | ppl 1.27 | wps 5988.3 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 44064 | lr 0.000301292 | gnorm 0.331 | train_wall 40 | gb_free 10.2 | wall 46027
2022-08-17 10:58:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:58:51 | INFO | fairseq.trainer | begin training epoch 545
2022-08-17 10:58:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 10:59:10 | INFO | train_inner | epoch 545:     36 / 81 loss=3.382, nll_loss=0.348, ppl=1.27, wps=6541.1, ups=1.19, wpb=5512.6, bsz=349.2, num_updates=44100, lr=0.000301169, gnorm=0.31, train_wall=49, gb_free=10.1, wall=46046
2022-08-17 10:59:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 10:59:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 10:59:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 10:59:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 10:59:41 | INFO | valid | epoch 545 | valid on 'valid' subset | loss 5.118 | nll_loss 2.502 | ppl 5.67 | bleu 57 | wps 1890.1 | wpb 933.5 | bsz 59.6 | num_updates 44145 | best_bleu 57.45
2022-08-17 10:59:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 545 @ 44145 updates
2022-08-17 10:59:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint545.pt
2022-08-17 10:59:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint545.pt
2022-08-17 10:59:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint545.pt (epoch 545 @ 44145 updates, score 57.0) (writing took 14.348219208419323 seconds)
2022-08-17 10:59:56 | INFO | fairseq_cli.train | end of epoch 545 (average epoch stats below)
2022-08-17 10:59:56 | INFO | train | epoch 545 | loss 3.381 | nll_loss 0.346 | ppl 1.27 | wps 6853.6 | ups 1.24 | wpb 5523.2 | bsz 358 | num_updates 44145 | lr 0.000301016 | gnorm 0.286 | train_wall 39 | gb_free 10.2 | wall 46092
2022-08-17 10:59:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 10:59:56 | INFO | fairseq.trainer | begin training epoch 546
2022-08-17 10:59:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:00:25 | INFO | train_inner | epoch 546:     55 / 81 loss=3.381, nll_loss=0.347, ppl=1.27, wps=7409.4, ups=1.33, wpb=5552.8, bsz=364.2, num_updates=44200, lr=0.000300828, gnorm=0.287, train_wall=49, gb_free=10, wall=46121
2022-08-17 11:00:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:00:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:00:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:00:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:00:47 | INFO | valid | epoch 546 | valid on 'valid' subset | loss 5.105 | nll_loss 2.482 | ppl 5.59 | bleu 57.03 | wps 1830.8 | wpb 933.5 | bsz 59.6 | num_updates 44226 | best_bleu 57.45
2022-08-17 11:00:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 546 @ 44226 updates
2022-08-17 11:00:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint546.pt
2022-08-17 11:00:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint546.pt
2022-08-17 11:01:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint546.pt (epoch 546 @ 44226 updates, score 57.03) (writing took 36.042399138212204 seconds)
2022-08-17 11:01:24 | INFO | fairseq_cli.train | end of epoch 546 (average epoch stats below)
2022-08-17 11:01:24 | INFO | train | epoch 546 | loss 3.382 | nll_loss 0.348 | ppl 1.27 | wps 5106.8 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 44226 | lr 0.00030074 | gnorm 0.308 | train_wall 40 | gb_free 10.2 | wall 46180
2022-08-17 11:01:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:01:24 | INFO | fairseq.trainer | begin training epoch 547
2022-08-17 11:01:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:02:02 | INFO | train_inner | epoch 547:     74 / 81 loss=3.382, nll_loss=0.347, ppl=1.27, wps=5699.8, ups=1.03, wpb=5529.4, bsz=358.3, num_updates=44300, lr=0.000300489, gnorm=0.293, train_wall=49, gb_free=10.1, wall=46218
2022-08-17 11:02:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:02:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:02:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:02:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:02:15 | INFO | valid | epoch 547 | valid on 'valid' subset | loss 5.098 | nll_loss 2.473 | ppl 5.55 | bleu 56.95 | wps 1857 | wpb 933.5 | bsz 59.6 | num_updates 44307 | best_bleu 57.45
2022-08-17 11:02:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 547 @ 44307 updates
2022-08-17 11:02:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint547.pt
2022-08-17 11:02:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint547.pt
2022-08-17 11:02:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint547.pt (epoch 547 @ 44307 updates, score 56.95) (writing took 18.81830755621195 seconds)
2022-08-17 11:02:34 | INFO | fairseq_cli.train | end of epoch 547 (average epoch stats below)
2022-08-17 11:02:34 | INFO | train | epoch 547 | loss 3.381 | nll_loss 0.347 | ppl 1.27 | wps 6345.9 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 44307 | lr 0.000300465 | gnorm 0.274 | train_wall 40 | gb_free 10.1 | wall 46250
2022-08-17 11:02:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:02:34 | INFO | fairseq.trainer | begin training epoch 548
2022-08-17 11:02:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:03:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:03:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:03:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:03:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:03:25 | INFO | valid | epoch 548 | valid on 'valid' subset | loss 5.117 | nll_loss 2.497 | ppl 5.64 | bleu 57.3 | wps 1791.2 | wpb 933.5 | bsz 59.6 | num_updates 44388 | best_bleu 57.45
2022-08-17 11:03:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 548 @ 44388 updates
2022-08-17 11:03:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint548.pt
2022-08-17 11:03:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint548.pt
2022-08-17 11:03:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint548.pt (epoch 548 @ 44388 updates, score 57.3) (writing took 16.726577151566744 seconds)
2022-08-17 11:03:42 | INFO | fairseq_cli.train | end of epoch 548 (average epoch stats below)
2022-08-17 11:03:42 | INFO | train | epoch 548 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 6588 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 44388 | lr 0.000300191 | gnorm 0.301 | train_wall 39 | gb_free 10.2 | wall 46318
2022-08-17 11:03:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:03:42 | INFO | fairseq.trainer | begin training epoch 549
2022-08-17 11:03:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:03:49 | INFO | train_inner | epoch 549:     12 / 81 loss=3.381, nll_loss=0.346, ppl=1.27, wps=5139.1, ups=0.93, wpb=5499.9, bsz=358.7, num_updates=44400, lr=0.00030015, gnorm=0.29, train_wall=49, gb_free=10.1, wall=46325
2022-08-17 11:04:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:04:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:04:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:04:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:04:33 | INFO | valid | epoch 549 | valid on 'valid' subset | loss 5.105 | nll_loss 2.482 | ppl 5.59 | bleu 56.87 | wps 1795.8 | wpb 933.5 | bsz 59.6 | num_updates 44469 | best_bleu 57.45
2022-08-17 11:04:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 549 @ 44469 updates
2022-08-17 11:04:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint549.pt
2022-08-17 11:04:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint549.pt
2022-08-17 11:04:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint549.pt (epoch 549 @ 44469 updates, score 56.87) (writing took 25.679045509546995 seconds)
2022-08-17 11:04:59 | INFO | fairseq_cli.train | end of epoch 549 (average epoch stats below)
2022-08-17 11:04:59 | INFO | train | epoch 549 | loss 3.382 | nll_loss 0.347 | ppl 1.27 | wps 5782.5 | ups 1.05 | wpb 5523.2 | bsz 358 | num_updates 44469 | lr 0.000299917 | gnorm 0.288 | train_wall 40 | gb_free 10.2 | wall 46396
2022-08-17 11:05:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:05:00 | INFO | fairseq.trainer | begin training epoch 550
2022-08-17 11:05:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:05:16 | INFO | train_inner | epoch 550:     31 / 81 loss=3.381, nll_loss=0.347, ppl=1.27, wps=6407.8, ups=1.16, wpb=5547, bsz=356.2, num_updates=44500, lr=0.000299813, gnorm=0.288, train_wall=49, gb_free=10.1, wall=46412
2022-08-17 11:05:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:05:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:05:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:05:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:05:51 | INFO | valid | epoch 550 | valid on 'valid' subset | loss 5.105 | nll_loss 2.487 | ppl 5.6 | bleu 57.15 | wps 1725.8 | wpb 933.5 | bsz 59.6 | num_updates 44550 | best_bleu 57.45
2022-08-17 11:05:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 550 @ 44550 updates
2022-08-17 11:05:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint550.pt
2022-08-17 11:05:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint550.pt
2022-08-17 11:06:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint550.pt (epoch 550 @ 44550 updates, score 57.15) (writing took 16.805363971740007 seconds)
2022-08-17 11:06:08 | INFO | fairseq_cli.train | end of epoch 550 (average epoch stats below)
2022-08-17 11:06:08 | INFO | train | epoch 550 | loss 3.382 | nll_loss 0.348 | ppl 1.27 | wps 6503.7 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 44550 | lr 0.000299644 | gnorm 0.285 | train_wall 40 | gb_free 10.1 | wall 46464
2022-08-17 11:06:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:06:08 | INFO | fairseq.trainer | begin training epoch 551
2022-08-17 11:06:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:06:35 | INFO | train_inner | epoch 551:     50 / 81 loss=3.382, nll_loss=0.348, ppl=1.27, wps=6988.8, ups=1.27, wpb=5517.8, bsz=357.4, num_updates=44600, lr=0.000299476, gnorm=0.296, train_wall=50, gb_free=10.1, wall=46491
2022-08-17 11:06:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:06:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:06:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:06:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:07:00 | INFO | valid | epoch 551 | valid on 'valid' subset | loss 5.125 | nll_loss 2.511 | ppl 5.7 | bleu 56.78 | wps 1777.9 | wpb 933.5 | bsz 59.6 | num_updates 44631 | best_bleu 57.45
2022-08-17 11:07:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 551 @ 44631 updates
2022-08-17 11:07:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint551.pt
2022-08-17 11:07:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint551.pt
2022-08-17 11:07:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint551.pt (epoch 551 @ 44631 updates, score 56.78) (writing took 2.513964056968689 seconds)
2022-08-17 11:07:03 | INFO | fairseq_cli.train | end of epoch 551 (average epoch stats below)
2022-08-17 11:07:03 | INFO | train | epoch 551 | loss 3.382 | nll_loss 0.347 | ppl 1.27 | wps 8223.2 | ups 1.49 | wpb 5523.2 | bsz 358 | num_updates 44631 | lr 0.000299372 | gnorm 0.29 | train_wall 40 | gb_free 10.2 | wall 46519
2022-08-17 11:07:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:07:03 | INFO | fairseq.trainer | begin training epoch 552
2022-08-17 11:07:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:07:39 | INFO | train_inner | epoch 552:     69 / 81 loss=3.381, nll_loss=0.347, ppl=1.27, wps=8498.6, ups=1.54, wpb=5505, bsz=357, num_updates=44700, lr=0.000299141, gnorm=0.3, train_wall=51, gb_free=10, wall=46556
2022-08-17 11:07:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:07:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:07:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:07:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:07:55 | INFO | valid | epoch 552 | valid on 'valid' subset | loss 5.122 | nll_loss 2.504 | ppl 5.67 | bleu 57.1 | wps 1735.3 | wpb 933.5 | bsz 59.6 | num_updates 44712 | best_bleu 57.45
2022-08-17 11:07:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 552 @ 44712 updates
2022-08-17 11:07:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint552.pt
2022-08-17 11:07:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint552.pt
2022-08-17 11:08:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint552.pt (epoch 552 @ 44712 updates, score 57.1) (writing took 19.56128877401352 seconds)
2022-08-17 11:08:15 | INFO | fairseq_cli.train | end of epoch 552 (average epoch stats below)
2022-08-17 11:08:15 | INFO | train | epoch 552 | loss 3.381 | nll_loss 0.347 | ppl 1.27 | wps 6180 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 44712 | lr 0.000299101 | gnorm 0.3 | train_wall 41 | gb_free 10 | wall 46591
2022-08-17 11:08:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:08:15 | INFO | fairseq.trainer | begin training epoch 553
2022-08-17 11:08:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:08:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:08:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:08:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:08:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:09:05 | INFO | valid | epoch 553 | valid on 'valid' subset | loss 5.106 | nll_loss 2.486 | ppl 5.6 | bleu 57.25 | wps 1828 | wpb 933.5 | bsz 59.6 | num_updates 44793 | best_bleu 57.45
2022-08-17 11:09:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 553 @ 44793 updates
2022-08-17 11:09:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint553.pt
2022-08-17 11:09:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint553.pt
2022-08-17 11:09:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint553.pt (epoch 553 @ 44793 updates, score 57.25) (writing took 25.704855784773827 seconds)
2022-08-17 11:09:31 | INFO | fairseq_cli.train | end of epoch 553 (average epoch stats below)
2022-08-17 11:09:31 | INFO | train | epoch 553 | loss 3.381 | nll_loss 0.347 | ppl 1.27 | wps 5859.9 | ups 1.06 | wpb 5523.2 | bsz 358 | num_updates 44793 | lr 0.00029883 | gnorm 0.303 | train_wall 39 | gb_free 10.2 | wall 46668
2022-08-17 11:09:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:09:31 | INFO | fairseq.trainer | begin training epoch 554
2022-08-17 11:09:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:09:36 | INFO | train_inner | epoch 554:      7 / 81 loss=3.381, nll_loss=0.346, ppl=1.27, wps=4755.8, ups=0.86, wpb=5539.9, bsz=361.1, num_updates=44800, lr=0.000298807, gnorm=0.287, train_wall=48, gb_free=10.1, wall=46672
2022-08-17 11:10:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:10:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:10:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:10:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:10:22 | INFO | valid | epoch 554 | valid on 'valid' subset | loss 5.127 | nll_loss 2.511 | ppl 5.7 | bleu 56.96 | wps 1787.1 | wpb 933.5 | bsz 59.6 | num_updates 44874 | best_bleu 57.45
2022-08-17 11:10:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 554 @ 44874 updates
2022-08-17 11:10:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint554.pt
2022-08-17 11:10:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint554.pt
2022-08-17 11:10:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint554.pt (epoch 554 @ 44874 updates, score 56.96) (writing took 13.179059445858002 seconds)
2022-08-17 11:10:36 | INFO | fairseq_cli.train | end of epoch 554 (average epoch stats below)
2022-08-17 11:10:36 | INFO | train | epoch 554 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 6942.8 | ups 1.26 | wpb 5523.2 | bsz 358 | num_updates 44874 | lr 0.000298561 | gnorm 0.271 | train_wall 40 | gb_free 10.1 | wall 46732
2022-08-17 11:10:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:10:36 | INFO | fairseq.trainer | begin training epoch 555
2022-08-17 11:10:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:10:50 | INFO | train_inner | epoch 555:     26 / 81 loss=3.381, nll_loss=0.346, ppl=1.27, wps=7447.9, ups=1.35, wpb=5512.5, bsz=356.4, num_updates=44900, lr=0.000298474, gnorm=0.289, train_wall=49, gb_free=10.1, wall=46746
2022-08-17 11:11:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:11:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:11:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:11:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:11:27 | INFO | valid | epoch 555 | valid on 'valid' subset | loss 5.114 | nll_loss 2.496 | ppl 5.64 | bleu 57.11 | wps 1835.3 | wpb 933.5 | bsz 59.6 | num_updates 44955 | best_bleu 57.45
2022-08-17 11:11:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 555 @ 44955 updates
2022-08-17 11:11:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint555.pt
2022-08-17 11:11:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint555.pt
2022-08-17 11:11:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint555.pt (epoch 555 @ 44955 updates, score 57.11) (writing took 16.426294770091772 seconds)
2022-08-17 11:11:43 | INFO | fairseq_cli.train | end of epoch 555 (average epoch stats below)
2022-08-17 11:11:43 | INFO | train | epoch 555 | loss 3.382 | nll_loss 0.348 | ppl 1.27 | wps 6618.1 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 44955 | lr 0.000298292 | gnorm 0.356 | train_wall 39 | gb_free 10.1 | wall 46800
2022-08-17 11:11:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:11:44 | INFO | fairseq.trainer | begin training epoch 556
2022-08-17 11:11:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:12:07 | INFO | train_inner | epoch 556:     45 / 81 loss=3.381, nll_loss=0.347, ppl=1.27, wps=7232.5, ups=1.3, wpb=5544.5, bsz=365.1, num_updates=45000, lr=0.000298142, gnorm=0.317, train_wall=48, gb_free=10.1, wall=46823
2022-08-17 11:12:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:12:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:12:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:12:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:12:34 | INFO | valid | epoch 556 | valid on 'valid' subset | loss 5.122 | nll_loss 2.502 | ppl 5.67 | bleu 56.57 | wps 1826.5 | wpb 933.5 | bsz 59.6 | num_updates 45036 | best_bleu 57.45
2022-08-17 11:12:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 556 @ 45036 updates
2022-08-17 11:12:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint556.pt
2022-08-17 11:12:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint556.pt
2022-08-17 11:12:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint556.pt (epoch 556 @ 45036 updates, score 56.57) (writing took 2.500975340604782 seconds)
2022-08-17 11:12:37 | INFO | fairseq_cli.train | end of epoch 556 (average epoch stats below)
2022-08-17 11:12:37 | INFO | train | epoch 556 | loss 3.382 | nll_loss 0.347 | ppl 1.27 | wps 8378.1 | ups 1.52 | wpb 5523.2 | bsz 358 | num_updates 45036 | lr 0.000298023 | gnorm 0.255 | train_wall 39 | gb_free 10.2 | wall 46853
2022-08-17 11:12:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:12:37 | INFO | fairseq.trainer | begin training epoch 557
2022-08-17 11:12:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:13:11 | INFO | train_inner | epoch 557:     64 / 81 loss=3.382, nll_loss=0.348, ppl=1.27, wps=8602.8, ups=1.56, wpb=5522.9, bsz=353, num_updates=45100, lr=0.000297812, gnorm=0.318, train_wall=50, gb_free=10.1, wall=46887
2022-08-17 11:13:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:13:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:13:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:13:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:13:29 | INFO | valid | epoch 557 | valid on 'valid' subset | loss 5.106 | nll_loss 2.484 | ppl 5.59 | bleu 57 | wps 1780.8 | wpb 933.5 | bsz 59.6 | num_updates 45117 | best_bleu 57.45
2022-08-17 11:13:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 557 @ 45117 updates
2022-08-17 11:13:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint557.pt
2022-08-17 11:13:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint557.pt
2022-08-17 11:14:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint557.pt (epoch 557 @ 45117 updates, score 57.0) (writing took 30.401222195476294 seconds)
2022-08-17 11:14:00 | INFO | fairseq_cli.train | end of epoch 557 (average epoch stats below)
2022-08-17 11:14:00 | INFO | train | epoch 557 | loss 3.382 | nll_loss 0.348 | ppl 1.27 | wps 5372.3 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 45117 | lr 0.000297756 | gnorm 0.342 | train_wall 41 | gb_free 10.1 | wall 46936
2022-08-17 11:14:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:14:00 | INFO | fairseq.trainer | begin training epoch 558
2022-08-17 11:14:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:14:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:14:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:14:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:14:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:14:52 | INFO | valid | epoch 558 | valid on 'valid' subset | loss 5.102 | nll_loss 2.479 | ppl 5.57 | bleu 57.22 | wps 1671.8 | wpb 933.5 | bsz 59.6 | num_updates 45198 | best_bleu 57.45
2022-08-17 11:14:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 558 @ 45198 updates
2022-08-17 11:14:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint558.pt
2022-08-17 11:14:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint558.pt
2022-08-17 11:15:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint558.pt (epoch 558 @ 45198 updates, score 57.22) (writing took 18.703685775399208 seconds)
2022-08-17 11:15:11 | INFO | fairseq_cli.train | end of epoch 558 (average epoch stats below)
2022-08-17 11:15:11 | INFO | train | epoch 558 | loss 3.381 | nll_loss 0.347 | ppl 1.27 | wps 6296.2 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 45198 | lr 0.000297489 | gnorm 0.432 | train_wall 40 | gb_free 10.2 | wall 47007
2022-08-17 11:15:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:15:11 | INFO | fairseq.trainer | begin training epoch 559
2022-08-17 11:15:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:15:13 | INFO | train_inner | epoch 559:      2 / 81 loss=3.381, nll_loss=0.347, ppl=1.27, wps=4487.9, ups=0.82, wpb=5496.9, bsz=355.8, num_updates=45200, lr=0.000297482, gnorm=0.412, train_wall=50, gb_free=10.1, wall=47009
2022-08-17 11:15:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:15:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:15:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:15:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:16:05 | INFO | valid | epoch 559 | valid on 'valid' subset | loss 5.135 | nll_loss 2.523 | ppl 5.75 | bleu 56.85 | wps 1810.2 | wpb 933.5 | bsz 59.6 | num_updates 45279 | best_bleu 57.45
2022-08-17 11:16:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 559 @ 45279 updates
2022-08-17 11:16:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint559.pt
2022-08-17 11:16:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint559.pt
2022-08-17 11:16:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint559.pt (epoch 559 @ 45279 updates, score 56.85) (writing took 30.94767278060317 seconds)
2022-08-17 11:16:36 | INFO | fairseq_cli.train | end of epoch 559 (average epoch stats below)
2022-08-17 11:16:36 | INFO | train | epoch 559 | loss 3.38 | nll_loss 0.345 | ppl 1.27 | wps 5266.9 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 45279 | lr 0.000297222 | gnorm 0.356 | train_wall 39 | gb_free 10.1 | wall 47092
2022-08-17 11:16:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:16:36 | INFO | fairseq.trainer | begin training epoch 560
2022-08-17 11:16:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:16:48 | INFO | train_inner | epoch 560:     21 / 81 loss=3.38, nll_loss=0.345, ppl=1.27, wps=5876.1, ups=1.06, wpb=5540.4, bsz=356.9, num_updates=45300, lr=0.000297154, gnorm=0.338, train_wall=49, gb_free=10.1, wall=47104
2022-08-17 11:17:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:17:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:17:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:17:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:17:28 | INFO | valid | epoch 560 | valid on 'valid' subset | loss 5.113 | nll_loss 2.496 | ppl 5.64 | bleu 57.47 | wps 1812.9 | wpb 933.5 | bsz 59.6 | num_updates 45360 | best_bleu 57.47
2022-08-17 11:17:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 560 @ 45360 updates
2022-08-17 11:17:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint560.pt
2022-08-17 11:17:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint560.pt
2022-08-17 11:18:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint560.pt (epoch 560 @ 45360 updates, score 57.47) (writing took 44.30050199478865 seconds)
2022-08-17 11:18:12 | INFO | fairseq_cli.train | end of epoch 560 (average epoch stats below)
2022-08-17 11:18:12 | INFO | train | epoch 560 | loss 3.381 | nll_loss 0.347 | ppl 1.27 | wps 4657.3 | ups 0.84 | wpb 5523.2 | bsz 358 | num_updates 45360 | lr 0.000296957 | gnorm 0.298 | train_wall 40 | gb_free 10.1 | wall 47188
2022-08-17 11:18:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:18:12 | INFO | fairseq.trainer | begin training epoch 561
2022-08-17 11:18:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:18:34 | INFO | train_inner | epoch 561:     40 / 81 loss=3.381, nll_loss=0.347, ppl=1.27, wps=5156.5, ups=0.94, wpb=5508.9, bsz=358.7, num_updates=45400, lr=0.000296826, gnorm=0.31, train_wall=51, gb_free=10.1, wall=47211
2022-08-17 11:18:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:18:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:18:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:18:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:19:05 | INFO | valid | epoch 561 | valid on 'valid' subset | loss 5.125 | nll_loss 2.512 | ppl 5.71 | bleu 56.42 | wps 1548.1 | wpb 933.5 | bsz 59.6 | num_updates 45441 | best_bleu 57.47
2022-08-17 11:19:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 561 @ 45441 updates
2022-08-17 11:19:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint561.pt
2022-08-17 11:19:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint561.pt
2022-08-17 11:19:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint561.pt (epoch 561 @ 45441 updates, score 56.42) (writing took 19.97594825923443 seconds)
2022-08-17 11:19:26 | INFO | fairseq_cli.train | end of epoch 561 (average epoch stats below)
2022-08-17 11:19:26 | INFO | train | epoch 561 | loss 3.382 | nll_loss 0.348 | ppl 1.27 | wps 6089.3 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 45441 | lr 0.000296692 | gnorm 0.542 | train_wall 41 | gb_free 10.1 | wall 47262
2022-08-17 11:19:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:19:26 | INFO | fairseq.trainer | begin training epoch 562
2022-08-17 11:19:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:20:00 | INFO | train_inner | epoch 562:     59 / 81 loss=3.382, nll_loss=0.348, ppl=1.27, wps=6434.6, ups=1.16, wpb=5524.7, bsz=356.9, num_updates=45500, lr=0.0002965, gnorm=0.49, train_wall=48, gb_free=10.1, wall=47296
2022-08-17 11:20:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:20:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:20:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:20:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:20:22 | INFO | valid | epoch 562 | valid on 'valid' subset | loss 5.121 | nll_loss 2.506 | ppl 5.68 | bleu 56.44 | wps 1869.6 | wpb 933.5 | bsz 59.6 | num_updates 45522 | best_bleu 57.47
2022-08-17 11:20:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 562 @ 45522 updates
2022-08-17 11:20:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint562.pt
2022-08-17 11:20:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint562.pt
2022-08-17 11:20:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint562.pt (epoch 562 @ 45522 updates, score 56.44) (writing took 15.087171584367752 seconds)
2022-08-17 11:20:38 | INFO | fairseq_cli.train | end of epoch 562 (average epoch stats below)
2022-08-17 11:20:38 | INFO | train | epoch 562 | loss 3.381 | nll_loss 0.347 | ppl 1.27 | wps 6210.3 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 45522 | lr 0.000296428 | gnorm 0.305 | train_wall 39 | gb_free 10.1 | wall 47334
2022-08-17 11:20:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:20:38 | INFO | fairseq.trainer | begin training epoch 563
2022-08-17 11:20:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:21:18 | INFO | train_inner | epoch 563:     78 / 81 loss=3.381, nll_loss=0.347, ppl=1.27, wps=7182.4, ups=1.29, wpb=5551.9, bsz=362.2, num_updates=45600, lr=0.000296174, gnorm=0.323, train_wall=49, gb_free=10.1, wall=47374
2022-08-17 11:21:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:21:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:21:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:21:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:21:29 | INFO | valid | epoch 563 | valid on 'valid' subset | loss 5.113 | nll_loss 2.496 | ppl 5.64 | bleu 57.46 | wps 1597.7 | wpb 933.5 | bsz 59.6 | num_updates 45603 | best_bleu 57.47
2022-08-17 11:21:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 563 @ 45603 updates
2022-08-17 11:21:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint563.pt
2022-08-17 11:21:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint563.pt
2022-08-17 11:21:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint563.pt (epoch 563 @ 45603 updates, score 57.46) (writing took 20.549440655857325 seconds)
2022-08-17 11:21:50 | INFO | fairseq_cli.train | end of epoch 563 (average epoch stats below)
2022-08-17 11:21:50 | INFO | train | epoch 563 | loss 3.381 | nll_loss 0.347 | ppl 1.27 | wps 6176.3 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 45603 | lr 0.000296165 | gnorm 0.321 | train_wall 40 | gb_free 10.2 | wall 47406
2022-08-17 11:21:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:21:50 | INFO | fairseq.trainer | begin training epoch 564
2022-08-17 11:21:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:22:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:22:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:22:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:22:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:22:42 | INFO | valid | epoch 564 | valid on 'valid' subset | loss 5.115 | nll_loss 2.499 | ppl 5.65 | bleu 57.28 | wps 1772.8 | wpb 933.5 | bsz 59.6 | num_updates 45684 | best_bleu 57.47
2022-08-17 11:22:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 564 @ 45684 updates
2022-08-17 11:22:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint564.pt
2022-08-17 11:22:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint564.pt
2022-08-17 11:23:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint564.pt (epoch 564 @ 45684 updates, score 57.28) (writing took 37.4494575150311 seconds)
2022-08-17 11:23:19 | INFO | fairseq_cli.train | end of epoch 564 (average epoch stats below)
2022-08-17 11:23:19 | INFO | train | epoch 564 | loss 3.381 | nll_loss 0.346 | ppl 1.27 | wps 5004.1 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 45684 | lr 0.000295902 | gnorm 0.314 | train_wall 40 | gb_free 10.2 | wall 47496
2022-08-17 11:23:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:23:20 | INFO | fairseq.trainer | begin training epoch 565
2022-08-17 11:23:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:23:29 | INFO | train_inner | epoch 565:     16 / 81 loss=3.381, nll_loss=0.346, ppl=1.27, wps=4200.2, ups=0.76, wpb=5509.9, bsz=354.8, num_updates=45700, lr=0.00029585, gnorm=0.303, train_wall=49, gb_free=10.1, wall=47505
2022-08-17 11:24:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:24:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:24:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:24:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:24:12 | INFO | valid | epoch 565 | valid on 'valid' subset | loss 5.135 | nll_loss 2.52 | ppl 5.74 | bleu 57.17 | wps 1709.2 | wpb 933.5 | bsz 59.6 | num_updates 45765 | best_bleu 57.47
2022-08-17 11:24:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 565 @ 45765 updates
2022-08-17 11:24:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint565.pt
2022-08-17 11:24:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint565.pt
2022-08-17 11:24:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint565.pt (epoch 565 @ 45765 updates, score 57.17) (writing took 2.4081666953861713 seconds)
2022-08-17 11:24:14 | INFO | fairseq_cli.train | end of epoch 565 (average epoch stats below)
2022-08-17 11:24:14 | INFO | train | epoch 565 | loss 3.382 | nll_loss 0.348 | ppl 1.27 | wps 8161.6 | ups 1.48 | wpb 5523.2 | bsz 358 | num_updates 45765 | lr 0.00029564 | gnorm 0.363 | train_wall 40 | gb_free 10.1 | wall 47551
2022-08-17 11:24:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:24:14 | INFO | fairseq.trainer | begin training epoch 566
2022-08-17 11:24:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:24:34 | INFO | train_inner | epoch 566:     35 / 81 loss=3.382, nll_loss=0.348, ppl=1.27, wps=8466.5, ups=1.53, wpb=5532.3, bsz=360.4, num_updates=45800, lr=0.000295527, gnorm=0.348, train_wall=49, gb_free=10.1, wall=47570
2022-08-17 11:24:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:24:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:24:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:24:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:25:06 | INFO | valid | epoch 566 | valid on 'valid' subset | loss 5.128 | nll_loss 2.512 | ppl 5.71 | bleu 56.64 | wps 1820 | wpb 933.5 | bsz 59.6 | num_updates 45846 | best_bleu 57.47
2022-08-17 11:25:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 566 @ 45846 updates
2022-08-17 11:25:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint566.pt
2022-08-17 11:25:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint566.pt
2022-08-17 11:25:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint566.pt (epoch 566 @ 45846 updates, score 56.64) (writing took 14.190977334976196 seconds)
2022-08-17 11:25:21 | INFO | fairseq_cli.train | end of epoch 566 (average epoch stats below)
2022-08-17 11:25:21 | INFO | train | epoch 566 | loss 3.381 | nll_loss 0.347 | ppl 1.27 | wps 6738.4 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 45846 | lr 0.000295379 | gnorm 0.332 | train_wall 40 | gb_free 10.1 | wall 47617
2022-08-17 11:25:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:25:21 | INFO | fairseq.trainer | begin training epoch 567
2022-08-17 11:25:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:25:49 | INFO | train_inner | epoch 567:     54 / 81 loss=3.382, nll_loss=0.348, ppl=1.27, wps=7376, ups=1.34, wpb=5501.5, bsz=360.4, num_updates=45900, lr=0.000295205, gnorm=0.337, train_wall=49, gb_free=10.1, wall=47645
2022-08-17 11:26:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:26:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:26:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:26:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:26:11 | INFO | valid | epoch 567 | valid on 'valid' subset | loss 5.119 | nll_loss 2.499 | ppl 5.65 | bleu 57.13 | wps 1903.3 | wpb 933.5 | bsz 59.6 | num_updates 45927 | best_bleu 57.47
2022-08-17 11:26:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 567 @ 45927 updates
2022-08-17 11:26:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint567.pt
2022-08-17 11:26:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint567.pt
2022-08-17 11:26:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint567.pt (epoch 567 @ 45927 updates, score 57.13) (writing took 27.58773624524474 seconds)
2022-08-17 11:26:39 | INFO | fairseq_cli.train | end of epoch 567 (average epoch stats below)
2022-08-17 11:26:39 | INFO | train | epoch 567 | loss 3.383 | nll_loss 0.349 | ppl 1.27 | wps 5710.1 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 45927 | lr 0.000295118 | gnorm 0.306 | train_wall 40 | gb_free 10.1 | wall 47695
2022-08-17 11:26:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:26:39 | INFO | fairseq.trainer | begin training epoch 568
2022-08-17 11:26:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:27:17 | INFO | train_inner | epoch 568:     73 / 81 loss=3.382, nll_loss=0.348, ppl=1.27, wps=6249.5, ups=1.13, wpb=5544.6, bsz=357.8, num_updates=46000, lr=0.000294884, gnorm=0.338, train_wall=50, gb_free=10.1, wall=47734
2022-08-17 11:27:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:27:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:27:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:27:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:27:30 | INFO | valid | epoch 568 | valid on 'valid' subset | loss 5.124 | nll_loss 2.507 | ppl 5.69 | bleu 57.42 | wps 1830.4 | wpb 933.5 | bsz 59.6 | num_updates 46008 | best_bleu 57.47
2022-08-17 11:27:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 568 @ 46008 updates
2022-08-17 11:27:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint568.pt
2022-08-17 11:27:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint568.pt
2022-08-17 11:27:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint568.pt (epoch 568 @ 46008 updates, score 57.42) (writing took 2.424200512468815 seconds)
2022-08-17 11:27:33 | INFO | fairseq_cli.train | end of epoch 568 (average epoch stats below)
2022-08-17 11:27:33 | INFO | train | epoch 568 | loss 3.381 | nll_loss 0.347 | ppl 1.27 | wps 8296 | ups 1.5 | wpb 5523.2 | bsz 358 | num_updates 46008 | lr 0.000294858 | gnorm 0.345 | train_wall 40 | gb_free 10.2 | wall 47749
2022-08-17 11:27:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:27:33 | INFO | fairseq.trainer | begin training epoch 569
2022-08-17 11:27:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:28:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:28:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:28:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:28:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:28:29 | INFO | valid | epoch 569 | valid on 'valid' subset | loss 5.131 | nll_loss 2.515 | ppl 5.72 | bleu 57.32 | wps 1665.6 | wpb 933.5 | bsz 59.6 | num_updates 46089 | best_bleu 57.47
2022-08-17 11:28:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 569 @ 46089 updates
2022-08-17 11:28:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint569.pt
2022-08-17 11:28:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint569.pt
2022-08-17 11:28:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint569.pt (epoch 569 @ 46089 updates, score 57.32) (writing took 15.711205501109362 seconds)
2022-08-17 11:28:45 | INFO | fairseq_cli.train | end of epoch 569 (average epoch stats below)
2022-08-17 11:28:45 | INFO | train | epoch 569 | loss 3.382 | nll_loss 0.348 | ppl 1.27 | wps 6249.8 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 46089 | lr 0.000294599 | gnorm 0.389 | train_wall 40 | gb_free 10.2 | wall 47821
2022-08-17 11:28:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:28:45 | INFO | fairseq.trainer | begin training epoch 570
2022-08-17 11:28:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:28:52 | INFO | train_inner | epoch 570:     11 / 81 loss=3.382, nll_loss=0.348, ppl=1.27, wps=5783.9, ups=1.05, wpb=5486.1, bsz=355.9, num_updates=46100, lr=0.000294564, gnorm=0.37, train_wall=49, gb_free=10.1, wall=47828
2022-08-17 11:29:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:29:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:29:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:29:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:29:40 | INFO | valid | epoch 570 | valid on 'valid' subset | loss 5.138 | nll_loss 2.522 | ppl 5.75 | bleu 57.1 | wps 1873.6 | wpb 933.5 | bsz 59.6 | num_updates 46170 | best_bleu 57.47
2022-08-17 11:29:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 570 @ 46170 updates
2022-08-17 11:29:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint570.pt
2022-08-17 11:29:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint570.pt
2022-08-17 11:29:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint570.pt (epoch 570 @ 46170 updates, score 57.1) (writing took 18.84011398255825 seconds)
2022-08-17 11:29:59 | INFO | fairseq_cli.train | end of epoch 570 (average epoch stats below)
2022-08-17 11:29:59 | INFO | train | epoch 570 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 6000.4 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 46170 | lr 0.000294341 | gnorm 0.301 | train_wall 41 | gb_free 10.1 | wall 47895
2022-08-17 11:29:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:29:59 | INFO | fairseq.trainer | begin training epoch 571
2022-08-17 11:29:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:30:16 | INFO | train_inner | epoch 571:     30 / 81 loss=3.381, nll_loss=0.346, ppl=1.27, wps=6636, ups=1.2, wpb=5540.8, bsz=357.8, num_updates=46200, lr=0.000294245, gnorm=0.314, train_wall=50, gb_free=10.1, wall=47912
2022-08-17 11:30:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:30:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:30:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:30:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:30:51 | INFO | valid | epoch 571 | valid on 'valid' subset | loss 5.13 | nll_loss 2.514 | ppl 5.71 | bleu 57.29 | wps 1776.2 | wpb 933.5 | bsz 59.6 | num_updates 46251 | best_bleu 57.47
2022-08-17 11:30:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 571 @ 46251 updates
2022-08-17 11:30:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint571.pt
2022-08-17 11:30:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint571.pt
2022-08-17 11:31:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint571.pt (epoch 571 @ 46251 updates, score 57.29) (writing took 17.04475025460124 seconds)
2022-08-17 11:31:08 | INFO | fairseq_cli.train | end of epoch 571 (average epoch stats below)
2022-08-17 11:31:08 | INFO | train | epoch 571 | loss 3.381 | nll_loss 0.347 | ppl 1.27 | wps 6468.4 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 46251 | lr 0.000294083 | gnorm 0.324 | train_wall 40 | gb_free 10.2 | wall 47965
2022-08-17 11:31:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:31:09 | INFO | fairseq.trainer | begin training epoch 572
2022-08-17 11:31:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:31:34 | INFO | train_inner | epoch 572:     49 / 81 loss=3.381, nll_loss=0.348, ppl=1.27, wps=7062.2, ups=1.28, wpb=5530.3, bsz=360.2, num_updates=46300, lr=0.000293927, gnorm=0.342, train_wall=49, gb_free=10, wall=47990
2022-08-17 11:31:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:31:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:31:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:31:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:31:59 | INFO | valid | epoch 572 | valid on 'valid' subset | loss 5.128 | nll_loss 2.514 | ppl 5.71 | bleu 56.66 | wps 1900.9 | wpb 933.5 | bsz 59.6 | num_updates 46332 | best_bleu 57.47
2022-08-17 11:31:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 572 @ 46332 updates
2022-08-17 11:31:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint572.pt
2022-08-17 11:32:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint572.pt
2022-08-17 11:32:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint572.pt (epoch 572 @ 46332 updates, score 56.66) (writing took 30.43033040687442 seconds)
2022-08-17 11:32:29 | INFO | fairseq_cli.train | end of epoch 572 (average epoch stats below)
2022-08-17 11:32:29 | INFO | train | epoch 572 | loss 3.382 | nll_loss 0.348 | ppl 1.27 | wps 5524.5 | ups 1 | wpb 5523.2 | bsz 358 | num_updates 46332 | lr 0.000293825 | gnorm 0.343 | train_wall 39 | gb_free 10.1 | wall 48045
2022-08-17 11:32:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:32:30 | INFO | fairseq.trainer | begin training epoch 573
2022-08-17 11:32:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:33:05 | INFO | train_inner | epoch 573:     68 / 81 loss=3.381, nll_loss=0.347, ppl=1.27, wps=6108.8, ups=1.1, wpb=5533.7, bsz=358.5, num_updates=46400, lr=0.00029361, gnorm=0.305, train_wall=49, gb_free=10.1, wall=48081
2022-08-17 11:33:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:33:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:33:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:33:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:33:21 | INFO | valid | epoch 573 | valid on 'valid' subset | loss 5.138 | nll_loss 2.524 | ppl 5.75 | bleu 57.29 | wps 1775 | wpb 933.5 | bsz 59.6 | num_updates 46413 | best_bleu 57.47
2022-08-17 11:33:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 573 @ 46413 updates
2022-08-17 11:33:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint573.pt
2022-08-17 11:33:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint573.pt
2022-08-17 11:33:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint573.pt (epoch 573 @ 46413 updates, score 57.29) (writing took 17.37468619272113 seconds)
2022-08-17 11:33:38 | INFO | fairseq_cli.train | end of epoch 573 (average epoch stats below)
2022-08-17 11:33:38 | INFO | train | epoch 573 | loss 3.381 | nll_loss 0.347 | ppl 1.27 | wps 6496.1 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 46413 | lr 0.000293569 | gnorm 0.302 | train_wall 40 | gb_free 10.1 | wall 48114
2022-08-17 11:33:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:33:38 | INFO | fairseq.trainer | begin training epoch 574
2022-08-17 11:33:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:34:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:34:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:34:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:34:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:34:31 | INFO | valid | epoch 574 | valid on 'valid' subset | loss 5.131 | nll_loss 2.513 | ppl 5.71 | bleu 57.46 | wps 1892.3 | wpb 933.5 | bsz 59.6 | num_updates 46494 | best_bleu 57.47
2022-08-17 11:34:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 574 @ 46494 updates
2022-08-17 11:34:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint574.pt
2022-08-17 11:34:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint574.pt
2022-08-17 11:34:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint574.pt (epoch 574 @ 46494 updates, score 57.46) (writing took 2.5283878594636917 seconds)
2022-08-17 11:34:34 | INFO | fairseq_cli.train | end of epoch 574 (average epoch stats below)
2022-08-17 11:34:34 | INFO | train | epoch 574 | loss 3.381 | nll_loss 0.347 | ppl 1.27 | wps 8049.2 | ups 1.46 | wpb 5523.2 | bsz 358 | num_updates 46494 | lr 0.000293313 | gnorm 0.293 | train_wall 40 | gb_free 10.1 | wall 48170
2022-08-17 11:34:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:34:34 | INFO | fairseq.trainer | begin training epoch 575
2022-08-17 11:34:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:34:38 | INFO | train_inner | epoch 575:      6 / 81 loss=3.381, nll_loss=0.346, ppl=1.27, wps=5867.3, ups=1.07, wpb=5505.2, bsz=357.6, num_updates=46500, lr=0.000293294, gnorm=0.292, train_wall=49, gb_free=10.1, wall=48175
2022-08-17 11:35:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:35:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:35:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:35:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:35:25 | INFO | valid | epoch 575 | valid on 'valid' subset | loss 5.12 | nll_loss 2.504 | ppl 5.67 | bleu 57.16 | wps 1885.3 | wpb 933.5 | bsz 59.6 | num_updates 46575 | best_bleu 57.47
2022-08-17 11:35:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 575 @ 46575 updates
2022-08-17 11:35:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint575.pt
2022-08-17 11:35:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint575.pt
2022-08-17 11:35:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint575.pt (epoch 575 @ 46575 updates, score 57.16) (writing took 23.85134708508849 seconds)
2022-08-17 11:35:50 | INFO | fairseq_cli.train | end of epoch 575 (average epoch stats below)
2022-08-17 11:35:50 | INFO | train | epoch 575 | loss 3.381 | nll_loss 0.346 | ppl 1.27 | wps 5902.9 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 46575 | lr 0.000293058 | gnorm 0.301 | train_wall 41 | gb_free 10.1 | wall 48246
2022-08-17 11:35:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:35:50 | INFO | fairseq.trainer | begin training epoch 576
2022-08-17 11:35:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:36:04 | INFO | train_inner | epoch 576:     25 / 81 loss=3.381, nll_loss=0.346, ppl=1.27, wps=6501.7, ups=1.17, wpb=5536.4, bsz=352.1, num_updates=46600, lr=0.000292979, gnorm=0.297, train_wall=50, gb_free=10, wall=48260
2022-08-17 11:36:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:36:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:36:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:36:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:36:41 | INFO | valid | epoch 576 | valid on 'valid' subset | loss 5.116 | nll_loss 2.5 | ppl 5.66 | bleu 57.05 | wps 1830.2 | wpb 933.5 | bsz 59.6 | num_updates 46656 | best_bleu 57.47
2022-08-17 11:36:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 576 @ 46656 updates
2022-08-17 11:36:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint576.pt
2022-08-17 11:36:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint576.pt
2022-08-17 11:36:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint576.pt (epoch 576 @ 46656 updates, score 57.05) (writing took 17.02623152732849 seconds)
2022-08-17 11:36:59 | INFO | fairseq_cli.train | end of epoch 576 (average epoch stats below)
2022-08-17 11:36:59 | INFO | train | epoch 576 | loss 3.381 | nll_loss 0.347 | ppl 1.27 | wps 6485.6 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 46656 | lr 0.000292803 | gnorm 0.342 | train_wall 39 | gb_free 10.2 | wall 48315
2022-08-17 11:36:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:36:59 | INFO | fairseq.trainer | begin training epoch 577
2022-08-17 11:36:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:37:22 | INFO | train_inner | epoch 577:     44 / 81 loss=3.38, nll_loss=0.346, ppl=1.27, wps=7011.5, ups=1.27, wpb=5526.4, bsz=363.4, num_updates=46700, lr=0.000292666, gnorm=0.327, train_wall=48, gb_free=10.1, wall=48339
2022-08-17 11:37:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:37:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:37:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:37:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:37:50 | INFO | valid | epoch 577 | valid on 'valid' subset | loss 5.141 | nll_loss 2.527 | ppl 5.76 | bleu 57.09 | wps 1835.2 | wpb 933.5 | bsz 59.6 | num_updates 46737 | best_bleu 57.47
2022-08-17 11:37:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 577 @ 46737 updates
2022-08-17 11:37:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint577.pt
2022-08-17 11:37:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint577.pt
2022-08-17 11:38:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint577.pt (epoch 577 @ 46737 updates, score 57.09) (writing took 21.158997528254986 seconds)
2022-08-17 11:38:12 | INFO | fairseq_cli.train | end of epoch 577 (average epoch stats below)
2022-08-17 11:38:12 | INFO | train | epoch 577 | loss 3.381 | nll_loss 0.346 | ppl 1.27 | wps 6122.9 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 46737 | lr 0.00029255 | gnorm 0.29 | train_wall 40 | gb_free 10.2 | wall 48388
2022-08-17 11:38:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:38:12 | INFO | fairseq.trainer | begin training epoch 578
2022-08-17 11:38:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:38:47 | INFO | train_inner | epoch 578:     63 / 81 loss=3.381, nll_loss=0.347, ppl=1.27, wps=6532.5, ups=1.18, wpb=5521.6, bsz=360.6, num_updates=46800, lr=0.000292353, gnorm=0.305, train_wall=48, gb_free=10.1, wall=48423
2022-08-17 11:38:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:38:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:38:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:38:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:39:05 | INFO | valid | epoch 578 | valid on 'valid' subset | loss 5.148 | nll_loss 2.54 | ppl 5.82 | bleu 56.44 | wps 1823.2 | wpb 933.5 | bsz 59.6 | num_updates 46818 | best_bleu 57.47
2022-08-17 11:39:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 578 @ 46818 updates
2022-08-17 11:39:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint578.pt
2022-08-17 11:39:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint578.pt
2022-08-17 11:39:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint578.pt (epoch 578 @ 46818 updates, score 56.44) (writing took 36.039744950830936 seconds)
2022-08-17 11:39:41 | INFO | fairseq_cli.train | end of epoch 578 (average epoch stats below)
2022-08-17 11:39:41 | INFO | train | epoch 578 | loss 3.381 | nll_loss 0.347 | ppl 1.27 | wps 5002.8 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 46818 | lr 0.000292296 | gnorm 0.301 | train_wall 39 | gb_free 10.1 | wall 48477
2022-08-17 11:39:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:39:41 | INFO | fairseq.trainer | begin training epoch 579
2022-08-17 11:39:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:40:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:40:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:40:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:40:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:40:32 | INFO | valid | epoch 579 | valid on 'valid' subset | loss 5.136 | nll_loss 2.525 | ppl 5.76 | bleu 57.23 | wps 1895.7 | wpb 933.5 | bsz 59.6 | num_updates 46899 | best_bleu 57.47
2022-08-17 11:40:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 579 @ 46899 updates
2022-08-17 11:40:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint579.pt
2022-08-17 11:40:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint579.pt
2022-08-17 11:40:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint579.pt (epoch 579 @ 46899 updates, score 57.23) (writing took 14.09857613965869 seconds)
2022-08-17 11:40:46 | INFO | fairseq_cli.train | end of epoch 579 (average epoch stats below)
2022-08-17 11:40:46 | INFO | train | epoch 579 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 6884.7 | ups 1.25 | wpb 5523.2 | bsz 358 | num_updates 46899 | lr 0.000292044 | gnorm 0.303 | train_wall 39 | gb_free 10.3 | wall 48542
2022-08-17 11:40:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:40:46 | INFO | fairseq.trainer | begin training epoch 580
2022-08-17 11:40:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:40:48 | INFO | train_inner | epoch 580:      1 / 81 loss=3.381, nll_loss=0.347, ppl=1.27, wps=4564.7, ups=0.83, wpb=5507.5, bsz=353.4, num_updates=46900, lr=0.000292041, gnorm=0.304, train_wall=47, gb_free=10.1, wall=48544
2022-08-17 11:41:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:41:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:41:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:41:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:41:37 | INFO | valid | epoch 580 | valid on 'valid' subset | loss 5.137 | nll_loss 2.528 | ppl 5.77 | bleu 57.05 | wps 1898.2 | wpb 933.5 | bsz 59.6 | num_updates 46980 | best_bleu 57.47
2022-08-17 11:41:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 580 @ 46980 updates
2022-08-17 11:41:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint580.pt
2022-08-17 11:41:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint580.pt
2022-08-17 11:41:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint580.pt (epoch 580 @ 46980 updates, score 57.05) (writing took 20.403608210384846 seconds)
2022-08-17 11:41:57 | INFO | fairseq_cli.train | end of epoch 580 (average epoch stats below)
2022-08-17 11:41:58 | INFO | train | epoch 580 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 6257.6 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 46980 | lr 0.000291792 | gnorm 0.334 | train_wall 40 | gb_free 10.1 | wall 48614
2022-08-17 11:41:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:41:58 | INFO | fairseq.trainer | begin training epoch 581
2022-08-17 11:41:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:42:08 | INFO | train_inner | epoch 581:     20 / 81 loss=3.38, nll_loss=0.345, ppl=1.27, wps=6814.7, ups=1.24, wpb=5500.9, bsz=356.9, num_updates=47000, lr=0.00029173, gnorm=0.315, train_wall=49, gb_free=10.1, wall=48625
2022-08-17 11:42:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:42:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:42:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:42:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:42:49 | INFO | valid | epoch 581 | valid on 'valid' subset | loss 5.14 | nll_loss 2.529 | ppl 5.77 | bleu 56.79 | wps 1789.2 | wpb 933.5 | bsz 59.6 | num_updates 47061 | best_bleu 57.47
2022-08-17 11:42:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 581 @ 47061 updates
2022-08-17 11:42:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint581.pt
2022-08-17 11:42:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint581.pt
2022-08-17 11:43:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint581.pt (epoch 581 @ 47061 updates, score 56.79) (writing took 16.50704511255026 seconds)
2022-08-17 11:43:05 | INFO | fairseq_cli.train | end of epoch 581 (average epoch stats below)
2022-08-17 11:43:05 | INFO | train | epoch 581 | loss 3.38 | nll_loss 0.345 | ppl 1.27 | wps 6591.7 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 47061 | lr 0.000291541 | gnorm 0.282 | train_wall 40 | gb_free 10.1 | wall 48682
2022-08-17 11:43:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:43:06 | INFO | fairseq.trainer | begin training epoch 582
2022-08-17 11:43:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:43:26 | INFO | train_inner | epoch 582:     39 / 81 loss=3.38, nll_loss=0.346, ppl=1.27, wps=7126.5, ups=1.28, wpb=5550.7, bsz=359.5, num_updates=47100, lr=0.00029142, gnorm=0.313, train_wall=50, gb_free=10.1, wall=48702
2022-08-17 11:43:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:43:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:43:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:43:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:43:56 | INFO | valid | epoch 582 | valid on 'valid' subset | loss 5.148 | nll_loss 2.537 | ppl 5.8 | bleu 56.55 | wps 1787.9 | wpb 933.5 | bsz 59.6 | num_updates 47142 | best_bleu 57.47
2022-08-17 11:43:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 582 @ 47142 updates
2022-08-17 11:43:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint582.pt
2022-08-17 11:43:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint582.pt
2022-08-17 11:44:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint582.pt (epoch 582 @ 47142 updates, score 56.55) (writing took 22.64505250379443 seconds)
2022-08-17 11:44:19 | INFO | fairseq_cli.train | end of epoch 582 (average epoch stats below)
2022-08-17 11:44:19 | INFO | train | epoch 582 | loss 3.381 | nll_loss 0.347 | ppl 1.27 | wps 6054 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 47142 | lr 0.00029129 | gnorm 0.325 | train_wall 40 | gb_free 10.2 | wall 48755
2022-08-17 11:44:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:44:19 | INFO | fairseq.trainer | begin training epoch 583
2022-08-17 11:44:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:44:50 | INFO | train_inner | epoch 583:     58 / 81 loss=3.381, nll_loss=0.347, ppl=1.27, wps=6639.3, ups=1.2, wpb=5525.1, bsz=360.6, num_updates=47200, lr=0.000291111, gnorm=0.293, train_wall=49, gb_free=10.1, wall=48786
2022-08-17 11:45:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:45:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:45:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:45:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:45:11 | INFO | valid | epoch 583 | valid on 'valid' subset | loss 5.15 | nll_loss 2.542 | ppl 5.82 | bleu 56.82 | wps 1833.3 | wpb 933.5 | bsz 59.6 | num_updates 47223 | best_bleu 57.47
2022-08-17 11:45:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 583 @ 47223 updates
2022-08-17 11:45:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint583.pt
2022-08-17 11:45:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint583.pt
2022-08-17 11:45:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint583.pt (epoch 583 @ 47223 updates, score 56.82) (writing took 40.55813301354647 seconds)
2022-08-17 11:45:51 | INFO | fairseq_cli.train | end of epoch 583 (average epoch stats below)
2022-08-17 11:45:51 | INFO | train | epoch 583 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 4863.6 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 47223 | lr 0.00029104 | gnorm 0.288 | train_wall 40 | gb_free 10.1 | wall 48847
2022-08-17 11:45:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:45:51 | INFO | fairseq.trainer | begin training epoch 584
2022-08-17 11:45:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:46:31 | INFO | train_inner | epoch 584:     77 / 81 loss=3.38, nll_loss=0.346, ppl=1.27, wps=5463.4, ups=0.99, wpb=5532.2, bsz=356.7, num_updates=47300, lr=0.000290803, gnorm=0.33, train_wall=49, gb_free=10.2, wall=48887
2022-08-17 11:46:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:46:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:46:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:46:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:46:42 | INFO | valid | epoch 584 | valid on 'valid' subset | loss 5.143 | nll_loss 2.536 | ppl 5.8 | bleu 57 | wps 1821 | wpb 933.5 | bsz 59.6 | num_updates 47304 | best_bleu 57.47
2022-08-17 11:46:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 584 @ 47304 updates
2022-08-17 11:46:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint584.pt
2022-08-17 11:46:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint584.pt
2022-08-17 11:47:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint584.pt (epoch 584 @ 47304 updates, score 57.0) (writing took 21.335608407855034 seconds)
2022-08-17 11:47:03 | INFO | fairseq_cli.train | end of epoch 584 (average epoch stats below)
2022-08-17 11:47:03 | INFO | train | epoch 584 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 6205.6 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 47304 | lr 0.000290791 | gnorm 0.332 | train_wall 39 | gb_free 10.1 | wall 48920
2022-08-17 11:47:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:47:04 | INFO | fairseq.trainer | begin training epoch 585
2022-08-17 11:47:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:47:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:47:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:47:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:47:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:47:56 | INFO | valid | epoch 585 | valid on 'valid' subset | loss 5.153 | nll_loss 2.544 | ppl 5.83 | bleu 56.62 | wps 1781.2 | wpb 933.5 | bsz 59.6 | num_updates 47385 | best_bleu 57.47
2022-08-17 11:47:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 585 @ 47385 updates
2022-08-17 11:47:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint585.pt
2022-08-17 11:47:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint585.pt
2022-08-17 11:48:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint585.pt (epoch 585 @ 47385 updates, score 56.62) (writing took 24.246521651744843 seconds)
2022-08-17 11:48:20 | INFO | fairseq_cli.train | end of epoch 585 (average epoch stats below)
2022-08-17 11:48:20 | INFO | train | epoch 585 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 5833.9 | ups 1.06 | wpb 5523.2 | bsz 358 | num_updates 47385 | lr 0.000290542 | gnorm 0.33 | train_wall 40 | gb_free 10.2 | wall 48996
2022-08-17 11:48:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:48:20 | INFO | fairseq.trainer | begin training epoch 586
2022-08-17 11:48:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:48:29 | INFO | train_inner | epoch 586:     15 / 81 loss=3.381, nll_loss=0.346, ppl=1.27, wps=4672.2, ups=0.85, wpb=5520.9, bsz=359.4, num_updates=47400, lr=0.000290496, gnorm=0.32, train_wall=49, gb_free=10, wall=49005
2022-08-17 11:49:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:49:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:49:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:49:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:49:12 | INFO | valid | epoch 586 | valid on 'valid' subset | loss 5.143 | nll_loss 2.534 | ppl 5.79 | bleu 56.65 | wps 1771.6 | wpb 933.5 | bsz 59.6 | num_updates 47466 | best_bleu 57.47
2022-08-17 11:49:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 586 @ 47466 updates
2022-08-17 11:49:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint586.pt
2022-08-17 11:49:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint586.pt
2022-08-17 11:49:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint586.pt (epoch 586 @ 47466 updates, score 56.65) (writing took 15.171060178428888 seconds)
2022-08-17 11:49:27 | INFO | fairseq_cli.train | end of epoch 586 (average epoch stats below)
2022-08-17 11:49:27 | INFO | train | epoch 586 | loss 3.381 | nll_loss 0.347 | ppl 1.27 | wps 6671.1 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 47466 | lr 0.000290294 | gnorm 0.285 | train_wall 40 | gb_free 10.2 | wall 49063
2022-08-17 11:49:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:49:27 | INFO | fairseq.trainer | begin training epoch 587
2022-08-17 11:49:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:49:46 | INFO | train_inner | epoch 587:     34 / 81 loss=3.381, nll_loss=0.347, ppl=1.27, wps=7170.7, ups=1.3, wpb=5522.8, bsz=353.5, num_updates=47500, lr=0.000290191, gnorm=0.298, train_wall=50, gb_free=10.1, wall=49082
2022-08-17 11:50:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:50:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:50:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:50:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:50:18 | INFO | valid | epoch 587 | valid on 'valid' subset | loss 5.161 | nll_loss 2.557 | ppl 5.88 | bleu 56.29 | wps 1726.1 | wpb 933.5 | bsz 59.6 | num_updates 47547 | best_bleu 57.47
2022-08-17 11:50:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 587 @ 47547 updates
2022-08-17 11:50:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint587.pt
2022-08-17 11:50:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint587.pt
2022-08-17 11:50:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint587.pt (epoch 587 @ 47547 updates, score 56.29) (writing took 24.01517541334033 seconds)
2022-08-17 11:50:43 | INFO | fairseq_cli.train | end of epoch 587 (average epoch stats below)
2022-08-17 11:50:43 | INFO | train | epoch 587 | loss 3.381 | nll_loss 0.347 | ppl 1.27 | wps 5926.7 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 47547 | lr 0.000290047 | gnorm 0.351 | train_wall 39 | gb_free 10.2 | wall 49139
2022-08-17 11:50:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:50:43 | INFO | fairseq.trainer | begin training epoch 588
2022-08-17 11:50:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:51:11 | INFO | train_inner | epoch 588:     53 / 81 loss=3.381, nll_loss=0.347, ppl=1.27, wps=6563.4, ups=1.18, wpb=5552.3, bsz=361.6, num_updates=47600, lr=0.000289886, gnorm=0.335, train_wall=48, gb_free=10, wall=49167
2022-08-17 11:51:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:51:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:51:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:51:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:51:35 | INFO | valid | epoch 588 | valid on 'valid' subset | loss 5.141 | nll_loss 2.532 | ppl 5.78 | bleu 56.89 | wps 1628.7 | wpb 933.5 | bsz 59.6 | num_updates 47628 | best_bleu 57.47
2022-08-17 11:51:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 588 @ 47628 updates
2022-08-17 11:51:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint588.pt
2022-08-17 11:51:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint588.pt
2022-08-17 11:52:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint588.pt (epoch 588 @ 47628 updates, score 56.89) (writing took 42.02116998657584 seconds)
2022-08-17 11:52:17 | INFO | fairseq_cli.train | end of epoch 588 (average epoch stats below)
2022-08-17 11:52:17 | INFO | train | epoch 588 | loss 3.381 | nll_loss 0.347 | ppl 1.27 | wps 4715.3 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 47628 | lr 0.0002898 | gnorm 0.303 | train_wall 40 | gb_free 10.2 | wall 49234
2022-08-17 11:52:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:52:18 | INFO | fairseq.trainer | begin training epoch 589
2022-08-17 11:52:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:52:55 | INFO | train_inner | epoch 589:     72 / 81 loss=3.38, nll_loss=0.346, ppl=1.27, wps=5294.4, ups=0.96, wpb=5525.3, bsz=356.9, num_updates=47700, lr=0.000289581, gnorm=0.303, train_wall=50, gb_free=10.1, wall=49271
2022-08-17 11:52:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:53:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:53:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:53:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:53:09 | INFO | valid | epoch 589 | valid on 'valid' subset | loss 5.139 | nll_loss 2.522 | ppl 5.75 | bleu 56.66 | wps 1741.6 | wpb 933.5 | bsz 59.6 | num_updates 47709 | best_bleu 57.47
2022-08-17 11:53:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 589 @ 47709 updates
2022-08-17 11:53:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint589.pt
2022-08-17 11:53:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint589.pt
2022-08-17 11:53:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint589.pt (epoch 589 @ 47709 updates, score 56.66) (writing took 2.3953951708972454 seconds)
2022-08-17 11:53:11 | INFO | fairseq_cli.train | end of epoch 589 (average epoch stats below)
2022-08-17 11:53:11 | INFO | train | epoch 589 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 8326.7 | ups 1.51 | wpb 5523.2 | bsz 358 | num_updates 47709 | lr 0.000289554 | gnorm 0.306 | train_wall 39 | gb_free 10.1 | wall 49287
2022-08-17 11:53:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:53:11 | INFO | fairseq.trainer | begin training epoch 590
2022-08-17 11:53:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:53:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:53:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:53:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:53:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:54:02 | INFO | valid | epoch 590 | valid on 'valid' subset | loss 5.135 | nll_loss 2.52 | ppl 5.74 | bleu 56.54 | wps 1780.9 | wpb 933.5 | bsz 59.6 | num_updates 47790 | best_bleu 57.47
2022-08-17 11:54:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 590 @ 47790 updates
2022-08-17 11:54:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint590.pt
2022-08-17 11:54:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint590.pt
2022-08-17 11:54:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint590.pt (epoch 590 @ 47790 updates, score 56.54) (writing took 14.31394561380148 seconds)
2022-08-17 11:54:17 | INFO | fairseq_cli.train | end of epoch 590 (average epoch stats below)
2022-08-17 11:54:17 | INFO | train | epoch 590 | loss 3.379 | nll_loss 0.345 | ppl 1.27 | wps 6845.5 | ups 1.24 | wpb 5523.2 | bsz 358 | num_updates 47790 | lr 0.000289309 | gnorm 0.286 | train_wall 39 | gb_free 10.1 | wall 49353
2022-08-17 11:54:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:54:17 | INFO | fairseq.trainer | begin training epoch 591
2022-08-17 11:54:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:54:23 | INFO | train_inner | epoch 591:     10 / 81 loss=3.379, nll_loss=0.345, ppl=1.27, wps=6236.7, ups=1.14, wpb=5488.3, bsz=357, num_updates=47800, lr=0.000289278, gnorm=0.288, train_wall=48, gb_free=10, wall=49359
2022-08-17 11:54:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:55:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:55:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:55:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:55:09 | INFO | valid | epoch 591 | valid on 'valid' subset | loss 5.14 | nll_loss 2.527 | ppl 5.76 | bleu 56.97 | wps 1805 | wpb 933.5 | bsz 59.6 | num_updates 47871 | best_bleu 57.47
2022-08-17 11:55:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 591 @ 47871 updates
2022-08-17 11:55:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint591.pt
2022-08-17 11:55:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint591.pt
2022-08-17 11:55:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint591.pt (epoch 591 @ 47871 updates, score 56.97) (writing took 32.38773176074028 seconds)
2022-08-17 11:55:42 | INFO | fairseq_cli.train | end of epoch 591 (average epoch stats below)
2022-08-17 11:55:42 | INFO | train | epoch 591 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 5261.3 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 47871 | lr 0.000289064 | gnorm 0.288 | train_wall 41 | gb_free 10.1 | wall 49438
2022-08-17 11:55:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:55:42 | INFO | fairseq.trainer | begin training epoch 592
2022-08-17 11:55:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:55:59 | INFO | train_inner | epoch 592:     29 / 81 loss=3.38, nll_loss=0.346, ppl=1.27, wps=5771.3, ups=1.05, wpb=5516.6, bsz=357.4, num_updates=47900, lr=0.000288976, gnorm=0.295, train_wall=50, gb_free=10.1, wall=49455
2022-08-17 11:56:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:56:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:56:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:56:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:56:35 | INFO | valid | epoch 592 | valid on 'valid' subset | loss 5.144 | nll_loss 2.535 | ppl 5.8 | bleu 57.06 | wps 1866.3 | wpb 933.5 | bsz 59.6 | num_updates 47952 | best_bleu 57.47
2022-08-17 11:56:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 592 @ 47952 updates
2022-08-17 11:56:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint592.pt
2022-08-17 11:56:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint592.pt
2022-08-17 11:56:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint592.pt (epoch 592 @ 47952 updates, score 57.06) (writing took 17.234528731554747 seconds)
2022-08-17 11:56:52 | INFO | fairseq_cli.train | end of epoch 592 (average epoch stats below)
2022-08-17 11:56:52 | INFO | train | epoch 592 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 6353.5 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 47952 | lr 0.00028882 | gnorm 0.31 | train_wall 40 | gb_free 10.2 | wall 49508
2022-08-17 11:56:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:56:52 | INFO | fairseq.trainer | begin training epoch 593
2022-08-17 11:56:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:57:18 | INFO | train_inner | epoch 593:     48 / 81 loss=3.38, nll_loss=0.346, ppl=1.27, wps=7005.2, ups=1.26, wpb=5568.6, bsz=357.4, num_updates=48000, lr=0.000288675, gnorm=0.298, train_wall=50, gb_free=10, wall=49534
2022-08-17 11:57:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:57:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:57:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:57:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:57:44 | INFO | valid | epoch 593 | valid on 'valid' subset | loss 5.144 | nll_loss 2.538 | ppl 5.81 | bleu 56.19 | wps 1722 | wpb 933.5 | bsz 59.6 | num_updates 48033 | best_bleu 57.47
2022-08-17 11:57:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 593 @ 48033 updates
2022-08-17 11:57:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint593.pt
2022-08-17 11:57:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint593.pt
2022-08-17 11:58:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint593.pt (epoch 593 @ 48033 updates, score 56.19) (writing took 26.972094107419252 seconds)
2022-08-17 11:58:11 | INFO | fairseq_cli.train | end of epoch 593 (average epoch stats below)
2022-08-17 11:58:11 | INFO | train | epoch 593 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 5650.7 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 48033 | lr 0.000288576 | gnorm 0.294 | train_wall 40 | gb_free 10.1 | wall 49587
2022-08-17 11:58:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:58:11 | INFO | fairseq.trainer | begin training epoch 594
2022-08-17 11:58:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:58:46 | INFO | train_inner | epoch 594:     67 / 81 loss=3.38, nll_loss=0.347, ppl=1.27, wps=6261.1, ups=1.13, wpb=5518.4, bsz=364.1, num_updates=48100, lr=0.000288375, gnorm=0.321, train_wall=49, gb_free=10, wall=49622
2022-08-17 11:58:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:58:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:58:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:58:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:59:03 | INFO | valid | epoch 594 | valid on 'valid' subset | loss 5.134 | nll_loss 2.521 | ppl 5.74 | bleu 56.74 | wps 1549.8 | wpb 933.5 | bsz 59.6 | num_updates 48114 | best_bleu 57.47
2022-08-17 11:59:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 594 @ 48114 updates
2022-08-17 11:59:03 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint594.pt
2022-08-17 11:59:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint594.pt
2022-08-17 11:59:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint594.pt (epoch 594 @ 48114 updates, score 56.74) (writing took 2.468146339058876 seconds)
2022-08-17 11:59:06 | INFO | fairseq_cli.train | end of epoch 594 (average epoch stats below)
2022-08-17 11:59:06 | INFO | train | epoch 594 | loss 3.381 | nll_loss 0.347 | ppl 1.27 | wps 8160 | ups 1.48 | wpb 5523.2 | bsz 358 | num_updates 48114 | lr 0.000288333 | gnorm 0.329 | train_wall 40 | gb_free 10.1 | wall 49642
2022-08-17 11:59:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 11:59:06 | INFO | fairseq.trainer | begin training epoch 595
2022-08-17 11:59:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 11:59:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 11:59:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 11:59:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 11:59:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 11:59:57 | INFO | valid | epoch 595 | valid on 'valid' subset | loss 5.145 | nll_loss 2.536 | ppl 5.8 | bleu 56.78 | wps 1708.8 | wpb 933.5 | bsz 59.6 | num_updates 48195 | best_bleu 57.47
2022-08-17 11:59:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 595 @ 48195 updates
2022-08-17 11:59:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint595.pt
2022-08-17 11:59:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint595.pt
2022-08-17 12:00:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint595.pt (epoch 595 @ 48195 updates, score 56.78) (writing took 2.536526381969452 seconds)
2022-08-17 12:00:00 | INFO | fairseq_cli.train | end of epoch 595 (average epoch stats below)
2022-08-17 12:00:00 | INFO | train | epoch 595 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 8291.5 | ups 1.5 | wpb 5523.2 | bsz 358 | num_updates 48195 | lr 0.000288091 | gnorm 0.291 | train_wall 40 | gb_free 10.3 | wall 49696
2022-08-17 12:00:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:00:00 | INFO | fairseq.trainer | begin training epoch 596
2022-08-17 12:00:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:00:04 | INFO | train_inner | epoch 596:      5 / 81 loss=3.38, nll_loss=0.346, ppl=1.27, wps=7069.2, ups=1.29, wpb=5481.8, bsz=353.8, num_updates=48200, lr=0.000288076, gnorm=0.294, train_wall=48, gb_free=10.1, wall=49700
2022-08-17 12:00:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:00:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:00:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:00:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:00:52 | INFO | valid | epoch 596 | valid on 'valid' subset | loss 5.134 | nll_loss 2.519 | ppl 5.73 | bleu 56.91 | wps 1726.7 | wpb 933.5 | bsz 59.6 | num_updates 48276 | best_bleu 57.47
2022-08-17 12:00:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 596 @ 48276 updates
2022-08-17 12:00:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint596.pt
2022-08-17 12:00:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint596.pt
2022-08-17 12:01:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint596.pt (epoch 596 @ 48276 updates, score 56.91) (writing took 25.095997348427773 seconds)
2022-08-17 12:01:17 | INFO | fairseq_cli.train | end of epoch 596 (average epoch stats below)
2022-08-17 12:01:17 | INFO | train | epoch 596 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 5790.8 | ups 1.05 | wpb 5523.2 | bsz 358 | num_updates 48276 | lr 0.000287849 | gnorm 0.32 | train_wall 40 | gb_free 10.1 | wall 49773
2022-08-17 12:01:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:01:17 | INFO | fairseq.trainer | begin training epoch 597
2022-08-17 12:01:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:01:31 | INFO | train_inner | epoch 597:     24 / 81 loss=3.38, nll_loss=0.346, ppl=1.27, wps=6375.4, ups=1.15, wpb=5539.1, bsz=356.3, num_updates=48300, lr=0.000287777, gnorm=0.317, train_wall=50, gb_free=10.1, wall=49787
2022-08-17 12:01:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:02:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:02:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:02:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:02:09 | INFO | valid | epoch 597 | valid on 'valid' subset | loss 5.125 | nll_loss 2.507 | ppl 5.68 | bleu 56.86 | wps 1836.3 | wpb 933.5 | bsz 59.6 | num_updates 48357 | best_bleu 57.47
2022-08-17 12:02:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 597 @ 48357 updates
2022-08-17 12:02:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint597.pt
2022-08-17 12:02:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint597.pt
2022-08-17 12:02:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint597.pt (epoch 597 @ 48357 updates, score 56.86) (writing took 18.287889380007982 seconds)
2022-08-17 12:02:27 | INFO | fairseq_cli.train | end of epoch 597 (average epoch stats below)
2022-08-17 12:02:27 | INFO | train | epoch 597 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 6411 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 48357 | lr 0.000287608 | gnorm 0.309 | train_wall 40 | gb_free 10.2 | wall 49843
2022-08-17 12:02:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:02:27 | INFO | fairseq.trainer | begin training epoch 598
2022-08-17 12:02:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:02:50 | INFO | train_inner | epoch 598:     43 / 81 loss=3.38, nll_loss=0.346, ppl=1.27, wps=6936.8, ups=1.25, wpb=5530.4, bsz=361.9, num_updates=48400, lr=0.00028748, gnorm=0.306, train_wall=50, gb_free=10, wall=49867
2022-08-17 12:03:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:03:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:03:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:03:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:03:18 | INFO | valid | epoch 598 | valid on 'valid' subset | loss 5.14 | nll_loss 2.528 | ppl 5.77 | bleu 57.18 | wps 1885.4 | wpb 933.5 | bsz 59.6 | num_updates 48438 | best_bleu 57.47
2022-08-17 12:03:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 598 @ 48438 updates
2022-08-17 12:03:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint598.pt
2022-08-17 12:03:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint598.pt
2022-08-17 12:03:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint598.pt (epoch 598 @ 48438 updates, score 57.18) (writing took 16.388838559389114 seconds)
2022-08-17 12:03:35 | INFO | fairseq_cli.train | end of epoch 598 (average epoch stats below)
2022-08-17 12:03:35 | INFO | train | epoch 598 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 6599.2 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 48438 | lr 0.000287367 | gnorm 0.292 | train_wall 40 | gb_free 10.2 | wall 49911
2022-08-17 12:03:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:03:35 | INFO | fairseq.trainer | begin training epoch 599
2022-08-17 12:03:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:04:08 | INFO | train_inner | epoch 599:     62 / 81 loss=3.379, nll_loss=0.345, ppl=1.27, wps=7057.3, ups=1.28, wpb=5508.8, bsz=359.4, num_updates=48500, lr=0.000287183, gnorm=0.309, train_wall=49, gb_free=10, wall=49945
2022-08-17 12:04:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:04:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:04:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:04:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:04:27 | INFO | valid | epoch 599 | valid on 'valid' subset | loss 5.15 | nll_loss 2.54 | ppl 5.82 | bleu 57.18 | wps 1862.1 | wpb 933.5 | bsz 59.6 | num_updates 48519 | best_bleu 57.47
2022-08-17 12:04:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 599 @ 48519 updates
2022-08-17 12:04:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint599.pt
2022-08-17 12:04:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint599.pt
2022-08-17 12:04:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint599.pt (epoch 599 @ 48519 updates, score 57.18) (writing took 15.406453151255846 seconds)
2022-08-17 12:04:42 | INFO | fairseq_cli.train | end of epoch 599 (average epoch stats below)
2022-08-17 12:04:42 | INFO | train | epoch 599 | loss 3.379 | nll_loss 0.345 | ppl 1.27 | wps 6633.9 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 48519 | lr 0.000287127 | gnorm 0.317 | train_wall 39 | gb_free 10.1 | wall 49979
2022-08-17 12:04:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:04:42 | INFO | fairseq.trainer | begin training epoch 600
2022-08-17 12:04:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:05:23 | INFO | train_inner | epoch 600:     81 / 81 loss=3.38, nll_loss=0.346, ppl=1.27, wps=7386.1, ups=1.34, wpb=5509.1, bsz=353.6, num_updates=48600, lr=0.000286888, gnorm=0.295, train_wall=48, gb_free=10.1, wall=50019
2022-08-17 12:05:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:05:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:05:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:05:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:05:33 | INFO | valid | epoch 600 | valid on 'valid' subset | loss 5.14 | nll_loss 2.532 | ppl 5.78 | bleu 56.49 | wps 1766.4 | wpb 933.5 | bsz 59.6 | num_updates 48600 | best_bleu 57.47
2022-08-17 12:05:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 600 @ 48600 updates
2022-08-17 12:05:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint600.pt
2022-08-17 12:05:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint600.pt
2022-08-17 12:05:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint600.pt (epoch 600 @ 48600 updates, score 56.49) (writing took 21.899244654923677 seconds)
2022-08-17 12:05:55 | INFO | fairseq_cli.train | end of epoch 600 (average epoch stats below)
2022-08-17 12:05:55 | INFO | train | epoch 600 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 6164.1 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 48600 | lr 0.000286888 | gnorm 0.288 | train_wall 39 | gb_free 10.1 | wall 50051
2022-08-17 12:05:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:05:55 | INFO | fairseq.trainer | begin training epoch 601
2022-08-17 12:05:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:06:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:06:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:06:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:06:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:06:49 | INFO | valid | epoch 601 | valid on 'valid' subset | loss 5.171 | nll_loss 2.563 | ppl 5.91 | bleu 56.9 | wps 1647.9 | wpb 933.5 | bsz 59.6 | num_updates 48681 | best_bleu 57.47
2022-08-17 12:06:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 601 @ 48681 updates
2022-08-17 12:06:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint601.pt
2022-08-17 12:06:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint601.pt
2022-08-17 12:07:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint601.pt (epoch 601 @ 48681 updates, score 56.9) (writing took 34.224238350987434 seconds)
2022-08-17 12:07:24 | INFO | fairseq_cli.train | end of epoch 601 (average epoch stats below)
2022-08-17 12:07:24 | INFO | train | epoch 601 | loss 3.379 | nll_loss 0.346 | ppl 1.27 | wps 5046.4 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 48681 | lr 0.000286649 | gnorm 0.29 | train_wall 40 | gb_free 10.1 | wall 50140
2022-08-17 12:07:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:07:24 | INFO | fairseq.trainer | begin training epoch 602
2022-08-17 12:07:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:07:34 | INFO | train_inner | epoch 602:     19 / 81 loss=3.379, nll_loss=0.345, ppl=1.27, wps=4235.4, ups=0.76, wpb=5552.7, bsz=359.4, num_updates=48700, lr=0.000286593, gnorm=0.286, train_wall=49, gb_free=10.1, wall=50150
2022-08-17 12:08:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:08:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:08:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:08:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:08:15 | INFO | valid | epoch 602 | valid on 'valid' subset | loss 5.162 | nll_loss 2.556 | ppl 5.88 | bleu 56.96 | wps 1686.1 | wpb 933.5 | bsz 59.6 | num_updates 48762 | best_bleu 57.47
2022-08-17 12:08:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 602 @ 48762 updates
2022-08-17 12:08:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint602.pt
2022-08-17 12:08:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint602.pt
2022-08-17 12:08:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint602.pt (epoch 602 @ 48762 updates, score 56.96) (writing took 22.330549828708172 seconds)
2022-08-17 12:08:38 | INFO | fairseq_cli.train | end of epoch 602 (average epoch stats below)
2022-08-17 12:08:38 | INFO | train | epoch 602 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 6040.4 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 48762 | lr 0.000286411 | gnorm 0.313 | train_wall 40 | gb_free 10.1 | wall 50214
2022-08-17 12:08:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:08:38 | INFO | fairseq.trainer | begin training epoch 603
2022-08-17 12:08:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:09:00 | INFO | train_inner | epoch 603:     38 / 81 loss=3.38, nll_loss=0.346, ppl=1.27, wps=6437.1, ups=1.17, wpb=5503.4, bsz=359, num_updates=48800, lr=0.000286299, gnorm=0.33, train_wall=49, gb_free=10.1, wall=50236
2022-08-17 12:09:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:09:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:09:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:09:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:09:33 | INFO | valid | epoch 603 | valid on 'valid' subset | loss 5.15 | nll_loss 2.536 | ppl 5.8 | bleu 56.73 | wps 1746.2 | wpb 933.5 | bsz 59.6 | num_updates 48843 | best_bleu 57.47
2022-08-17 12:09:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 603 @ 48843 updates
2022-08-17 12:09:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint603.pt
2022-08-17 12:09:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint603.pt
2022-08-17 12:10:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint603.pt (epoch 603 @ 48843 updates, score 56.73) (writing took 36.734164863824844 seconds)
2022-08-17 12:10:10 | INFO | fairseq_cli.train | end of epoch 603 (average epoch stats below)
2022-08-17 12:10:10 | INFO | train | epoch 603 | loss 3.38 | nll_loss 0.347 | ppl 1.27 | wps 4865.3 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 48843 | lr 0.000286173 | gnorm 0.324 | train_wall 40 | gb_free 10.1 | wall 50306
2022-08-17 12:10:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:10:10 | INFO | fairseq.trainer | begin training epoch 604
2022-08-17 12:10:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:10:40 | INFO | train_inner | epoch 604:     57 / 81 loss=3.38, nll_loss=0.346, ppl=1.27, wps=5515.2, ups=0.99, wpb=5546.4, bsz=355.8, num_updates=48900, lr=0.000286006, gnorm=0.293, train_wall=50, gb_free=10, wall=50336
2022-08-17 12:10:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:10:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:10:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:10:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:11:02 | INFO | valid | epoch 604 | valid on 'valid' subset | loss 5.156 | nll_loss 2.549 | ppl 5.85 | bleu 56.89 | wps 1847.4 | wpb 933.5 | bsz 59.6 | num_updates 48924 | best_bleu 57.47
2022-08-17 12:11:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 604 @ 48924 updates
2022-08-17 12:11:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint604.pt
2022-08-17 12:11:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint604.pt
2022-08-17 12:11:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint604.pt (epoch 604 @ 48924 updates, score 56.89) (writing took 2.348452340811491 seconds)
2022-08-17 12:11:04 | INFO | fairseq_cli.train | end of epoch 604 (average epoch stats below)
2022-08-17 12:11:04 | INFO | train | epoch 604 | loss 3.38 | nll_loss 0.345 | ppl 1.27 | wps 8200.3 | ups 1.48 | wpb 5523.2 | bsz 358 | num_updates 48924 | lr 0.000285936 | gnorm 0.293 | train_wall 41 | gb_free 10.1 | wall 50360
2022-08-17 12:11:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:11:04 | INFO | fairseq.trainer | begin training epoch 605
2022-08-17 12:11:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:11:45 | INFO | train_inner | epoch 605:     76 / 81 loss=3.38, nll_loss=0.346, ppl=1.27, wps=8482.3, ups=1.54, wpb=5520.5, bsz=357.2, num_updates=49000, lr=0.000285714, gnorm=0.308, train_wall=51, gb_free=10, wall=50401
2022-08-17 12:11:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:11:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:11:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:11:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:11:57 | INFO | valid | epoch 605 | valid on 'valid' subset | loss 5.158 | nll_loss 2.549 | ppl 5.85 | bleu 56.91 | wps 1810.3 | wpb 933.5 | bsz 59.6 | num_updates 49005 | best_bleu 57.47
2022-08-17 12:11:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 605 @ 49005 updates
2022-08-17 12:11:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint605.pt
2022-08-17 12:11:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint605.pt
2022-08-17 12:12:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint605.pt (epoch 605 @ 49005 updates, score 56.91) (writing took 17.12865725159645 seconds)
2022-08-17 12:12:15 | INFO | fairseq_cli.train | end of epoch 605 (average epoch stats below)
2022-08-17 12:12:15 | INFO | train | epoch 605 | loss 3.379 | nll_loss 0.345 | ppl 1.27 | wps 6343.5 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 49005 | lr 0.0002857 | gnorm 0.312 | train_wall 42 | gb_free 10.1 | wall 50431
2022-08-17 12:12:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:12:15 | INFO | fairseq.trainer | begin training epoch 606
2022-08-17 12:12:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:12:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:13:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:13:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:13:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:13:08 | INFO | valid | epoch 606 | valid on 'valid' subset | loss 5.145 | nll_loss 2.536 | ppl 5.8 | bleu 56.6 | wps 1807.1 | wpb 933.5 | bsz 59.6 | num_updates 49086 | best_bleu 57.47
2022-08-17 12:13:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 606 @ 49086 updates
2022-08-17 12:13:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint606.pt
2022-08-17 12:13:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint606.pt
2022-08-17 12:13:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint606.pt (epoch 606 @ 49086 updates, score 56.6) (writing took 14.424621511250734 seconds)
2022-08-17 12:13:23 | INFO | fairseq_cli.train | end of epoch 606 (average epoch stats below)
2022-08-17 12:13:23 | INFO | train | epoch 606 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 6565.8 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 49086 | lr 0.000285464 | gnorm 0.33 | train_wall 40 | gb_free 10.2 | wall 50499
2022-08-17 12:13:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:13:23 | INFO | fairseq.trainer | begin training epoch 607
2022-08-17 12:13:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:13:31 | INFO | train_inner | epoch 607:     14 / 81 loss=3.379, nll_loss=0.345, ppl=1.27, wps=5217.3, ups=0.94, wpb=5523.1, bsz=362.2, num_updates=49100, lr=0.000285423, gnorm=0.326, train_wall=50, gb_free=10, wall=50507
2022-08-17 12:14:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:14:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:14:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:14:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:14:16 | INFO | valid | epoch 607 | valid on 'valid' subset | loss 5.167 | nll_loss 2.562 | ppl 5.91 | bleu 56.19 | wps 1734.6 | wpb 933.5 | bsz 59.6 | num_updates 49167 | best_bleu 57.47
2022-08-17 12:14:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 607 @ 49167 updates
2022-08-17 12:14:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint607.pt
2022-08-17 12:14:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint607.pt
2022-08-17 12:14:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint607.pt (epoch 607 @ 49167 updates, score 56.19) (writing took 22.257996078580618 seconds)
2022-08-17 12:14:38 | INFO | fairseq_cli.train | end of epoch 607 (average epoch stats below)
2022-08-17 12:14:38 | INFO | train | epoch 607 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 5938.6 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 49167 | lr 0.000285229 | gnorm 0.306 | train_wall 41 | gb_free 10.1 | wall 50574
2022-08-17 12:14:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:14:38 | INFO | fairseq.trainer | begin training epoch 608
2022-08-17 12:14:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:14:56 | INFO | train_inner | epoch 608:     33 / 81 loss=3.38, nll_loss=0.346, ppl=1.27, wps=6503.7, ups=1.18, wpb=5500.5, bsz=352.2, num_updates=49200, lr=0.000285133, gnorm=0.302, train_wall=50, gb_free=10.1, wall=50592
2022-08-17 12:15:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:15:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:15:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:15:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:15:29 | INFO | valid | epoch 608 | valid on 'valid' subset | loss 5.147 | nll_loss 2.541 | ppl 5.82 | bleu 56.75 | wps 1828.7 | wpb 933.5 | bsz 59.6 | num_updates 49248 | best_bleu 57.47
2022-08-17 12:15:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 608 @ 49248 updates
2022-08-17 12:15:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint608.pt
2022-08-17 12:15:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint608.pt
2022-08-17 12:16:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint608.pt (epoch 608 @ 49248 updates, score 56.75) (writing took 36.447268925607204 seconds)
2022-08-17 12:16:06 | INFO | fairseq_cli.train | end of epoch 608 (average epoch stats below)
2022-08-17 12:16:06 | INFO | train | epoch 608 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 5103.8 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 49248 | lr 0.000284994 | gnorm 0.303 | train_wall 39 | gb_free 10.3 | wall 50662
2022-08-17 12:16:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:16:06 | INFO | fairseq.trainer | begin training epoch 609
2022-08-17 12:16:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:16:33 | INFO | train_inner | epoch 609:     52 / 81 loss=3.38, nll_loss=0.346, ppl=1.27, wps=5723.6, ups=1.03, wpb=5561.2, bsz=361, num_updates=49300, lr=0.000284844, gnorm=0.435, train_wall=49, gb_free=10, wall=50689
2022-08-17 12:16:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:16:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:16:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:16:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:16:56 | INFO | valid | epoch 609 | valid on 'valid' subset | loss 5.138 | nll_loss 2.524 | ppl 5.75 | bleu 56.6 | wps 1782.1 | wpb 933.5 | bsz 59.6 | num_updates 49329 | best_bleu 57.47
2022-08-17 12:16:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 609 @ 49329 updates
2022-08-17 12:16:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint609.pt
2022-08-17 12:16:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint609.pt
2022-08-17 12:17:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint609.pt (epoch 609 @ 49329 updates, score 56.6) (writing took 19.90074123069644 seconds)
2022-08-17 12:17:17 | INFO | fairseq_cli.train | end of epoch 609 (average epoch stats below)
2022-08-17 12:17:17 | INFO | train | epoch 609 | loss 3.38 | nll_loss 0.347 | ppl 1.27 | wps 6324.7 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 49329 | lr 0.00028476 | gnorm 0.463 | train_wall 39 | gb_free 10.1 | wall 50733
2022-08-17 12:17:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:17:17 | INFO | fairseq.trainer | begin training epoch 610
2022-08-17 12:17:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:17:54 | INFO | train_inner | epoch 610:     71 / 81 loss=3.381, nll_loss=0.347, ppl=1.27, wps=6764.1, ups=1.23, wpb=5498, bsz=358.8, num_updates=49400, lr=0.000284555, gnorm=0.37, train_wall=49, gb_free=10, wall=50770
2022-08-17 12:17:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:18:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:18:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:18:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:18:09 | INFO | valid | epoch 610 | valid on 'valid' subset | loss 5.145 | nll_loss 2.533 | ppl 5.79 | bleu 57.15 | wps 1812.6 | wpb 933.5 | bsz 59.6 | num_updates 49410 | best_bleu 57.47
2022-08-17 12:18:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 610 @ 49410 updates
2022-08-17 12:18:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint610.pt
2022-08-17 12:18:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint610.pt
2022-08-17 12:18:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint610.pt (epoch 610 @ 49410 updates, score 57.15) (writing took 20.275952145457268 seconds)
2022-08-17 12:18:29 | INFO | fairseq_cli.train | end of epoch 610 (average epoch stats below)
2022-08-17 12:18:29 | INFO | train | epoch 610 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 6163.3 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 49410 | lr 0.000284526 | gnorm 0.377 | train_wall 41 | gb_free 10.1 | wall 50805
2022-08-17 12:18:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:18:29 | INFO | fairseq.trainer | begin training epoch 611
2022-08-17 12:18:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:19:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:19:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:19:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:19:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:19:21 | INFO | valid | epoch 611 | valid on 'valid' subset | loss 5.129 | nll_loss 2.518 | ppl 5.73 | bleu 56.99 | wps 1738 | wpb 933.5 | bsz 59.6 | num_updates 49491 | best_bleu 57.47
2022-08-17 12:19:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 611 @ 49491 updates
2022-08-17 12:19:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint611.pt
2022-08-17 12:19:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint611.pt
2022-08-17 12:19:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint611.pt (epoch 611 @ 49491 updates, score 56.99) (writing took 23.234136022627354 seconds)
2022-08-17 12:19:44 | INFO | fairseq_cli.train | end of epoch 611 (average epoch stats below)
2022-08-17 12:19:44 | INFO | train | epoch 611 | loss 3.379 | nll_loss 0.345 | ppl 1.27 | wps 5959.2 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 49491 | lr 0.000284293 | gnorm 0.3 | train_wall 40 | gb_free 10.1 | wall 50880
2022-08-17 12:19:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:19:44 | INFO | fairseq.trainer | begin training epoch 612
2022-08-17 12:19:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:19:50 | INFO | train_inner | epoch 612:      9 / 81 loss=3.379, nll_loss=0.345, ppl=1.27, wps=4737.7, ups=0.86, wpb=5508.2, bsz=357.8, num_updates=49500, lr=0.000284268, gnorm=0.295, train_wall=50, gb_free=10.2, wall=50887
2022-08-17 12:20:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:20:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:20:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:20:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:20:36 | INFO | valid | epoch 612 | valid on 'valid' subset | loss 5.155 | nll_loss 2.551 | ppl 5.86 | bleu 56.09 | wps 1802.7 | wpb 933.5 | bsz 59.6 | num_updates 49572 | best_bleu 57.47
2022-08-17 12:20:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 612 @ 49572 updates
2022-08-17 12:20:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint612.pt
2022-08-17 12:20:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint612.pt
2022-08-17 12:20:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint612.pt (epoch 612 @ 49572 updates, score 56.09) (writing took 2.607143010944128 seconds)
2022-08-17 12:20:38 | INFO | fairseq_cli.train | end of epoch 612 (average epoch stats below)
2022-08-17 12:20:38 | INFO | train | epoch 612 | loss 3.379 | nll_loss 0.346 | ppl 1.27 | wps 8262.8 | ups 1.5 | wpb 5523.2 | bsz 358 | num_updates 49572 | lr 0.000284061 | gnorm 0.268 | train_wall 40 | gb_free 10.1 | wall 50935
2022-08-17 12:20:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:20:39 | INFO | fairseq.trainer | begin training epoch 613
2022-08-17 12:20:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:20:54 | INFO | train_inner | epoch 613:     28 / 81 loss=3.379, nll_loss=0.345, ppl=1.27, wps=8709.8, ups=1.57, wpb=5545.6, bsz=364.6, num_updates=49600, lr=0.000283981, gnorm=0.263, train_wall=50, gb_free=10.1, wall=50950
2022-08-17 12:21:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:21:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:21:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:21:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:21:30 | INFO | valid | epoch 613 | valid on 'valid' subset | loss 5.127 | nll_loss 2.513 | ppl 5.71 | bleu 57.09 | wps 1800.5 | wpb 933.5 | bsz 59.6 | num_updates 49653 | best_bleu 57.47
2022-08-17 12:21:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 613 @ 49653 updates
2022-08-17 12:21:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint613.pt
2022-08-17 12:21:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint613.pt
2022-08-17 12:22:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint613.pt (epoch 613 @ 49653 updates, score 57.09) (writing took 34.72358867153525 seconds)
2022-08-17 12:22:05 | INFO | fairseq_cli.train | end of epoch 613 (average epoch stats below)
2022-08-17 12:22:05 | INFO | train | epoch 613 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 5150.6 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 49653 | lr 0.000283829 | gnorm 0.338 | train_wall 41 | gb_free 10.1 | wall 51021
2022-08-17 12:22:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:22:05 | INFO | fairseq.trainer | begin training epoch 614
2022-08-17 12:22:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:22:30 | INFO | train_inner | epoch 614:     47 / 81 loss=3.38, nll_loss=0.346, ppl=1.27, wps=5776.5, ups=1.04, wpb=5534.8, bsz=352.2, num_updates=49700, lr=0.000283695, gnorm=0.365, train_wall=49, gb_free=10, wall=51046
2022-08-17 12:22:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:22:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:22:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:22:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:22:56 | INFO | valid | epoch 614 | valid on 'valid' subset | loss 5.15 | nll_loss 2.541 | ppl 5.82 | bleu 57.13 | wps 1799.7 | wpb 933.5 | bsz 59.6 | num_updates 49734 | best_bleu 57.47
2022-08-17 12:22:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 614 @ 49734 updates
2022-08-17 12:22:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint614.pt
2022-08-17 12:22:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint614.pt
2022-08-17 12:23:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint614.pt (epoch 614 @ 49734 updates, score 57.13) (writing took 27.640273604542017 seconds)
2022-08-17 12:23:24 | INFO | fairseq_cli.train | end of epoch 614 (average epoch stats below)
2022-08-17 12:23:24 | INFO | train | epoch 614 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 5664.2 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 49734 | lr 0.000283598 | gnorm 0.315 | train_wall 40 | gb_free 10.2 | wall 51100
2022-08-17 12:23:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:23:24 | INFO | fairseq.trainer | begin training epoch 615
2022-08-17 12:23:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:23:59 | INFO | train_inner | epoch 615:     66 / 81 loss=3.38, nll_loss=0.346, ppl=1.27, wps=6163.3, ups=1.12, wpb=5514.2, bsz=356.7, num_updates=49800, lr=0.00028341, gnorm=0.313, train_wall=50, gb_free=10.1, wall=51136
2022-08-17 12:24:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:24:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:24:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:24:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:24:17 | INFO | valid | epoch 615 | valid on 'valid' subset | loss 5.148 | nll_loss 2.541 | ppl 5.82 | bleu 56.78 | wps 1600.1 | wpb 933.5 | bsz 59.6 | num_updates 49815 | best_bleu 57.47
2022-08-17 12:24:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 615 @ 49815 updates
2022-08-17 12:24:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint615.pt
2022-08-17 12:24:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint615.pt
2022-08-17 12:24:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint615.pt (epoch 615 @ 49815 updates, score 56.78) (writing took 18.61419814825058 seconds)
2022-08-17 12:24:36 | INFO | fairseq_cli.train | end of epoch 615 (average epoch stats below)
2022-08-17 12:24:36 | INFO | train | epoch 615 | loss 3.379 | nll_loss 0.345 | ppl 1.27 | wps 6235.6 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 49815 | lr 0.000283367 | gnorm 0.322 | train_wall 40 | gb_free 10.2 | wall 51172
2022-08-17 12:24:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:24:36 | INFO | fairseq.trainer | begin training epoch 616
2022-08-17 12:24:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:25:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:25:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:25:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:25:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:25:27 | INFO | valid | epoch 616 | valid on 'valid' subset | loss 5.135 | nll_loss 2.524 | ppl 5.75 | bleu 57.26 | wps 1854.9 | wpb 933.5 | bsz 59.6 | num_updates 49896 | best_bleu 57.47
2022-08-17 12:25:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 616 @ 49896 updates
2022-08-17 12:25:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint616.pt
2022-08-17 12:25:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint616.pt
2022-08-17 12:25:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint616.pt (epoch 616 @ 49896 updates, score 57.26) (writing took 15.06332989782095 seconds)
2022-08-17 12:25:42 | INFO | fairseq_cli.train | end of epoch 616 (average epoch stats below)
2022-08-17 12:25:42 | INFO | train | epoch 616 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 6781.7 | ups 1.23 | wpb 5523.2 | bsz 358 | num_updates 49896 | lr 0.000283137 | gnorm 0.318 | train_wall 39 | gb_free 10.1 | wall 51238
2022-08-17 12:25:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:25:42 | INFO | fairseq.trainer | begin training epoch 617
2022-08-17 12:25:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:25:45 | INFO | train_inner | epoch 617:      4 / 81 loss=3.379, nll_loss=0.346, ppl=1.27, wps=5186.9, ups=0.94, wpb=5501.6, bsz=358.2, num_updates=49900, lr=0.000283126, gnorm=0.315, train_wall=49, gb_free=10.1, wall=51242
2022-08-17 12:26:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:26:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:26:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:26:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:26:37 | INFO | valid | epoch 617 | valid on 'valid' subset | loss 5.134 | nll_loss 2.527 | ppl 5.76 | bleu 56.71 | wps 1893.8 | wpb 933.5 | bsz 59.6 | num_updates 49977 | best_bleu 57.47
2022-08-17 12:26:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 617 @ 49977 updates
2022-08-17 12:26:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint617.pt
2022-08-17 12:26:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint617.pt
2022-08-17 12:26:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint617.pt (epoch 617 @ 49977 updates, score 56.71) (writing took 20.44143634662032 seconds)
2022-08-17 12:26:58 | INFO | fairseq_cli.train | end of epoch 617 (average epoch stats below)
2022-08-17 12:26:58 | INFO | train | epoch 617 | loss 3.379 | nll_loss 0.345 | ppl 1.27 | wps 5901.2 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 49977 | lr 0.000282908 | gnorm 0.498 | train_wall 41 | gb_free 10.1 | wall 51314
2022-08-17 12:26:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:26:58 | INFO | fairseq.trainer | begin training epoch 618
2022-08-17 12:26:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:27:10 | INFO | train_inner | epoch 618:     23 / 81 loss=3.379, nll_loss=0.345, ppl=1.27, wps=6533.6, ups=1.18, wpb=5547.9, bsz=355.6, num_updates=50000, lr=0.000282843, gnorm=0.476, train_wall=50, gb_free=10.1, wall=51327
2022-08-17 12:27:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:27:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:27:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:27:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:27:50 | INFO | valid | epoch 618 | valid on 'valid' subset | loss 5.136 | nll_loss 2.525 | ppl 5.75 | bleu 57.03 | wps 1689.2 | wpb 933.5 | bsz 59.6 | num_updates 50058 | best_bleu 57.47
2022-08-17 12:27:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 618 @ 50058 updates
2022-08-17 12:27:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint618.pt
2022-08-17 12:27:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint618.pt
2022-08-17 12:28:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint618.pt (epoch 618 @ 50058 updates, score 57.03) (writing took 42.37201212719083 seconds)
2022-08-17 12:28:32 | INFO | fairseq_cli.train | end of epoch 618 (average epoch stats below)
2022-08-17 12:28:32 | INFO | train | epoch 618 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 4725.8 | ups 0.86 | wpb 5523.2 | bsz 358 | num_updates 50058 | lr 0.000282679 | gnorm 0.329 | train_wall 40 | gb_free 10.1 | wall 51409
2022-08-17 12:28:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:28:33 | INFO | fairseq.trainer | begin training epoch 619
2022-08-17 12:28:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:28:55 | INFO | train_inner | epoch 619:     42 / 81 loss=3.379, nll_loss=0.345, ppl=1.27, wps=5253.1, ups=0.96, wpb=5492.1, bsz=359.8, num_updates=50100, lr=0.00028256, gnorm=0.315, train_wall=50, gb_free=10.1, wall=51431
2022-08-17 12:29:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:29:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:29:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:29:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:29:24 | INFO | valid | epoch 619 | valid on 'valid' subset | loss 5.142 | nll_loss 2.529 | ppl 5.77 | bleu 56.72 | wps 1850.2 | wpb 933.5 | bsz 59.6 | num_updates 50139 | best_bleu 57.47
2022-08-17 12:29:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 619 @ 50139 updates
2022-08-17 12:29:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint619.pt
2022-08-17 12:29:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint619.pt
2022-08-17 12:29:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint619.pt (epoch 619 @ 50139 updates, score 56.72) (writing took 22.374218333512545 seconds)
2022-08-17 12:29:47 | INFO | fairseq_cli.train | end of epoch 619 (average epoch stats below)
2022-08-17 12:29:47 | INFO | train | epoch 619 | loss 3.379 | nll_loss 0.345 | ppl 1.27 | wps 6005.2 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 50139 | lr 0.00028245 | gnorm 0.398 | train_wall 40 | gb_free 10.1 | wall 51483
2022-08-17 12:29:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:29:47 | INFO | fairseq.trainer | begin training epoch 620
2022-08-17 12:29:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:30:19 | INFO | train_inner | epoch 620:     61 / 81 loss=3.38, nll_loss=0.347, ppl=1.27, wps=6575.8, ups=1.18, wpb=5551.1, bsz=361.4, num_updates=50200, lr=0.000282279, gnorm=0.376, train_wall=51, gb_free=10.1, wall=51516
2022-08-17 12:30:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:30:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:30:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:30:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:30:40 | INFO | valid | epoch 620 | valid on 'valid' subset | loss 5.144 | nll_loss 2.537 | ppl 5.8 | bleu 57.02 | wps 1738.1 | wpb 933.5 | bsz 59.6 | num_updates 50220 | best_bleu 57.47
2022-08-17 12:30:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 620 @ 50220 updates
2022-08-17 12:30:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint620.pt
2022-08-17 12:30:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint620.pt
2022-08-17 12:30:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint620.pt (epoch 620 @ 50220 updates, score 57.02) (writing took 2.3807034976780415 seconds)
2022-08-17 12:30:42 | INFO | fairseq_cli.train | end of epoch 620 (average epoch stats below)
2022-08-17 12:30:42 | INFO | train | epoch 620 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 8103.9 | ups 1.47 | wpb 5523.2 | bsz 358 | num_updates 50220 | lr 0.000282223 | gnorm 0.302 | train_wall 41 | gb_free 10.1 | wall 51538
2022-08-17 12:30:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:30:42 | INFO | fairseq.trainer | begin training epoch 621
2022-08-17 12:30:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:31:26 | INFO | train_inner | epoch 621:     80 / 81 loss=3.379, nll_loss=0.345, ppl=1.27, wps=8237.4, ups=1.49, wpb=5522.6, bsz=356.6, num_updates=50300, lr=0.000281998, gnorm=0.271, train_wall=50, gb_free=10, wall=51583
2022-08-17 12:31:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:31:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:31:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:31:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:31:37 | INFO | valid | epoch 621 | valid on 'valid' subset | loss 5.141 | nll_loss 2.535 | ppl 5.79 | bleu 56.59 | wps 1816.2 | wpb 933.5 | bsz 59.6 | num_updates 50301 | best_bleu 57.47
2022-08-17 12:31:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 621 @ 50301 updates
2022-08-17 12:31:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint621.pt
2022-08-17 12:31:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint621.pt
2022-08-17 12:31:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint621.pt (epoch 621 @ 50301 updates, score 56.59) (writing took 22.25923489779234 seconds)
2022-08-17 12:31:59 | INFO | fairseq_cli.train | end of epoch 621 (average epoch stats below)
2022-08-17 12:31:59 | INFO | train | epoch 621 | loss 3.379 | nll_loss 0.344 | ppl 1.27 | wps 5818.6 | ups 1.05 | wpb 5523.2 | bsz 358 | num_updates 50301 | lr 0.000281995 | gnorm 0.268 | train_wall 41 | gb_free 10.3 | wall 51615
2022-08-17 12:31:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:31:59 | INFO | fairseq.trainer | begin training epoch 622
2022-08-17 12:31:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:32:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:32:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:32:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:32:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:32:51 | INFO | valid | epoch 622 | valid on 'valid' subset | loss 5.144 | nll_loss 2.533 | ppl 5.79 | bleu 56.93 | wps 1722.3 | wpb 933.5 | bsz 59.6 | num_updates 50382 | best_bleu 57.47
2022-08-17 12:32:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 622 @ 50382 updates
2022-08-17 12:32:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint622.pt
2022-08-17 12:32:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint622.pt
2022-08-17 12:33:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint622.pt (epoch 622 @ 50382 updates, score 56.93) (writing took 16.542368203401566 seconds)
2022-08-17 12:33:08 | INFO | fairseq_cli.train | end of epoch 622 (average epoch stats below)
2022-08-17 12:33:08 | INFO | train | epoch 622 | loss 3.378 | nll_loss 0.344 | ppl 1.27 | wps 6516.8 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 50382 | lr 0.000281768 | gnorm 0.29 | train_wall 40 | gb_free 10 | wall 51684
2022-08-17 12:33:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:33:08 | INFO | fairseq.trainer | begin training epoch 623
2022-08-17 12:33:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:33:18 | INFO | train_inner | epoch 623:     18 / 81 loss=3.378, nll_loss=0.344, ppl=1.27, wps=4936.8, ups=0.9, wpb=5506.9, bsz=357.8, num_updates=50400, lr=0.000281718, gnorm=0.289, train_wall=49, gb_free=10, wall=51694
2022-08-17 12:33:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:33:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:33:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:33:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:33:59 | INFO | valid | epoch 623 | valid on 'valid' subset | loss 5.149 | nll_loss 2.539 | ppl 5.81 | bleu 57.01 | wps 1798.4 | wpb 933.5 | bsz 59.6 | num_updates 50463 | best_bleu 57.47
2022-08-17 12:33:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 623 @ 50463 updates
2022-08-17 12:33:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint623.pt
2022-08-17 12:34:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint623.pt
2022-08-17 12:34:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint623.pt (epoch 623 @ 50463 updates, score 57.01) (writing took 20.526061989367008 seconds)
2022-08-17 12:34:20 | INFO | fairseq_cli.train | end of epoch 623 (average epoch stats below)
2022-08-17 12:34:20 | INFO | train | epoch 623 | loss 3.379 | nll_loss 0.346 | ppl 1.27 | wps 6210.3 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 50463 | lr 0.000281542 | gnorm 0.31 | train_wall 40 | gb_free 10.1 | wall 51756
2022-08-17 12:34:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:34:20 | INFO | fairseq.trainer | begin training epoch 624
2022-08-17 12:34:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:34:39 | INFO | train_inner | epoch 624:     37 / 81 loss=3.38, nll_loss=0.346, ppl=1.27, wps=6802.3, ups=1.23, wpb=5531.1, bsz=357.6, num_updates=50500, lr=0.000281439, gnorm=0.303, train_wall=49, gb_free=10.1, wall=51775
2022-08-17 12:35:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:35:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:35:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:35:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:35:11 | INFO | valid | epoch 624 | valid on 'valid' subset | loss 5.142 | nll_loss 2.533 | ppl 5.79 | bleu 56.63 | wps 1805.7 | wpb 933.5 | bsz 59.6 | num_updates 50544 | best_bleu 57.47
2022-08-17 12:35:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 624 @ 50544 updates
2022-08-17 12:35:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint624.pt
2022-08-17 12:35:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint624.pt
2022-08-17 12:35:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint624.pt (epoch 624 @ 50544 updates, score 56.63) (writing took 19.632589522749186 seconds)
2022-08-17 12:35:31 | INFO | fairseq_cli.train | end of epoch 624 (average epoch stats below)
2022-08-17 12:35:31 | INFO | train | epoch 624 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 6304.1 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 50544 | lr 0.000281316 | gnorm 0.282 | train_wall 40 | gb_free 10.2 | wall 51827
2022-08-17 12:35:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:35:31 | INFO | fairseq.trainer | begin training epoch 625
2022-08-17 12:35:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:36:00 | INFO | train_inner | epoch 625:     56 / 81 loss=3.38, nll_loss=0.346, ppl=1.27, wps=6810.2, ups=1.23, wpb=5528.5, bsz=356.2, num_updates=50600, lr=0.000281161, gnorm=0.321, train_wall=50, gb_free=10.1, wall=51857
2022-08-17 12:36:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:36:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:36:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:36:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:36:23 | INFO | valid | epoch 625 | valid on 'valid' subset | loss 5.135 | nll_loss 2.525 | ppl 5.75 | bleu 56.84 | wps 1762.1 | wpb 933.5 | bsz 59.6 | num_updates 50625 | best_bleu 57.47
2022-08-17 12:36:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 625 @ 50625 updates
2022-08-17 12:36:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint625.pt
2022-08-17 12:36:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint625.pt
2022-08-17 12:36:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint625.pt (epoch 625 @ 50625 updates, score 56.84) (writing took 19.30320242419839 seconds)
2022-08-17 12:36:42 | INFO | fairseq_cli.train | end of epoch 625 (average epoch stats below)
2022-08-17 12:36:42 | INFO | train | epoch 625 | loss 3.379 | nll_loss 0.345 | ppl 1.27 | wps 6230.9 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 50625 | lr 0.000281091 | gnorm 0.323 | train_wall 41 | gb_free 10.3 | wall 51899
2022-08-17 12:36:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:36:43 | INFO | fairseq.trainer | begin training epoch 626
2022-08-17 12:36:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:37:21 | INFO | train_inner | epoch 626:     75 / 81 loss=3.379, nll_loss=0.345, ppl=1.27, wps=6827, ups=1.24, wpb=5524.7, bsz=361.5, num_updates=50700, lr=0.000280883, gnorm=0.273, train_wall=50, gb_free=10, wall=51938
2022-08-17 12:37:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:37:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:37:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:37:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:37:34 | INFO | valid | epoch 626 | valid on 'valid' subset | loss 5.126 | nll_loss 2.51 | ppl 5.7 | bleu 57.3 | wps 1856.6 | wpb 933.5 | bsz 59.6 | num_updates 50706 | best_bleu 57.47
2022-08-17 12:37:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 626 @ 50706 updates
2022-08-17 12:37:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint626.pt
2022-08-17 12:37:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint626.pt
2022-08-17 12:37:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint626.pt (epoch 626 @ 50706 updates, score 57.3) (writing took 23.11966034770012 seconds)
2022-08-17 12:37:57 | INFO | fairseq_cli.train | end of epoch 626 (average epoch stats below)
2022-08-17 12:37:57 | INFO | train | epoch 626 | loss 3.379 | nll_loss 0.345 | ppl 1.27 | wps 6000.4 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 50706 | lr 0.000280867 | gnorm 0.3 | train_wall 40 | gb_free 10.2 | wall 51973
2022-08-17 12:37:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:37:57 | INFO | fairseq.trainer | begin training epoch 627
2022-08-17 12:37:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:38:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:38:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:38:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:38:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:38:57 | INFO | valid | epoch 627 | valid on 'valid' subset | loss 5.144 | nll_loss 2.535 | ppl 5.8 | bleu 56.71 | wps 1771.7 | wpb 933.5 | bsz 59.6 | num_updates 50787 | best_bleu 57.47
2022-08-17 12:38:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 627 @ 50787 updates
2022-08-17 12:38:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint627.pt
2022-08-17 12:38:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint627.pt
2022-08-17 12:38:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint627.pt (epoch 627 @ 50787 updates, score 56.71) (writing took 2.2817623429000378 seconds)
2022-08-17 12:38:59 | INFO | fairseq_cli.train | end of epoch 627 (average epoch stats below)
2022-08-17 12:38:59 | INFO | train | epoch 627 | loss 3.379 | nll_loss 0.345 | ppl 1.27 | wps 7190.2 | ups 1.3 | wpb 5523.2 | bsz 358 | num_updates 50787 | lr 0.000280643 | gnorm 0.313 | train_wall 40 | gb_free 10.1 | wall 52035
2022-08-17 12:38:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:38:59 | INFO | fairseq.trainer | begin training epoch 628
2022-08-17 12:38:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:39:07 | INFO | train_inner | epoch 628:     13 / 81 loss=3.379, nll_loss=0.345, ppl=1.27, wps=5221.1, ups=0.95, wpb=5499.6, bsz=354.2, num_updates=50800, lr=0.000280607, gnorm=0.322, train_wall=49, gb_free=10, wall=52043
2022-08-17 12:39:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:39:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:39:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:39:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:39:52 | INFO | valid | epoch 628 | valid on 'valid' subset | loss 5.147 | nll_loss 2.539 | ppl 5.81 | bleu 57.08 | wps 1634.7 | wpb 933.5 | bsz 59.6 | num_updates 50868 | best_bleu 57.47
2022-08-17 12:39:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 628 @ 50868 updates
2022-08-17 12:39:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint628.pt
2022-08-17 12:39:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint628.pt
2022-08-17 12:40:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint628.pt (epoch 628 @ 50868 updates, score 57.08) (writing took 30.90750404074788 seconds)
2022-08-17 12:40:23 | INFO | fairseq_cli.train | end of epoch 628 (average epoch stats below)
2022-08-17 12:40:23 | INFO | train | epoch 628 | loss 3.379 | nll_loss 0.345 | ppl 1.27 | wps 5330.8 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 50868 | lr 0.000280419 | gnorm 0.297 | train_wall 41 | gb_free 10.1 | wall 52119
2022-08-17 12:40:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:40:23 | INFO | fairseq.trainer | begin training epoch 629
2022-08-17 12:40:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:40:41 | INFO | train_inner | epoch 629:     32 / 81 loss=3.379, nll_loss=0.346, ppl=1.27, wps=5833.4, ups=1.06, wpb=5499.5, bsz=357.4, num_updates=50900, lr=0.000280331, gnorm=0.308, train_wall=50, gb_free=10, wall=52137
2022-08-17 12:41:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:41:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:41:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:41:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:41:17 | INFO | valid | epoch 629 | valid on 'valid' subset | loss 5.141 | nll_loss 2.53 | ppl 5.78 | bleu 56.66 | wps 1741.8 | wpb 933.5 | bsz 59.6 | num_updates 50949 | best_bleu 57.47
2022-08-17 12:41:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 629 @ 50949 updates
2022-08-17 12:41:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint629.pt
2022-08-17 12:41:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint629.pt
2022-08-17 12:41:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint629.pt (epoch 629 @ 50949 updates, score 56.66) (writing took 15.843920275568962 seconds)
2022-08-17 12:41:33 | INFO | fairseq_cli.train | end of epoch 629 (average epoch stats below)
2022-08-17 12:41:33 | INFO | train | epoch 629 | loss 3.379 | nll_loss 0.345 | ppl 1.27 | wps 6427 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 50949 | lr 0.000280196 | gnorm 0.313 | train_wall 40 | gb_free 10.1 | wall 52189
2022-08-17 12:41:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:41:33 | INFO | fairseq.trainer | begin training epoch 630
2022-08-17 12:41:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:41:59 | INFO | train_inner | epoch 630:     51 / 81 loss=3.379, nll_loss=0.345, ppl=1.27, wps=7139.6, ups=1.28, wpb=5558.3, bsz=357.4, num_updates=51000, lr=0.000280056, gnorm=0.326, train_wall=49, gb_free=10.1, wall=52215
2022-08-17 12:42:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:42:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:42:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:42:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:42:24 | INFO | valid | epoch 630 | valid on 'valid' subset | loss 5.142 | nll_loss 2.53 | ppl 5.78 | bleu 56.74 | wps 1852.7 | wpb 933.5 | bsz 59.6 | num_updates 51030 | best_bleu 57.47
2022-08-17 12:42:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 630 @ 51030 updates
2022-08-17 12:42:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint630.pt
2022-08-17 12:42:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint630.pt
2022-08-17 12:42:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint630.pt (epoch 630 @ 51030 updates, score 56.74) (writing took 2.4109046310186386 seconds)
2022-08-17 12:42:26 | INFO | fairseq_cli.train | end of epoch 630 (average epoch stats below)
2022-08-17 12:42:26 | INFO | train | epoch 630 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 8394.5 | ups 1.52 | wpb 5523.2 | bsz 358 | num_updates 51030 | lr 0.000279974 | gnorm 0.342 | train_wall 40 | gb_free 10.1 | wall 52242
2022-08-17 12:42:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:42:26 | INFO | fairseq.trainer | begin training epoch 631
2022-08-17 12:42:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:43:03 | INFO | train_inner | epoch 631:     70 / 81 loss=3.38, nll_loss=0.346, ppl=1.27, wps=8707.2, ups=1.57, wpb=5548.7, bsz=364.8, num_updates=51100, lr=0.000279782, gnorm=0.35, train_wall=50, gb_free=10.1, wall=52279
2022-08-17 12:43:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:43:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:43:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:43:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:43:18 | INFO | valid | epoch 631 | valid on 'valid' subset | loss 5.138 | nll_loss 2.527 | ppl 5.76 | bleu 56.63 | wps 1784.3 | wpb 933.5 | bsz 59.6 | num_updates 51111 | best_bleu 57.47
2022-08-17 12:43:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 631 @ 51111 updates
2022-08-17 12:43:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint631.pt
2022-08-17 12:43:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint631.pt
2022-08-17 12:43:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint631.pt (epoch 631 @ 51111 updates, score 56.63) (writing took 28.23005112633109 seconds)
2022-08-17 12:43:46 | INFO | fairseq_cli.train | end of epoch 631 (average epoch stats below)
2022-08-17 12:43:46 | INFO | train | epoch 631 | loss 3.379 | nll_loss 0.345 | ppl 1.27 | wps 5584.6 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 51111 | lr 0.000279752 | gnorm 0.348 | train_wall 40 | gb_free 10.2 | wall 52322
2022-08-17 12:43:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:43:46 | INFO | fairseq.trainer | begin training epoch 632
2022-08-17 12:43:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:44:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:44:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:44:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:44:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:44:38 | INFO | valid | epoch 632 | valid on 'valid' subset | loss 5.133 | nll_loss 2.522 | ppl 5.74 | bleu 56.84 | wps 1819.4 | wpb 933.5 | bsz 59.6 | num_updates 51192 | best_bleu 57.47
2022-08-17 12:44:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 632 @ 51192 updates
2022-08-17 12:44:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint632.pt
2022-08-17 12:44:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint632.pt
2022-08-17 12:44:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint632.pt (epoch 632 @ 51192 updates, score 56.84) (writing took 17.343962006270885 seconds)
2022-08-17 12:44:56 | INFO | fairseq_cli.train | end of epoch 632 (average epoch stats below)
2022-08-17 12:44:56 | INFO | train | epoch 632 | loss 3.38 | nll_loss 0.346 | ppl 1.27 | wps 6448.1 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 51192 | lr 0.00027953 | gnorm 0.326 | train_wall 40 | gb_free 10.1 | wall 52392
2022-08-17 12:44:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:44:56 | INFO | fairseq.trainer | begin training epoch 633
2022-08-17 12:44:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:45:01 | INFO | train_inner | epoch 633:      8 / 81 loss=3.38, nll_loss=0.346, ppl=1.27, wps=4638.1, ups=0.85, wpb=5485.8, bsz=352.4, num_updates=51200, lr=0.000279508, gnorm=0.33, train_wall=50, gb_free=10, wall=52397
2022-08-17 12:45:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:45:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:45:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:45:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:45:48 | INFO | valid | epoch 633 | valid on 'valid' subset | loss 5.136 | nll_loss 2.525 | ppl 5.75 | bleu 56.94 | wps 1712.4 | wpb 933.5 | bsz 59.6 | num_updates 51273 | best_bleu 57.47
2022-08-17 12:45:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 633 @ 51273 updates
2022-08-17 12:45:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint633.pt
2022-08-17 12:45:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint633.pt
2022-08-17 12:46:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint633.pt (epoch 633 @ 51273 updates, score 56.94) (writing took 23.280130576342344 seconds)
2022-08-17 12:46:11 | INFO | fairseq_cli.train | end of epoch 633 (average epoch stats below)
2022-08-17 12:46:11 | INFO | train | epoch 633 | loss 3.379 | nll_loss 0.345 | ppl 1.27 | wps 5935.6 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 51273 | lr 0.000279309 | gnorm 0.284 | train_wall 40 | gb_free 10.2 | wall 52467
2022-08-17 12:46:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:46:11 | INFO | fairseq.trainer | begin training epoch 634
2022-08-17 12:46:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:46:26 | INFO | train_inner | epoch 634:     27 / 81 loss=3.379, nll_loss=0.345, ppl=1.27, wps=6544.2, ups=1.18, wpb=5540.9, bsz=359.8, num_updates=51300, lr=0.000279236, gnorm=0.287, train_wall=49, gb_free=10.1, wall=52482
2022-08-17 12:46:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:46:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:46:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:46:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:47:02 | INFO | valid | epoch 634 | valid on 'valid' subset | loss 5.127 | nll_loss 2.517 | ppl 5.72 | bleu 56.8 | wps 1820.8 | wpb 933.5 | bsz 59.6 | num_updates 51354 | best_bleu 57.47
2022-08-17 12:47:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 634 @ 51354 updates
2022-08-17 12:47:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint634.pt
2022-08-17 12:47:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint634.pt
2022-08-17 12:47:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint634.pt (epoch 634 @ 51354 updates, score 56.8) (writing took 28.269972749054432 seconds)
2022-08-17 12:47:30 | INFO | fairseq_cli.train | end of epoch 634 (average epoch stats below)
2022-08-17 12:47:30 | INFO | train | epoch 634 | loss 3.378 | nll_loss 0.344 | ppl 1.27 | wps 5656.5 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 51354 | lr 0.000279089 | gnorm 0.306 | train_wall 40 | gb_free 10.1 | wall 52546
2022-08-17 12:47:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:47:30 | INFO | fairseq.trainer | begin training epoch 635
2022-08-17 12:47:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:47:55 | INFO | train_inner | epoch 635:     46 / 81 loss=3.378, nll_loss=0.345, ppl=1.27, wps=6189, ups=1.12, wpb=5505.8, bsz=356, num_updates=51400, lr=0.000278964, gnorm=0.311, train_wall=49, gb_free=10.1, wall=52571
2022-08-17 12:48:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:48:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:48:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:48:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:48:21 | INFO | valid | epoch 635 | valid on 'valid' subset | loss 5.154 | nll_loss 2.549 | ppl 5.85 | bleu 56.72 | wps 1807 | wpb 933.5 | bsz 59.6 | num_updates 51435 | best_bleu 57.47
2022-08-17 12:48:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 635 @ 51435 updates
2022-08-17 12:48:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint635.pt
2022-08-17 12:48:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint635.pt
2022-08-17 12:48:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint635.pt (epoch 635 @ 51435 updates, score 56.72) (writing took 18.928135693073273 seconds)
2022-08-17 12:48:40 | INFO | fairseq_cli.train | end of epoch 635 (average epoch stats below)
2022-08-17 12:48:40 | INFO | train | epoch 635 | loss 3.379 | nll_loss 0.346 | ppl 1.27 | wps 6400.1 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 51435 | lr 0.000278869 | gnorm 0.311 | train_wall 40 | gb_free 10.1 | wall 52616
2022-08-17 12:48:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:48:40 | INFO | fairseq.trainer | begin training epoch 636
2022-08-17 12:48:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:49:13 | INFO | train_inner | epoch 636:     65 / 81 loss=3.38, nll_loss=0.346, ppl=1.27, wps=7085.8, ups=1.27, wpb=5559.6, bsz=360.2, num_updates=51500, lr=0.000278693, gnorm=0.325, train_wall=48, gb_free=10, wall=52649
2022-08-17 12:49:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:49:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:49:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:49:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:49:31 | INFO | valid | epoch 636 | valid on 'valid' subset | loss 5.143 | nll_loss 2.528 | ppl 5.77 | bleu 56.97 | wps 1660.2 | wpb 933.5 | bsz 59.6 | num_updates 51516 | best_bleu 57.47
2022-08-17 12:49:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 636 @ 51516 updates
2022-08-17 12:49:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint636.pt
2022-08-17 12:49:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint636.pt
2022-08-17 12:49:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint636.pt (epoch 636 @ 51516 updates, score 56.97) (writing took 18.008430067449808 seconds)
2022-08-17 12:49:49 | INFO | fairseq_cli.train | end of epoch 636 (average epoch stats below)
2022-08-17 12:49:49 | INFO | train | epoch 636 | loss 3.379 | nll_loss 0.346 | ppl 1.27 | wps 6453.2 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 51516 | lr 0.00027865 | gnorm 0.342 | train_wall 39 | gb_free 10.2 | wall 52686
2022-08-17 12:49:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:49:50 | INFO | fairseq.trainer | begin training epoch 637
2022-08-17 12:49:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:50:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:50:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:50:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:50:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:50:42 | INFO | valid | epoch 637 | valid on 'valid' subset | loss 5.146 | nll_loss 2.537 | ppl 5.81 | bleu 57.04 | wps 1818.1 | wpb 933.5 | bsz 59.6 | num_updates 51597 | best_bleu 57.47
2022-08-17 12:50:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 637 @ 51597 updates
2022-08-17 12:50:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint637.pt
2022-08-17 12:50:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint637.pt
2022-08-17 12:50:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint637.pt (epoch 637 @ 51597 updates, score 57.04) (writing took 15.400870852172375 seconds)
2022-08-17 12:50:58 | INFO | fairseq_cli.train | end of epoch 637 (average epoch stats below)
2022-08-17 12:50:58 | INFO | train | epoch 637 | loss 3.379 | nll_loss 0.345 | ppl 1.27 | wps 6550 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 51597 | lr 0.000278431 | gnorm 0.29 | train_wall 40 | gb_free 10.2 | wall 52754
2022-08-17 12:50:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:50:58 | INFO | fairseq.trainer | begin training epoch 638
2022-08-17 12:50:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:51:00 | INFO | train_inner | epoch 638:      3 / 81 loss=3.379, nll_loss=0.345, ppl=1.27, wps=5113.9, ups=0.93, wpb=5489, bsz=356.6, num_updates=51600, lr=0.000278423, gnorm=0.297, train_wall=49, gb_free=10.1, wall=52757
2022-08-17 12:51:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:51:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:51:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:51:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:51:48 | INFO | valid | epoch 638 | valid on 'valid' subset | loss 5.142 | nll_loss 2.532 | ppl 5.78 | bleu 56.82 | wps 1743.7 | wpb 933.5 | bsz 59.6 | num_updates 51678 | best_bleu 57.47
2022-08-17 12:51:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 638 @ 51678 updates
2022-08-17 12:51:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint638.pt
2022-08-17 12:51:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint638.pt
2022-08-17 12:52:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint638.pt (epoch 638 @ 51678 updates, score 56.82) (writing took 20.05013107135892 seconds)
2022-08-17 12:52:09 | INFO | fairseq_cli.train | end of epoch 638 (average epoch stats below)
2022-08-17 12:52:09 | INFO | train | epoch 638 | loss 3.379 | nll_loss 0.345 | ppl 1.27 | wps 6294.5 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 51678 | lr 0.000278213 | gnorm 0.28 | train_wall 39 | gb_free 10.1 | wall 52825
2022-08-17 12:52:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:52:09 | INFO | fairseq.trainer | begin training epoch 639
2022-08-17 12:52:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:52:21 | INFO | train_inner | epoch 639:     22 / 81 loss=3.379, nll_loss=0.345, ppl=1.27, wps=6833.6, ups=1.24, wpb=5498.4, bsz=357.4, num_updates=51700, lr=0.000278154, gnorm=0.306, train_wall=49, gb_free=10.1, wall=52837
2022-08-17 12:52:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:52:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:52:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:52:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:53:01 | INFO | valid | epoch 639 | valid on 'valid' subset | loss 5.155 | nll_loss 2.545 | ppl 5.84 | bleu 56.96 | wps 1802.1 | wpb 933.5 | bsz 59.6 | num_updates 51759 | best_bleu 57.47
2022-08-17 12:53:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 639 @ 51759 updates
2022-08-17 12:53:01 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint639.pt
2022-08-17 12:53:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint639.pt
2022-08-17 12:53:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint639.pt (epoch 639 @ 51759 updates, score 56.96) (writing took 16.47484189271927 seconds)
2022-08-17 12:53:18 | INFO | fairseq_cli.train | end of epoch 639 (average epoch stats below)
2022-08-17 12:53:18 | INFO | train | epoch 639 | loss 3.38 | nll_loss 0.347 | ppl 1.27 | wps 6463.2 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 51759 | lr 0.000277995 | gnorm 0.359 | train_wall 41 | gb_free 10.1 | wall 52894
2022-08-17 12:53:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:53:18 | INFO | fairseq.trainer | begin training epoch 640
2022-08-17 12:53:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:53:40 | INFO | train_inner | epoch 640:     41 / 81 loss=3.379, nll_loss=0.346, ppl=1.27, wps=7040.8, ups=1.27, wpb=5563.3, bsz=362.7, num_updates=51800, lr=0.000277885, gnorm=0.327, train_wall=50, gb_free=10, wall=52916
2022-08-17 12:53:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:54:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:54:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:54:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:54:09 | INFO | valid | epoch 640 | valid on 'valid' subset | loss 5.143 | nll_loss 2.534 | ppl 5.79 | bleu 57.24 | wps 1821.4 | wpb 933.5 | bsz 59.6 | num_updates 51840 | best_bleu 57.47
2022-08-17 12:54:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 640 @ 51840 updates
2022-08-17 12:54:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint640.pt
2022-08-17 12:54:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint640.pt
2022-08-17 12:54:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint640.pt (epoch 640 @ 51840 updates, score 57.24) (writing took 2.355039060115814 seconds)
2022-08-17 12:54:11 | INFO | fairseq_cli.train | end of epoch 640 (average epoch stats below)
2022-08-17 12:54:11 | INFO | train | epoch 640 | loss 3.379 | nll_loss 0.346 | ppl 1.27 | wps 8388.4 | ups 1.52 | wpb 5523.2 | bsz 358 | num_updates 51840 | lr 0.000277778 | gnorm 0.3 | train_wall 39 | gb_free 10.1 | wall 52947
2022-08-17 12:54:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:54:11 | INFO | fairseq.trainer | begin training epoch 641
2022-08-17 12:54:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:54:43 | INFO | train_inner | epoch 641:     60 / 81 loss=3.379, nll_loss=0.346, ppl=1.27, wps=8718.9, ups=1.58, wpb=5520.6, bsz=358.2, num_updates=51900, lr=0.000277617, gnorm=0.293, train_wall=49, gb_free=10.1, wall=52979
2022-08-17 12:54:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:54:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:54:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:54:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:55:03 | INFO | valid | epoch 641 | valid on 'valid' subset | loss 5.145 | nll_loss 2.538 | ppl 5.81 | bleu 57.36 | wps 1800.9 | wpb 933.5 | bsz 59.6 | num_updates 51921 | best_bleu 57.47
2022-08-17 12:55:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 641 @ 51921 updates
2022-08-17 12:55:03 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint641.pt
2022-08-17 12:55:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint641.pt
2022-08-17 12:55:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint641.pt (epoch 641 @ 51921 updates, score 57.36) (writing took 15.901772651821375 seconds)
2022-08-17 12:55:19 | INFO | fairseq_cli.train | end of epoch 641 (average epoch stats below)
2022-08-17 12:55:19 | INFO | train | epoch 641 | loss 3.379 | nll_loss 0.345 | ppl 1.27 | wps 6581 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 51921 | lr 0.000277561 | gnorm 0.312 | train_wall 40 | gb_free 10.2 | wall 53015
2022-08-17 12:55:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:55:20 | INFO | fairseq.trainer | begin training epoch 642
2022-08-17 12:55:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:56:00 | INFO | train_inner | epoch 642:     79 / 81 loss=3.379, nll_loss=0.346, ppl=1.27, wps=7233.9, ups=1.31, wpb=5529.2, bsz=356.6, num_updates=52000, lr=0.00027735, gnorm=0.334, train_wall=49, gb_free=10.1, wall=53056
2022-08-17 12:56:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:56:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:56:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:56:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:56:10 | INFO | valid | epoch 642 | valid on 'valid' subset | loss 5.14 | nll_loss 2.527 | ppl 5.77 | bleu 56.75 | wps 1777.9 | wpb 933.5 | bsz 59.6 | num_updates 52002 | best_bleu 57.47
2022-08-17 12:56:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 642 @ 52002 updates
2022-08-17 12:56:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint642.pt
2022-08-17 12:56:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint642.pt
2022-08-17 12:56:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint642.pt (epoch 642 @ 52002 updates, score 56.75) (writing took 25.522261008620262 seconds)
2022-08-17 12:56:36 | INFO | fairseq_cli.train | end of epoch 642 (average epoch stats below)
2022-08-17 12:56:36 | INFO | train | epoch 642 | loss 3.379 | nll_loss 0.345 | ppl 1.27 | wps 5842 | ups 1.06 | wpb 5523.2 | bsz 358 | num_updates 52002 | lr 0.000277345 | gnorm 0.319 | train_wall 39 | gb_free 10 | wall 53092
2022-08-17 12:56:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:56:36 | INFO | fairseq.trainer | begin training epoch 643
2022-08-17 12:56:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:57:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:57:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:57:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:57:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:57:28 | INFO | valid | epoch 643 | valid on 'valid' subset | loss 5.15 | nll_loss 2.544 | ppl 5.83 | bleu 56.79 | wps 1679.6 | wpb 933.5 | bsz 59.6 | num_updates 52083 | best_bleu 57.47
2022-08-17 12:57:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 643 @ 52083 updates
2022-08-17 12:57:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint643.pt
2022-08-17 12:57:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint643.pt
2022-08-17 12:57:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint643.pt (epoch 643 @ 52083 updates, score 56.79) (writing took 20.57785042375326 seconds)
2022-08-17 12:57:49 | INFO | fairseq_cli.train | end of epoch 643 (average epoch stats below)
2022-08-17 12:57:49 | INFO | train | epoch 643 | loss 3.379 | nll_loss 0.345 | ppl 1.27 | wps 6125.7 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 52083 | lr 0.000277129 | gnorm 0.327 | train_wall 40 | gb_free 10.2 | wall 53165
2022-08-17 12:57:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:57:49 | INFO | fairseq.trainer | begin training epoch 644
2022-08-17 12:57:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:57:58 | INFO | train_inner | epoch 644:     17 / 81 loss=3.379, nll_loss=0.345, ppl=1.27, wps=4656.8, ups=0.84, wpb=5518.9, bsz=355.5, num_updates=52100, lr=0.000277084, gnorm=0.316, train_wall=49, gb_free=10, wall=53174
2022-08-17 12:58:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:58:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:58:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:58:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 12:58:40 | INFO | valid | epoch 644 | valid on 'valid' subset | loss 5.157 | nll_loss 2.552 | ppl 5.86 | bleu 56.62 | wps 1713.4 | wpb 933.5 | bsz 59.6 | num_updates 52164 | best_bleu 57.47
2022-08-17 12:58:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 644 @ 52164 updates
2022-08-17 12:58:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint644.pt
2022-08-17 12:58:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint644.pt
2022-08-17 12:59:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint644.pt (epoch 644 @ 52164 updates, score 56.62) (writing took 32.03709761798382 seconds)
2022-08-17 12:59:12 | INFO | fairseq_cli.train | end of epoch 644 (average epoch stats below)
2022-08-17 12:59:12 | INFO | train | epoch 644 | loss 3.379 | nll_loss 0.345 | ppl 1.27 | wps 5393.8 | ups 0.98 | wpb 5523.2 | bsz 358 | num_updates 52164 | lr 0.000276914 | gnorm 0.281 | train_wall 39 | gb_free 10.1 | wall 53248
2022-08-17 12:59:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 12:59:12 | INFO | fairseq.trainer | begin training epoch 645
2022-08-17 12:59:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 12:59:31 | INFO | train_inner | epoch 645:     36 / 81 loss=3.378, nll_loss=0.344, ppl=1.27, wps=5945.6, ups=1.08, wpb=5523.3, bsz=358.1, num_updates=52200, lr=0.000276818, gnorm=0.271, train_wall=49, gb_free=10.1, wall=53267
2022-08-17 12:59:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 12:59:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 12:59:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 12:59:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:00:04 | INFO | valid | epoch 645 | valid on 'valid' subset | loss 5.166 | nll_loss 2.563 | ppl 5.91 | bleu 56.87 | wps 1829.1 | wpb 933.5 | bsz 59.6 | num_updates 52245 | best_bleu 57.47
2022-08-17 13:00:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 645 @ 52245 updates
2022-08-17 13:00:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint645.pt
2022-08-17 13:00:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint645.pt
2022-08-17 13:00:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint645.pt (epoch 645 @ 52245 updates, score 56.87) (writing took 17.576646901667118 seconds)
2022-08-17 13:00:22 | INFO | fairseq_cli.train | end of epoch 645 (average epoch stats below)
2022-08-17 13:00:22 | INFO | train | epoch 645 | loss 3.378 | nll_loss 0.344 | ppl 1.27 | wps 6412.3 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 52245 | lr 0.000276699 | gnorm 0.297 | train_wall 41 | gb_free 10.2 | wall 53318
2022-08-17 13:00:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:00:22 | INFO | fairseq.trainer | begin training epoch 646
2022-08-17 13:00:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:00:51 | INFO | train_inner | epoch 646:     55 / 81 loss=3.378, nll_loss=0.344, ppl=1.27, wps=6946.6, ups=1.25, wpb=5546.8, bsz=359.9, num_updates=52300, lr=0.000276553, gnorm=0.312, train_wall=50, gb_free=10.1, wall=53347
2022-08-17 13:01:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:01:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:01:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:01:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:01:14 | INFO | valid | epoch 646 | valid on 'valid' subset | loss 5.162 | nll_loss 2.555 | ppl 5.88 | bleu 56.87 | wps 1764.8 | wpb 933.5 | bsz 59.6 | num_updates 52326 | best_bleu 57.47
2022-08-17 13:01:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 646 @ 52326 updates
2022-08-17 13:01:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint646.pt
2022-08-17 13:01:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint646.pt
2022-08-17 13:01:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint646.pt (epoch 646 @ 52326 updates, score 56.87) (writing took 20.230508781969547 seconds)
2022-08-17 13:01:34 | INFO | fairseq_cli.train | end of epoch 646 (average epoch stats below)
2022-08-17 13:01:34 | INFO | train | epoch 646 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 6182.2 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 52326 | lr 0.000276485 | gnorm 0.29 | train_wall 40 | gb_free 10.1 | wall 53390
2022-08-17 13:01:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:01:34 | INFO | fairseq.trainer | begin training epoch 647
2022-08-17 13:01:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:02:13 | INFO | train_inner | epoch 647:     74 / 81 loss=3.379, nll_loss=0.345, ppl=1.27, wps=6689, ups=1.22, wpb=5494.9, bsz=358.5, num_updates=52400, lr=0.000276289, gnorm=0.3, train_wall=49, gb_free=10.1, wall=53429
2022-08-17 13:02:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:02:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:02:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:02:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:02:26 | INFO | valid | epoch 647 | valid on 'valid' subset | loss 5.152 | nll_loss 2.547 | ppl 5.84 | bleu 56.95 | wps 1841.8 | wpb 933.5 | bsz 59.6 | num_updates 52407 | best_bleu 57.47
2022-08-17 13:02:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 647 @ 52407 updates
2022-08-17 13:02:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint647.pt
2022-08-17 13:02:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint647.pt
2022-08-17 13:02:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint647.pt (epoch 647 @ 52407 updates, score 56.95) (writing took 20.32156726717949 seconds)
2022-08-17 13:02:47 | INFO | fairseq_cli.train | end of epoch 647 (average epoch stats below)
2022-08-17 13:02:47 | INFO | train | epoch 647 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 6166.1 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 52407 | lr 0.000276271 | gnorm 0.306 | train_wall 40 | gb_free 10.1 | wall 53463
2022-08-17 13:02:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:02:47 | INFO | fairseq.trainer | begin training epoch 648
2022-08-17 13:02:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:03:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:03:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:03:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:03:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:03:45 | INFO | valid | epoch 648 | valid on 'valid' subset | loss 5.167 | nll_loss 2.565 | ppl 5.92 | bleu 56.83 | wps 1743.5 | wpb 933.5 | bsz 59.6 | num_updates 52488 | best_bleu 57.47
2022-08-17 13:03:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 648 @ 52488 updates
2022-08-17 13:03:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint648.pt
2022-08-17 13:03:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint648.pt
2022-08-17 13:04:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint648.pt (epoch 648 @ 52488 updates, score 56.83) (writing took 25.593165054917336 seconds)
2022-08-17 13:04:11 | INFO | fairseq_cli.train | end of epoch 648 (average epoch stats below)
2022-08-17 13:04:11 | INFO | train | epoch 648 | loss 3.378 | nll_loss 0.344 | ppl 1.27 | wps 5319.3 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 52488 | lr 0.000276058 | gnorm 0.276 | train_wall 40 | gb_free 10.2 | wall 53547
2022-08-17 13:04:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:04:11 | INFO | fairseq.trainer | begin training epoch 649
2022-08-17 13:04:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:04:18 | INFO | train_inner | epoch 649:     12 / 81 loss=3.378, nll_loss=0.345, ppl=1.27, wps=4389.6, ups=0.8, wpb=5508.6, bsz=357.4, num_updates=52500, lr=0.000276026, gnorm=0.276, train_wall=50, gb_free=10, wall=53555
2022-08-17 13:04:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:04:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:04:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:04:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:05:04 | INFO | valid | epoch 649 | valid on 'valid' subset | loss 5.16 | nll_loss 2.555 | ppl 5.88 | bleu 56.26 | wps 1858.2 | wpb 933.5 | bsz 59.6 | num_updates 52569 | best_bleu 57.47
2022-08-17 13:05:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 649 @ 52569 updates
2022-08-17 13:05:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint649.pt
2022-08-17 13:05:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint649.pt
2022-08-17 13:05:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint649.pt (epoch 649 @ 52569 updates, score 56.26) (writing took 14.999449890106916 seconds)
2022-08-17 13:05:19 | INFO | fairseq_cli.train | end of epoch 649 (average epoch stats below)
2022-08-17 13:05:19 | INFO | train | epoch 649 | loss 3.379 | nll_loss 0.345 | ppl 1.27 | wps 6573 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 52569 | lr 0.000275845 | gnorm 0.307 | train_wall 41 | gb_free 10.3 | wall 53615
2022-08-17 13:05:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:05:19 | INFO | fairseq.trainer | begin training epoch 650
2022-08-17 13:05:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:05:36 | INFO | train_inner | epoch 650:     31 / 81 loss=3.378, nll_loss=0.344, ppl=1.27, wps=7166.7, ups=1.29, wpb=5543.4, bsz=357.7, num_updates=52600, lr=0.000275764, gnorm=0.311, train_wall=51, gb_free=10.1, wall=53632
2022-08-17 13:06:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:06:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:06:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:06:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:06:11 | INFO | valid | epoch 650 | valid on 'valid' subset | loss 5.164 | nll_loss 2.558 | ppl 5.89 | bleu 56.47 | wps 1653.6 | wpb 933.5 | bsz 59.6 | num_updates 52650 | best_bleu 57.47
2022-08-17 13:06:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 650 @ 52650 updates
2022-08-17 13:06:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint650.pt
2022-08-17 13:06:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint650.pt
2022-08-17 13:06:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint650.pt (epoch 650 @ 52650 updates, score 56.47) (writing took 17.931158155202866 seconds)
2022-08-17 13:06:29 | INFO | fairseq_cli.train | end of epoch 650 (average epoch stats below)
2022-08-17 13:06:29 | INFO | train | epoch 650 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 6394.6 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 52650 | lr 0.000275633 | gnorm 0.29 | train_wall 39 | gb_free 10.1 | wall 53685
2022-08-17 13:06:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:06:29 | INFO | fairseq.trainer | begin training epoch 651
2022-08-17 13:06:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:06:55 | INFO | train_inner | epoch 651:     50 / 81 loss=3.379, nll_loss=0.345, ppl=1.27, wps=6957, ups=1.26, wpb=5530.8, bsz=357, num_updates=52700, lr=0.000275502, gnorm=0.295, train_wall=49, gb_free=10.1, wall=53712
2022-08-17 13:07:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:07:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:07:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:07:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:07:20 | INFO | valid | epoch 651 | valid on 'valid' subset | loss 5.174 | nll_loss 2.573 | ppl 5.95 | bleu 55.65 | wps 1789.7 | wpb 933.5 | bsz 59.6 | num_updates 52731 | best_bleu 57.47
2022-08-17 13:07:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 651 @ 52731 updates
2022-08-17 13:07:20 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint651.pt
2022-08-17 13:07:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint651.pt
2022-08-17 13:07:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint651.pt (epoch 651 @ 52731 updates, score 55.65) (writing took 26.826778702437878 seconds)
2022-08-17 13:07:47 | INFO | fairseq_cli.train | end of epoch 651 (average epoch stats below)
2022-08-17 13:07:47 | INFO | train | epoch 651 | loss 3.379 | nll_loss 0.345 | ppl 1.27 | wps 5695 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 52731 | lr 0.000275421 | gnorm 0.315 | train_wall 40 | gb_free 10.2 | wall 53763
2022-08-17 13:07:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:07:47 | INFO | fairseq.trainer | begin training epoch 652
2022-08-17 13:07:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:08:23 | INFO | train_inner | epoch 652:     69 / 81 loss=3.379, nll_loss=0.346, ppl=1.27, wps=6259.2, ups=1.14, wpb=5508.6, bsz=359.2, num_updates=52800, lr=0.000275241, gnorm=0.399, train_wall=49, gb_free=10.1, wall=53800
2022-08-17 13:08:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:08:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:08:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:08:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:08:39 | INFO | valid | epoch 652 | valid on 'valid' subset | loss 5.158 | nll_loss 2.554 | ppl 5.87 | bleu 57.02 | wps 1849.2 | wpb 933.5 | bsz 59.6 | num_updates 52812 | best_bleu 57.47
2022-08-17 13:08:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 652 @ 52812 updates
2022-08-17 13:08:39 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint652.pt
2022-08-17 13:08:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint652.pt
2022-08-17 13:08:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint652.pt (epoch 652 @ 52812 updates, score 57.02) (writing took 15.453126169741154 seconds)
2022-08-17 13:08:54 | INFO | fairseq_cli.train | end of epoch 652 (average epoch stats below)
2022-08-17 13:08:54 | INFO | train | epoch 652 | loss 3.379 | nll_loss 0.346 | ppl 1.27 | wps 6671.5 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 52812 | lr 0.00027521 | gnorm 0.406 | train_wall 40 | gb_free 10.1 | wall 53830
2022-08-17 13:08:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:08:54 | INFO | fairseq.trainer | begin training epoch 653
2022-08-17 13:08:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:09:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:09:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:09:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:09:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:09:45 | INFO | valid | epoch 653 | valid on 'valid' subset | loss 5.157 | nll_loss 2.553 | ppl 5.87 | bleu 57.21 | wps 1883.4 | wpb 933.5 | bsz 59.6 | num_updates 52893 | best_bleu 57.47
2022-08-17 13:09:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 653 @ 52893 updates
2022-08-17 13:09:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint653.pt
2022-08-17 13:09:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint653.pt
2022-08-17 13:10:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint653.pt (epoch 653 @ 52893 updates, score 57.21) (writing took 33.341938741505146 seconds)
2022-08-17 13:10:19 | INFO | fairseq_cli.train | end of epoch 653 (average epoch stats below)
2022-08-17 13:10:19 | INFO | train | epoch 653 | loss 3.378 | nll_loss 0.344 | ppl 1.27 | wps 5299.2 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 52893 | lr 0.000274999 | gnorm 0.4 | train_wall 40 | gb_free 10.1 | wall 53915
2022-08-17 13:10:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:10:19 | INFO | fairseq.trainer | begin training epoch 654
2022-08-17 13:10:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:10:24 | INFO | train_inner | epoch 654:      7 / 81 loss=3.378, nll_loss=0.344, ppl=1.27, wps=4578.9, ups=0.83, wpb=5519, bsz=359.2, num_updates=52900, lr=0.000274981, gnorm=0.375, train_wall=49, gb_free=10.1, wall=53920
2022-08-17 13:11:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:11:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:11:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:11:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:11:14 | INFO | valid | epoch 654 | valid on 'valid' subset | loss 5.158 | nll_loss 2.549 | ppl 5.85 | bleu 56.61 | wps 1774 | wpb 933.5 | bsz 59.6 | num_updates 52974 | best_bleu 57.47
2022-08-17 13:11:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 654 @ 52974 updates
2022-08-17 13:11:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint654.pt
2022-08-17 13:11:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint654.pt
2022-08-17 13:11:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint654.pt (epoch 654 @ 52974 updates, score 56.61) (writing took 2.368366315960884 seconds)
2022-08-17 13:11:16 | INFO | fairseq_cli.train | end of epoch 654 (average epoch stats below)
2022-08-17 13:11:16 | INFO | train | epoch 654 | loss 3.379 | nll_loss 0.345 | ppl 1.27 | wps 7766 | ups 1.41 | wpb 5523.2 | bsz 358 | num_updates 52974 | lr 0.000274789 | gnorm 0.398 | train_wall 40 | gb_free 10 | wall 53973
2022-08-17 13:11:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:11:17 | INFO | fairseq.trainer | begin training epoch 655
2022-08-17 13:11:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:11:31 | INFO | train_inner | epoch 655:     26 / 81 loss=3.379, nll_loss=0.345, ppl=1.27, wps=8270.9, ups=1.49, wpb=5538.2, bsz=359.6, num_updates=53000, lr=0.000274721, gnorm=0.38, train_wall=50, gb_free=10.2, wall=53987
2022-08-17 13:11:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:11:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:11:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:11:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:12:08 | INFO | valid | epoch 655 | valid on 'valid' subset | loss 5.162 | nll_loss 2.555 | ppl 5.88 | bleu 56.39 | wps 1712.2 | wpb 933.5 | bsz 59.6 | num_updates 53055 | best_bleu 57.47
2022-08-17 13:12:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 655 @ 53055 updates
2022-08-17 13:12:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint655.pt
2022-08-17 13:12:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint655.pt
2022-08-17 13:12:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint655.pt (epoch 655 @ 53055 updates, score 56.39) (writing took 16.17711642757058 seconds)
2022-08-17 13:12:24 | INFO | fairseq_cli.train | end of epoch 655 (average epoch stats below)
2022-08-17 13:12:24 | INFO | train | epoch 655 | loss 3.379 | nll_loss 0.345 | ppl 1.27 | wps 6596.8 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 53055 | lr 0.000274579 | gnorm 0.332 | train_wall 39 | gb_free 10.1 | wall 54040
2022-08-17 13:12:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:12:24 | INFO | fairseq.trainer | begin training epoch 656
2022-08-17 13:12:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:12:49 | INFO | train_inner | epoch 656:     45 / 81 loss=3.38, nll_loss=0.347, ppl=1.27, wps=7093.4, ups=1.29, wpb=5515.5, bsz=355.3, num_updates=53100, lr=0.000274462, gnorm=5.973, train_wall=49, gb_free=10.1, wall=54065
2022-08-17 13:13:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:13:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:13:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:13:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:13:16 | INFO | valid | epoch 656 | valid on 'valid' subset | loss 5.177 | nll_loss 2.573 | ppl 5.95 | bleu 56.32 | wps 1756.7 | wpb 933.5 | bsz 59.6 | num_updates 53136 | best_bleu 57.47
2022-08-17 13:13:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 656 @ 53136 updates
2022-08-17 13:13:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint656.pt
2022-08-17 13:13:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint656.pt
2022-08-17 13:13:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint656.pt (epoch 656 @ 53136 updates, score 56.32) (writing took 29.735805198550224 seconds)
2022-08-17 13:13:46 | INFO | fairseq_cli.train | end of epoch 656 (average epoch stats below)
2022-08-17 13:13:46 | INFO | train | epoch 656 | loss 3.379 | nll_loss 0.346 | ppl 1.27 | wps 5489.5 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 53136 | lr 0.000274369 | gnorm 7.279 | train_wall 40 | gb_free 10.2 | wall 54122
2022-08-17 13:13:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:13:46 | INFO | fairseq.trainer | begin training epoch 657
2022-08-17 13:13:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:14:19 | INFO | train_inner | epoch 657:     64 / 81 loss=3.378, nll_loss=0.345, ppl=1.27, wps=6118.7, ups=1.1, wpb=5546.3, bsz=358.5, num_updates=53200, lr=0.000274204, gnorm=0.294, train_wall=49, gb_free=10.1, wall=54155
2022-08-17 13:14:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:14:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:14:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:14:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:14:37 | INFO | valid | epoch 657 | valid on 'valid' subset | loss 5.154 | nll_loss 2.548 | ppl 5.85 | bleu 55.83 | wps 1786.1 | wpb 933.5 | bsz 59.6 | num_updates 53217 | best_bleu 57.47
2022-08-17 13:14:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 657 @ 53217 updates
2022-08-17 13:14:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint657.pt
2022-08-17 13:14:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint657.pt
2022-08-17 13:14:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint657.pt (epoch 657 @ 53217 updates, score 55.83) (writing took 18.763517800718546 seconds)
2022-08-17 13:14:56 | INFO | fairseq_cli.train | end of epoch 657 (average epoch stats below)
2022-08-17 13:14:56 | INFO | train | epoch 657 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 6345.7 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 53217 | lr 0.00027416 | gnorm 0.287 | train_wall 40 | gb_free 10.1 | wall 54192
2022-08-17 13:14:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:14:56 | INFO | fairseq.trainer | begin training epoch 658
2022-08-17 13:14:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:15:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:15:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:15:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:15:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:15:53 | INFO | valid | epoch 658 | valid on 'valid' subset | loss 5.166 | nll_loss 2.561 | ppl 5.9 | bleu 56.2 | wps 1788.1 | wpb 933.5 | bsz 59.6 | num_updates 53298 | best_bleu 57.47
2022-08-17 13:15:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 658 @ 53298 updates
2022-08-17 13:15:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint658.pt
2022-08-17 13:15:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint658.pt
2022-08-17 13:16:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint658.pt (epoch 658 @ 53298 updates, score 56.2) (writing took 37.68639611080289 seconds)
2022-08-17 13:16:30 | INFO | fairseq_cli.train | end of epoch 658 (average epoch stats below)
2022-08-17 13:16:30 | INFO | train | epoch 658 | loss 3.379 | nll_loss 0.346 | ppl 1.27 | wps 4742.4 | ups 0.86 | wpb 5523.2 | bsz 358 | num_updates 53298 | lr 0.000273952 | gnorm 0.34 | train_wall 39 | gb_free 10.3 | wall 54287
2022-08-17 13:16:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:16:31 | INFO | fairseq.trainer | begin training epoch 659
2022-08-17 13:16:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:16:33 | INFO | train_inner | epoch 659:      2 / 81 loss=3.379, nll_loss=0.346, ppl=1.27, wps=4111.6, ups=0.75, wpb=5488.2, bsz=355.9, num_updates=53300, lr=0.000273947, gnorm=0.339, train_wall=48, gb_free=10.1, wall=54289
2022-08-17 13:17:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:17:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:17:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:17:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:17:22 | INFO | valid | epoch 659 | valid on 'valid' subset | loss 5.161 | nll_loss 2.559 | ppl 5.89 | bleu 56.23 | wps 1776.4 | wpb 933.5 | bsz 59.6 | num_updates 53379 | best_bleu 57.47
2022-08-17 13:17:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 659 @ 53379 updates
2022-08-17 13:17:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint659.pt
2022-08-17 13:17:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint659.pt
2022-08-17 13:17:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint659.pt (epoch 659 @ 53379 updates, score 56.23) (writing took 17.441612854599953 seconds)
2022-08-17 13:17:40 | INFO | fairseq_cli.train | end of epoch 659 (average epoch stats below)
2022-08-17 13:17:40 | INFO | train | epoch 659 | loss 3.378 | nll_loss 0.344 | ppl 1.27 | wps 6448.6 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 53379 | lr 0.000273744 | gnorm 0.266 | train_wall 40 | gb_free 10.2 | wall 54356
2022-08-17 13:17:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:17:40 | INFO | fairseq.trainer | begin training epoch 660
2022-08-17 13:17:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:17:52 | INFO | train_inner | epoch 660:     21 / 81 loss=3.377, nll_loss=0.343, ppl=1.27, wps=7000.4, ups=1.27, wpb=5531.5, bsz=360.2, num_updates=53400, lr=0.00027369, gnorm=0.254, train_wall=50, gb_free=10.1, wall=54368
2022-08-17 13:18:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:18:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:18:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:18:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:18:32 | INFO | valid | epoch 660 | valid on 'valid' subset | loss 5.168 | nll_loss 2.563 | ppl 5.91 | bleu 56.18 | wps 1852.7 | wpb 933.5 | bsz 59.6 | num_updates 53460 | best_bleu 57.47
2022-08-17 13:18:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 660 @ 53460 updates
2022-08-17 13:18:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint660.pt
2022-08-17 13:18:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint660.pt
2022-08-17 13:18:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint660.pt (epoch 660 @ 53460 updates, score 56.18) (writing took 18.544478937983513 seconds)
2022-08-17 13:18:50 | INFO | fairseq_cli.train | end of epoch 660 (average epoch stats below)
2022-08-17 13:18:50 | INFO | train | epoch 660 | loss 3.378 | nll_loss 0.344 | ppl 1.27 | wps 6343.6 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 53460 | lr 0.000273537 | gnorm 0.296 | train_wall 41 | gb_free 10.1 | wall 54427
2022-08-17 13:18:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:18:51 | INFO | fairseq.trainer | begin training epoch 661
2022-08-17 13:18:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:19:12 | INFO | train_inner | epoch 661:     40 / 81 loss=3.378, nll_loss=0.344, ppl=1.27, wps=6926.3, ups=1.25, wpb=5529.5, bsz=358.7, num_updates=53500, lr=0.000273434, gnorm=0.306, train_wall=50, gb_free=10.1, wall=54448
2022-08-17 13:19:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:19:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:19:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:19:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:19:42 | INFO | valid | epoch 661 | valid on 'valid' subset | loss 5.162 | nll_loss 2.554 | ppl 5.87 | bleu 56.9 | wps 1847.7 | wpb 933.5 | bsz 59.6 | num_updates 53541 | best_bleu 57.47
2022-08-17 13:19:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 661 @ 53541 updates
2022-08-17 13:19:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint661.pt
2022-08-17 13:19:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint661.pt
2022-08-17 13:19:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint661.pt (epoch 661 @ 53541 updates, score 56.9) (writing took 15.740094546228647 seconds)
2022-08-17 13:19:58 | INFO | fairseq_cli.train | end of epoch 661 (average epoch stats below)
2022-08-17 13:19:58 | INFO | train | epoch 661 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 6622.2 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 53541 | lr 0.00027333 | gnorm 0.29 | train_wall 41 | gb_free 10.2 | wall 54494
2022-08-17 13:19:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:19:58 | INFO | fairseq.trainer | begin training epoch 662
2022-08-17 13:19:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:20:29 | INFO | train_inner | epoch 662:     59 / 81 loss=3.378, nll_loss=0.344, ppl=1.27, wps=7163.3, ups=1.3, wpb=5523.4, bsz=357.4, num_updates=53600, lr=0.000273179, gnorm=0.289, train_wall=50, gb_free=10, wall=54525
2022-08-17 13:20:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:20:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:20:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:20:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:20:49 | INFO | valid | epoch 662 | valid on 'valid' subset | loss 5.164 | nll_loss 2.562 | ppl 5.9 | bleu 56.67 | wps 1837.3 | wpb 933.5 | bsz 59.6 | num_updates 53622 | best_bleu 57.47
2022-08-17 13:20:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 662 @ 53622 updates
2022-08-17 13:20:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint662.pt
2022-08-17 13:20:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint662.pt
2022-08-17 13:20:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint662.pt (epoch 662 @ 53622 updates, score 56.67) (writing took 2.4480032846331596 seconds)
2022-08-17 13:20:52 | INFO | fairseq_cli.train | end of epoch 662 (average epoch stats below)
2022-08-17 13:20:52 | INFO | train | epoch 662 | loss 3.378 | nll_loss 0.344 | ppl 1.27 | wps 8312.6 | ups 1.51 | wpb 5523.2 | bsz 358 | num_updates 53622 | lr 0.000273123 | gnorm 0.305 | train_wall 40 | gb_free 10.1 | wall 54548
2022-08-17 13:20:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:20:52 | INFO | fairseq.trainer | begin training epoch 663
2022-08-17 13:20:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:21:33 | INFO | train_inner | epoch 663:     78 / 81 loss=3.379, nll_loss=0.346, ppl=1.27, wps=8571.7, ups=1.55, wpb=5527.6, bsz=357, num_updates=53700, lr=0.000272925, gnorm=0.414, train_wall=51, gb_free=10.2, wall=54589
2022-08-17 13:21:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:21:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:21:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:21:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:21:44 | INFO | valid | epoch 663 | valid on 'valid' subset | loss 5.184 | nll_loss 2.582 | ppl 5.99 | bleu 56.27 | wps 1754.3 | wpb 933.5 | bsz 59.6 | num_updates 53703 | best_bleu 57.47
2022-08-17 13:21:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 663 @ 53703 updates
2022-08-17 13:21:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint663.pt
2022-08-17 13:21:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint663.pt
2022-08-17 13:22:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint663.pt (epoch 663 @ 53703 updates, score 56.27) (writing took 17.52525496482849 seconds)
2022-08-17 13:22:02 | INFO | fairseq_cli.train | end of epoch 663 (average epoch stats below)
2022-08-17 13:22:02 | INFO | train | epoch 663 | loss 3.379 | nll_loss 0.346 | ppl 1.27 | wps 6390.5 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 53703 | lr 0.000272917 | gnorm 0.433 | train_wall 41 | gb_free 10.1 | wall 54618
2022-08-17 13:22:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:22:02 | INFO | fairseq.trainer | begin training epoch 664
2022-08-17 13:22:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:22:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:22:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:22:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:22:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:22:58 | INFO | valid | epoch 664 | valid on 'valid' subset | loss 5.16 | nll_loss 2.554 | ppl 5.87 | bleu 56.35 | wps 1542.8 | wpb 933.5 | bsz 59.6 | num_updates 53784 | best_bleu 57.47
2022-08-17 13:22:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 664 @ 53784 updates
2022-08-17 13:22:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint664.pt
2022-08-17 13:23:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint664.pt
2022-08-17 13:23:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint664.pt (epoch 664 @ 53784 updates, score 56.35) (writing took 13.817287873476744 seconds)
2022-08-17 13:23:12 | INFO | fairseq_cli.train | end of epoch 664 (average epoch stats below)
2022-08-17 13:23:12 | INFO | train | epoch 664 | loss 3.379 | nll_loss 0.346 | ppl 1.27 | wps 6357.9 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 53784 | lr 0.000272711 | gnorm 0.436 | train_wall 40 | gb_free 10.1 | wall 54688
2022-08-17 13:23:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:23:12 | INFO | fairseq.trainer | begin training epoch 665
2022-08-17 13:23:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:23:22 | INFO | train_inner | epoch 665:     16 / 81 loss=3.378, nll_loss=0.345, ppl=1.27, wps=5022.5, ups=0.92, wpb=5475.4, bsz=355, num_updates=53800, lr=0.000272671, gnorm=0.409, train_wall=50, gb_free=10.2, wall=54698
2022-08-17 13:23:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:23:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:23:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:23:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:24:06 | INFO | valid | epoch 665 | valid on 'valid' subset | loss 5.174 | nll_loss 2.572 | ppl 5.95 | bleu 56.36 | wps 1860.2 | wpb 933.5 | bsz 59.6 | num_updates 53865 | best_bleu 57.47
2022-08-17 13:24:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 665 @ 53865 updates
2022-08-17 13:24:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint665.pt
2022-08-17 13:24:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint665.pt
2022-08-17 13:24:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint665.pt (epoch 665 @ 53865 updates, score 56.36) (writing took 17.892437163740396 seconds)
2022-08-17 13:24:24 | INFO | fairseq_cli.train | end of epoch 665 (average epoch stats below)
2022-08-17 13:24:24 | INFO | train | epoch 665 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 6246 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 53865 | lr 0.000272506 | gnorm 0.281 | train_wall 41 | gb_free 10.1 | wall 54760
2022-08-17 13:24:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:24:24 | INFO | fairseq.trainer | begin training epoch 666
2022-08-17 13:24:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:24:44 | INFO | train_inner | epoch 666:     35 / 81 loss=3.379, nll_loss=0.345, ppl=1.27, wps=6757.5, ups=1.22, wpb=5545.5, bsz=359.8, num_updates=53900, lr=0.000272418, gnorm=0.306, train_wall=49, gb_free=10.1, wall=54781
2022-08-17 13:25:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:25:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:25:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:25:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:25:19 | INFO | valid | epoch 666 | valid on 'valid' subset | loss 5.159 | nll_loss 2.555 | ppl 5.87 | bleu 56.2 | wps 1724.4 | wpb 933.5 | bsz 59.6 | num_updates 53946 | best_bleu 57.47
2022-08-17 13:25:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 666 @ 53946 updates
2022-08-17 13:25:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint666.pt
2022-08-17 13:25:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint666.pt
2022-08-17 13:25:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint666.pt (epoch 666 @ 53946 updates, score 56.2) (writing took 29.44753597304225 seconds)
2022-08-17 13:25:49 | INFO | fairseq_cli.train | end of epoch 666 (average epoch stats below)
2022-08-17 13:25:49 | INFO | train | epoch 666 | loss 3.379 | nll_loss 0.345 | ppl 1.27 | wps 5246.2 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 53946 | lr 0.000272302 | gnorm 0.326 | train_wall 39 | gb_free 10.1 | wall 54845
2022-08-17 13:25:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:25:49 | INFO | fairseq.trainer | begin training epoch 667
2022-08-17 13:25:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:26:17 | INFO | train_inner | epoch 667:     54 / 81 loss=3.378, nll_loss=0.345, ppl=1.27, wps=5978.9, ups=1.08, wpb=5539.2, bsz=363.3, num_updates=54000, lr=0.000272166, gnorm=0.301, train_wall=48, gb_free=10.1, wall=54873
2022-08-17 13:26:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:26:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:26:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:26:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:26:41 | INFO | valid | epoch 667 | valid on 'valid' subset | loss 5.166 | nll_loss 2.561 | ppl 5.9 | bleu 56.3 | wps 1771.6 | wpb 933.5 | bsz 59.6 | num_updates 54027 | best_bleu 57.47
2022-08-17 13:26:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 667 @ 54027 updates
2022-08-17 13:26:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint667.pt
2022-08-17 13:26:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint667.pt
2022-08-17 13:26:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint667.pt (epoch 667 @ 54027 updates, score 56.3) (writing took 17.722539130598307 seconds)
2022-08-17 13:26:59 | INFO | fairseq_cli.train | end of epoch 667 (average epoch stats below)
2022-08-17 13:26:59 | INFO | train | epoch 667 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 6440.8 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 54027 | lr 0.000272098 | gnorm 0.489 | train_wall 40 | gb_free 10.1 | wall 54915
2022-08-17 13:26:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:26:59 | INFO | fairseq.trainer | begin training epoch 668
2022-08-17 13:26:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:27:38 | INFO | train_inner | epoch 668:     73 / 81 loss=3.378, nll_loss=0.345, ppl=1.27, wps=6814.5, ups=1.23, wpb=5544.5, bsz=357.7, num_updates=54100, lr=0.000271914, gnorm=0.437, train_wall=51, gb_free=10.1, wall=54955
2022-08-17 13:27:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:27:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:27:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:27:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:27:56 | INFO | valid | epoch 668 | valid on 'valid' subset | loss 5.159 | nll_loss 2.552 | ppl 5.86 | bleu 56.59 | wps 1281.7 | wpb 933.5 | bsz 59.6 | num_updates 54108 | best_bleu 57.47
2022-08-17 13:27:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 668 @ 54108 updates
2022-08-17 13:27:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint668.pt
2022-08-17 13:27:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint668.pt
2022-08-17 13:28:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint668.pt (epoch 668 @ 54108 updates, score 56.59) (writing took 37.10136981680989 seconds)
2022-08-17 13:28:34 | INFO | fairseq_cli.train | end of epoch 668 (average epoch stats below)
2022-08-17 13:28:34 | INFO | train | epoch 668 | loss 3.378 | nll_loss 0.344 | ppl 1.27 | wps 4708.3 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 54108 | lr 0.000271894 | gnorm 0.276 | train_wall 42 | gb_free 10.1 | wall 55010
2022-08-17 13:28:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:28:34 | INFO | fairseq.trainer | begin training epoch 669
2022-08-17 13:28:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:29:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:29:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:29:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:29:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:29:57 | INFO | valid | epoch 669 | valid on 'valid' subset | loss 5.159 | nll_loss 2.552 | ppl 5.87 | bleu 56.65 | wps 1095.9 | wpb 933.5 | bsz 59.6 | num_updates 54189 | best_bleu 57.47
2022-08-17 13:29:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 669 @ 54189 updates
2022-08-17 13:29:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint669.pt
2022-08-17 13:29:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint669.pt
2022-08-17 13:30:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint669.pt (epoch 669 @ 54189 updates, score 56.65) (writing took 16.34918235987425 seconds)
2022-08-17 13:30:13 | INFO | fairseq_cli.train | end of epoch 669 (average epoch stats below)
2022-08-17 13:30:13 | INFO | train | epoch 669 | loss 3.378 | nll_loss 0.344 | ppl 1.27 | wps 4496 | ups 0.81 | wpb 5523.2 | bsz 358 | num_updates 54189 | lr 0.00027169 | gnorm 0.298 | train_wall 65 | gb_free 10 | wall 55109
2022-08-17 13:30:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:30:13 | INFO | fairseq.trainer | begin training epoch 670
2022-08-17 13:30:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:30:20 | INFO | train_inner | epoch 670:     11 / 81 loss=3.378, nll_loss=0.344, ppl=1.27, wps=3406, ups=0.62, wpb=5500.9, bsz=355, num_updates=54200, lr=0.000271663, gnorm=0.291, train_wall=75, gb_free=10.1, wall=55116
2022-08-17 13:30:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:30:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:30:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:30:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:31:03 | INFO | valid | epoch 670 | valid on 'valid' subset | loss 5.14 | nll_loss 2.527 | ppl 5.76 | bleu 56.58 | wps 1826.7 | wpb 933.5 | bsz 59.6 | num_updates 54270 | best_bleu 57.47
2022-08-17 13:31:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 670 @ 54270 updates
2022-08-17 13:31:03 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint670.pt
2022-08-17 13:31:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint670.pt
2022-08-17 13:31:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint670.pt (epoch 670 @ 54270 updates, score 56.58) (writing took 2.4993702210485935 seconds)
2022-08-17 13:31:06 | INFO | fairseq_cli.train | end of epoch 670 (average epoch stats below)
2022-08-17 13:31:06 | INFO | train | epoch 670 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 8488.8 | ups 1.54 | wpb 5523.2 | bsz 358 | num_updates 54270 | lr 0.000271488 | gnorm 0.314 | train_wall 39 | gb_free 10.2 | wall 55162
2022-08-17 13:31:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:31:06 | INFO | fairseq.trainer | begin training epoch 671
2022-08-17 13:31:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:31:22 | INFO | train_inner | epoch 671:     30 / 81 loss=3.379, nll_loss=0.346, ppl=1.27, wps=8865.5, ups=1.61, wpb=5515.9, bsz=357.8, num_updates=54300, lr=0.000271413, gnorm=0.321, train_wall=48, gb_free=10, wall=55178
2022-08-17 13:31:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:31:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:31:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:31:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:31:56 | INFO | valid | epoch 671 | valid on 'valid' subset | loss 5.163 | nll_loss 2.556 | ppl 5.88 | bleu 56.4 | wps 1779.8 | wpb 933.5 | bsz 59.6 | num_updates 54351 | best_bleu 57.47
2022-08-17 13:31:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 671 @ 54351 updates
2022-08-17 13:31:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint671.pt
2022-08-17 13:31:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint671.pt
2022-08-17 13:32:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint671.pt (epoch 671 @ 54351 updates, score 56.4) (writing took 27.245173059403896 seconds)
2022-08-17 13:32:24 | INFO | fairseq_cli.train | end of epoch 671 (average epoch stats below)
2022-08-17 13:32:24 | INFO | train | epoch 671 | loss 3.379 | nll_loss 0.345 | ppl 1.27 | wps 5736.9 | ups 1.04 | wpb 5523.2 | bsz 358 | num_updates 54351 | lr 0.000271285 | gnorm 0.293 | train_wall 39 | gb_free 10.1 | wall 55240
2022-08-17 13:32:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:32:24 | INFO | fairseq.trainer | begin training epoch 672
2022-08-17 13:32:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:32:49 | INFO | train_inner | epoch 672:     49 / 81 loss=3.378, nll_loss=0.344, ppl=1.27, wps=6383.1, ups=1.15, wpb=5541.4, bsz=357.2, num_updates=54400, lr=0.000271163, gnorm=0.276, train_wall=48, gb_free=10, wall=55265
2022-08-17 13:33:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:33:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:33:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:33:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:33:14 | INFO | valid | epoch 672 | valid on 'valid' subset | loss 5.166 | nll_loss 2.564 | ppl 5.91 | bleu 56.57 | wps 1828.7 | wpb 933.5 | bsz 59.6 | num_updates 54432 | best_bleu 57.47
2022-08-17 13:33:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 672 @ 54432 updates
2022-08-17 13:33:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint672.pt
2022-08-17 13:33:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint672.pt
2022-08-17 13:33:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint672.pt (epoch 672 @ 54432 updates, score 56.57) (writing took 24.666413739323616 seconds)
2022-08-17 13:33:39 | INFO | fairseq_cli.train | end of epoch 672 (average epoch stats below)
2022-08-17 13:33:39 | INFO | train | epoch 672 | loss 3.377 | nll_loss 0.343 | ppl 1.27 | wps 5929.7 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 54432 | lr 0.000271083 | gnorm 0.281 | train_wall 39 | gb_free 10.1 | wall 55315
2022-08-17 13:33:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:33:39 | INFO | fairseq.trainer | begin training epoch 673
2022-08-17 13:33:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:34:14 | INFO | train_inner | epoch 673:     68 / 81 loss=3.377, nll_loss=0.344, ppl=1.27, wps=6502.4, ups=1.18, wpb=5520.9, bsz=359.5, num_updates=54500, lr=0.000270914, gnorm=0.291, train_wall=49, gb_free=10.1, wall=55350
2022-08-17 13:34:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:34:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:34:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:34:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:34:30 | INFO | valid | epoch 673 | valid on 'valid' subset | loss 5.155 | nll_loss 2.551 | ppl 5.86 | bleu 56.63 | wps 1699 | wpb 933.5 | bsz 59.6 | num_updates 54513 | best_bleu 57.47
2022-08-17 13:34:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 673 @ 54513 updates
2022-08-17 13:34:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint673.pt
2022-08-17 13:34:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint673.pt
2022-08-17 13:34:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint673.pt (epoch 673 @ 54513 updates, score 56.63) (writing took 23.94260599464178 seconds)
2022-08-17 13:34:54 | INFO | fairseq_cli.train | end of epoch 673 (average epoch stats below)
2022-08-17 13:34:54 | INFO | train | epoch 673 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 5996.3 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 54513 | lr 0.000270882 | gnorm 0.279 | train_wall 39 | gb_free 10 | wall 55390
2022-08-17 13:34:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:34:54 | INFO | fairseq.trainer | begin training epoch 674
2022-08-17 13:34:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:35:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:35:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:35:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:35:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:35:43 | INFO | valid | epoch 674 | valid on 'valid' subset | loss 5.182 | nll_loss 2.582 | ppl 5.99 | bleu 56.25 | wps 1882.1 | wpb 933.5 | bsz 59.6 | num_updates 54594 | best_bleu 57.47
2022-08-17 13:35:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 674 @ 54594 updates
2022-08-17 13:35:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint674.pt
2022-08-17 13:35:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint674.pt
2022-08-17 13:36:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint674.pt (epoch 674 @ 54594 updates, score 56.25) (writing took 24.082466065883636 seconds)
2022-08-17 13:36:07 | INFO | fairseq_cli.train | end of epoch 674 (average epoch stats below)
2022-08-17 13:36:07 | INFO | train | epoch 674 | loss 3.378 | nll_loss 0.344 | ppl 1.27 | wps 6075.4 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 54594 | lr 0.000270681 | gnorm 0.288 | train_wall 39 | gb_free 10.1 | wall 55464
2022-08-17 13:36:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:36:08 | INFO | fairseq.trainer | begin training epoch 675
2022-08-17 13:36:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:36:12 | INFO | train_inner | epoch 675:      6 / 81 loss=3.378, nll_loss=0.345, ppl=1.27, wps=4675.5, ups=0.85, wpb=5511.6, bsz=354.7, num_updates=54600, lr=0.000270666, gnorm=0.286, train_wall=47, gb_free=10.1, wall=55468
2022-08-17 13:36:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:36:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:36:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:36:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:36:59 | INFO | valid | epoch 675 | valid on 'valid' subset | loss 5.166 | nll_loss 2.563 | ppl 5.91 | bleu 56.75 | wps 1668 | wpb 933.5 | bsz 59.6 | num_updates 54675 | best_bleu 57.47
2022-08-17 13:36:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 675 @ 54675 updates
2022-08-17 13:36:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint675.pt
2022-08-17 13:37:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint675.pt
2022-08-17 13:37:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint675.pt (epoch 675 @ 54675 updates, score 56.75) (writing took 29.972900059074163 seconds)
2022-08-17 13:37:29 | INFO | fairseq_cli.train | end of epoch 675 (average epoch stats below)
2022-08-17 13:37:29 | INFO | train | epoch 675 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 5500.5 | ups 1 | wpb 5523.2 | bsz 358 | num_updates 54675 | lr 0.00027048 | gnorm 0.478 | train_wall 39 | gb_free 10.2 | wall 55545
2022-08-17 13:37:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:37:29 | INFO | fairseq.trainer | begin training epoch 676
2022-08-17 13:37:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:37:43 | INFO | train_inner | epoch 676:     25 / 81 loss=3.378, nll_loss=0.344, ppl=1.27, wps=6087, ups=1.1, wpb=5531.5, bsz=364.5, num_updates=54700, lr=0.000270418, gnorm=0.438, train_wall=48, gb_free=10.1, wall=55559
2022-08-17 13:38:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:38:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:38:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:38:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:38:20 | INFO | valid | epoch 676 | valid on 'valid' subset | loss 5.153 | nll_loss 2.549 | ppl 5.85 | bleu 56.47 | wps 1820.2 | wpb 933.5 | bsz 59.6 | num_updates 54756 | best_bleu 57.47
2022-08-17 13:38:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 676 @ 54756 updates
2022-08-17 13:38:20 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint676.pt
2022-08-17 13:38:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint676.pt
2022-08-17 13:38:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint676.pt (epoch 676 @ 54756 updates, score 56.47) (writing took 15.221148312091827 seconds)
2022-08-17 13:38:35 | INFO | fairseq_cli.train | end of epoch 676 (average epoch stats below)
2022-08-17 13:38:35 | INFO | train | epoch 676 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 6768.1 | ups 1.23 | wpb 5523.2 | bsz 358 | num_updates 54756 | lr 0.00027028 | gnorm 0.321 | train_wall 39 | gb_free 10.1 | wall 55611
2022-08-17 13:38:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:38:35 | INFO | fairseq.trainer | begin training epoch 677
2022-08-17 13:38:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:38:58 | INFO | train_inner | epoch 677:     44 / 81 loss=3.379, nll_loss=0.345, ppl=1.27, wps=7261.5, ups=1.32, wpb=5514, bsz=353.8, num_updates=54800, lr=0.000270172, gnorm=0.319, train_wall=49, gb_free=10.1, wall=55635
2022-08-17 13:39:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:39:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:39:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:39:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:39:25 | INFO | valid | epoch 677 | valid on 'valid' subset | loss 5.158 | nll_loss 2.552 | ppl 5.87 | bleu 56.29 | wps 1850.6 | wpb 933.5 | bsz 59.6 | num_updates 54837 | best_bleu 57.47
2022-08-17 13:39:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 677 @ 54837 updates
2022-08-17 13:39:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint677.pt
2022-08-17 13:39:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint677.pt
2022-08-17 13:39:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint677.pt (epoch 677 @ 54837 updates, score 56.29) (writing took 19.125879116356373 seconds)
2022-08-17 13:39:45 | INFO | fairseq_cli.train | end of epoch 677 (average epoch stats below)
2022-08-17 13:39:45 | INFO | train | epoch 677 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 6412.2 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 54837 | lr 0.00027008 | gnorm 0.315 | train_wall 39 | gb_free 10.1 | wall 55681
2022-08-17 13:39:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:39:45 | INFO | fairseq.trainer | begin training epoch 678
2022-08-17 13:39:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:40:17 | INFO | train_inner | epoch 678:     63 / 81 loss=3.378, nll_loss=0.345, ppl=1.27, wps=7054.8, ups=1.27, wpb=5546.3, bsz=359.6, num_updates=54900, lr=0.000269925, gnorm=0.295, train_wall=48, gb_free=10.1, wall=55713
2022-08-17 13:40:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:40:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:40:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:40:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:40:37 | INFO | valid | epoch 678 | valid on 'valid' subset | loss 5.152 | nll_loss 2.546 | ppl 5.84 | bleu 56.65 | wps 1533.1 | wpb 933.5 | bsz 59.6 | num_updates 54918 | best_bleu 57.47
2022-08-17 13:40:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 678 @ 54918 updates
2022-08-17 13:40:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint678.pt
2022-08-17 13:40:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint678.pt
2022-08-17 13:41:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint678.pt (epoch 678 @ 54918 updates, score 56.65) (writing took 38.65133771672845 seconds)
2022-08-17 13:41:16 | INFO | fairseq_cli.train | end of epoch 678 (average epoch stats below)
2022-08-17 13:41:16 | INFO | train | epoch 678 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 4923.1 | ups 0.89 | wpb 5523.2 | bsz 358 | num_updates 54918 | lr 0.000269881 | gnorm 0.269 | train_wall 39 | gb_free 10.1 | wall 55772
2022-08-17 13:41:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:41:16 | INFO | fairseq.trainer | begin training epoch 679
2022-08-17 13:41:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:41:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:41:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:41:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:41:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:42:05 | INFO | valid | epoch 679 | valid on 'valid' subset | loss 5.175 | nll_loss 2.575 | ppl 5.96 | bleu 56.84 | wps 1820.5 | wpb 933.5 | bsz 59.6 | num_updates 54999 | best_bleu 57.47
2022-08-17 13:42:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 679 @ 54999 updates
2022-08-17 13:42:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint679.pt
2022-08-17 13:42:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint679.pt
2022-08-17 13:42:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint679.pt (epoch 679 @ 54999 updates, score 56.84) (writing took 22.736355125904083 seconds)
2022-08-17 13:42:28 | INFO | fairseq_cli.train | end of epoch 679 (average epoch stats below)
2022-08-17 13:42:28 | INFO | train | epoch 679 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 6194.3 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 54999 | lr 0.000269682 | gnorm 0.337 | train_wall 38 | gb_free 10.1 | wall 55844
2022-08-17 13:42:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:42:28 | INFO | fairseq.trainer | begin training epoch 680
2022-08-17 13:42:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:42:30 | INFO | train_inner | epoch 680:      1 / 81 loss=3.378, nll_loss=0.345, ppl=1.27, wps=4144.1, ups=0.75, wpb=5489.3, bsz=355.4, num_updates=55000, lr=0.00026968, gnorm=0.324, train_wall=47, gb_free=10, wall=55846
2022-08-17 13:43:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:43:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:43:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:43:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:43:18 | INFO | valid | epoch 680 | valid on 'valid' subset | loss 5.153 | nll_loss 2.548 | ppl 5.85 | bleu 56.64 | wps 1794.9 | wpb 933.5 | bsz 59.6 | num_updates 55080 | best_bleu 57.47
2022-08-17 13:43:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 680 @ 55080 updates
2022-08-17 13:43:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint680.pt
2022-08-17 13:43:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint680.pt
2022-08-17 13:43:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint680.pt (epoch 680 @ 55080 updates, score 56.64) (writing took 2.425608679652214 seconds)
2022-08-17 13:43:21 | INFO | fairseq_cli.train | end of epoch 680 (average epoch stats below)
2022-08-17 13:43:21 | INFO | train | epoch 680 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 8484.7 | ups 1.54 | wpb 5523.2 | bsz 358 | num_updates 55080 | lr 0.000269484 | gnorm 0.52 | train_wall 39 | gb_free 10.1 | wall 55897
2022-08-17 13:43:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:43:21 | INFO | fairseq.trainer | begin training epoch 681
2022-08-17 13:43:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:43:32 | INFO | train_inner | epoch 681:     20 / 81 loss=3.378, nll_loss=0.344, ppl=1.27, wps=8905.6, ups=1.61, wpb=5531.9, bsz=359.8, num_updates=55100, lr=0.000269435, gnorm=0.478, train_wall=48, gb_free=10.1, wall=55908
2022-08-17 13:44:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:44:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:44:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:44:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:44:11 | INFO | valid | epoch 681 | valid on 'valid' subset | loss 5.163 | nll_loss 2.559 | ppl 5.89 | bleu 56.76 | wps 1832.6 | wpb 933.5 | bsz 59.6 | num_updates 55161 | best_bleu 57.47
2022-08-17 13:44:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 681 @ 55161 updates
2022-08-17 13:44:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint681.pt
2022-08-17 13:44:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint681.pt
2022-08-17 13:44:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint681.pt (epoch 681 @ 55161 updates, score 56.76) (writing took 35.47838598489761 seconds)
2022-08-17 13:44:47 | INFO | fairseq_cli.train | end of epoch 681 (average epoch stats below)
2022-08-17 13:44:47 | INFO | train | epoch 681 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 5188.9 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 55161 | lr 0.000269286 | gnorm 0.349 | train_wall 40 | gb_free 10.2 | wall 55983
2022-08-17 13:44:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:44:47 | INFO | fairseq.trainer | begin training epoch 682
2022-08-17 13:44:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:45:09 | INFO | train_inner | epoch 682:     39 / 81 loss=3.378, nll_loss=0.345, ppl=1.27, wps=5714.3, ups=1.03, wpb=5547, bsz=356.2, num_updates=55200, lr=0.000269191, gnorm=0.341, train_wall=49, gb_free=10.1, wall=56005
2022-08-17 13:45:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:45:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:45:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:45:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:45:40 | INFO | valid | epoch 682 | valid on 'valid' subset | loss 5.16 | nll_loss 2.553 | ppl 5.87 | bleu 56.84 | wps 1764.5 | wpb 933.5 | bsz 59.6 | num_updates 55242 | best_bleu 57.47
2022-08-17 13:45:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 682 @ 55242 updates
2022-08-17 13:45:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint682.pt
2022-08-17 13:45:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint682.pt
2022-08-17 13:46:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint682.pt (epoch 682 @ 55242 updates, score 56.84) (writing took 19.967973455786705 seconds)
2022-08-17 13:46:00 | INFO | fairseq_cli.train | end of epoch 682 (average epoch stats below)
2022-08-17 13:46:00 | INFO | train | epoch 682 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 6069.4 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 55242 | lr 0.000269089 | gnorm 0.34 | train_wall 40 | gb_free 10.1 | wall 56057
2022-08-17 13:46:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:46:01 | INFO | fairseq.trainer | begin training epoch 683
2022-08-17 13:46:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:46:31 | INFO | train_inner | epoch 683:     58 / 81 loss=3.378, nll_loss=0.345, ppl=1.27, wps=6742.1, ups=1.22, wpb=5531.1, bsz=362.4, num_updates=55300, lr=0.000268947, gnorm=0.336, train_wall=49, gb_free=10.1, wall=56087
2022-08-17 13:46:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:46:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:46:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:46:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:46:51 | INFO | valid | epoch 683 | valid on 'valid' subset | loss 5.175 | nll_loss 2.579 | ppl 5.98 | bleu 55.93 | wps 1794.4 | wpb 933.5 | bsz 59.6 | num_updates 55323 | best_bleu 57.47
2022-08-17 13:46:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 683 @ 55323 updates
2022-08-17 13:46:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint683.pt
2022-08-17 13:46:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint683.pt
2022-08-17 13:47:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint683.pt (epoch 683 @ 55323 updates, score 55.93) (writing took 17.7649577409029 seconds)
2022-08-17 13:47:09 | INFO | fairseq_cli.train | end of epoch 683 (average epoch stats below)
2022-08-17 13:47:09 | INFO | train | epoch 683 | loss 3.378 | nll_loss 0.344 | ppl 1.27 | wps 6492.8 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 55323 | lr 0.000268892 | gnorm 0.325 | train_wall 39 | gb_free 10.2 | wall 56126
2022-08-17 13:47:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:47:10 | INFO | fairseq.trainer | begin training epoch 684
2022-08-17 13:47:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:47:49 | INFO | train_inner | epoch 684:     77 / 81 loss=3.378, nll_loss=0.345, ppl=1.27, wps=7018.9, ups=1.27, wpb=5507.9, bsz=354.4, num_updates=55400, lr=0.000268705, gnorm=0.363, train_wall=49, gb_free=10.1, wall=56166
2022-08-17 13:47:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:47:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:47:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:47:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:48:01 | INFO | valid | epoch 684 | valid on 'valid' subset | loss 5.154 | nll_loss 2.546 | ppl 5.84 | bleu 56.56 | wps 1770 | wpb 933.5 | bsz 59.6 | num_updates 55404 | best_bleu 57.47
2022-08-17 13:48:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 684 @ 55404 updates
2022-08-17 13:48:01 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint684.pt
2022-08-17 13:48:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint684.pt
2022-08-17 13:48:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint684.pt (epoch 684 @ 55404 updates, score 56.56) (writing took 13.701926078647375 seconds)
2022-08-17 13:48:15 | INFO | fairseq_cli.train | end of epoch 684 (average epoch stats below)
2022-08-17 13:48:15 | INFO | train | epoch 684 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 6814.8 | ups 1.23 | wpb 5523.2 | bsz 358 | num_updates 55404 | lr 0.000268695 | gnorm 0.357 | train_wall 40 | gb_free 10.1 | wall 56191
2022-08-17 13:48:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:48:15 | INFO | fairseq.trainer | begin training epoch 685
2022-08-17 13:48:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:48:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:48:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:48:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:48:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:49:06 | INFO | valid | epoch 685 | valid on 'valid' subset | loss 5.16 | nll_loss 2.553 | ppl 5.87 | bleu 55.85 | wps 1786.6 | wpb 933.5 | bsz 59.6 | num_updates 55485 | best_bleu 57.47
2022-08-17 13:49:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 685 @ 55485 updates
2022-08-17 13:49:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint685.pt
2022-08-17 13:49:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint685.pt
2022-08-17 13:49:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint685.pt (epoch 685 @ 55485 updates, score 55.85) (writing took 19.705319453030825 seconds)
2022-08-17 13:49:26 | INFO | fairseq_cli.train | end of epoch 685 (average epoch stats below)
2022-08-17 13:49:26 | INFO | train | epoch 685 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 6335.2 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 55485 | lr 0.000268499 | gnorm 0.369 | train_wall 39 | gb_free 10.1 | wall 56262
2022-08-17 13:49:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:49:26 | INFO | fairseq.trainer | begin training epoch 686
2022-08-17 13:49:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:49:34 | INFO | train_inner | epoch 686:     15 / 81 loss=3.378, nll_loss=0.345, ppl=1.27, wps=5229.4, ups=0.95, wpb=5494.1, bsz=356.1, num_updates=55500, lr=0.000268462, gnorm=0.356, train_wall=49, gb_free=10.1, wall=56271
2022-08-17 13:50:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:50:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:50:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:50:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:50:16 | INFO | valid | epoch 686 | valid on 'valid' subset | loss 5.158 | nll_loss 2.553 | ppl 5.87 | bleu 56.46 | wps 1829 | wpb 933.5 | bsz 59.6 | num_updates 55566 | best_bleu 57.47
2022-08-17 13:50:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 686 @ 55566 updates
2022-08-17 13:50:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint686.pt
2022-08-17 13:50:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint686.pt
2022-08-17 13:50:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint686.pt (epoch 686 @ 55566 updates, score 56.46) (writing took 33.19752883911133 seconds)
2022-08-17 13:50:50 | INFO | fairseq_cli.train | end of epoch 686 (average epoch stats below)
2022-08-17 13:50:50 | INFO | train | epoch 686 | loss 3.379 | nll_loss 0.346 | ppl 1.27 | wps 5324.1 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 55566 | lr 0.000268303 | gnorm 0.376 | train_wall 40 | gb_free 10.1 | wall 56346
2022-08-17 13:50:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:50:50 | INFO | fairseq.trainer | begin training epoch 687
2022-08-17 13:50:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:51:08 | INFO | train_inner | epoch 687:     34 / 81 loss=3.378, nll_loss=0.345, ppl=1.27, wps=5893.9, ups=1.06, wpb=5541.7, bsz=364.3, num_updates=55600, lr=0.000268221, gnorm=0.373, train_wall=49, gb_free=10.1, wall=56365
2022-08-17 13:51:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:51:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:51:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:51:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:51:42 | INFO | valid | epoch 687 | valid on 'valid' subset | loss 5.159 | nll_loss 2.554 | ppl 5.87 | bleu 55.99 | wps 1618.2 | wpb 933.5 | bsz 59.6 | num_updates 55647 | best_bleu 57.47
2022-08-17 13:51:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 687 @ 55647 updates
2022-08-17 13:51:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint687.pt
2022-08-17 13:51:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint687.pt
2022-08-17 13:52:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint687.pt (epoch 687 @ 55647 updates, score 55.99) (writing took 20.820882208645344 seconds)
2022-08-17 13:52:03 | INFO | fairseq_cli.train | end of epoch 687 (average epoch stats below)
2022-08-17 13:52:03 | INFO | train | epoch 687 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 6128.4 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 55647 | lr 0.000268108 | gnorm 0.461 | train_wall 40 | gb_free 10.2 | wall 56419
2022-08-17 13:52:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:52:03 | INFO | fairseq.trainer | begin training epoch 688
2022-08-17 13:52:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:52:34 | INFO | train_inner | epoch 688:     53 / 81 loss=3.378, nll_loss=0.344, ppl=1.27, wps=6463.3, ups=1.17, wpb=5520.1, bsz=353.5, num_updates=55700, lr=0.00026798, gnorm=0.418, train_wall=49, gb_free=10.1, wall=56450
2022-08-17 13:52:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:52:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:52:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:52:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:52:59 | INFO | valid | epoch 688 | valid on 'valid' subset | loss 5.16 | nll_loss 2.553 | ppl 5.87 | bleu 56.33 | wps 1830.2 | wpb 933.5 | bsz 59.6 | num_updates 55728 | best_bleu 57.47
2022-08-17 13:52:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 688 @ 55728 updates
2022-08-17 13:52:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint688.pt
2022-08-17 13:53:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint688.pt
2022-08-17 13:53:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint688.pt (epoch 688 @ 55728 updates, score 56.33) (writing took 2.5622841231524944 seconds)
2022-08-17 13:53:02 | INFO | fairseq_cli.train | end of epoch 688 (average epoch stats below)
2022-08-17 13:53:02 | INFO | train | epoch 688 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 7544.5 | ups 1.37 | wpb 5523.2 | bsz 358 | num_updates 55728 | lr 0.000267913 | gnorm 0.296 | train_wall 40 | gb_free 10.3 | wall 56478
2022-08-17 13:53:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:53:02 | INFO | fairseq.trainer | begin training epoch 689
2022-08-17 13:53:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:53:39 | INFO | train_inner | epoch 689:     72 / 81 loss=3.377, nll_loss=0.344, ppl=1.27, wps=8488, ups=1.54, wpb=5526.4, bsz=358, num_updates=55800, lr=0.00026774, gnorm=0.317, train_wall=49, gb_free=10, wall=56515
2022-08-17 13:53:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:53:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:53:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:53:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:53:53 | INFO | valid | epoch 689 | valid on 'valid' subset | loss 5.179 | nll_loss 2.581 | ppl 5.98 | bleu 55.88 | wps 1796.1 | wpb 933.5 | bsz 59.6 | num_updates 55809 | best_bleu 57.47
2022-08-17 13:53:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 689 @ 55809 updates
2022-08-17 13:53:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint689.pt
2022-08-17 13:53:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint689.pt
2022-08-17 13:54:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint689.pt (epoch 689 @ 55809 updates, score 55.88) (writing took 32.271156284958124 seconds)
2022-08-17 13:54:25 | INFO | fairseq_cli.train | end of epoch 689 (average epoch stats below)
2022-08-17 13:54:25 | INFO | train | epoch 689 | loss 3.377 | nll_loss 0.343 | ppl 1.27 | wps 5381.7 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 55809 | lr 0.000267718 | gnorm 0.328 | train_wall 39 | gb_free 10.1 | wall 56561
2022-08-17 13:54:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:54:25 | INFO | fairseq.trainer | begin training epoch 690
2022-08-17 13:54:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:55:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:55:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:55:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:55:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:55:16 | INFO | valid | epoch 690 | valid on 'valid' subset | loss 5.152 | nll_loss 2.545 | ppl 5.84 | bleu 56.48 | wps 1760 | wpb 933.5 | bsz 59.6 | num_updates 55890 | best_bleu 57.47
2022-08-17 13:55:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 690 @ 55890 updates
2022-08-17 13:55:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint690.pt
2022-08-17 13:55:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint690.pt
2022-08-17 13:55:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint690.pt (epoch 690 @ 55890 updates, score 56.48) (writing took 17.865760907530785 seconds)
2022-08-17 13:55:34 | INFO | fairseq_cli.train | end of epoch 690 (average epoch stats below)
2022-08-17 13:55:34 | INFO | train | epoch 690 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 6466.3 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 55890 | lr 0.000267524 | gnorm 0.43 | train_wall 39 | gb_free 10 | wall 56631
2022-08-17 13:55:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:55:35 | INFO | fairseq.trainer | begin training epoch 691
2022-08-17 13:55:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:55:41 | INFO | train_inner | epoch 691:     10 / 81 loss=3.378, nll_loss=0.345, ppl=1.27, wps=4496.6, ups=0.82, wpb=5510.9, bsz=358.7, num_updates=55900, lr=0.0002675, gnorm=0.405, train_wall=48, gb_free=10.1, wall=56638
2022-08-17 13:56:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:56:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:56:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:56:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:56:29 | INFO | valid | epoch 691 | valid on 'valid' subset | loss 5.159 | nll_loss 2.554 | ppl 5.87 | bleu 56.41 | wps 1668.7 | wpb 933.5 | bsz 59.6 | num_updates 55971 | best_bleu 57.47
2022-08-17 13:56:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 691 @ 55971 updates
2022-08-17 13:56:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint691.pt
2022-08-17 13:56:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint691.pt
2022-08-17 13:57:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint691.pt (epoch 691 @ 55971 updates, score 56.41) (writing took 37.624432630836964 seconds)
2022-08-17 13:57:07 | INFO | fairseq_cli.train | end of epoch 691 (average epoch stats below)
2022-08-17 13:57:07 | INFO | train | epoch 691 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 4836.5 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 55971 | lr 0.00026733 | gnorm 0.34 | train_wall 39 | gb_free 10.1 | wall 56723
2022-08-17 13:57:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:57:07 | INFO | fairseq.trainer | begin training epoch 692
2022-08-17 13:57:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:57:23 | INFO | train_inner | epoch 692:     29 / 81 loss=3.378, nll_loss=0.345, ppl=1.27, wps=5476.1, ups=0.99, wpb=5540.5, bsz=356.2, num_updates=56000, lr=0.000267261, gnorm=0.327, train_wall=48, gb_free=10.1, wall=56739
2022-08-17 13:57:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:57:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:57:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:57:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:57:57 | INFO | valid | epoch 692 | valid on 'valid' subset | loss 5.172 | nll_loss 2.568 | ppl 5.93 | bleu 56.34 | wps 1841.9 | wpb 933.5 | bsz 59.6 | num_updates 56052 | best_bleu 57.47
2022-08-17 13:57:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 692 @ 56052 updates
2022-08-17 13:57:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint692.pt
2022-08-17 13:57:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint692.pt
2022-08-17 13:58:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint692.pt (epoch 692 @ 56052 updates, score 56.34) (writing took 18.486947119235992 seconds)
2022-08-17 13:58:16 | INFO | fairseq_cli.train | end of epoch 692 (average epoch stats below)
2022-08-17 13:58:16 | INFO | train | epoch 692 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 6462 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 56052 | lr 0.000267137 | gnorm 0.277 | train_wall 39 | gb_free 10.2 | wall 56792
2022-08-17 13:58:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:58:16 | INFO | fairseq.trainer | begin training epoch 693
2022-08-17 13:58:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:58:41 | INFO | train_inner | epoch 693:     48 / 81 loss=3.378, nll_loss=0.344, ppl=1.27, wps=7023.9, ups=1.27, wpb=5525.3, bsz=359.1, num_updates=56100, lr=0.000267023, gnorm=0.294, train_wall=48, gb_free=10.1, wall=56818
2022-08-17 13:58:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 13:58:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 13:58:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 13:58:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 13:59:06 | INFO | valid | epoch 693 | valid on 'valid' subset | loss 5.18 | nll_loss 2.577 | ppl 5.97 | bleu 56.3 | wps 1859.2 | wpb 933.5 | bsz 59.6 | num_updates 56133 | best_bleu 57.47
2022-08-17 13:59:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 693 @ 56133 updates
2022-08-17 13:59:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint693.pt
2022-08-17 13:59:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint693.pt
2022-08-17 13:59:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint693.pt (epoch 693 @ 56133 updates, score 56.3) (writing took 18.240582577884197 seconds)
2022-08-17 13:59:24 | INFO | fairseq_cli.train | end of epoch 693 (average epoch stats below)
2022-08-17 13:59:24 | INFO | train | epoch 693 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 6545.3 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 56133 | lr 0.000266944 | gnorm 0.323 | train_wall 38 | gb_free 10.1 | wall 56861
2022-08-17 13:59:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 13:59:25 | INFO | fairseq.trainer | begin training epoch 694
2022-08-17 13:59:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 13:59:58 | INFO | train_inner | epoch 694:     67 / 81 loss=3.378, nll_loss=0.345, ppl=1.27, wps=7182, ups=1.3, wpb=5532.8, bsz=360.8, num_updates=56200, lr=0.000266785, gnorm=0.325, train_wall=48, gb_free=10, wall=56895
2022-08-17 14:00:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:00:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:00:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:00:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:00:14 | INFO | valid | epoch 694 | valid on 'valid' subset | loss 5.166 | nll_loss 2.561 | ppl 5.9 | bleu 56.02 | wps 1770.9 | wpb 933.5 | bsz 59.6 | num_updates 56214 | best_bleu 57.47
2022-08-17 14:00:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 694 @ 56214 updates
2022-08-17 14:00:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint694.pt
2022-08-17 14:00:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint694.pt
2022-08-17 14:00:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint694.pt (epoch 694 @ 56214 updates, score 56.02) (writing took 33.033460799604654 seconds)
2022-08-17 14:00:48 | INFO | fairseq_cli.train | end of epoch 694 (average epoch stats below)
2022-08-17 14:00:48 | INFO | train | epoch 694 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 5377.9 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 56214 | lr 0.000266752 | gnorm 0.327 | train_wall 38 | gb_free 10.1 | wall 56944
2022-08-17 14:00:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:00:48 | INFO | fairseq.trainer | begin training epoch 695
2022-08-17 14:00:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:01:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:01:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:01:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:01:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:01:39 | INFO | valid | epoch 695 | valid on 'valid' subset | loss 5.171 | nll_loss 2.573 | ppl 5.95 | bleu 56.21 | wps 1804 | wpb 933.5 | bsz 59.6 | num_updates 56295 | best_bleu 57.47
2022-08-17 14:01:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 695 @ 56295 updates
2022-08-17 14:01:39 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint695.pt
2022-08-17 14:01:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint695.pt
2022-08-17 14:01:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint695.pt (epoch 695 @ 56295 updates, score 56.21) (writing took 2.3464971147477627 seconds)
2022-08-17 14:01:41 | INFO | fairseq_cli.train | end of epoch 695 (average epoch stats below)
2022-08-17 14:01:41 | INFO | train | epoch 695 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 8314.6 | ups 1.51 | wpb 5523.2 | bsz 358 | num_updates 56295 | lr 0.00026656 | gnorm 0.324 | train_wall 39 | gb_free 10.3 | wall 56998
2022-08-17 14:01:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:01:42 | INFO | fairseq.trainer | begin training epoch 696
2022-08-17 14:01:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:01:45 | INFO | train_inner | epoch 696:      5 / 81 loss=3.378, nll_loss=0.345, ppl=1.27, wps=5150.3, ups=0.94, wpb=5496, bsz=353.1, num_updates=56300, lr=0.000266548, gnorm=0.327, train_wall=48, gb_free=10.1, wall=57001
2022-08-17 14:02:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:02:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:02:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:02:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:02:33 | INFO | valid | epoch 696 | valid on 'valid' subset | loss 5.175 | nll_loss 2.573 | ppl 5.95 | bleu 56.24 | wps 1759 | wpb 933.5 | bsz 59.6 | num_updates 56376 | best_bleu 57.47
2022-08-17 14:02:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 696 @ 56376 updates
2022-08-17 14:02:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint696.pt
2022-08-17 14:02:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint696.pt
2022-08-17 14:02:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint696.pt (epoch 696 @ 56376 updates, score 56.24) (writing took 17.18709732964635 seconds)
2022-08-17 14:02:50 | INFO | fairseq_cli.train | end of epoch 696 (average epoch stats below)
2022-08-17 14:02:50 | INFO | train | epoch 696 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 6529.3 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 56376 | lr 0.000266369 | gnorm 0.278 | train_wall 40 | gb_free 10.2 | wall 57066
2022-08-17 14:02:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:02:50 | INFO | fairseq.trainer | begin training epoch 697
2022-08-17 14:02:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:03:03 | INFO | train_inner | epoch 697:     24 / 81 loss=3.377, nll_loss=0.344, ppl=1.27, wps=7129.5, ups=1.29, wpb=5526, bsz=357, num_updates=56400, lr=0.000266312, gnorm=0.324, train_wall=48, gb_free=10.1, wall=57079
2022-08-17 14:03:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:03:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:03:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:03:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:03:40 | INFO | valid | epoch 697 | valid on 'valid' subset | loss 5.172 | nll_loss 2.571 | ppl 5.94 | bleu 56.05 | wps 1811.1 | wpb 933.5 | bsz 59.6 | num_updates 56457 | best_bleu 57.47
2022-08-17 14:03:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 697 @ 56457 updates
2022-08-17 14:03:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint697.pt
2022-08-17 14:03:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint697.pt
2022-08-17 14:03:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint697.pt (epoch 697 @ 56457 updates, score 56.05) (writing took 12.382029350847006 seconds)
2022-08-17 14:03:52 | INFO | fairseq_cli.train | end of epoch 697 (average epoch stats below)
2022-08-17 14:03:52 | INFO | train | epoch 697 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 7189.2 | ups 1.3 | wpb 5523.2 | bsz 358 | num_updates 56457 | lr 0.000266177 | gnorm 0.359 | train_wall 38 | gb_free 10.1 | wall 57128
2022-08-17 14:03:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:03:52 | INFO | fairseq.trainer | begin training epoch 698
2022-08-17 14:03:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:04:15 | INFO | train_inner | epoch 698:     43 / 81 loss=3.378, nll_loss=0.345, ppl=1.27, wps=7636.8, ups=1.38, wpb=5524.6, bsz=356.6, num_updates=56500, lr=0.000266076, gnorm=0.319, train_wall=48, gb_free=10.1, wall=57151
2022-08-17 14:04:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:04:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:04:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:04:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:04:42 | INFO | valid | epoch 698 | valid on 'valid' subset | loss 5.179 | nll_loss 2.578 | ppl 5.97 | bleu 56.55 | wps 1877.6 | wpb 933.5 | bsz 59.6 | num_updates 56538 | best_bleu 57.47
2022-08-17 14:04:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 698 @ 56538 updates
2022-08-17 14:04:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint698.pt
2022-08-17 14:04:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint698.pt
2022-08-17 14:04:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint698.pt (epoch 698 @ 56538 updates, score 56.55) (writing took 16.22117055952549 seconds)
2022-08-17 14:04:59 | INFO | fairseq_cli.train | end of epoch 698 (average epoch stats below)
2022-08-17 14:04:59 | INFO | train | epoch 698 | loss 3.378 | nll_loss 0.344 | ppl 1.27 | wps 6724.5 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 56538 | lr 0.000265987 | gnorm 0.333 | train_wall 39 | gb_free 10.1 | wall 57195
2022-08-17 14:04:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:04:59 | INFO | fairseq.trainer | begin training epoch 699
2022-08-17 14:04:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:05:30 | INFO | train_inner | epoch 699:     62 / 81 loss=3.378, nll_loss=0.344, ppl=1.27, wps=7383.8, ups=1.33, wpb=5547.8, bsz=366.8, num_updates=56600, lr=0.000265841, gnorm=0.314, train_wall=48, gb_free=10.1, wall=57226
2022-08-17 14:05:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:05:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:05:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:05:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:05:49 | INFO | valid | epoch 699 | valid on 'valid' subset | loss 5.165 | nll_loss 2.562 | ppl 5.91 | bleu 56.64 | wps 1736 | wpb 933.5 | bsz 59.6 | num_updates 56619 | best_bleu 57.47
2022-08-17 14:05:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 699 @ 56619 updates
2022-08-17 14:05:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint699.pt
2022-08-17 14:05:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint699.pt
2022-08-17 14:06:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint699.pt (epoch 699 @ 56619 updates, score 56.64) (writing took 16.691722318530083 seconds)
2022-08-17 14:06:06 | INFO | fairseq_cli.train | end of epoch 699 (average epoch stats below)
2022-08-17 14:06:06 | INFO | train | epoch 699 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 6620.8 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 56619 | lr 0.000265796 | gnorm 0.315 | train_wall 39 | gb_free 10.1 | wall 57262
2022-08-17 14:06:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:06:06 | INFO | fairseq.trainer | begin training epoch 700
2022-08-17 14:06:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:06:47 | INFO | train_inner | epoch 700:     81 / 81 loss=3.378, nll_loss=0.345, ppl=1.27, wps=7098.6, ups=1.29, wpb=5494, bsz=353.1, num_updates=56700, lr=0.000265606, gnorm=0.344, train_wall=49, gb_free=10.1, wall=57304
2022-08-17 14:06:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:06:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:06:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:06:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:06:58 | INFO | valid | epoch 700 | valid on 'valid' subset | loss 5.18 | nll_loss 2.581 | ppl 5.99 | bleu 55.99 | wps 1634.2 | wpb 933.5 | bsz 59.6 | num_updates 56700 | best_bleu 57.47
2022-08-17 14:06:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 700 @ 56700 updates
2022-08-17 14:06:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint700.pt
2022-08-17 14:06:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint700.pt
2022-08-17 14:07:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint700.pt (epoch 700 @ 56700 updates, score 55.99) (writing took 20.63814442232251 seconds)
2022-08-17 14:07:19 | INFO | fairseq_cli.train | end of epoch 700 (average epoch stats below)
2022-08-17 14:07:19 | INFO | train | epoch 700 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 6178.4 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 56700 | lr 0.000265606 | gnorm 0.337 | train_wall 39 | gb_free 10.1 | wall 57335
2022-08-17 14:07:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:07:19 | INFO | fairseq.trainer | begin training epoch 701
2022-08-17 14:07:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:08:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:08:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:08:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:08:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:08:11 | INFO | valid | epoch 701 | valid on 'valid' subset | loss 5.159 | nll_loss 2.553 | ppl 5.87 | bleu 56.68 | wps 1830.3 | wpb 933.5 | bsz 59.6 | num_updates 56781 | best_bleu 57.47
2022-08-17 14:08:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 701 @ 56781 updates
2022-08-17 14:08:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint701.pt
2022-08-17 14:08:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint701.pt
2022-08-17 14:08:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint701.pt (epoch 701 @ 56781 updates, score 56.68) (writing took 14.45586433634162 seconds)
2022-08-17 14:08:25 | INFO | fairseq_cli.train | end of epoch 701 (average epoch stats below)
2022-08-17 14:08:25 | INFO | train | epoch 701 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 6702.1 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 56781 | lr 0.000265417 | gnorm 0.364 | train_wall 41 | gb_free 10.3 | wall 57402
2022-08-17 14:08:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:08:26 | INFO | fairseq.trainer | begin training epoch 702
2022-08-17 14:08:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:08:36 | INFO | train_inner | epoch 702:     19 / 81 loss=3.378, nll_loss=0.344, ppl=1.27, wps=5093.6, ups=0.92, wpb=5536.1, bsz=357.8, num_updates=56800, lr=0.000265372, gnorm=0.356, train_wall=50, gb_free=10.1, wall=57412
2022-08-17 14:09:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:09:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:09:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:09:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:09:18 | INFO | valid | epoch 702 | valid on 'valid' subset | loss 5.166 | nll_loss 2.564 | ppl 5.91 | bleu 56.23 | wps 1671.2 | wpb 933.5 | bsz 59.6 | num_updates 56862 | best_bleu 57.47
2022-08-17 14:09:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 702 @ 56862 updates
2022-08-17 14:09:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint702.pt
2022-08-17 14:09:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint702.pt
2022-08-17 14:09:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint702.pt (epoch 702 @ 56862 updates, score 56.23) (writing took 14.335699517279863 seconds)
2022-08-17 14:09:33 | INFO | fairseq_cli.train | end of epoch 702 (average epoch stats below)
2022-08-17 14:09:33 | INFO | train | epoch 702 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 6634 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 56862 | lr 0.000265228 | gnorm 0.322 | train_wall 41 | gb_free 10.2 | wall 57469
2022-08-17 14:09:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:09:33 | INFO | fairseq.trainer | begin training epoch 703
2022-08-17 14:09:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:09:53 | INFO | train_inner | epoch 703:     38 / 81 loss=3.378, nll_loss=0.345, ppl=1.27, wps=7198.8, ups=1.3, wpb=5545, bsz=360.3, num_updates=56900, lr=0.000265139, gnorm=0.329, train_wall=50, gb_free=10.1, wall=57489
2022-08-17 14:10:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:10:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:10:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:10:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:10:24 | INFO | valid | epoch 703 | valid on 'valid' subset | loss 5.158 | nll_loss 2.553 | ppl 5.87 | bleu 56.56 | wps 1864.2 | wpb 933.5 | bsz 59.6 | num_updates 56943 | best_bleu 57.47
2022-08-17 14:10:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 703 @ 56943 updates
2022-08-17 14:10:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint703.pt
2022-08-17 14:10:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint703.pt
2022-08-17 14:10:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint703.pt (epoch 703 @ 56943 updates, score 56.56) (writing took 17.416802801191807 seconds)
2022-08-17 14:10:42 | INFO | fairseq_cli.train | end of epoch 703 (average epoch stats below)
2022-08-17 14:10:42 | INFO | train | epoch 703 | loss 3.378 | nll_loss 0.344 | ppl 1.27 | wps 6508.5 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 56943 | lr 0.000265039 | gnorm 0.324 | train_wall 40 | gb_free 10.2 | wall 57538
2022-08-17 14:10:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:10:42 | INFO | fairseq.trainer | begin training epoch 704
2022-08-17 14:10:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:11:11 | INFO | train_inner | epoch 704:     57 / 81 loss=3.377, nll_loss=0.344, ppl=1.27, wps=7073.5, ups=1.28, wpb=5508.6, bsz=357.2, num_updates=57000, lr=0.000264906, gnorm=0.314, train_wall=49, gb_free=10.1, wall=57567
2022-08-17 14:11:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:11:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:11:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:11:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:11:33 | INFO | valid | epoch 704 | valid on 'valid' subset | loss 5.167 | nll_loss 2.567 | ppl 5.93 | bleu 56.24 | wps 1778.4 | wpb 933.5 | bsz 59.6 | num_updates 57024 | best_bleu 57.47
2022-08-17 14:11:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 704 @ 57024 updates
2022-08-17 14:11:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint704.pt
2022-08-17 14:11:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint704.pt
2022-08-17 14:11:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint704.pt (epoch 704 @ 57024 updates, score 56.24) (writing took 17.15339943766594 seconds)
2022-08-17 14:11:50 | INFO | fairseq_cli.train | end of epoch 704 (average epoch stats below)
2022-08-17 14:11:50 | INFO | train | epoch 704 | loss 3.378 | nll_loss 0.344 | ppl 1.27 | wps 6551.5 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 57024 | lr 0.000264851 | gnorm 0.31 | train_wall 40 | gb_free 10.1 | wall 57606
2022-08-17 14:11:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:11:50 | INFO | fairseq.trainer | begin training epoch 705
2022-08-17 14:11:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:12:29 | INFO | train_inner | epoch 705:     76 / 81 loss=3.378, nll_loss=0.345, ppl=1.27, wps=7103.9, ups=1.28, wpb=5533, bsz=358.2, num_updates=57100, lr=0.000264674, gnorm=0.305, train_wall=49, gb_free=10.2, wall=57645
2022-08-17 14:12:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:12:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:12:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:12:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:12:41 | INFO | valid | epoch 705 | valid on 'valid' subset | loss 5.174 | nll_loss 2.572 | ppl 5.95 | bleu 56.05 | wps 1811.4 | wpb 933.5 | bsz 59.6 | num_updates 57105 | best_bleu 57.47
2022-08-17 14:12:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 705 @ 57105 updates
2022-08-17 14:12:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint705.pt
2022-08-17 14:12:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint705.pt
2022-08-17 14:12:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint705.pt (epoch 705 @ 57105 updates, score 56.05) (writing took 14.254513677209616 seconds)
2022-08-17 14:12:55 | INFO | fairseq_cli.train | end of epoch 705 (average epoch stats below)
2022-08-17 14:12:55 | INFO | train | epoch 705 | loss 3.378 | nll_loss 0.344 | ppl 1.27 | wps 6874.8 | ups 1.24 | wpb 5523.2 | bsz 358 | num_updates 57105 | lr 0.000264663 | gnorm 0.306 | train_wall 39 | gb_free 10.1 | wall 57671
2022-08-17 14:12:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:12:55 | INFO | fairseq.trainer | begin training epoch 706
2022-08-17 14:12:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:13:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:13:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:13:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:13:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:13:46 | INFO | valid | epoch 706 | valid on 'valid' subset | loss 5.179 | nll_loss 2.577 | ppl 5.97 | bleu 56.37 | wps 1800.2 | wpb 933.5 | bsz 59.6 | num_updates 57186 | best_bleu 57.47
2022-08-17 14:13:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 706 @ 57186 updates
2022-08-17 14:13:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint706.pt
2022-08-17 14:13:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint706.pt
2022-08-17 14:14:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint706.pt (epoch 706 @ 57186 updates, score 56.37) (writing took 19.88735195621848 seconds)
2022-08-17 14:14:06 | INFO | fairseq_cli.train | end of epoch 706 (average epoch stats below)
2022-08-17 14:14:06 | INFO | train | epoch 706 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 6265.2 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 57186 | lr 0.000264475 | gnorm 0.319 | train_wall 39 | gb_free 10.2 | wall 57743
2022-08-17 14:14:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:14:07 | INFO | fairseq.trainer | begin training epoch 707
2022-08-17 14:14:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:14:15 | INFO | train_inner | epoch 707:     14 / 81 loss=3.377, nll_loss=0.344, ppl=1.27, wps=5217.9, ups=0.95, wpb=5510.9, bsz=358.3, num_updates=57200, lr=0.000264443, gnorm=0.318, train_wall=47, gb_free=10.1, wall=57751
2022-08-17 14:14:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:14:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:14:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:14:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:14:57 | INFO | valid | epoch 707 | valid on 'valid' subset | loss 5.17 | nll_loss 2.569 | ppl 5.93 | bleu 56.22 | wps 1873 | wpb 933.5 | bsz 59.6 | num_updates 57267 | best_bleu 57.47
2022-08-17 14:14:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 707 @ 57267 updates
2022-08-17 14:14:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint707.pt
2022-08-17 14:14:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint707.pt
2022-08-17 14:15:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint707.pt (epoch 707 @ 57267 updates, score 56.22) (writing took 16.824349764734507 seconds)
2022-08-17 14:15:14 | INFO | fairseq_cli.train | end of epoch 707 (average epoch stats below)
2022-08-17 14:15:14 | INFO | train | epoch 707 | loss 3.378 | nll_loss 0.344 | ppl 1.27 | wps 6614.2 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 57267 | lr 0.000264288 | gnorm 0.305 | train_wall 40 | gb_free 10.1 | wall 57810
2022-08-17 14:15:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:15:14 | INFO | fairseq.trainer | begin training epoch 708
2022-08-17 14:15:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:15:32 | INFO | train_inner | epoch 708:     33 / 81 loss=3.378, nll_loss=0.344, ppl=1.27, wps=7127.2, ups=1.29, wpb=5513.7, bsz=353.5, num_updates=57300, lr=0.000264212, gnorm=0.31, train_wall=49, gb_free=10.1, wall=57828
2022-08-17 14:15:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:15:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:15:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:15:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:16:05 | INFO | valid | epoch 708 | valid on 'valid' subset | loss 5.176 | nll_loss 2.577 | ppl 5.97 | bleu 56.11 | wps 1779.9 | wpb 933.5 | bsz 59.6 | num_updates 57348 | best_bleu 57.47
2022-08-17 14:16:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 708 @ 57348 updates
2022-08-17 14:16:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint708.pt
2022-08-17 14:16:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint708.pt
2022-08-17 14:16:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint708.pt (epoch 708 @ 57348 updates, score 56.11) (writing took 24.048207126557827 seconds)
2022-08-17 14:16:30 | INFO | fairseq_cli.train | end of epoch 708 (average epoch stats below)
2022-08-17 14:16:30 | INFO | train | epoch 708 | loss 3.377 | nll_loss 0.343 | ppl 1.27 | wps 5916 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 57348 | lr 0.000264101 | gnorm 0.314 | train_wall 40 | gb_free 10.1 | wall 57886
2022-08-17 14:16:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:16:30 | INFO | fairseq.trainer | begin training epoch 709
2022-08-17 14:16:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:17:00 | INFO | train_inner | epoch 709:     52 / 81 loss=3.377, nll_loss=0.344, ppl=1.27, wps=6253, ups=1.13, wpb=5521.7, bsz=361.5, num_updates=57400, lr=0.000263982, gnorm=0.307, train_wall=49, gb_free=10, wall=57916
2022-08-17 14:17:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:17:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:17:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:17:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:17:27 | INFO | valid | epoch 709 | valid on 'valid' subset | loss 5.171 | nll_loss 2.57 | ppl 5.94 | bleu 56.81 | wps 1609.6 | wpb 933.5 | bsz 59.6 | num_updates 57429 | best_bleu 57.47
2022-08-17 14:17:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 709 @ 57429 updates
2022-08-17 14:17:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint709.pt
2022-08-17 14:17:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint709.pt
2022-08-17 14:18:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint709.pt (epoch 709 @ 57429 updates, score 56.81) (writing took 38.44550395011902 seconds)
2022-08-17 14:18:06 | INFO | fairseq_cli.train | end of epoch 709 (average epoch stats below)
2022-08-17 14:18:06 | INFO | train | epoch 709 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 4664.5 | ups 0.84 | wpb 5523.2 | bsz 358 | num_updates 57429 | lr 0.000263915 | gnorm 0.307 | train_wall 39 | gb_free 10.3 | wall 57982
2022-08-17 14:18:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:18:06 | INFO | fairseq.trainer | begin training epoch 710
2022-08-17 14:18:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:18:43 | INFO | train_inner | epoch 710:     71 / 81 loss=3.377, nll_loss=0.344, ppl=1.27, wps=5419.9, ups=0.97, wpb=5563.1, bsz=359.3, num_updates=57500, lr=0.000263752, gnorm=0.343, train_wall=49, gb_free=10.2, wall=58019
2022-08-17 14:18:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:18:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:18:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:18:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:18:57 | INFO | valid | epoch 710 | valid on 'valid' subset | loss 5.169 | nll_loss 2.566 | ppl 5.92 | bleu 56.01 | wps 1828 | wpb 933.5 | bsz 59.6 | num_updates 57510 | best_bleu 57.47
2022-08-17 14:18:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 710 @ 57510 updates
2022-08-17 14:18:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint710.pt
2022-08-17 14:18:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint710.pt
2022-08-17 14:19:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint710.pt (epoch 710 @ 57510 updates, score 56.01) (writing took 18.686017364263535 seconds)
2022-08-17 14:19:16 | INFO | fairseq_cli.train | end of epoch 710 (average epoch stats below)
2022-08-17 14:19:16 | INFO | train | epoch 710 | loss 3.378 | nll_loss 0.344 | ppl 1.27 | wps 6371.5 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 57510 | lr 0.000263729 | gnorm 0.37 | train_wall 40 | gb_free 10.1 | wall 58052
2022-08-17 14:19:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:19:16 | INFO | fairseq.trainer | begin training epoch 711
2022-08-17 14:19:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:19:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:19:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:19:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:19:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:20:07 | INFO | valid | epoch 711 | valid on 'valid' subset | loss 5.162 | nll_loss 2.557 | ppl 5.89 | bleu 56.23 | wps 1843.1 | wpb 933.5 | bsz 59.6 | num_updates 57591 | best_bleu 57.47
2022-08-17 14:20:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 711 @ 57591 updates
2022-08-17 14:20:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint711.pt
2022-08-17 14:20:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint711.pt
2022-08-17 14:20:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint711.pt (epoch 711 @ 57591 updates, score 56.23) (writing took 18.601512506604195 seconds)
2022-08-17 14:20:26 | INFO | fairseq_cli.train | end of epoch 711 (average epoch stats below)
2022-08-17 14:20:26 | INFO | train | epoch 711 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 6387.8 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 57591 | lr 0.000263544 | gnorm 0.328 | train_wall 40 | gb_free 10 | wall 58122
2022-08-17 14:20:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:20:26 | INFO | fairseq.trainer | begin training epoch 712
2022-08-17 14:20:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:20:32 | INFO | train_inner | epoch 712:      9 / 81 loss=3.377, nll_loss=0.344, ppl=1.27, wps=5041.1, ups=0.92, wpb=5485.7, bsz=356.6, num_updates=57600, lr=0.000263523, gnorm=0.337, train_wall=48, gb_free=10.1, wall=58128
2022-08-17 14:21:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:21:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:21:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:21:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:21:20 | INFO | valid | epoch 712 | valid on 'valid' subset | loss 5.166 | nll_loss 2.568 | ppl 5.93 | bleu 55.7 | wps 1806.8 | wpb 933.5 | bsz 59.6 | num_updates 57672 | best_bleu 57.47
2022-08-17 14:21:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 712 @ 57672 updates
2022-08-17 14:21:20 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint712.pt
2022-08-17 14:21:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint712.pt
2022-08-17 14:21:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint712.pt (epoch 712 @ 57672 updates, score 55.7) (writing took 22.5706839710474 seconds)
2022-08-17 14:21:43 | INFO | fairseq_cli.train | end of epoch 712 (average epoch stats below)
2022-08-17 14:21:43 | INFO | train | epoch 712 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 5813.4 | ups 1.05 | wpb 5523.2 | bsz 358 | num_updates 57672 | lr 0.000263359 | gnorm 0.375 | train_wall 38 | gb_free 10.2 | wall 58199
2022-08-17 14:21:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:21:43 | INFO | fairseq.trainer | begin training epoch 713
2022-08-17 14:21:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:21:59 | INFO | train_inner | epoch 713:     28 / 81 loss=3.378, nll_loss=0.344, ppl=1.27, wps=6340.3, ups=1.15, wpb=5508.4, bsz=353.9, num_updates=57700, lr=0.000263295, gnorm=0.366, train_wall=48, gb_free=10.1, wall=58215
2022-08-17 14:22:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:22:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:22:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:22:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:22:35 | INFO | valid | epoch 713 | valid on 'valid' subset | loss 5.169 | nll_loss 2.57 | ppl 5.94 | bleu 56.97 | wps 1740.9 | wpb 933.5 | bsz 59.6 | num_updates 57753 | best_bleu 57.47
2022-08-17 14:22:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 713 @ 57753 updates
2022-08-17 14:22:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint713.pt
2022-08-17 14:22:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint713.pt
2022-08-17 14:22:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint713.pt (epoch 713 @ 57753 updates, score 56.97) (writing took 17.795137632638216 seconds)
2022-08-17 14:22:54 | INFO | fairseq_cli.train | end of epoch 713 (average epoch stats below)
2022-08-17 14:22:54 | INFO | train | epoch 713 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 6263.6 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 57753 | lr 0.000263174 | gnorm 0.384 | train_wall 40 | gb_free 10.1 | wall 58270
2022-08-17 14:22:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:22:54 | INFO | fairseq.trainer | begin training epoch 714
2022-08-17 14:22:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:23:19 | INFO | train_inner | epoch 714:     47 / 81 loss=3.378, nll_loss=0.345, ppl=1.27, wps=6850.9, ups=1.24, wpb=5536.4, bsz=364.9, num_updates=57800, lr=0.000263067, gnorm=0.411, train_wall=50, gb_free=10, wall=58296
2022-08-17 14:23:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:23:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:23:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:23:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:23:46 | INFO | valid | epoch 714 | valid on 'valid' subset | loss 5.152 | nll_loss 2.551 | ppl 5.86 | bleu 56.46 | wps 1830.2 | wpb 933.5 | bsz 59.6 | num_updates 57834 | best_bleu 57.47
2022-08-17 14:23:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 714 @ 57834 updates
2022-08-17 14:23:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint714.pt
2022-08-17 14:23:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint714.pt
2022-08-17 14:24:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint714.pt (epoch 714 @ 57834 updates, score 56.46) (writing took 16.78294289112091 seconds)
2022-08-17 14:24:03 | INFO | fairseq_cli.train | end of epoch 714 (average epoch stats below)
2022-08-17 14:24:03 | INFO | train | epoch 714 | loss 3.379 | nll_loss 0.346 | ppl 1.27 | wps 6481.9 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 57834 | lr 0.000262989 | gnorm 0.397 | train_wall 41 | gb_free 10.1 | wall 58339
2022-08-17 14:24:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:24:03 | INFO | fairseq.trainer | begin training epoch 715
2022-08-17 14:24:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:24:37 | INFO | train_inner | epoch 715:     66 / 81 loss=3.379, nll_loss=0.346, ppl=1.27, wps=7152.6, ups=1.29, wpb=5533.7, bsz=356.6, num_updates=57900, lr=0.00026284, gnorm=0.422, train_wall=49, gb_free=10, wall=58373
2022-08-17 14:24:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:24:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:24:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:24:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:24:55 | INFO | valid | epoch 715 | valid on 'valid' subset | loss 5.163 | nll_loss 2.558 | ppl 5.89 | bleu 56.52 | wps 1621.4 | wpb 933.5 | bsz 59.6 | num_updates 57915 | best_bleu 57.47
2022-08-17 14:24:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 715 @ 57915 updates
2022-08-17 14:24:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint715.pt
2022-08-17 14:24:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint715.pt
2022-08-17 14:25:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint715.pt (epoch 715 @ 57915 updates, score 56.52) (writing took 15.631080761551857 seconds)
2022-08-17 14:25:10 | INFO | fairseq_cli.train | end of epoch 715 (average epoch stats below)
2022-08-17 14:25:10 | INFO | train | epoch 715 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 6666.3 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 57915 | lr 0.000262806 | gnorm 0.41 | train_wall 39 | gb_free 10 | wall 58407
2022-08-17 14:25:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:25:11 | INFO | fairseq.trainer | begin training epoch 716
2022-08-17 14:25:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:25:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:25:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:25:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:25:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:26:02 | INFO | valid | epoch 716 | valid on 'valid' subset | loss 5.165 | nll_loss 2.562 | ppl 5.9 | bleu 56.62 | wps 1813.6 | wpb 933.5 | bsz 59.6 | num_updates 57996 | best_bleu 57.47
2022-08-17 14:26:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 716 @ 57996 updates
2022-08-17 14:26:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint716.pt
2022-08-17 14:26:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint716.pt
2022-08-17 14:26:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint716.pt (epoch 716 @ 57996 updates, score 56.62) (writing took 24.226503524929285 seconds)
2022-08-17 14:26:27 | INFO | fairseq_cli.train | end of epoch 716 (average epoch stats below)
2022-08-17 14:26:27 | INFO | train | epoch 716 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 5850.1 | ups 1.06 | wpb 5523.2 | bsz 358 | num_updates 57996 | lr 0.000262622 | gnorm 0.354 | train_wall 41 | gb_free 10.1 | wall 58483
2022-08-17 14:26:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:26:27 | INFO | fairseq.trainer | begin training epoch 717
2022-08-17 14:26:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:26:30 | INFO | train_inner | epoch 717:      4 / 81 loss=3.378, nll_loss=0.345, ppl=1.27, wps=4855.7, ups=0.88, wpb=5510.3, bsz=357.1, num_updates=58000, lr=0.000262613, gnorm=0.343, train_wall=50, gb_free=10, wall=58487
2022-08-17 14:27:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:27:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:27:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:27:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:27:18 | INFO | valid | epoch 717 | valid on 'valid' subset | loss 5.153 | nll_loss 2.543 | ppl 5.83 | bleu 56.81 | wps 1781.5 | wpb 933.5 | bsz 59.6 | num_updates 58077 | best_bleu 57.47
2022-08-17 14:27:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 717 @ 58077 updates
2022-08-17 14:27:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint717.pt
2022-08-17 14:27:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint717.pt
2022-08-17 14:28:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint717.pt (epoch 717 @ 58077 updates, score 56.81) (writing took 44.97925281152129 seconds)
2022-08-17 14:28:03 | INFO | fairseq_cli.train | end of epoch 717 (average epoch stats below)
2022-08-17 14:28:03 | INFO | train | epoch 717 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 4645.3 | ups 0.84 | wpb 5523.2 | bsz 358 | num_updates 58077 | lr 0.000262439 | gnorm 0.311 | train_wall 40 | gb_free 10.1 | wall 58579
2022-08-17 14:28:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:28:03 | INFO | fairseq.trainer | begin training epoch 718
2022-08-17 14:28:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:28:16 | INFO | train_inner | epoch 718:     23 / 81 loss=3.377, nll_loss=0.344, ppl=1.27, wps=5244.3, ups=0.95, wpb=5526.3, bsz=354.5, num_updates=58100, lr=0.000262387, gnorm=0.306, train_wall=49, gb_free=10.2, wall=58592
2022-08-17 14:28:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:28:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:28:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:28:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:28:53 | INFO | valid | epoch 718 | valid on 'valid' subset | loss 5.181 | nll_loss 2.583 | ppl 5.99 | bleu 56.3 | wps 1879.8 | wpb 933.5 | bsz 59.6 | num_updates 58158 | best_bleu 57.47
2022-08-17 14:28:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 718 @ 58158 updates
2022-08-17 14:28:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint718.pt
2022-08-17 14:28:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint718.pt
2022-08-17 14:28:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint718.pt (epoch 718 @ 58158 updates, score 56.3) (writing took 2.3618439733982086 seconds)
2022-08-17 14:28:56 | INFO | fairseq_cli.train | end of epoch 718 (average epoch stats below)
2022-08-17 14:28:56 | INFO | train | epoch 718 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 8491.1 | ups 1.54 | wpb 5523.2 | bsz 358 | num_updates 58158 | lr 0.000262256 | gnorm 0.339 | train_wall 39 | gb_free 10.2 | wall 58632
2022-08-17 14:28:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:28:56 | INFO | fairseq.trainer | begin training epoch 719
2022-08-17 14:28:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:29:18 | INFO | train_inner | epoch 719:     42 / 81 loss=3.377, nll_loss=0.344, ppl=1.27, wps=8897.4, ups=1.61, wpb=5532, bsz=359.3, num_updates=58200, lr=0.000262161, gnorm=0.317, train_wall=49, gb_free=10.1, wall=58654
2022-08-17 14:29:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:29:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:29:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:29:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:29:46 | INFO | valid | epoch 719 | valid on 'valid' subset | loss 5.156 | nll_loss 2.55 | ppl 5.86 | bleu 57.09 | wps 1848.7 | wpb 933.5 | bsz 59.6 | num_updates 58239 | best_bleu 57.47
2022-08-17 14:29:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 719 @ 58239 updates
2022-08-17 14:29:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint719.pt
2022-08-17 14:29:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint719.pt
2022-08-17 14:30:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint719.pt (epoch 719 @ 58239 updates, score 57.09) (writing took 18.549138259142637 seconds)
2022-08-17 14:30:05 | INFO | fairseq_cli.train | end of epoch 719 (average epoch stats below)
2022-08-17 14:30:05 | INFO | train | epoch 719 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 6477.8 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 58239 | lr 0.000262073 | gnorm 0.304 | train_wall 39 | gb_free 10.1 | wall 58701
2022-08-17 14:30:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:30:05 | INFO | fairseq.trainer | begin training epoch 720
2022-08-17 14:30:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:30:37 | INFO | train_inner | epoch 720:     61 / 81 loss=3.378, nll_loss=0.345, ppl=1.27, wps=7032.1, ups=1.27, wpb=5531.2, bsz=361.6, num_updates=58300, lr=0.000261936, gnorm=0.353, train_wall=49, gb_free=10.1, wall=58733
2022-08-17 14:30:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:30:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:30:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:30:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:30:56 | INFO | valid | epoch 720 | valid on 'valid' subset | loss 5.164 | nll_loss 2.56 | ppl 5.9 | bleu 57.07 | wps 1709.6 | wpb 933.5 | bsz 59.6 | num_updates 58320 | best_bleu 57.47
2022-08-17 14:30:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 720 @ 58320 updates
2022-08-17 14:30:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint720.pt
2022-08-17 14:30:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint720.pt
2022-08-17 14:31:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint720.pt (epoch 720 @ 58320 updates, score 57.07) (writing took 22.580317441374063 seconds)
2022-08-17 14:31:18 | INFO | fairseq_cli.train | end of epoch 720 (average epoch stats below)
2022-08-17 14:31:18 | INFO | train | epoch 720 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 6087.2 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 58320 | lr 0.000261891 | gnorm 0.344 | train_wall 39 | gb_free 10.1 | wall 58775
2022-08-17 14:31:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:31:19 | INFO | fairseq.trainer | begin training epoch 721
2022-08-17 14:31:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:32:00 | INFO | train_inner | epoch 721:     80 / 81 loss=3.377, nll_loss=0.344, ppl=1.27, wps=6637, ups=1.2, wpb=5525.1, bsz=357.3, num_updates=58400, lr=0.000261712, gnorm=0.28, train_wall=48, gb_free=10.1, wall=58816
2022-08-17 14:32:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:32:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:32:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:32:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:32:10 | INFO | valid | epoch 721 | valid on 'valid' subset | loss 5.159 | nll_loss 2.553 | ppl 5.87 | bleu 57.2 | wps 1710.5 | wpb 933.5 | bsz 59.6 | num_updates 58401 | best_bleu 57.47
2022-08-17 14:32:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 721 @ 58401 updates
2022-08-17 14:32:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint721.pt
2022-08-17 14:32:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint721.pt
2022-08-17 14:32:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint721.pt (epoch 721 @ 58401 updates, score 57.2) (writing took 18.378445267677307 seconds)
2022-08-17 14:32:28 | INFO | fairseq_cli.train | end of epoch 721 (average epoch stats below)
2022-08-17 14:32:28 | INFO | train | epoch 721 | loss 3.377 | nll_loss 0.343 | ppl 1.27 | wps 6386.5 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 58401 | lr 0.00026171 | gnorm 0.273 | train_wall 39 | gb_free 10.1 | wall 58845
2022-08-17 14:32:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:32:29 | INFO | fairseq.trainer | begin training epoch 722
2022-08-17 14:32:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:33:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:33:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:33:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:33:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:33:21 | INFO | valid | epoch 722 | valid on 'valid' subset | loss 5.153 | nll_loss 2.548 | ppl 5.85 | bleu 56.73 | wps 1725.5 | wpb 933.5 | bsz 59.6 | num_updates 58482 | best_bleu 57.47
2022-08-17 14:33:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 722 @ 58482 updates
2022-08-17 14:33:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint722.pt
2022-08-17 14:33:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint722.pt
2022-08-17 14:33:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint722.pt (epoch 722 @ 58482 updates, score 56.73) (writing took 34.09135101735592 seconds)
2022-08-17 14:33:55 | INFO | fairseq_cli.train | end of epoch 722 (average epoch stats below)
2022-08-17 14:33:55 | INFO | train | epoch 722 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 5167.7 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 58482 | lr 0.000261528 | gnorm 0.352 | train_wall 39 | gb_free 10.3 | wall 58931
2022-08-17 14:33:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:33:55 | INFO | fairseq.trainer | begin training epoch 723
2022-08-17 14:33:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:34:05 | INFO | train_inner | epoch 723:     18 / 81 loss=3.377, nll_loss=0.343, ppl=1.27, wps=4381.9, ups=0.8, wpb=5489.5, bsz=353.6, num_updates=58500, lr=0.000261488, gnorm=0.384, train_wall=48, gb_free=10.1, wall=58941
2022-08-17 14:34:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:34:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:34:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:34:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:34:47 | INFO | valid | epoch 723 | valid on 'valid' subset | loss 5.168 | nll_loss 2.562 | ppl 5.9 | bleu 56.8 | wps 1671.2 | wpb 933.5 | bsz 59.6 | num_updates 58563 | best_bleu 57.47
2022-08-17 14:34:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 723 @ 58563 updates
2022-08-17 14:34:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint723.pt
2022-08-17 14:34:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint723.pt
2022-08-17 14:35:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint723.pt (epoch 723 @ 58563 updates, score 56.8) (writing took 15.166182160377502 seconds)
2022-08-17 14:35:02 | INFO | fairseq_cli.train | end of epoch 723 (average epoch stats below)
2022-08-17 14:35:02 | INFO | train | epoch 723 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 6656.4 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 58563 | lr 0.000261347 | gnorm 0.445 | train_wall 39 | gb_free 10.1 | wall 58998
2022-08-17 14:35:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:35:03 | INFO | fairseq.trainer | begin training epoch 724
2022-08-17 14:35:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:35:23 | INFO | train_inner | epoch 724:     37 / 81 loss=3.378, nll_loss=0.345, ppl=1.27, wps=7162.8, ups=1.29, wpb=5547.9, bsz=363.5, num_updates=58600, lr=0.000261265, gnorm=0.375, train_wall=50, gb_free=10, wall=59019
2022-08-17 14:35:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:35:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:35:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:35:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:35:53 | INFO | valid | epoch 724 | valid on 'valid' subset | loss 5.144 | nll_loss 2.539 | ppl 5.81 | bleu 56.73 | wps 1863.3 | wpb 933.5 | bsz 59.6 | num_updates 58644 | best_bleu 57.47
2022-08-17 14:35:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 724 @ 58644 updates
2022-08-17 14:35:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint724.pt
2022-08-17 14:35:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint724.pt
2022-08-17 14:36:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint724.pt (epoch 724 @ 58644 updates, score 56.73) (writing took 22.15459580719471 seconds)
2022-08-17 14:36:15 | INFO | fairseq_cli.train | end of epoch 724 (average epoch stats below)
2022-08-17 14:36:15 | INFO | train | epoch 724 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 6134.3 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 58644 | lr 0.000261167 | gnorm 0.342 | train_wall 40 | gb_free 10 | wall 59071
2022-08-17 14:36:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:36:15 | INFO | fairseq.trainer | begin training epoch 725
2022-08-17 14:36:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:36:44 | INFO | train_inner | epoch 725:     56 / 81 loss=3.377, nll_loss=0.344, ppl=1.27, wps=6776.6, ups=1.23, wpb=5525.8, bsz=356.7, num_updates=58700, lr=0.000261042, gnorm=0.335, train_wall=48, gb_free=10.1, wall=59100
2022-08-17 14:36:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:36:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:36:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:36:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:37:06 | INFO | valid | epoch 725 | valid on 'valid' subset | loss 5.154 | nll_loss 2.549 | ppl 5.85 | bleu 56.67 | wps 1770.7 | wpb 933.5 | bsz 59.6 | num_updates 58725 | best_bleu 57.47
2022-08-17 14:37:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 725 @ 58725 updates
2022-08-17 14:37:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint725.pt
2022-08-17 14:37:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint725.pt
2022-08-17 14:37:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint725.pt (epoch 725 @ 58725 updates, score 56.67) (writing took 33.34092292562127 seconds)
2022-08-17 14:37:40 | INFO | fairseq_cli.train | end of epoch 725 (average epoch stats below)
2022-08-17 14:37:40 | INFO | train | epoch 725 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 5306.8 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 58725 | lr 0.000260987 | gnorm 0.313 | train_wall 39 | gb_free 10.1 | wall 59156
2022-08-17 14:37:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:37:40 | INFO | fairseq.trainer | begin training epoch 726
2022-08-17 14:37:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:38:18 | INFO | train_inner | epoch 726:     75 / 81 loss=3.377, nll_loss=0.344, ppl=1.27, wps=5861.6, ups=1.06, wpb=5511.9, bsz=356, num_updates=58800, lr=0.00026082, gnorm=0.318, train_wall=49, gb_free=10.1, wall=59194
2022-08-17 14:38:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:38:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:38:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:38:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:38:30 | INFO | valid | epoch 726 | valid on 'valid' subset | loss 5.158 | nll_loss 2.557 | ppl 5.88 | bleu 56.43 | wps 1832.6 | wpb 933.5 | bsz 59.6 | num_updates 58806 | best_bleu 57.47
2022-08-17 14:38:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 726 @ 58806 updates
2022-08-17 14:38:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint726.pt
2022-08-17 14:38:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint726.pt
2022-08-17 14:38:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint726.pt (epoch 726 @ 58806 updates, score 56.43) (writing took 2.4148390516638756 seconds)
2022-08-17 14:38:33 | INFO | fairseq_cli.train | end of epoch 726 (average epoch stats below)
2022-08-17 14:38:33 | INFO | train | epoch 726 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 8381.7 | ups 1.52 | wpb 5523.2 | bsz 358 | num_updates 58806 | lr 0.000260807 | gnorm 0.31 | train_wall 39 | gb_free 10.2 | wall 59209
2022-08-17 14:38:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:38:33 | INFO | fairseq.trainer | begin training epoch 727
2022-08-17 14:38:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:39:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:39:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:39:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:39:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:39:24 | INFO | valid | epoch 727 | valid on 'valid' subset | loss 5.168 | nll_loss 2.565 | ppl 5.92 | bleu 56.61 | wps 1856.3 | wpb 933.5 | bsz 59.6 | num_updates 58887 | best_bleu 57.47
2022-08-17 14:39:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 727 @ 58887 updates
2022-08-17 14:39:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint727.pt
2022-08-17 14:39:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint727.pt
2022-08-17 14:39:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint727.pt (epoch 727 @ 58887 updates, score 56.61) (writing took 16.727563872933388 seconds)
2022-08-17 14:39:41 | INFO | fairseq_cli.train | end of epoch 727 (average epoch stats below)
2022-08-17 14:39:41 | INFO | train | epoch 727 | loss 3.377 | nll_loss 0.343 | ppl 1.27 | wps 6584.1 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 58887 | lr 0.000260628 | gnorm 0.309 | train_wall 40 | gb_free 10.2 | wall 59277
2022-08-17 14:39:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:39:41 | INFO | fairseq.trainer | begin training epoch 728
2022-08-17 14:39:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:39:48 | INFO | train_inner | epoch 728:     13 / 81 loss=3.377, nll_loss=0.343, ppl=1.27, wps=6134.6, ups=1.11, wpb=5534.5, bsz=356.2, num_updates=58900, lr=0.000260599, gnorm=0.312, train_wall=49, gb_free=10.1, wall=59285
2022-08-17 14:40:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:40:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:40:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:40:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:40:33 | INFO | valid | epoch 728 | valid on 'valid' subset | loss 5.174 | nll_loss 2.569 | ppl 5.94 | bleu 55.87 | wps 1566.4 | wpb 933.5 | bsz 59.6 | num_updates 58968 | best_bleu 57.47
2022-08-17 14:40:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 728 @ 58968 updates
2022-08-17 14:40:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint728.pt
2022-08-17 14:40:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint728.pt
2022-08-17 14:40:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint728.pt (epoch 728 @ 58968 updates, score 55.87) (writing took 20.529238916933537 seconds)
2022-08-17 14:40:54 | INFO | fairseq_cli.train | end of epoch 728 (average epoch stats below)
2022-08-17 14:40:54 | INFO | train | epoch 728 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 6151.9 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 58968 | lr 0.000260448 | gnorm 0.324 | train_wall 40 | gb_free 10.2 | wall 59350
2022-08-17 14:40:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:40:54 | INFO | fairseq.trainer | begin training epoch 729
2022-08-17 14:40:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:41:12 | INFO | train_inner | epoch 729:     32 / 81 loss=3.377, nll_loss=0.344, ppl=1.27, wps=6608.5, ups=1.2, wpb=5524.8, bsz=363.4, num_updates=59000, lr=0.000260378, gnorm=0.301, train_wall=48, gb_free=10.1, wall=59368
2022-08-17 14:41:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:41:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:41:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:41:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:41:47 | INFO | valid | epoch 729 | valid on 'valid' subset | loss 5.17 | nll_loss 2.565 | ppl 5.92 | bleu 56.58 | wps 1882.1 | wpb 933.5 | bsz 59.6 | num_updates 59049 | best_bleu 57.47
2022-08-17 14:41:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 729 @ 59049 updates
2022-08-17 14:41:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint729.pt
2022-08-17 14:41:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint729.pt
2022-08-17 14:42:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint729.pt (epoch 729 @ 59049 updates, score 56.58) (writing took 20.300276316702366 seconds)
2022-08-17 14:42:07 | INFO | fairseq_cli.train | end of epoch 729 (average epoch stats below)
2022-08-17 14:42:07 | INFO | train | epoch 729 | loss 3.377 | nll_loss 0.343 | ppl 1.27 | wps 6086.6 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 59049 | lr 0.00026027 | gnorm 0.299 | train_wall 39 | gb_free 10.2 | wall 59423
2022-08-17 14:42:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:42:07 | INFO | fairseq.trainer | begin training epoch 730
2022-08-17 14:42:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:42:33 | INFO | train_inner | epoch 730:     51 / 81 loss=3.377, nll_loss=0.344, ppl=1.27, wps=6750.8, ups=1.23, wpb=5495.3, bsz=356.3, num_updates=59100, lr=0.000260157, gnorm=0.321, train_wall=48, gb_free=10, wall=59450
2022-08-17 14:42:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:42:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:42:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:42:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:42:57 | INFO | valid | epoch 730 | valid on 'valid' subset | loss 5.17 | nll_loss 2.569 | ppl 5.94 | bleu 56.22 | wps 1830.9 | wpb 933.5 | bsz 59.6 | num_updates 59130 | best_bleu 57.47
2022-08-17 14:42:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 730 @ 59130 updates
2022-08-17 14:42:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint730.pt
2022-08-17 14:42:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint730.pt
2022-08-17 14:43:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint730.pt (epoch 730 @ 59130 updates, score 56.22) (writing took 29.738951064646244 seconds)
2022-08-17 14:43:27 | INFO | fairseq_cli.train | end of epoch 730 (average epoch stats below)
2022-08-17 14:43:27 | INFO | train | epoch 730 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 5584.2 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 59130 | lr 0.000260091 | gnorm 0.33 | train_wall 39 | gb_free 10.1 | wall 59503
2022-08-17 14:43:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:43:27 | INFO | fairseq.trainer | begin training epoch 731
2022-08-17 14:43:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:44:04 | INFO | train_inner | epoch 731:     70 / 81 loss=3.377, nll_loss=0.345, ppl=1.27, wps=6144.5, ups=1.1, wpb=5582.1, bsz=362, num_updates=59200, lr=0.000259938, gnorm=0.344, train_wall=49, gb_free=10.1, wall=59540
2022-08-17 14:44:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:44:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:44:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:44:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:44:19 | INFO | valid | epoch 731 | valid on 'valid' subset | loss 5.18 | nll_loss 2.58 | ppl 5.98 | bleu 56.34 | wps 1821.9 | wpb 933.5 | bsz 59.6 | num_updates 59211 | best_bleu 57.47
2022-08-17 14:44:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 731 @ 59211 updates
2022-08-17 14:44:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint731.pt
2022-08-17 14:44:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint731.pt
2022-08-17 14:44:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint731.pt (epoch 731 @ 59211 updates, score 56.34) (writing took 20.66455864906311 seconds)
2022-08-17 14:44:40 | INFO | fairseq_cli.train | end of epoch 731 (average epoch stats below)
2022-08-17 14:44:40 | INFO | train | epoch 731 | loss 3.378 | nll_loss 0.345 | ppl 1.27 | wps 6180.7 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 59211 | lr 0.000259913 | gnorm 0.345 | train_wall 40 | gb_free 10.2 | wall 59576
2022-08-17 14:44:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:44:40 | INFO | fairseq.trainer | begin training epoch 732
2022-08-17 14:44:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:45:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:45:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:45:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:45:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:45:31 | INFO | valid | epoch 732 | valid on 'valid' subset | loss 5.158 | nll_loss 2.547 | ppl 5.84 | bleu 57.15 | wps 1781.8 | wpb 933.5 | bsz 59.6 | num_updates 59292 | best_bleu 57.47
2022-08-17 14:45:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 732 @ 59292 updates
2022-08-17 14:45:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint732.pt
2022-08-17 14:45:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint732.pt
2022-08-17 14:45:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint732.pt (epoch 732 @ 59292 updates, score 57.15) (writing took 19.922063175588846 seconds)
2022-08-17 14:45:51 | INFO | fairseq_cli.train | end of epoch 732 (average epoch stats below)
2022-08-17 14:45:51 | INFO | train | epoch 732 | loss 3.377 | nll_loss 0.343 | ppl 1.27 | wps 6234.8 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 59292 | lr 0.000259736 | gnorm 0.304 | train_wall 40 | gb_free 10.2 | wall 59648
2022-08-17 14:45:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:45:52 | INFO | fairseq.trainer | begin training epoch 733
2022-08-17 14:45:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:45:57 | INFO | train_inner | epoch 733:      8 / 81 loss=3.377, nll_loss=0.343, ppl=1.27, wps=4862.6, ups=0.89, wpb=5461.1, bsz=351.2, num_updates=59300, lr=0.000259718, gnorm=0.307, train_wall=48, gb_free=10.1, wall=59653
2022-08-17 14:46:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:46:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:46:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:46:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:46:41 | INFO | valid | epoch 733 | valid on 'valid' subset | loss 5.163 | nll_loss 2.558 | ppl 5.89 | bleu 56.75 | wps 1774.9 | wpb 933.5 | bsz 59.6 | num_updates 59373 | best_bleu 57.47
2022-08-17 14:46:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 733 @ 59373 updates
2022-08-17 14:46:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint733.pt
2022-08-17 14:46:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint733.pt
2022-08-17 14:46:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint733.pt (epoch 733 @ 59373 updates, score 56.75) (writing took 14.63204088062048 seconds)
2022-08-17 14:46:56 | INFO | fairseq_cli.train | end of epoch 733 (average epoch stats below)
2022-08-17 14:46:56 | INFO | train | epoch 733 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 6904.5 | ups 1.25 | wpb 5523.2 | bsz 358 | num_updates 59373 | lr 0.000259559 | gnorm 0.334 | train_wall 38 | gb_free 10.1 | wall 59712
2022-08-17 14:46:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:46:56 | INFO | fairseq.trainer | begin training epoch 734
2022-08-17 14:46:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:47:12 | INFO | train_inner | epoch 734:     27 / 81 loss=3.377, nll_loss=0.343, ppl=1.27, wps=7363.4, ups=1.33, wpb=5538.5, bsz=357.9, num_updates=59400, lr=0.0002595, gnorm=0.35, train_wall=47, gb_free=10.1, wall=59728
2022-08-17 14:47:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:47:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:47:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:47:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:47:49 | INFO | valid | epoch 734 | valid on 'valid' subset | loss 5.158 | nll_loss 2.553 | ppl 5.87 | bleu 56.57 | wps 1754.5 | wpb 933.5 | bsz 59.6 | num_updates 59454 | best_bleu 57.47
2022-08-17 14:47:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 734 @ 59454 updates
2022-08-17 14:47:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint734.pt
2022-08-17 14:47:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint734.pt
2022-08-17 14:48:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint734.pt (epoch 734 @ 59454 updates, score 56.57) (writing took 20.408360052853823 seconds)
2022-08-17 14:48:09 | INFO | fairseq_cli.train | end of epoch 734 (average epoch stats below)
2022-08-17 14:48:09 | INFO | train | epoch 734 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 6108.9 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 59454 | lr 0.000259382 | gnorm 0.325 | train_wall 40 | gb_free 10.1 | wall 59786
2022-08-17 14:48:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:48:10 | INFO | fairseq.trainer | begin training epoch 735
2022-08-17 14:48:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:48:34 | INFO | train_inner | epoch 735:     46 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=6726.1, ups=1.22, wpb=5532.2, bsz=362.7, num_updates=59500, lr=0.000259281, gnorm=0.285, train_wall=49, gb_free=10.1, wall=59810
2022-08-17 14:48:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:48:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:48:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:48:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:49:01 | INFO | valid | epoch 735 | valid on 'valid' subset | loss 5.147 | nll_loss 2.541 | ppl 5.82 | bleu 57.19 | wps 1768.8 | wpb 933.5 | bsz 59.6 | num_updates 59535 | best_bleu 57.47
2022-08-17 14:49:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 735 @ 59535 updates
2022-08-17 14:49:01 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint735.pt
2022-08-17 14:49:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint735.pt
2022-08-17 14:49:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint735.pt (epoch 735 @ 59535 updates, score 57.19) (writing took 2.536837689578533 seconds)
2022-08-17 14:49:03 | INFO | fairseq_cli.train | end of epoch 735 (average epoch stats below)
2022-08-17 14:49:03 | INFO | train | epoch 735 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 8269.3 | ups 1.5 | wpb 5523.2 | bsz 358 | num_updates 59535 | lr 0.000259205 | gnorm 0.279 | train_wall 39 | gb_free 10.3 | wall 59840
2022-08-17 14:49:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:49:04 | INFO | fairseq.trainer | begin training epoch 736
2022-08-17 14:49:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:49:38 | INFO | train_inner | epoch 736:     65 / 81 loss=3.378, nll_loss=0.345, ppl=1.27, wps=8696.6, ups=1.57, wpb=5544, bsz=358.8, num_updates=59600, lr=0.000259064, gnorm=0.31, train_wall=50, gb_free=10.1, wall=59874
2022-08-17 14:49:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:49:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:49:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:49:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:49:55 | INFO | valid | epoch 736 | valid on 'valid' subset | loss 5.173 | nll_loss 2.571 | ppl 5.94 | bleu 56.66 | wps 1802.8 | wpb 933.5 | bsz 59.6 | num_updates 59616 | best_bleu 57.47
2022-08-17 14:49:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 736 @ 59616 updates
2022-08-17 14:49:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint736.pt
2022-08-17 14:49:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint736.pt
2022-08-17 14:50:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint736.pt (epoch 736 @ 59616 updates, score 56.66) (writing took 27.39163387566805 seconds)
2022-08-17 14:50:22 | INFO | fairseq_cli.train | end of epoch 736 (average epoch stats below)
2022-08-17 14:50:22 | INFO | train | epoch 736 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 5676 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 59616 | lr 0.000259029 | gnorm 0.302 | train_wall 40 | gb_free 10.1 | wall 59919
2022-08-17 14:50:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:50:22 | INFO | fairseq.trainer | begin training epoch 737
2022-08-17 14:50:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:51:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:51:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:51:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:51:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:51:14 | INFO | valid | epoch 737 | valid on 'valid' subset | loss 5.165 | nll_loss 2.56 | ppl 5.9 | bleu 57.04 | wps 1865.3 | wpb 933.5 | bsz 59.6 | num_updates 59697 | best_bleu 57.47
2022-08-17 14:51:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 737 @ 59697 updates
2022-08-17 14:51:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint737.pt
2022-08-17 14:51:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint737.pt
2022-08-17 14:51:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint737.pt (epoch 737 @ 59697 updates, score 57.04) (writing took 2.5424418300390244 seconds)
2022-08-17 14:51:17 | INFO | fairseq_cli.train | end of epoch 737 (average epoch stats below)
2022-08-17 14:51:17 | INFO | train | epoch 737 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 8165.5 | ups 1.48 | wpb 5523.2 | bsz 358 | num_updates 59697 | lr 0.000258853 | gnorm 0.327 | train_wall 39 | gb_free 10.2 | wall 59973
2022-08-17 14:51:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:51:17 | INFO | fairseq.trainer | begin training epoch 738
2022-08-17 14:51:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:51:21 | INFO | train_inner | epoch 738:      3 / 81 loss=3.377, nll_loss=0.344, ppl=1.27, wps=5333.1, ups=0.97, wpb=5481.5, bsz=353, num_updates=59700, lr=0.000258847, gnorm=0.313, train_wall=48, gb_free=10.1, wall=59977
2022-08-17 14:51:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:52:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:52:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:52:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:52:08 | INFO | valid | epoch 738 | valid on 'valid' subset | loss 5.172 | nll_loss 2.57 | ppl 5.94 | bleu 56.24 | wps 1807.7 | wpb 933.5 | bsz 59.6 | num_updates 59778 | best_bleu 57.47
2022-08-17 14:52:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 738 @ 59778 updates
2022-08-17 14:52:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint738.pt
2022-08-17 14:52:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint738.pt
2022-08-17 14:52:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint738.pt (epoch 738 @ 59778 updates, score 56.24) (writing took 2.4427049681544304 seconds)
2022-08-17 14:52:11 | INFO | fairseq_cli.train | end of epoch 738 (average epoch stats below)
2022-08-17 14:52:11 | INFO | train | epoch 738 | loss 3.377 | nll_loss 0.343 | ppl 1.27 | wps 8325.5 | ups 1.51 | wpb 5523.2 | bsz 358 | num_updates 59778 | lr 0.000258678 | gnorm 0.288 | train_wall 40 | gb_free 10.1 | wall 60027
2022-08-17 14:52:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:52:11 | INFO | fairseq.trainer | begin training epoch 739
2022-08-17 14:52:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:52:23 | INFO | train_inner | epoch 739:     22 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=8899.8, ups=1.61, wpb=5538.4, bsz=360.2, num_updates=59800, lr=0.00025863, gnorm=0.298, train_wall=48, gb_free=10.1, wall=60039
2022-08-17 14:52:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:52:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:52:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:52:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:53:02 | INFO | valid | epoch 739 | valid on 'valid' subset | loss 5.171 | nll_loss 2.566 | ppl 5.92 | bleu 57.03 | wps 1799.7 | wpb 933.5 | bsz 59.6 | num_updates 59859 | best_bleu 57.47
2022-08-17 14:53:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 739 @ 59859 updates
2022-08-17 14:53:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint739.pt
2022-08-17 14:53:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint739.pt
2022-08-17 14:53:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint739.pt (epoch 739 @ 59859 updates, score 57.03) (writing took 27.808564834296703 seconds)
2022-08-17 14:53:30 | INFO | fairseq_cli.train | end of epoch 739 (average epoch stats below)
2022-08-17 14:53:30 | INFO | train | epoch 739 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 5673.6 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 59859 | lr 0.000258503 | gnorm 0.357 | train_wall 40 | gb_free 10 | wall 60106
2022-08-17 14:53:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:53:30 | INFO | fairseq.trainer | begin training epoch 740
2022-08-17 14:53:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:53:52 | INFO | train_inner | epoch 740:     41 / 81 loss=3.377, nll_loss=0.344, ppl=1.27, wps=6237.2, ups=1.12, wpb=5548.7, bsz=359.5, num_updates=59900, lr=0.000258414, gnorm=0.338, train_wall=49, gb_free=10.1, wall=60128
2022-08-17 14:54:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:54:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:54:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:54:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:54:21 | INFO | valid | epoch 740 | valid on 'valid' subset | loss 5.177 | nll_loss 2.574 | ppl 5.95 | bleu 56.5 | wps 1851.4 | wpb 933.5 | bsz 59.6 | num_updates 59940 | best_bleu 57.47
2022-08-17 14:54:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 740 @ 59940 updates
2022-08-17 14:54:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint740.pt
2022-08-17 14:54:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint740.pt
2022-08-17 14:54:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint740.pt (epoch 740 @ 59940 updates, score 56.5) (writing took 2.5132533945143223 seconds)
2022-08-17 14:54:23 | INFO | fairseq_cli.train | end of epoch 740 (average epoch stats below)
2022-08-17 14:54:23 | INFO | train | epoch 740 | loss 3.377 | nll_loss 0.343 | ppl 1.27 | wps 8339.2 | ups 1.51 | wpb 5523.2 | bsz 358 | num_updates 59940 | lr 0.000258328 | gnorm 0.315 | train_wall 39 | gb_free 10.3 | wall 60160
2022-08-17 14:54:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:54:24 | INFO | fairseq.trainer | begin training epoch 741
2022-08-17 14:54:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:54:54 | INFO | train_inner | epoch 741:     60 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=8855.2, ups=1.61, wpb=5515.4, bsz=359.3, num_updates=60000, lr=0.000258199, gnorm=0.34, train_wall=49, gb_free=10.1, wall=60190
2022-08-17 14:55:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:55:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:55:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:55:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:55:14 | INFO | valid | epoch 741 | valid on 'valid' subset | loss 5.18 | nll_loss 2.581 | ppl 5.98 | bleu 56.56 | wps 1790.2 | wpb 933.5 | bsz 59.6 | num_updates 60021 | best_bleu 57.47
2022-08-17 14:55:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 741 @ 60021 updates
2022-08-17 14:55:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint741.pt
2022-08-17 14:55:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint741.pt
2022-08-17 14:55:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint741.pt (epoch 741 @ 60021 updates, score 56.56) (writing took 28.140585362911224 seconds)
2022-08-17 14:55:42 | INFO | fairseq_cli.train | end of epoch 741 (average epoch stats below)
2022-08-17 14:55:42 | INFO | train | epoch 741 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 5670.2 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 60021 | lr 0.000258154 | gnorm 0.36 | train_wall 39 | gb_free 10.1 | wall 60238
2022-08-17 14:55:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:55:42 | INFO | fairseq.trainer | begin training epoch 742
2022-08-17 14:55:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:56:22 | INFO | train_inner | epoch 742:     79 / 81 loss=3.377, nll_loss=0.344, ppl=1.27, wps=6255.5, ups=1.13, wpb=5524.7, bsz=357.1, num_updates=60100, lr=0.000257984, gnorm=0.34, train_wall=48, gb_free=10.1, wall=60279
2022-08-17 14:56:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:56:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:56:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:56:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:56:32 | INFO | valid | epoch 742 | valid on 'valid' subset | loss 5.179 | nll_loss 2.581 | ppl 5.98 | bleu 56.52 | wps 1853.3 | wpb 933.5 | bsz 59.6 | num_updates 60102 | best_bleu 57.47
2022-08-17 14:56:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 742 @ 60102 updates
2022-08-17 14:56:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint742.pt
2022-08-17 14:56:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint742.pt
2022-08-17 14:56:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint742.pt (epoch 742 @ 60102 updates, score 56.52) (writing took 15.786578219383955 seconds)
2022-08-17 14:56:48 | INFO | fairseq_cli.train | end of epoch 742 (average epoch stats below)
2022-08-17 14:56:48 | INFO | train | epoch 742 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 6769.1 | ups 1.23 | wpb 5523.2 | bsz 358 | num_updates 60102 | lr 0.00025798 | gnorm 0.325 | train_wall 39 | gb_free 10.1 | wall 60305
2022-08-17 14:56:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:56:49 | INFO | fairseq.trainer | begin training epoch 743
2022-08-17 14:56:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:57:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:57:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:57:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:57:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:57:40 | INFO | valid | epoch 743 | valid on 'valid' subset | loss 5.175 | nll_loss 2.571 | ppl 5.94 | bleu 56.79 | wps 1628.2 | wpb 933.5 | bsz 59.6 | num_updates 60183 | best_bleu 57.47
2022-08-17 14:57:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 743 @ 60183 updates
2022-08-17 14:57:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint743.pt
2022-08-17 14:57:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint743.pt
2022-08-17 14:57:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint743.pt (epoch 743 @ 60183 updates, score 56.79) (writing took 18.221188690513372 seconds)
2022-08-17 14:57:59 | INFO | fairseq_cli.train | end of epoch 743 (average epoch stats below)
2022-08-17 14:57:59 | INFO | train | epoch 743 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 6360.3 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 60183 | lr 0.000257806 | gnorm 0.367 | train_wall 39 | gb_free 10 | wall 60375
2022-08-17 14:57:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:57:59 | INFO | fairseq.trainer | begin training epoch 744
2022-08-17 14:57:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:58:08 | INFO | train_inner | epoch 744:     17 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=5196.8, ups=0.95, wpb=5498.6, bsz=354.6, num_updates=60200, lr=0.00025777, gnorm=0.354, train_wall=48, gb_free=10.1, wall=60384
2022-08-17 14:58:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 14:58:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 14:58:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 14:58:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 14:58:50 | INFO | valid | epoch 744 | valid on 'valid' subset | loss 5.167 | nll_loss 2.567 | ppl 5.93 | bleu 57.22 | wps 1813.5 | wpb 933.5 | bsz 59.6 | num_updates 60264 | best_bleu 57.47
2022-08-17 14:58:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 744 @ 60264 updates
2022-08-17 14:58:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint744.pt
2022-08-17 14:58:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint744.pt
2022-08-17 14:59:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint744.pt (epoch 744 @ 60264 updates, score 57.22) (writing took 34.8046427257359 seconds)
2022-08-17 14:59:25 | INFO | fairseq_cli.train | end of epoch 744 (average epoch stats below)
2022-08-17 14:59:25 | INFO | train | epoch 744 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 5201.4 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 60264 | lr 0.000257633 | gnorm 0.359 | train_wall 40 | gb_free 10 | wall 60461
2022-08-17 14:59:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 14:59:25 | INFO | fairseq.trainer | begin training epoch 745
2022-08-17 14:59:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 14:59:45 | INFO | train_inner | epoch 745:     36 / 81 loss=3.377, nll_loss=0.344, ppl=1.27, wps=5707.2, ups=1.03, wpb=5548.9, bsz=361.6, num_updates=60300, lr=0.000257556, gnorm=0.383, train_wall=50, gb_free=10, wall=60482
2022-08-17 15:00:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:00:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:00:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:00:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:00:17 | INFO | valid | epoch 745 | valid on 'valid' subset | loss 5.169 | nll_loss 2.567 | ppl 5.93 | bleu 56.88 | wps 1911.8 | wpb 933.5 | bsz 59.6 | num_updates 60345 | best_bleu 57.47
2022-08-17 15:00:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 745 @ 60345 updates
2022-08-17 15:00:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint745.pt
2022-08-17 15:00:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint745.pt
2022-08-17 15:00:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint745.pt (epoch 745 @ 60345 updates, score 56.88) (writing took 2.3885076604783535 seconds)
2022-08-17 15:00:20 | INFO | fairseq_cli.train | end of epoch 745 (average epoch stats below)
2022-08-17 15:00:20 | INFO | train | epoch 745 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 8093.5 | ups 1.47 | wpb 5523.2 | bsz 358 | num_updates 60345 | lr 0.00025746 | gnorm 0.333 | train_wall 41 | gb_free 10.1 | wall 60516
2022-08-17 15:00:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:00:20 | INFO | fairseq.trainer | begin training epoch 746
2022-08-17 15:00:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:00:50 | INFO | train_inner | epoch 746:     55 / 81 loss=3.377, nll_loss=0.345, ppl=1.27, wps=8519.5, ups=1.55, wpb=5497.4, bsz=355.4, num_updates=60400, lr=0.000257343, gnorm=0.34, train_wall=51, gb_free=10.1, wall=60546
2022-08-17 15:01:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:01:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:01:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:01:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:01:13 | INFO | valid | epoch 746 | valid on 'valid' subset | loss 5.178 | nll_loss 2.58 | ppl 5.98 | bleu 56.93 | wps 1756.8 | wpb 933.5 | bsz 59.6 | num_updates 60426 | best_bleu 57.47
2022-08-17 15:01:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 746 @ 60426 updates
2022-08-17 15:01:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint746.pt
2022-08-17 15:01:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint746.pt
2022-08-17 15:01:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint746.pt (epoch 746 @ 60426 updates, score 56.93) (writing took 16.326664555817842 seconds)
2022-08-17 15:01:30 | INFO | fairseq_cli.train | end of epoch 746 (average epoch stats below)
2022-08-17 15:01:30 | INFO | train | epoch 746 | loss 3.377 | nll_loss 0.345 | ppl 1.27 | wps 6417.8 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 60426 | lr 0.000257287 | gnorm 0.376 | train_wall 42 | gb_free 10.2 | wall 60586
2022-08-17 15:01:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:01:30 | INFO | fairseq.trainer | begin training epoch 747
2022-08-17 15:01:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:02:07 | INFO | train_inner | epoch 747:     74 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=7221.9, ups=1.3, wpb=5552.9, bsz=363.6, num_updates=60500, lr=0.00025713, gnorm=0.31, train_wall=49, gb_free=10, wall=60623
2022-08-17 15:02:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:02:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:02:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:02:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:02:19 | INFO | valid | epoch 747 | valid on 'valid' subset | loss 5.156 | nll_loss 2.551 | ppl 5.86 | bleu 56.73 | wps 1886.7 | wpb 933.5 | bsz 59.6 | num_updates 60507 | best_bleu 57.47
2022-08-17 15:02:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 747 @ 60507 updates
2022-08-17 15:02:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint747.pt
2022-08-17 15:02:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint747.pt
2022-08-17 15:02:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint747.pt (epoch 747 @ 60507 updates, score 56.73) (writing took 25.7323777936399 seconds)
2022-08-17 15:02:45 | INFO | fairseq_cli.train | end of epoch 747 (average epoch stats below)
2022-08-17 15:02:45 | INFO | train | epoch 747 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 5924.1 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 60507 | lr 0.000257115 | gnorm 0.296 | train_wall 39 | gb_free 10.1 | wall 60661
2022-08-17 15:02:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:02:45 | INFO | fairseq.trainer | begin training epoch 748
2022-08-17 15:02:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:03:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:03:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:03:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:03:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:03:36 | INFO | valid | epoch 748 | valid on 'valid' subset | loss 5.169 | nll_loss 2.564 | ppl 5.91 | bleu 56.73 | wps 1782.7 | wpb 933.5 | bsz 59.6 | num_updates 60588 | best_bleu 57.47
2022-08-17 15:03:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 748 @ 60588 updates
2022-08-17 15:03:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint748.pt
2022-08-17 15:03:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint748.pt
2022-08-17 15:03:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint748.pt (epoch 748 @ 60588 updates, score 56.73) (writing took 19.303508147597313 seconds)
2022-08-17 15:03:55 | INFO | fairseq_cli.train | end of epoch 748 (average epoch stats below)
2022-08-17 15:03:55 | INFO | train | epoch 748 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 6372.4 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 60588 | lr 0.000256943 | gnorm 0.284 | train_wall 39 | gb_free 10.1 | wall 60732
2022-08-17 15:03:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:03:56 | INFO | fairseq.trainer | begin training epoch 749
2022-08-17 15:03:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:04:03 | INFO | train_inner | epoch 749:     12 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=4725.8, ups=0.86, wpb=5489, bsz=351.8, num_updates=60600, lr=0.000256917, gnorm=0.29, train_wall=48, gb_free=10.1, wall=60739
2022-08-17 15:04:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:04:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:04:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:04:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:04:47 | INFO | valid | epoch 749 | valid on 'valid' subset | loss 5.173 | nll_loss 2.571 | ppl 5.94 | bleu 56.59 | wps 1763.4 | wpb 933.5 | bsz 59.6 | num_updates 60669 | best_bleu 57.47
2022-08-17 15:04:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 749 @ 60669 updates
2022-08-17 15:04:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint749.pt
2022-08-17 15:04:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint749.pt
2022-08-17 15:05:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint749.pt (epoch 749 @ 60669 updates, score 56.59) (writing took 43.511155400425196 seconds)
2022-08-17 15:05:31 | INFO | fairseq_cli.train | end of epoch 749 (average epoch stats below)
2022-08-17 15:05:31 | INFO | train | epoch 749 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 4705.7 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 60669 | lr 0.000256771 | gnorm 0.349 | train_wall 38 | gb_free 10.1 | wall 60827
2022-08-17 15:05:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:05:31 | INFO | fairseq.trainer | begin training epoch 750
2022-08-17 15:05:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:05:47 | INFO | train_inner | epoch 750:     31 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=5307.4, ups=0.96, wpb=5540.2, bsz=358.3, num_updates=60700, lr=0.000256706, gnorm=0.365, train_wall=48, gb_free=10.1, wall=60844
2022-08-17 15:06:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:06:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:06:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:06:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:06:22 | INFO | valid | epoch 750 | valid on 'valid' subset | loss 5.157 | nll_loss 2.548 | ppl 5.85 | bleu 57.52 | wps 1770.2 | wpb 933.5 | bsz 59.6 | num_updates 60750 | best_bleu 57.52
2022-08-17 15:06:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 750 @ 60750 updates
2022-08-17 15:06:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint750.pt
2022-08-17 15:06:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint750.pt
2022-08-17 15:07:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint750.pt (epoch 750 @ 60750 updates, score 57.52) (writing took 43.90963697806001 seconds)
2022-08-17 15:07:06 | INFO | fairseq_cli.train | end of epoch 750 (average epoch stats below)
2022-08-17 15:07:06 | INFO | train | epoch 750 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 4667.6 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 60750 | lr 0.0002566 | gnorm 0.345 | train_wall 39 | gb_free 10.2 | wall 60923
2022-08-17 15:07:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:07:07 | INFO | fairseq.trainer | begin training epoch 751
2022-08-17 15:07:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:07:32 | INFO | train_inner | epoch 751:     50 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=5282.8, ups=0.96, wpb=5529, bsz=359, num_updates=60800, lr=0.000256495, gnorm=0.343, train_wall=49, gb_free=10.1, wall=60948
2022-08-17 15:07:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:07:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:07:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:07:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:07:56 | INFO | valid | epoch 751 | valid on 'valid' subset | loss 5.168 | nll_loss 2.567 | ppl 5.92 | bleu 56.83 | wps 1842.7 | wpb 933.5 | bsz 59.6 | num_updates 60831 | best_bleu 57.52
2022-08-17 15:07:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 751 @ 60831 updates
2022-08-17 15:07:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint751.pt
2022-08-17 15:07:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint751.pt
2022-08-17 15:07:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint751.pt (epoch 751 @ 60831 updates, score 56.83) (writing took 2.5283959731459618 seconds)
2022-08-17 15:07:59 | INFO | fairseq_cli.train | end of epoch 751 (average epoch stats below)
2022-08-17 15:07:59 | INFO | train | epoch 751 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 8523.8 | ups 1.54 | wpb 5523.2 | bsz 358 | num_updates 60831 | lr 0.000256429 | gnorm 0.342 | train_wall 39 | gb_free 10 | wall 60975
2022-08-17 15:07:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:07:59 | INFO | fairseq.trainer | begin training epoch 752
2022-08-17 15:07:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:08:34 | INFO | train_inner | epoch 752:     69 / 81 loss=3.377, nll_loss=0.344, ppl=1.27, wps=8928.6, ups=1.62, wpb=5519.5, bsz=358.4, num_updates=60900, lr=0.000256284, gnorm=0.312, train_wall=48, gb_free=10.1, wall=61010
2022-08-17 15:08:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:08:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:08:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:08:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:08:49 | INFO | valid | epoch 752 | valid on 'valid' subset | loss 5.169 | nll_loss 2.569 | ppl 5.93 | bleu 56.82 | wps 1850.9 | wpb 933.5 | bsz 59.6 | num_updates 60912 | best_bleu 57.52
2022-08-17 15:08:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 752 @ 60912 updates
2022-08-17 15:08:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint752.pt
2022-08-17 15:08:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint752.pt
2022-08-17 15:09:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint752.pt (epoch 752 @ 60912 updates, score 56.82) (writing took 27.644065715372562 seconds)
2022-08-17 15:09:17 | INFO | fairseq_cli.train | end of epoch 752 (average epoch stats below)
2022-08-17 15:09:17 | INFO | train | epoch 752 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 5738.8 | ups 1.04 | wpb 5523.2 | bsz 358 | num_updates 60912 | lr 0.000256259 | gnorm 0.311 | train_wall 39 | gb_free 10.1 | wall 61053
2022-08-17 15:09:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:09:17 | INFO | fairseq.trainer | begin training epoch 753
2022-08-17 15:09:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:09:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:09:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:09:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:09:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:10:07 | INFO | valid | epoch 753 | valid on 'valid' subset | loss 5.161 | nll_loss 2.556 | ppl 5.88 | bleu 56.93 | wps 1855.4 | wpb 933.5 | bsz 59.6 | num_updates 60993 | best_bleu 57.52
2022-08-17 15:10:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 753 @ 60993 updates
2022-08-17 15:10:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint753.pt
2022-08-17 15:10:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint753.pt
2022-08-17 15:10:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint753.pt (epoch 753 @ 60993 updates, score 56.93) (writing took 21.919172193855047 seconds)
2022-08-17 15:10:29 | INFO | fairseq_cli.train | end of epoch 753 (average epoch stats below)
2022-08-17 15:10:29 | INFO | train | epoch 753 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 6176.4 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 60993 | lr 0.000256088 | gnorm 0.328 | train_wall 39 | gb_free 10.1 | wall 61125
2022-08-17 15:10:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:10:29 | INFO | fairseq.trainer | begin training epoch 754
2022-08-17 15:10:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:10:34 | INFO | train_inner | epoch 754:      7 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=4563.3, ups=0.83, wpb=5485.3, bsz=354.6, num_updates=61000, lr=0.000256074, gnorm=0.332, train_wall=48, gb_free=10, wall=61130
2022-08-17 15:11:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:11:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:11:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:11:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:11:23 | INFO | valid | epoch 754 | valid on 'valid' subset | loss 5.174 | nll_loss 2.577 | ppl 5.97 | bleu 57.04 | wps 1818.6 | wpb 933.5 | bsz 59.6 | num_updates 61074 | best_bleu 57.52
2022-08-17 15:11:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 754 @ 61074 updates
2022-08-17 15:11:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint754.pt
2022-08-17 15:11:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint754.pt
2022-08-17 15:11:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint754.pt (epoch 754 @ 61074 updates, score 57.04) (writing took 25.482639610767365 seconds)
2022-08-17 15:11:49 | INFO | fairseq_cli.train | end of epoch 754 (average epoch stats below)
2022-08-17 15:11:49 | INFO | train | epoch 754 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 5637.7 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 61074 | lr 0.000255919 | gnorm 0.433 | train_wall 39 | gb_free 10.1 | wall 61205
2022-08-17 15:11:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:11:49 | INFO | fairseq.trainer | begin training epoch 755
2022-08-17 15:11:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:12:03 | INFO | train_inner | epoch 755:     26 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=6245.5, ups=1.13, wpb=5535.3, bsz=362.2, num_updates=61100, lr=0.000255864, gnorm=0.391, train_wall=48, gb_free=10.2, wall=61219
2022-08-17 15:12:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:12:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:12:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:12:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:12:38 | INFO | valid | epoch 755 | valid on 'valid' subset | loss 5.163 | nll_loss 2.558 | ppl 5.89 | bleu 56.85 | wps 1846.1 | wpb 933.5 | bsz 59.6 | num_updates 61155 | best_bleu 57.52
2022-08-17 15:12:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 755 @ 61155 updates
2022-08-17 15:12:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint755.pt
2022-08-17 15:12:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint755.pt
2022-08-17 15:12:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint755.pt (epoch 755 @ 61155 updates, score 56.85) (writing took 19.475094202905893 seconds)
2022-08-17 15:12:58 | INFO | fairseq_cli.train | end of epoch 755 (average epoch stats below)
2022-08-17 15:12:58 | INFO | train | epoch 755 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 6436.4 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 61155 | lr 0.000255749 | gnorm 0.347 | train_wall 38 | gb_free 10.1 | wall 61274
2022-08-17 15:12:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:12:58 | INFO | fairseq.trainer | begin training epoch 756
2022-08-17 15:12:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:13:22 | INFO | train_inner | epoch 756:     45 / 81 loss=3.377, nll_loss=0.345, ppl=1.27, wps=7024.2, ups=1.27, wpb=5543.7, bsz=356.1, num_updates=61200, lr=0.000255655, gnorm=0.38, train_wall=48, gb_free=10.1, wall=61298
2022-08-17 15:13:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:13:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:13:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:13:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:13:49 | INFO | valid | epoch 756 | valid on 'valid' subset | loss 5.174 | nll_loss 2.573 | ppl 5.95 | bleu 56.47 | wps 1846.9 | wpb 933.5 | bsz 59.6 | num_updates 61236 | best_bleu 57.52
2022-08-17 15:13:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 756 @ 61236 updates
2022-08-17 15:13:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint756.pt
2022-08-17 15:13:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint756.pt
2022-08-17 15:14:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint756.pt (epoch 756 @ 61236 updates, score 56.47) (writing took 17.565592173486948 seconds)
2022-08-17 15:14:07 | INFO | fairseq_cli.train | end of epoch 756 (average epoch stats below)
2022-08-17 15:14:07 | INFO | train | epoch 756 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 6523.6 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 61236 | lr 0.00025558 | gnorm 0.361 | train_wall 40 | gb_free 10.1 | wall 61343
2022-08-17 15:14:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:14:07 | INFO | fairseq.trainer | begin training epoch 757
2022-08-17 15:14:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:14:40 | INFO | train_inner | epoch 757:     64 / 81 loss=3.376, nll_loss=0.344, ppl=1.27, wps=7061.6, ups=1.28, wpb=5533.9, bsz=360.5, num_updates=61300, lr=0.000255446, gnorm=0.333, train_wall=49, gb_free=10.1, wall=61376
2022-08-17 15:14:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:14:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:14:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:14:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:14:58 | INFO | valid | epoch 757 | valid on 'valid' subset | loss 5.148 | nll_loss 2.54 | ppl 5.81 | bleu 57.31 | wps 1746.2 | wpb 933.5 | bsz 59.6 | num_updates 61317 | best_bleu 57.52
2022-08-17 15:14:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 757 @ 61317 updates
2022-08-17 15:14:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint757.pt
2022-08-17 15:15:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint757.pt
2022-08-17 15:15:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint757.pt (epoch 757 @ 61317 updates, score 57.31) (writing took 12.964656479656696 seconds)
2022-08-17 15:15:11 | INFO | fairseq_cli.train | end of epoch 757 (average epoch stats below)
2022-08-17 15:15:11 | INFO | train | epoch 757 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 6910.8 | ups 1.25 | wpb 5523.2 | bsz 358 | num_updates 61317 | lr 0.000255411 | gnorm 0.319 | train_wall 40 | gb_free 10.3 | wall 61408
2022-08-17 15:15:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:15:12 | INFO | fairseq.trainer | begin training epoch 758
2022-08-17 15:15:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:15:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:15:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:15:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:15:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:16:03 | INFO | valid | epoch 758 | valid on 'valid' subset | loss 5.165 | nll_loss 2.562 | ppl 5.91 | bleu 56.96 | wps 1798.7 | wpb 933.5 | bsz 59.6 | num_updates 61398 | best_bleu 57.52
2022-08-17 15:16:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 758 @ 61398 updates
2022-08-17 15:16:03 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint758.pt
2022-08-17 15:16:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint758.pt
2022-08-17 15:16:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint758.pt (epoch 758 @ 61398 updates, score 56.96) (writing took 18.058606814593077 seconds)
2022-08-17 15:16:21 | INFO | fairseq_cli.train | end of epoch 758 (average epoch stats below)
2022-08-17 15:16:21 | INFO | train | epoch 758 | loss 3.376 | nll_loss 0.342 | ppl 1.27 | wps 6414.2 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 61398 | lr 0.000255242 | gnorm 4.4 | train_wall 40 | gb_free 10.1 | wall 61477
2022-08-17 15:16:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:16:21 | INFO | fairseq.trainer | begin training epoch 759
2022-08-17 15:16:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:16:24 | INFO | train_inner | epoch 759:      2 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=5312.4, ups=0.96, wpb=5507.4, bsz=356, num_updates=61400, lr=0.000255238, gnorm=3.615, train_wall=49, gb_free=10.1, wall=61480
2022-08-17 15:17:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:17:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:17:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:17:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:17:15 | INFO | valid | epoch 759 | valid on 'valid' subset | loss 5.171 | nll_loss 2.566 | ppl 5.92 | bleu 56.57 | wps 1776.5 | wpb 933.5 | bsz 59.6 | num_updates 61479 | best_bleu 57.52
2022-08-17 15:17:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 759 @ 61479 updates
2022-08-17 15:17:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint759.pt
2022-08-17 15:17:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint759.pt
2022-08-17 15:17:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint759.pt (epoch 759 @ 61479 updates, score 56.57) (writing took 41.378204349428415 seconds)
2022-08-17 15:17:57 | INFO | fairseq_cli.train | end of epoch 759 (average epoch stats below)
2022-08-17 15:17:57 | INFO | train | epoch 759 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 4671.6 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 61479 | lr 0.000255074 | gnorm 0.409 | train_wall 39 | gb_free 10.1 | wall 61573
2022-08-17 15:17:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:17:57 | INFO | fairseq.trainer | begin training epoch 760
2022-08-17 15:17:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:18:09 | INFO | train_inner | epoch 760:     21 / 81 loss=3.377, nll_loss=0.343, ppl=1.27, wps=5225.1, ups=0.95, wpb=5503.9, bsz=357.8, num_updates=61500, lr=0.000255031, gnorm=0.399, train_wall=49, gb_free=10.1, wall=61585
2022-08-17 15:18:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:18:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:18:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:18:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:18:49 | INFO | valid | epoch 760 | valid on 'valid' subset | loss 5.175 | nll_loss 2.572 | ppl 5.94 | bleu 57.03 | wps 1776.6 | wpb 933.5 | bsz 59.6 | num_updates 61560 | best_bleu 57.52
2022-08-17 15:18:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 760 @ 61560 updates
2022-08-17 15:18:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint760.pt
2022-08-17 15:18:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint760.pt
2022-08-17 15:19:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint760.pt (epoch 760 @ 61560 updates, score 57.03) (writing took 16.452358309179544 seconds)
2022-08-17 15:19:06 | INFO | fairseq_cli.train | end of epoch 760 (average epoch stats below)
2022-08-17 15:19:06 | INFO | train | epoch 760 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 6502.8 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 61560 | lr 0.000254906 | gnorm 0.326 | train_wall 40 | gb_free 10.2 | wall 61642
2022-08-17 15:19:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:19:06 | INFO | fairseq.trainer | begin training epoch 761
2022-08-17 15:19:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:19:26 | INFO | train_inner | epoch 761:     40 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=7183, ups=1.29, wpb=5550.7, bsz=356.1, num_updates=61600, lr=0.000254824, gnorm=0.333, train_wall=49, gb_free=10.1, wall=61662
2022-08-17 15:19:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:19:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:19:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:19:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:19:56 | INFO | valid | epoch 761 | valid on 'valid' subset | loss 5.188 | nll_loss 2.593 | ppl 6.03 | bleu 56.69 | wps 1812.7 | wpb 933.5 | bsz 59.6 | num_updates 61641 | best_bleu 57.52
2022-08-17 15:19:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 761 @ 61641 updates
2022-08-17 15:19:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint761.pt
2022-08-17 15:19:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint761.pt
2022-08-17 15:19:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint761.pt (epoch 761 @ 61641 updates, score 56.69) (writing took 2.408185575157404 seconds)
2022-08-17 15:19:58 | INFO | fairseq_cli.train | end of epoch 761 (average epoch stats below)
2022-08-17 15:19:58 | INFO | train | epoch 761 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 8532.7 | ups 1.54 | wpb 5523.2 | bsz 358 | num_updates 61641 | lr 0.000254739 | gnorm 0.346 | train_wall 39 | gb_free 10.1 | wall 61694
2022-08-17 15:19:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:19:58 | INFO | fairseq.trainer | begin training epoch 762
2022-08-17 15:19:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:20:29 | INFO | train_inner | epoch 762:     59 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=8877.5, ups=1.6, wpb=5536.1, bsz=359.6, num_updates=61700, lr=0.000254617, gnorm=0.334, train_wall=48, gb_free=10, wall=61725
2022-08-17 15:20:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:20:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:20:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:20:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:20:49 | INFO | valid | epoch 762 | valid on 'valid' subset | loss 5.178 | nll_loss 2.578 | ppl 5.97 | bleu 57.11 | wps 1750.4 | wpb 933.5 | bsz 59.6 | num_updates 61722 | best_bleu 57.52
2022-08-17 15:20:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 762 @ 61722 updates
2022-08-17 15:20:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint762.pt
2022-08-17 15:20:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint762.pt
2022-08-17 15:21:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint762.pt (epoch 762 @ 61722 updates, score 57.11) (writing took 23.45843556150794 seconds)
2022-08-17 15:21:13 | INFO | fairseq_cli.train | end of epoch 762 (average epoch stats below)
2022-08-17 15:21:13 | INFO | train | epoch 762 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 5981.6 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 61722 | lr 0.000254572 | gnorm 0.324 | train_wall 40 | gb_free 10.1 | wall 61769
2022-08-17 15:21:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:21:13 | INFO | fairseq.trainer | begin training epoch 763
2022-08-17 15:21:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:21:57 | INFO | train_inner | epoch 763:     78 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=6210.7, ups=1.13, wpb=5515.7, bsz=360, num_updates=61800, lr=0.000254411, gnorm=0.318, train_wall=50, gb_free=10.1, wall=61814
2022-08-17 15:21:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:22:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:22:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:22:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:22:08 | INFO | valid | epoch 763 | valid on 'valid' subset | loss 5.174 | nll_loss 2.573 | ppl 5.95 | bleu 56.63 | wps 1785.7 | wpb 933.5 | bsz 59.6 | num_updates 61803 | best_bleu 57.52
2022-08-17 15:22:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 763 @ 61803 updates
2022-08-17 15:22:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint763.pt
2022-08-17 15:22:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint763.pt
2022-08-17 15:22:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint763.pt (epoch 763 @ 61803 updates, score 56.63) (writing took 14.45544596388936 seconds)
2022-08-17 15:22:23 | INFO | fairseq_cli.train | end of epoch 763 (average epoch stats below)
2022-08-17 15:22:23 | INFO | train | epoch 763 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 6389.1 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 61803 | lr 0.000254405 | gnorm 0.32 | train_wall 40 | gb_free 10 | wall 61839
2022-08-17 15:22:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:22:23 | INFO | fairseq.trainer | begin training epoch 764
2022-08-17 15:22:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:23:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:23:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:23:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:23:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:23:14 | INFO | valid | epoch 764 | valid on 'valid' subset | loss 5.188 | nll_loss 2.589 | ppl 6.02 | bleu 56.83 | wps 1835.5 | wpb 933.5 | bsz 59.6 | num_updates 61884 | best_bleu 57.52
2022-08-17 15:23:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 764 @ 61884 updates
2022-08-17 15:23:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint764.pt
2022-08-17 15:23:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint764.pt
2022-08-17 15:23:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint764.pt (epoch 764 @ 61884 updates, score 56.83) (writing took 19.950343877077103 seconds)
2022-08-17 15:23:34 | INFO | fairseq_cli.train | end of epoch 764 (average epoch stats below)
2022-08-17 15:23:34 | INFO | train | epoch 764 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 6310.8 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 61884 | lr 0.000254238 | gnorm 0.293 | train_wall 40 | gb_free 10.1 | wall 61910
2022-08-17 15:23:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:23:34 | INFO | fairseq.trainer | begin training epoch 765
2022-08-17 15:23:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:23:43 | INFO | train_inner | epoch 765:     16 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=5208.2, ups=0.95, wpb=5498.6, bsz=353, num_updates=61900, lr=0.000254205, gnorm=0.293, train_wall=49, gb_free=10.1, wall=61919
2022-08-17 15:24:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:24:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:24:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:24:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:24:24 | INFO | valid | epoch 765 | valid on 'valid' subset | loss 5.171 | nll_loss 2.567 | ppl 5.93 | bleu 56.81 | wps 1875.9 | wpb 933.5 | bsz 59.6 | num_updates 61965 | best_bleu 57.52
2022-08-17 15:24:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 765 @ 61965 updates
2022-08-17 15:24:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint765.pt
2022-08-17 15:24:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint765.pt
2022-08-17 15:25:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint765.pt (epoch 765 @ 61965 updates, score 56.81) (writing took 44.22782016918063 seconds)
2022-08-17 15:25:09 | INFO | fairseq_cli.train | end of epoch 765 (average epoch stats below)
2022-08-17 15:25:09 | INFO | train | epoch 765 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 4723.4 | ups 0.86 | wpb 5523.2 | bsz 358 | num_updates 61965 | lr 0.000254072 | gnorm 0.383 | train_wall 39 | gb_free 10.2 | wall 62005
2022-08-17 15:25:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:25:09 | INFO | fairseq.trainer | begin training epoch 766
2022-08-17 15:25:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:25:27 | INFO | train_inner | epoch 766:     35 / 81 loss=3.377, nll_loss=0.344, ppl=1.27, wps=5300.2, ups=0.96, wpb=5515.7, bsz=357, num_updates=62000, lr=0.000254, gnorm=0.362, train_wall=49, gb_free=10.1, wall=62023
2022-08-17 15:25:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:25:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:25:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:25:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:25:59 | INFO | valid | epoch 766 | valid on 'valid' subset | loss 5.156 | nll_loss 2.552 | ppl 5.86 | bleu 56.94 | wps 1948.5 | wpb 933.5 | bsz 59.6 | num_updates 62046 | best_bleu 57.52
2022-08-17 15:25:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 766 @ 62046 updates
2022-08-17 15:25:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint766.pt
2022-08-17 15:26:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint766.pt
2022-08-17 15:26:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint766.pt (epoch 766 @ 62046 updates, score 56.94) (writing took 2.386598702520132 seconds)
2022-08-17 15:26:02 | INFO | fairseq_cli.train | end of epoch 766 (average epoch stats below)
2022-08-17 15:26:02 | INFO | train | epoch 766 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 8411.2 | ups 1.52 | wpb 5523.2 | bsz 358 | num_updates 62046 | lr 0.000253906 | gnorm 0.318 | train_wall 40 | gb_free 10.1 | wall 62058
2022-08-17 15:26:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:26:02 | INFO | fairseq.trainer | begin training epoch 767
2022-08-17 15:26:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:26:30 | INFO | train_inner | epoch 767:     54 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=8895.8, ups=1.6, wpb=5555.3, bsz=364.1, num_updates=62100, lr=0.000253796, gnorm=0.356, train_wall=49, gb_free=10, wall=62086
2022-08-17 15:26:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:26:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:26:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:26:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:26:53 | INFO | valid | epoch 767 | valid on 'valid' subset | loss 5.161 | nll_loss 2.556 | ppl 5.88 | bleu 57.23 | wps 1711.4 | wpb 933.5 | bsz 59.6 | num_updates 62127 | best_bleu 57.52
2022-08-17 15:26:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 767 @ 62127 updates
2022-08-17 15:26:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint767.pt
2022-08-17 15:26:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint767.pt
2022-08-17 15:27:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint767.pt (epoch 767 @ 62127 updates, score 57.23) (writing took 18.934411369264126 seconds)
2022-08-17 15:27:12 | INFO | fairseq_cli.train | end of epoch 767 (average epoch stats below)
2022-08-17 15:27:12 | INFO | train | epoch 767 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 6361.3 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 62127 | lr 0.000253741 | gnorm 0.354 | train_wall 40 | gb_free 10.2 | wall 62128
2022-08-17 15:27:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:27:12 | INFO | fairseq.trainer | begin training epoch 768
2022-08-17 15:27:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:27:51 | INFO | train_inner | epoch 768:     73 / 81 loss=3.377, nll_loss=0.344, ppl=1.27, wps=6817.4, ups=1.23, wpb=5532.2, bsz=358, num_updates=62200, lr=0.000253592, gnorm=0.329, train_wall=50, gb_free=10, wall=62167
2022-08-17 15:27:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:27:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:27:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:27:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:28:04 | INFO | valid | epoch 768 | valid on 'valid' subset | loss 5.155 | nll_loss 2.549 | ppl 5.85 | bleu 56.98 | wps 1850.6 | wpb 933.5 | bsz 59.6 | num_updates 62208 | best_bleu 57.52
2022-08-17 15:28:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 768 @ 62208 updates
2022-08-17 15:28:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint768.pt
2022-08-17 15:28:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint768.pt
2022-08-17 15:28:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint768.pt (epoch 768 @ 62208 updates, score 56.98) (writing took 22.777081862092018 seconds)
2022-08-17 15:28:27 | INFO | fairseq_cli.train | end of epoch 768 (average epoch stats below)
2022-08-17 15:28:27 | INFO | train | epoch 768 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 5993.7 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 62208 | lr 0.000253575 | gnorm 0.33 | train_wall 40 | gb_free 10.1 | wall 62203
2022-08-17 15:28:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:28:27 | INFO | fairseq.trainer | begin training epoch 769
2022-08-17 15:28:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:29:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:29:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:29:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:29:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:29:18 | INFO | valid | epoch 769 | valid on 'valid' subset | loss 5.153 | nll_loss 2.551 | ppl 5.86 | bleu 57.1 | wps 1891.4 | wpb 933.5 | bsz 59.6 | num_updates 62289 | best_bleu 57.52
2022-08-17 15:29:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 769 @ 62289 updates
2022-08-17 15:29:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint769.pt
2022-08-17 15:29:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint769.pt
2022-08-17 15:29:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint769.pt (epoch 769 @ 62289 updates, score 57.1) (writing took 2.3978155851364136 seconds)
2022-08-17 15:29:20 | INFO | fairseq_cli.train | end of epoch 769 (average epoch stats below)
2022-08-17 15:29:20 | INFO | train | epoch 769 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 8356.9 | ups 1.51 | wpb 5523.2 | bsz 358 | num_updates 62289 | lr 0.00025341 | gnorm 0.325 | train_wall 40 | gb_free 10 | wall 62257
2022-08-17 15:29:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:29:21 | INFO | fairseq.trainer | begin training epoch 770
2022-08-17 15:29:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:29:27 | INFO | train_inner | epoch 770:     11 / 81 loss=3.376, nll_loss=0.342, ppl=1.27, wps=5716.6, ups=1.04, wpb=5510.6, bsz=357.4, num_updates=62300, lr=0.000253388, gnorm=0.313, train_wall=49, gb_free=10, wall=62263
2022-08-17 15:30:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:30:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:30:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:30:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:30:12 | INFO | valid | epoch 770 | valid on 'valid' subset | loss 5.162 | nll_loss 2.559 | ppl 5.89 | bleu 56.81 | wps 1835.6 | wpb 933.5 | bsz 59.6 | num_updates 62370 | best_bleu 57.52
2022-08-17 15:30:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 770 @ 62370 updates
2022-08-17 15:30:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint770.pt
2022-08-17 15:30:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint770.pt
2022-08-17 15:30:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint770.pt (epoch 770 @ 62370 updates, score 56.81) (writing took 14.408053264021873 seconds)
2022-08-17 15:30:27 | INFO | fairseq_cli.train | end of epoch 770 (average epoch stats below)
2022-08-17 15:30:27 | INFO | train | epoch 770 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 6750.1 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 62370 | lr 0.000253246 | gnorm 0.279 | train_wall 40 | gb_free 10.1 | wall 62323
2022-08-17 15:30:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:30:27 | INFO | fairseq.trainer | begin training epoch 771
2022-08-17 15:30:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:30:43 | INFO | train_inner | epoch 771:     30 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=7287.8, ups=1.32, wpb=5523.8, bsz=357.8, num_updates=62400, lr=0.000253185, gnorm=0.312, train_wall=50, gb_free=10.1, wall=62339
2022-08-17 15:31:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:31:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:31:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:31:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:31:16 | INFO | valid | epoch 771 | valid on 'valid' subset | loss 5.161 | nll_loss 2.556 | ppl 5.88 | bleu 56.81 | wps 1918.5 | wpb 933.5 | bsz 59.6 | num_updates 62451 | best_bleu 57.52
2022-08-17 15:31:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 771 @ 62451 updates
2022-08-17 15:31:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint771.pt
2022-08-17 15:31:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint771.pt
2022-08-17 15:31:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint771.pt (epoch 771 @ 62451 updates, score 56.81) (writing took 29.645935881882906 seconds)
2022-08-17 15:31:46 | INFO | fairseq_cli.train | end of epoch 771 (average epoch stats below)
2022-08-17 15:31:46 | INFO | train | epoch 771 | loss 3.376 | nll_loss 0.344 | ppl 1.27 | wps 5629 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 62451 | lr 0.000253081 | gnorm 0.351 | train_wall 39 | gb_free 10.2 | wall 62402
2022-08-17 15:31:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:31:46 | INFO | fairseq.trainer | begin training epoch 772
2022-08-17 15:31:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:32:12 | INFO | train_inner | epoch 772:     49 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=6200.6, ups=1.12, wpb=5516.6, bsz=358, num_updates=62500, lr=0.000252982, gnorm=0.343, train_wall=48, gb_free=10.1, wall=62428
2022-08-17 15:32:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:32:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:32:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:32:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:32:37 | INFO | valid | epoch 772 | valid on 'valid' subset | loss 5.154 | nll_loss 2.55 | ppl 5.86 | bleu 56.68 | wps 1905.2 | wpb 933.5 | bsz 59.6 | num_updates 62532 | best_bleu 57.52
2022-08-17 15:32:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 772 @ 62532 updates
2022-08-17 15:32:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint772.pt
2022-08-17 15:32:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint772.pt
2022-08-17 15:32:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint772.pt (epoch 772 @ 62532 updates, score 56.68) (writing took 16.539751958101988 seconds)
2022-08-17 15:32:54 | INFO | fairseq_cli.train | end of epoch 772 (average epoch stats below)
2022-08-17 15:32:54 | INFO | train | epoch 772 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 6621.6 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 62532 | lr 0.000252917 | gnorm 0.334 | train_wall 40 | gb_free 10.1 | wall 62470
2022-08-17 15:32:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:32:54 | INFO | fairseq.trainer | begin training epoch 773
2022-08-17 15:32:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:33:29 | INFO | train_inner | epoch 773:     68 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=7177.3, ups=1.29, wpb=5545.5, bsz=364.5, num_updates=62600, lr=0.00025278, gnorm=0.306, train_wall=50, gb_free=10, wall=62505
2022-08-17 15:33:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:33:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:33:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:33:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:33:45 | INFO | valid | epoch 773 | valid on 'valid' subset | loss 5.159 | nll_loss 2.555 | ppl 5.88 | bleu 57.05 | wps 1805.7 | wpb 933.5 | bsz 59.6 | num_updates 62613 | best_bleu 57.52
2022-08-17 15:33:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 773 @ 62613 updates
2022-08-17 15:33:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint773.pt
2022-08-17 15:33:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint773.pt
2022-08-17 15:34:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint773.pt (epoch 773 @ 62613 updates, score 57.05) (writing took 34.86330350860953 seconds)
2022-08-17 15:34:20 | INFO | fairseq_cli.train | end of epoch 773 (average epoch stats below)
2022-08-17 15:34:20 | INFO | train | epoch 773 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 5198.4 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 62613 | lr 0.000252754 | gnorm 0.376 | train_wall 40 | gb_free 10.1 | wall 62556
2022-08-17 15:34:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:34:20 | INFO | fairseq.trainer | begin training epoch 774
2022-08-17 15:34:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:35:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:35:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:35:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:35:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:35:12 | INFO | valid | epoch 774 | valid on 'valid' subset | loss 5.159 | nll_loss 2.553 | ppl 5.87 | bleu 57.32 | wps 1949.6 | wpb 933.5 | bsz 59.6 | num_updates 62694 | best_bleu 57.52
2022-08-17 15:35:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 774 @ 62694 updates
2022-08-17 15:35:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint774.pt
2022-08-17 15:35:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint774.pt
2022-08-17 15:35:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint774.pt (epoch 774 @ 62694 updates, score 57.32) (writing took 16.87847238779068 seconds)
2022-08-17 15:35:29 | INFO | fairseq_cli.train | end of epoch 774 (average epoch stats below)
2022-08-17 15:35:29 | INFO | train | epoch 774 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 6473 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 62694 | lr 0.00025259 | gnorm 0.334 | train_wall 41 | gb_free 10 | wall 62625
2022-08-17 15:35:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:35:29 | INFO | fairseq.trainer | begin training epoch 775
2022-08-17 15:35:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:35:33 | INFO | train_inner | epoch 775:      6 / 81 loss=3.377, nll_loss=0.344, ppl=1.27, wps=4422.3, ups=0.81, wpb=5488.5, bsz=351.3, num_updates=62700, lr=0.000252578, gnorm=0.391, train_wall=50, gb_free=10.1, wall=62629
2022-08-17 15:36:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:36:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:36:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:36:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:36:23 | INFO | valid | epoch 775 | valid on 'valid' subset | loss 5.156 | nll_loss 2.552 | ppl 5.87 | bleu 56.9 | wps 1795.2 | wpb 933.5 | bsz 59.6 | num_updates 62775 | best_bleu 57.52
2022-08-17 15:36:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 775 @ 62775 updates
2022-08-17 15:36:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint775.pt
2022-08-17 15:36:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint775.pt
2022-08-17 15:36:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint775.pt (epoch 775 @ 62775 updates, score 56.9) (writing took 20.925482638180256 seconds)
2022-08-17 15:36:44 | INFO | fairseq_cli.train | end of epoch 775 (average epoch stats below)
2022-08-17 15:36:44 | INFO | train | epoch 775 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 5951.8 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 62775 | lr 0.000252427 | gnorm 0.351 | train_wall 41 | gb_free 10.1 | wall 62700
2022-08-17 15:36:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:36:44 | INFO | fairseq.trainer | begin training epoch 776
2022-08-17 15:36:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:37:00 | INFO | train_inner | epoch 776:     25 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=6435.6, ups=1.16, wpb=5562.6, bsz=359.1, num_updates=62800, lr=0.000252377, gnorm=0.351, train_wall=51, gb_free=10, wall=62716
2022-08-17 15:37:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:37:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:37:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:37:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:37:38 | INFO | valid | epoch 776 | valid on 'valid' subset | loss 5.167 | nll_loss 2.564 | ppl 5.91 | bleu 56.47 | wps 1778.6 | wpb 933.5 | bsz 59.6 | num_updates 62856 | best_bleu 57.52
2022-08-17 15:37:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 776 @ 62856 updates
2022-08-17 15:37:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint776.pt
2022-08-17 15:37:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint776.pt
2022-08-17 15:37:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint776.pt (epoch 776 @ 62856 updates, score 56.47) (writing took 18.332850832492113 seconds)
2022-08-17 15:37:56 | INFO | fairseq_cli.train | end of epoch 776 (average epoch stats below)
2022-08-17 15:37:56 | INFO | train | epoch 776 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 6202.2 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 62856 | lr 0.000252265 | gnorm 0.348 | train_wall 40 | gb_free 10.1 | wall 62772
2022-08-17 15:37:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:37:56 | INFO | fairseq.trainer | begin training epoch 777
2022-08-17 15:37:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:38:19 | INFO | train_inner | epoch 777:     44 / 81 loss=3.376, nll_loss=0.344, ppl=1.27, wps=6931.4, ups=1.26, wpb=5520.1, bsz=364.5, num_updates=62900, lr=0.000252177, gnorm=0.346, train_wall=48, gb_free=10.1, wall=62796
2022-08-17 15:38:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:38:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:38:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:38:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:38:46 | INFO | valid | epoch 777 | valid on 'valid' subset | loss 5.176 | nll_loss 2.576 | ppl 5.96 | bleu 56.15 | wps 1907.9 | wpb 933.5 | bsz 59.6 | num_updates 62937 | best_bleu 57.52
2022-08-17 15:38:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 777 @ 62937 updates
2022-08-17 15:38:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint777.pt
2022-08-17 15:38:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint777.pt
2022-08-17 15:39:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint777.pt (epoch 777 @ 62937 updates, score 56.15) (writing took 18.103100806474686 seconds)
2022-08-17 15:39:04 | INFO | fairseq_cli.train | end of epoch 777 (average epoch stats below)
2022-08-17 15:39:04 | INFO | train | epoch 777 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 6557.2 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 62937 | lr 0.000252102 | gnorm 0.351 | train_wall 38 | gb_free 10.1 | wall 62841
2022-08-17 15:39:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:39:05 | INFO | fairseq.trainer | begin training epoch 778
2022-08-17 15:39:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:39:37 | INFO | train_inner | epoch 778:     63 / 81 loss=3.377, nll_loss=0.344, ppl=1.27, wps=7100.6, ups=1.29, wpb=5519.6, bsz=349.3, num_updates=63000, lr=0.000251976, gnorm=0.342, train_wall=48, gb_free=10.1, wall=62873
2022-08-17 15:39:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:39:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:39:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:39:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:39:55 | INFO | valid | epoch 778 | valid on 'valid' subset | loss 5.175 | nll_loss 2.571 | ppl 5.94 | bleu 56.71 | wps 1883.2 | wpb 933.5 | bsz 59.6 | num_updates 63018 | best_bleu 57.52
2022-08-17 15:39:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 778 @ 63018 updates
2022-08-17 15:39:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint778.pt
2022-08-17 15:39:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint778.pt
2022-08-17 15:40:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint778.pt (epoch 778 @ 63018 updates, score 56.71) (writing took 19.844387128949165 seconds)
2022-08-17 15:40:15 | INFO | fairseq_cli.train | end of epoch 778 (average epoch stats below)
2022-08-17 15:40:15 | INFO | train | epoch 778 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 6334 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 63018 | lr 0.00025194 | gnorm 0.321 | train_wall 39 | gb_free 10.1 | wall 62911
2022-08-17 15:40:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:40:15 | INFO | fairseq.trainer | begin training epoch 779
2022-08-17 15:40:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:40:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:40:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:40:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:40:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:41:06 | INFO | valid | epoch 779 | valid on 'valid' subset | loss 5.167 | nll_loss 2.561 | ppl 5.9 | bleu 56.85 | wps 1885.2 | wpb 933.5 | bsz 59.6 | num_updates 63099 | best_bleu 57.52
2022-08-17 15:41:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 779 @ 63099 updates
2022-08-17 15:41:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint779.pt
2022-08-17 15:41:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint779.pt
2022-08-17 15:41:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint779.pt (epoch 779 @ 63099 updates, score 56.85) (writing took 13.242326438426971 seconds)
2022-08-17 15:41:19 | INFO | fairseq_cli.train | end of epoch 779 (average epoch stats below)
2022-08-17 15:41:19 | INFO | train | epoch 779 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 6952.9 | ups 1.26 | wpb 5523.2 | bsz 358 | num_updates 63099 | lr 0.000251779 | gnorm 0.294 | train_wall 40 | gb_free 10.1 | wall 62976
2022-08-17 15:41:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:41:20 | INFO | fairseq.trainer | begin training epoch 780
2022-08-17 15:41:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:41:21 | INFO | train_inner | epoch 780:      1 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=5292.9, ups=0.96, wpb=5496.6, bsz=358.5, num_updates=63100, lr=0.000251777, gnorm=0.294, train_wall=49, gb_free=10.1, wall=62977
2022-08-17 15:42:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:42:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:42:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:42:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:42:10 | INFO | valid | epoch 780 | valid on 'valid' subset | loss 5.177 | nll_loss 2.579 | ppl 5.97 | bleu 56.46 | wps 1845.4 | wpb 933.5 | bsz 59.6 | num_updates 63180 | best_bleu 57.52
2022-08-17 15:42:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 780 @ 63180 updates
2022-08-17 15:42:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint780.pt
2022-08-17 15:42:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint780.pt
2022-08-17 15:42:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint780.pt (epoch 780 @ 63180 updates, score 56.46) (writing took 17.01302247494459 seconds)
2022-08-17 15:42:28 | INFO | fairseq_cli.train | end of epoch 780 (average epoch stats below)
2022-08-17 15:42:28 | INFO | train | epoch 780 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 6568.2 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 63180 | lr 0.000251617 | gnorm 0.329 | train_wall 39 | gb_free 10.2 | wall 63044
2022-08-17 15:42:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:42:28 | INFO | fairseq.trainer | begin training epoch 781
2022-08-17 15:42:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:42:38 | INFO | train_inner | epoch 781:     20 / 81 loss=3.376, nll_loss=0.342, ppl=1.27, wps=7128.5, ups=1.29, wpb=5517.3, bsz=360.2, num_updates=63200, lr=0.000251577, gnorm=0.324, train_wall=48, gb_free=10.2, wall=63055
2022-08-17 15:43:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:43:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:43:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:43:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:43:18 | INFO | valid | epoch 781 | valid on 'valid' subset | loss 5.168 | nll_loss 2.563 | ppl 5.91 | bleu 56.77 | wps 1877.9 | wpb 933.5 | bsz 59.6 | num_updates 63261 | best_bleu 57.52
2022-08-17 15:43:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 781 @ 63261 updates
2022-08-17 15:43:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint781.pt
2022-08-17 15:43:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint781.pt
2022-08-17 15:43:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint781.pt (epoch 781 @ 63261 updates, score 56.77) (writing took 18.559161748737097 seconds)
2022-08-17 15:43:37 | INFO | fairseq_cli.train | end of epoch 781 (average epoch stats below)
2022-08-17 15:43:37 | INFO | train | epoch 781 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 6452.1 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 63261 | lr 0.000251456 | gnorm 0.291 | train_wall 40 | gb_free 10 | wall 63113
2022-08-17 15:43:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:43:37 | INFO | fairseq.trainer | begin training epoch 782
2022-08-17 15:43:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:43:58 | INFO | train_inner | epoch 782:     39 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=6987.2, ups=1.26, wpb=5533.1, bsz=353.2, num_updates=63300, lr=0.000251379, gnorm=0.3, train_wall=49, gb_free=10, wall=63134
2022-08-17 15:44:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:44:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:44:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:44:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:44:28 | INFO | valid | epoch 782 | valid on 'valid' subset | loss 5.175 | nll_loss 2.574 | ppl 5.96 | bleu 56.5 | wps 1728 | wpb 933.5 | bsz 59.6 | num_updates 63342 | best_bleu 57.52
2022-08-17 15:44:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 782 @ 63342 updates
2022-08-17 15:44:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint782.pt
2022-08-17 15:44:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint782.pt
2022-08-17 15:44:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint782.pt (epoch 782 @ 63342 updates, score 56.5) (writing took 25.54348747059703 seconds)
2022-08-17 15:44:54 | INFO | fairseq_cli.train | end of epoch 782 (average epoch stats below)
2022-08-17 15:44:54 | INFO | train | epoch 782 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 5784.6 | ups 1.05 | wpb 5523.2 | bsz 358 | num_updates 63342 | lr 0.000251295 | gnorm 0.325 | train_wall 40 | gb_free 10.2 | wall 63190
2022-08-17 15:44:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:44:54 | INFO | fairseq.trainer | begin training epoch 783
2022-08-17 15:44:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:45:28 | INFO | train_inner | epoch 783:     58 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=6080.3, ups=1.1, wpb=5527.4, bsz=363.3, num_updates=63400, lr=0.00025118, gnorm=0.329, train_wall=50, gb_free=10.2, wall=63225
2022-08-17 15:45:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:45:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:45:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:45:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:45:50 | INFO | valid | epoch 783 | valid on 'valid' subset | loss 5.203 | nll_loss 2.61 | ppl 6.11 | bleu 56.9 | wps 1693.8 | wpb 933.5 | bsz 59.6 | num_updates 63423 | best_bleu 57.52
2022-08-17 15:45:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 783 @ 63423 updates
2022-08-17 15:45:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint783.pt
2022-08-17 15:45:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint783.pt
2022-08-17 15:46:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint783.pt (epoch 783 @ 63423 updates, score 56.9) (writing took 31.006603598594666 seconds)
2022-08-17 15:46:22 | INFO | fairseq_cli.train | end of epoch 783 (average epoch stats below)
2022-08-17 15:46:22 | INFO | train | epoch 783 | loss 3.376 | nll_loss 0.344 | ppl 1.27 | wps 5114.2 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 63423 | lr 0.000251135 | gnorm 0.333 | train_wall 41 | gb_free 10.1 | wall 63278
2022-08-17 15:46:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:46:22 | INFO | fairseq.trainer | begin training epoch 784
2022-08-17 15:46:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:47:01 | INFO | train_inner | epoch 784:     77 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=5991.4, ups=1.08, wpb=5538, bsz=358.1, num_updates=63500, lr=0.000250982, gnorm=0.338, train_wall=49, gb_free=10.1, wall=63317
2022-08-17 15:47:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:47:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:47:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:47:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:47:12 | INFO | valid | epoch 784 | valid on 'valid' subset | loss 5.188 | nll_loss 2.596 | ppl 6.04 | bleu 56.74 | wps 1912.1 | wpb 933.5 | bsz 59.6 | num_updates 63504 | best_bleu 57.52
2022-08-17 15:47:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 784 @ 63504 updates
2022-08-17 15:47:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint784.pt
2022-08-17 15:47:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint784.pt
2022-08-17 15:47:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint784.pt (epoch 784 @ 63504 updates, score 56.74) (writing took 17.74678935483098 seconds)
2022-08-17 15:47:30 | INFO | fairseq_cli.train | end of epoch 784 (average epoch stats below)
2022-08-17 15:47:30 | INFO | train | epoch 784 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 6585.7 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 63504 | lr 0.000250974 | gnorm 0.346 | train_wall 39 | gb_free 10.1 | wall 63346
2022-08-17 15:47:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:47:30 | INFO | fairseq.trainer | begin training epoch 785
2022-08-17 15:47:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:48:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:48:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:48:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:48:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:48:22 | INFO | valid | epoch 785 | valid on 'valid' subset | loss 5.174 | nll_loss 2.574 | ppl 5.95 | bleu 57.02 | wps 1838.9 | wpb 933.5 | bsz 59.6 | num_updates 63585 | best_bleu 57.52
2022-08-17 15:48:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 785 @ 63585 updates
2022-08-17 15:48:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint785.pt
2022-08-17 15:48:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint785.pt
2022-08-17 15:48:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint785.pt (epoch 785 @ 63585 updates, score 57.02) (writing took 18.19010267779231 seconds)
2022-08-17 15:48:40 | INFO | fairseq_cli.train | end of epoch 785 (average epoch stats below)
2022-08-17 15:48:40 | INFO | train | epoch 785 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 6357.4 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 63585 | lr 0.000250815 | gnorm 0.338 | train_wall 41 | gb_free 10.2 | wall 63416
2022-08-17 15:48:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:48:40 | INFO | fairseq.trainer | begin training epoch 786
2022-08-17 15:48:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:48:49 | INFO | train_inner | epoch 786:     15 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=5083.4, ups=0.92, wpb=5498.4, bsz=352, num_updates=63600, lr=0.000250785, gnorm=0.346, train_wall=50, gb_free=10.1, wall=63425
2022-08-17 15:49:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:49:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:49:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:49:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:49:32 | INFO | valid | epoch 786 | valid on 'valid' subset | loss 5.182 | nll_loss 2.581 | ppl 5.98 | bleu 57.03 | wps 1856 | wpb 933.5 | bsz 59.6 | num_updates 63666 | best_bleu 57.52
2022-08-17 15:49:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 786 @ 63666 updates
2022-08-17 15:49:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint786.pt
2022-08-17 15:49:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint786.pt
2022-08-17 15:50:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint786.pt (epoch 786 @ 63666 updates, score 57.03) (writing took 35.799684543162584 seconds)
2022-08-17 15:50:08 | INFO | fairseq_cli.train | end of epoch 786 (average epoch stats below)
2022-08-17 15:50:08 | INFO | train | epoch 786 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 5106.3 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 63666 | lr 0.000250655 | gnorm 0.327 | train_wall 40 | gb_free 10 | wall 63504
2022-08-17 15:50:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:50:08 | INFO | fairseq.trainer | begin training epoch 787
2022-08-17 15:50:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:50:26 | INFO | train_inner | epoch 787:     34 / 81 loss=3.375, nll_loss=0.343, ppl=1.27, wps=5676.3, ups=1.03, wpb=5522.5, bsz=362.6, num_updates=63700, lr=0.000250588, gnorm=0.308, train_wall=49, gb_free=10.1, wall=63523
2022-08-17 15:50:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:50:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:50:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:50:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:50:58 | INFO | valid | epoch 787 | valid on 'valid' subset | loss 5.156 | nll_loss 2.549 | ppl 5.85 | bleu 56.89 | wps 1887.4 | wpb 933.5 | bsz 59.6 | num_updates 63747 | best_bleu 57.52
2022-08-17 15:50:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 787 @ 63747 updates
2022-08-17 15:50:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint787.pt
2022-08-17 15:50:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint787.pt
2022-08-17 15:51:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint787.pt (epoch 787 @ 63747 updates, score 56.89) (writing took 18.937546759843826 seconds)
2022-08-17 15:51:17 | INFO | fairseq_cli.train | end of epoch 787 (average epoch stats below)
2022-08-17 15:51:17 | INFO | train | epoch 787 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 6452.5 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 63747 | lr 0.000250496 | gnorm 0.323 | train_wall 39 | gb_free 10.1 | wall 63573
2022-08-17 15:51:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:51:17 | INFO | fairseq.trainer | begin training epoch 788
2022-08-17 15:51:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:51:46 | INFO | train_inner | epoch 788:     53 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=6982.1, ups=1.25, wpb=5581.8, bsz=362.1, num_updates=63800, lr=0.000250392, gnorm=0.364, train_wall=48, gb_free=10.1, wall=63603
2022-08-17 15:52:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:52:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:52:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:52:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:52:10 | INFO | valid | epoch 788 | valid on 'valid' subset | loss 5.173 | nll_loss 2.573 | ppl 5.95 | bleu 56.73 | wps 1939.6 | wpb 933.5 | bsz 59.6 | num_updates 63828 | best_bleu 57.52
2022-08-17 15:52:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 788 @ 63828 updates
2022-08-17 15:52:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint788.pt
2022-08-17 15:52:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint788.pt
2022-08-17 15:52:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint788.pt (epoch 788 @ 63828 updates, score 56.73) (writing took 22.47464830055833 seconds)
2022-08-17 15:52:32 | INFO | fairseq_cli.train | end of epoch 788 (average epoch stats below)
2022-08-17 15:52:32 | INFO | train | epoch 788 | loss 3.377 | nll_loss 0.344 | ppl 1.27 | wps 5936.2 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 63828 | lr 0.000250337 | gnorm 0.356 | train_wall 40 | gb_free 10 | wall 63649
2022-08-17 15:52:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:52:33 | INFO | fairseq.trainer | begin training epoch 789
2022-08-17 15:52:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:53:09 | INFO | train_inner | epoch 789:     72 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=6597.8, ups=1.2, wpb=5486.4, bsz=354.6, num_updates=63900, lr=0.000250196, gnorm=0.297, train_wall=49, gb_free=10.1, wall=63686
2022-08-17 15:53:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:53:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:53:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:53:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:53:23 | INFO | valid | epoch 789 | valid on 'valid' subset | loss 5.184 | nll_loss 2.587 | ppl 6.01 | bleu 56.19 | wps 1816.4 | wpb 933.5 | bsz 59.6 | num_updates 63909 | best_bleu 57.52
2022-08-17 15:53:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 789 @ 63909 updates
2022-08-17 15:53:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint789.pt
2022-08-17 15:53:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint789.pt
2022-08-17 15:53:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint789.pt (epoch 789 @ 63909 updates, score 56.19) (writing took 20.12810719758272 seconds)
2022-08-17 15:53:43 | INFO | fairseq_cli.train | end of epoch 789 (average epoch stats below)
2022-08-17 15:53:43 | INFO | train | epoch 789 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 6299.2 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 63909 | lr 0.000250178 | gnorm 0.298 | train_wall 39 | gb_free 10.1 | wall 63720
2022-08-17 15:53:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:53:44 | INFO | fairseq.trainer | begin training epoch 790
2022-08-17 15:53:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:54:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:54:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:54:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:54:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:54:34 | INFO | valid | epoch 790 | valid on 'valid' subset | loss 5.178 | nll_loss 2.576 | ppl 5.96 | bleu 56.41 | wps 1864.7 | wpb 933.5 | bsz 59.6 | num_updates 63990 | best_bleu 57.52
2022-08-17 15:54:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 790 @ 63990 updates
2022-08-17 15:54:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint790.pt
2022-08-17 15:54:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint790.pt
2022-08-17 15:54:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint790.pt (epoch 790 @ 63990 updates, score 56.41) (writing took 20.63487571850419 seconds)
2022-08-17 15:54:55 | INFO | fairseq_cli.train | end of epoch 790 (average epoch stats below)
2022-08-17 15:54:55 | INFO | train | epoch 790 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 6226.3 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 63990 | lr 0.00025002 | gnorm 0.339 | train_wall 39 | gb_free 10.1 | wall 63791
2022-08-17 15:54:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:54:55 | INFO | fairseq.trainer | begin training epoch 791
2022-08-17 15:54:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:55:01 | INFO | train_inner | epoch 791:     10 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=4922.3, ups=0.89, wpb=5511.3, bsz=357.3, num_updates=64000, lr=0.00025, gnorm=0.339, train_wall=48, gb_free=10.1, wall=63798
2022-08-17 15:55:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:55:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:55:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:55:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:55:48 | INFO | valid | epoch 791 | valid on 'valid' subset | loss 5.18 | nll_loss 2.577 | ppl 5.97 | bleu 55.96 | wps 1711.5 | wpb 933.5 | bsz 59.6 | num_updates 64071 | best_bleu 57.52
2022-08-17 15:55:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 791 @ 64071 updates
2022-08-17 15:55:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint791.pt
2022-08-17 15:55:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint791.pt
2022-08-17 15:56:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint791.pt (epoch 791 @ 64071 updates, score 55.96) (writing took 34.465669363737106 seconds)
2022-08-17 15:56:22 | INFO | fairseq_cli.train | end of epoch 791 (average epoch stats below)
2022-08-17 15:56:22 | INFO | train | epoch 791 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 5141.5 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 64071 | lr 0.000249861 | gnorm 0.313 | train_wall 40 | gb_free 10.2 | wall 63878
2022-08-17 15:56:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:56:22 | INFO | fairseq.trainer | begin training epoch 792
2022-08-17 15:56:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:56:38 | INFO | train_inner | epoch 792:     29 / 81 loss=3.376, nll_loss=0.342, ppl=1.27, wps=5748.9, ups=1.04, wpb=5543.7, bsz=356.1, num_updates=64100, lr=0.000249805, gnorm=0.314, train_wall=50, gb_free=10, wall=63894
2022-08-17 15:57:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:57:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:57:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:57:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:57:14 | INFO | valid | epoch 792 | valid on 'valid' subset | loss 5.18 | nll_loss 2.582 | ppl 5.99 | bleu 56.37 | wps 1814.4 | wpb 933.5 | bsz 59.6 | num_updates 64152 | best_bleu 57.52
2022-08-17 15:57:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 792 @ 64152 updates
2022-08-17 15:57:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint792.pt
2022-08-17 15:57:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint792.pt
2022-08-17 15:57:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint792.pt (epoch 792 @ 64152 updates, score 56.37) (writing took 2.3659084737300873 seconds)
2022-08-17 15:57:16 | INFO | fairseq_cli.train | end of epoch 792 (average epoch stats below)
2022-08-17 15:57:16 | INFO | train | epoch 792 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 8282.7 | ups 1.5 | wpb 5523.2 | bsz 358 | num_updates 64152 | lr 0.000249704 | gnorm 0.286 | train_wall 40 | gb_free 10.2 | wall 63932
2022-08-17 15:57:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:57:16 | INFO | fairseq.trainer | begin training epoch 793
2022-08-17 15:57:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:57:42 | INFO | train_inner | epoch 793:     48 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=8610.9, ups=1.56, wpb=5510.4, bsz=359.5, num_updates=64200, lr=0.00024961, gnorm=0.307, train_wall=49, gb_free=10.1, wall=63958
2022-08-17 15:57:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:58:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:58:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:58:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:58:08 | INFO | valid | epoch 793 | valid on 'valid' subset | loss 5.193 | nll_loss 2.597 | ppl 6.05 | bleu 55.85 | wps 1814.9 | wpb 933.5 | bsz 59.6 | num_updates 64233 | best_bleu 57.52
2022-08-17 15:58:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 793 @ 64233 updates
2022-08-17 15:58:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint793.pt
2022-08-17 15:58:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint793.pt
2022-08-17 15:58:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint793.pt (epoch 793 @ 64233 updates, score 55.85) (writing took 16.12376455590129 seconds)
2022-08-17 15:58:24 | INFO | fairseq_cli.train | end of epoch 793 (average epoch stats below)
2022-08-17 15:58:24 | INFO | train | epoch 793 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 6597.8 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 64233 | lr 0.000249546 | gnorm 0.342 | train_wall 39 | gb_free 10.2 | wall 64000
2022-08-17 15:58:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:58:24 | INFO | fairseq.trainer | begin training epoch 794
2022-08-17 15:58:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 15:58:59 | INFO | train_inner | epoch 794:     67 / 81 loss=3.376, nll_loss=0.344, ppl=1.27, wps=7195.3, ups=1.3, wpb=5527.1, bsz=358.5, num_updates=64300, lr=0.000249416, gnorm=0.345, train_wall=49, gb_free=10.1, wall=64035
2022-08-17 15:59:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 15:59:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 15:59:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 15:59:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 15:59:14 | INFO | valid | epoch 794 | valid on 'valid' subset | loss 5.17 | nll_loss 2.565 | ppl 5.92 | bleu 56.72 | wps 1941.5 | wpb 933.5 | bsz 59.6 | num_updates 64314 | best_bleu 57.52
2022-08-17 15:59:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 794 @ 64314 updates
2022-08-17 15:59:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint794.pt
2022-08-17 15:59:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint794.pt
2022-08-17 15:59:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint794.pt (epoch 794 @ 64314 updates, score 56.72) (writing took 27.78155644237995 seconds)
2022-08-17 15:59:42 | INFO | fairseq_cli.train | end of epoch 794 (average epoch stats below)
2022-08-17 15:59:42 | INFO | train | epoch 794 | loss 3.376 | nll_loss 0.344 | ppl 1.27 | wps 5723.4 | ups 1.04 | wpb 5523.2 | bsz 358 | num_updates 64314 | lr 0.000249389 | gnorm 0.335 | train_wall 39 | gb_free 10.2 | wall 64078
2022-08-17 15:59:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 15:59:42 | INFO | fairseq.trainer | begin training epoch 795
2022-08-17 15:59:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:00:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:00:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:00:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:00:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:00:36 | INFO | valid | epoch 795 | valid on 'valid' subset | loss 5.18 | nll_loss 2.577 | ppl 5.97 | bleu 56.83 | wps 1778.9 | wpb 933.5 | bsz 59.6 | num_updates 64395 | best_bleu 57.52
2022-08-17 16:00:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 795 @ 64395 updates
2022-08-17 16:00:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint795.pt
2022-08-17 16:00:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint795.pt
2022-08-17 16:00:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint795.pt (epoch 795 @ 64395 updates, score 56.83) (writing took 20.398385379463434 seconds)
2022-08-17 16:00:57 | INFO | fairseq_cli.train | end of epoch 795 (average epoch stats below)
2022-08-17 16:00:57 | INFO | train | epoch 795 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 5994.7 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 64395 | lr 0.000249232 | gnorm 0.28 | train_wall 40 | gb_free 10.3 | wall 64153
2022-08-17 16:00:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:00:57 | INFO | fairseq.trainer | begin training epoch 796
2022-08-17 16:00:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:01:01 | INFO | train_inner | epoch 796:      5 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=4515.1, ups=0.82, wpb=5511.4, bsz=355.1, num_updates=64400, lr=0.000249222, gnorm=0.284, train_wall=49, gb_free=10.1, wall=64157
2022-08-17 16:01:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:01:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:01:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:01:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:01:52 | INFO | valid | epoch 796 | valid on 'valid' subset | loss 5.183 | nll_loss 2.586 | ppl 6 | bleu 56.34 | wps 1901 | wpb 933.5 | bsz 59.6 | num_updates 64476 | best_bleu 57.52
2022-08-17 16:01:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 796 @ 64476 updates
2022-08-17 16:01:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint796.pt
2022-08-17 16:01:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint796.pt
2022-08-17 16:02:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint796.pt (epoch 796 @ 64476 updates, score 56.34) (writing took 36.363200791180134 seconds)
2022-08-17 16:02:29 | INFO | fairseq_cli.train | end of epoch 796 (average epoch stats below)
2022-08-17 16:02:29 | INFO | train | epoch 796 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 4868.6 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 64476 | lr 0.000249075 | gnorm 0.374 | train_wall 40 | gb_free 10.2 | wall 64245
2022-08-17 16:02:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:02:29 | INFO | fairseq.trainer | begin training epoch 797
2022-08-17 16:02:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:02:42 | INFO | train_inner | epoch 797:     24 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=5471.7, ups=0.99, wpb=5552, bsz=360, num_updates=64500, lr=0.000249029, gnorm=0.361, train_wall=50, gb_free=10, wall=64258
2022-08-17 16:03:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:03:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:03:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:03:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:03:20 | INFO | valid | epoch 797 | valid on 'valid' subset | loss 5.188 | nll_loss 2.591 | ppl 6.03 | bleu 56.05 | wps 1925.8 | wpb 933.5 | bsz 59.6 | num_updates 64557 | best_bleu 57.52
2022-08-17 16:03:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 797 @ 64557 updates
2022-08-17 16:03:20 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint797.pt
2022-08-17 16:03:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint797.pt
2022-08-17 16:03:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint797.pt (epoch 797 @ 64557 updates, score 56.05) (writing took 18.163660172373056 seconds)
2022-08-17 16:03:38 | INFO | fairseq_cli.train | end of epoch 797 (average epoch stats below)
2022-08-17 16:03:38 | INFO | train | epoch 797 | loss 3.375 | nll_loss 0.343 | ppl 1.27 | wps 6460.5 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 64557 | lr 0.000248919 | gnorm 0.345 | train_wall 40 | gb_free 10.3 | wall 64314
2022-08-17 16:03:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:03:38 | INFO | fairseq.trainer | begin training epoch 798
2022-08-17 16:03:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:04:02 | INFO | train_inner | epoch 798:     43 / 81 loss=3.375, nll_loss=0.343, ppl=1.27, wps=6886.8, ups=1.25, wpb=5501.4, bsz=357.6, num_updates=64600, lr=0.000248836, gnorm=0.365, train_wall=51, gb_free=10.1, wall=64338
2022-08-17 16:04:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:04:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:04:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:04:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:04:31 | INFO | valid | epoch 798 | valid on 'valid' subset | loss 5.165 | nll_loss 2.562 | ppl 5.9 | bleu 56.83 | wps 1879.1 | wpb 933.5 | bsz 59.6 | num_updates 64638 | best_bleu 57.52
2022-08-17 16:04:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 798 @ 64638 updates
2022-08-17 16:04:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint798.pt
2022-08-17 16:04:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint798.pt
2022-08-17 16:04:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint798.pt (epoch 798 @ 64638 updates, score 56.83) (writing took 2.3666650727391243 seconds)
2022-08-17 16:04:33 | INFO | fairseq_cli.train | end of epoch 798 (average epoch stats below)
2022-08-17 16:04:33 | INFO | train | epoch 798 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 8090.9 | ups 1.46 | wpb 5523.2 | bsz 358 | num_updates 64638 | lr 0.000248763 | gnorm 0.369 | train_wall 42 | gb_free 10.1 | wall 64370
2022-08-17 16:04:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:04:33 | INFO | fairseq.trainer | begin training epoch 799
2022-08-17 16:04:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:05:07 | INFO | train_inner | epoch 799:     62 / 81 loss=3.375, nll_loss=0.343, ppl=1.27, wps=8561.8, ups=1.55, wpb=5531.8, bsz=358, num_updates=64700, lr=0.000248644, gnorm=0.454, train_wall=51, gb_free=10.1, wall=64403
2022-08-17 16:05:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:05:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:05:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:05:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:05:25 | INFO | valid | epoch 799 | valid on 'valid' subset | loss 5.166 | nll_loss 2.561 | ppl 5.9 | bleu 57.18 | wps 1853.6 | wpb 933.5 | bsz 59.6 | num_updates 64719 | best_bleu 57.52
2022-08-17 16:05:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 799 @ 64719 updates
2022-08-17 16:05:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint799.pt
2022-08-17 16:05:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint799.pt
2022-08-17 16:05:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint799.pt (epoch 799 @ 64719 updates, score 57.18) (writing took 31.38783483952284 seconds)
2022-08-17 16:05:57 | INFO | fairseq_cli.train | end of epoch 799 (average epoch stats below)
2022-08-17 16:05:57 | INFO | train | epoch 799 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 5358.1 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 64719 | lr 0.000248607 | gnorm 0.477 | train_wall 41 | gb_free 10.1 | wall 64453
2022-08-17 16:05:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:05:57 | INFO | fairseq.trainer | begin training epoch 800
2022-08-17 16:05:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:06:39 | INFO | train_inner | epoch 800:     81 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=5966.8, ups=1.09, wpb=5497.9, bsz=358.6, num_updates=64800, lr=0.000248452, gnorm=0.359, train_wall=49, gb_free=10.1, wall=64495
2022-08-17 16:06:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:06:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:06:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:06:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:06:49 | INFO | valid | epoch 800 | valid on 'valid' subset | loss 5.18 | nll_loss 2.576 | ppl 5.96 | bleu 56.4 | wps 1777.2 | wpb 933.5 | bsz 59.6 | num_updates 64800 | best_bleu 57.52
2022-08-17 16:06:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 800 @ 64800 updates
2022-08-17 16:06:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint800.pt
2022-08-17 16:06:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint800.pt
2022-08-17 16:06:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint800.pt (epoch 800 @ 64800 updates, score 56.4) (writing took 2.325487434864044 seconds)
2022-08-17 16:06:51 | INFO | fairseq_cli.train | end of epoch 800 (average epoch stats below)
2022-08-17 16:06:51 | INFO | train | epoch 800 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 8254 | ups 1.49 | wpb 5523.2 | bsz 358 | num_updates 64800 | lr 0.000248452 | gnorm 0.357 | train_wall 40 | gb_free 10.1 | wall 64507
2022-08-17 16:06:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:06:51 | INFO | fairseq.trainer | begin training epoch 801
2022-08-17 16:06:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:07:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:07:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:07:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:07:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:07:43 | INFO | valid | epoch 801 | valid on 'valid' subset | loss 5.178 | nll_loss 2.576 | ppl 5.96 | bleu 56.46 | wps 1846.4 | wpb 933.5 | bsz 59.6 | num_updates 64881 | best_bleu 57.52
2022-08-17 16:07:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 801 @ 64881 updates
2022-08-17 16:07:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint801.pt
2022-08-17 16:07:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint801.pt
2022-08-17 16:07:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint801.pt (epoch 801 @ 64881 updates, score 56.46) (writing took 15.913232892751694 seconds)
2022-08-17 16:07:59 | INFO | fairseq_cli.train | end of epoch 801 (average epoch stats below)
2022-08-17 16:07:59 | INFO | train | epoch 801 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 6580.8 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 64881 | lr 0.000248297 | gnorm 0.306 | train_wall 41 | gb_free 10.1 | wall 64575
2022-08-17 16:07:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:07:59 | INFO | fairseq.trainer | begin training epoch 802
2022-08-17 16:07:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:08:10 | INFO | train_inner | epoch 802:     19 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=6037.3, ups=1.1, wpb=5511.3, bsz=360.1, num_updates=64900, lr=0.000248261, gnorm=0.298, train_wall=50, gb_free=10.1, wall=64586
2022-08-17 16:08:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:08:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:08:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:08:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:08:50 | INFO | valid | epoch 802 | valid on 'valid' subset | loss 5.188 | nll_loss 2.588 | ppl 6.01 | bleu 56.48 | wps 1795.5 | wpb 933.5 | bsz 59.6 | num_updates 64962 | best_bleu 57.52
2022-08-17 16:08:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 802 @ 64962 updates
2022-08-17 16:08:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint802.pt
2022-08-17 16:08:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint802.pt
2022-08-17 16:09:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint802.pt (epoch 802 @ 64962 updates, score 56.48) (writing took 23.009084049612284 seconds)
2022-08-17 16:09:13 | INFO | fairseq_cli.train | end of epoch 802 (average epoch stats below)
2022-08-17 16:09:13 | INFO | train | epoch 802 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 6005.4 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 64962 | lr 0.000248142 | gnorm 0.262 | train_wall 39 | gb_free 10.2 | wall 64650
2022-08-17 16:09:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:09:14 | INFO | fairseq.trainer | begin training epoch 803
2022-08-17 16:09:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:09:33 | INFO | train_inner | epoch 803:     38 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=6676.3, ups=1.2, wpb=5559.4, bsz=360.2, num_updates=65000, lr=0.000248069, gnorm=0.275, train_wall=48, gb_free=10, wall=64670
2022-08-17 16:09:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:09:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:09:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:09:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:10:04 | INFO | valid | epoch 803 | valid on 'valid' subset | loss 5.167 | nll_loss 2.566 | ppl 5.92 | bleu 56.14 | wps 1837.3 | wpb 933.5 | bsz 59.6 | num_updates 65043 | best_bleu 57.52
2022-08-17 16:10:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 803 @ 65043 updates
2022-08-17 16:10:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint803.pt
2022-08-17 16:10:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint803.pt
2022-08-17 16:10:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint803.pt (epoch 803 @ 65043 updates, score 56.14) (writing took 20.17166542261839 seconds)
2022-08-17 16:10:24 | INFO | fairseq_cli.train | end of epoch 803 (average epoch stats below)
2022-08-17 16:10:24 | INFO | train | epoch 803 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 6320.2 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 65043 | lr 0.000247987 | gnorm 0.289 | train_wall 40 | gb_free 10.3 | wall 64720
2022-08-17 16:10:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:10:24 | INFO | fairseq.trainer | begin training epoch 804
2022-08-17 16:10:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:10:54 | INFO | train_inner | epoch 804:     57 / 81 loss=3.375, nll_loss=0.343, ppl=1.27, wps=6831.2, ups=1.24, wpb=5515.6, bsz=353.3, num_updates=65100, lr=0.000247879, gnorm=0.278, train_wall=49, gb_free=10.1, wall=64750
2022-08-17 16:11:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:11:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:11:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:11:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:11:15 | INFO | valid | epoch 804 | valid on 'valid' subset | loss 5.194 | nll_loss 2.6 | ppl 6.06 | bleu 55.95 | wps 1869.9 | wpb 933.5 | bsz 59.6 | num_updates 65124 | best_bleu 57.52
2022-08-17 16:11:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 804 @ 65124 updates
2022-08-17 16:11:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint804.pt
2022-08-17 16:11:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint804.pt
2022-08-17 16:11:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint804.pt (epoch 804 @ 65124 updates, score 55.95) (writing took 2.453263632953167 seconds)
2022-08-17 16:11:18 | INFO | fairseq_cli.train | end of epoch 804 (average epoch stats below)
2022-08-17 16:11:18 | INFO | train | epoch 804 | loss 3.375 | nll_loss 0.343 | ppl 1.27 | wps 8361.7 | ups 1.51 | wpb 5523.2 | bsz 358 | num_updates 65124 | lr 0.000247833 | gnorm 0.391 | train_wall 40 | gb_free 10 | wall 64774
2022-08-17 16:11:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:11:18 | INFO | fairseq.trainer | begin training epoch 805
2022-08-17 16:11:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:11:56 | INFO | train_inner | epoch 805:     76 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=8942.1, ups=1.61, wpb=5538, bsz=361.3, num_updates=65200, lr=0.000247689, gnorm=0.408, train_wall=49, gb_free=10.1, wall=64812
2022-08-17 16:11:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:12:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:12:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:12:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:12:08 | INFO | valid | epoch 805 | valid on 'valid' subset | loss 5.182 | nll_loss 2.583 | ppl 5.99 | bleu 56.35 | wps 1748.1 | wpb 933.5 | bsz 59.6 | num_updates 65205 | best_bleu 57.52
2022-08-17 16:12:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 805 @ 65205 updates
2022-08-17 16:12:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint805.pt
2022-08-17 16:12:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint805.pt
2022-08-17 16:12:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint805.pt (epoch 805 @ 65205 updates, score 56.35) (writing took 29.246915236115456 seconds)
2022-08-17 16:12:38 | INFO | fairseq_cli.train | end of epoch 805 (average epoch stats below)
2022-08-17 16:12:38 | INFO | train | epoch 805 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 5609.4 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 65205 | lr 0.000247679 | gnorm 0.32 | train_wall 39 | gb_free 10.1 | wall 64854
2022-08-17 16:12:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:12:38 | INFO | fairseq.trainer | begin training epoch 806
2022-08-17 16:12:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:13:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:13:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:13:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:13:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:13:28 | INFO | valid | epoch 806 | valid on 'valid' subset | loss 5.182 | nll_loss 2.58 | ppl 5.98 | bleu 56.17 | wps 1938.1 | wpb 933.5 | bsz 59.6 | num_updates 65286 | best_bleu 57.52
2022-08-17 16:13:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 806 @ 65286 updates
2022-08-17 16:13:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint806.pt
2022-08-17 16:13:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint806.pt
2022-08-17 16:13:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint806.pt (epoch 806 @ 65286 updates, score 56.17) (writing took 18.741530019789934 seconds)
2022-08-17 16:13:47 | INFO | fairseq_cli.train | end of epoch 806 (average epoch stats below)
2022-08-17 16:13:47 | INFO | train | epoch 806 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 6416.1 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 65286 | lr 0.000247526 | gnorm 0.375 | train_wall 39 | gb_free 10.1 | wall 64923
2022-08-17 16:13:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:13:47 | INFO | fairseq.trainer | begin training epoch 807
2022-08-17 16:13:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:13:56 | INFO | train_inner | epoch 807:     14 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=4599.5, ups=0.84, wpb=5504.3, bsz=357.4, num_updates=65300, lr=0.000247499, gnorm=0.368, train_wall=48, gb_free=10, wall=64932
2022-08-17 16:14:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:14:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:14:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:14:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:14:38 | INFO | valid | epoch 807 | valid on 'valid' subset | loss 5.182 | nll_loss 2.584 | ppl 6 | bleu 56.25 | wps 1847.8 | wpb 933.5 | bsz 59.6 | num_updates 65367 | best_bleu 57.52
2022-08-17 16:14:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 807 @ 65367 updates
2022-08-17 16:14:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint807.pt
2022-08-17 16:14:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint807.pt
2022-08-17 16:15:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint807.pt (epoch 807 @ 65367 updates, score 56.25) (writing took 21.872459068894386 seconds)
2022-08-17 16:15:00 | INFO | fairseq_cli.train | end of epoch 807 (average epoch stats below)
2022-08-17 16:15:00 | INFO | train | epoch 807 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 6178.8 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 65367 | lr 0.000247372 | gnorm 0.297 | train_wall 39 | gb_free 10.1 | wall 64996
2022-08-17 16:15:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:15:00 | INFO | fairseq.trainer | begin training epoch 808
2022-08-17 16:15:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:15:18 | INFO | train_inner | epoch 808:     33 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=6715.5, ups=1.21, wpb=5545.1, bsz=359.8, num_updates=65400, lr=0.00024731, gnorm=0.285, train_wall=48, gb_free=10.1, wall=65015
2022-08-17 16:15:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:15:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:15:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:15:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:15:52 | INFO | valid | epoch 808 | valid on 'valid' subset | loss 5.171 | nll_loss 2.57 | ppl 5.94 | bleu 56.64 | wps 1739.5 | wpb 933.5 | bsz 59.6 | num_updates 65448 | best_bleu 57.52
2022-08-17 16:15:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 808 @ 65448 updates
2022-08-17 16:15:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint808.pt
2022-08-17 16:15:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint808.pt
2022-08-17 16:16:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint808.pt (epoch 808 @ 65448 updates, score 56.64) (writing took 19.4244713075459 seconds)
2022-08-17 16:16:11 | INFO | fairseq_cli.train | end of epoch 808 (average epoch stats below)
2022-08-17 16:16:12 | INFO | train | epoch 808 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 6229.9 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 65448 | lr 0.000247219 | gnorm 0.306 | train_wall 39 | gb_free 10.2 | wall 65068
2022-08-17 16:16:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:16:12 | INFO | fairseq.trainer | begin training epoch 809
2022-08-17 16:16:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:16:40 | INFO | train_inner | epoch 809:     52 / 81 loss=3.375, nll_loss=0.343, ppl=1.27, wps=6787.1, ups=1.23, wpb=5518.6, bsz=353.3, num_updates=65500, lr=0.000247121, gnorm=0.335, train_wall=49, gb_free=10, wall=65096
2022-08-17 16:16:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:16:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:16:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:16:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:17:03 | INFO | valid | epoch 809 | valid on 'valid' subset | loss 5.176 | nll_loss 2.572 | ppl 5.95 | bleu 56.85 | wps 1930.6 | wpb 933.5 | bsz 59.6 | num_updates 65529 | best_bleu 57.52
2022-08-17 16:17:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 809 @ 65529 updates
2022-08-17 16:17:03 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint809.pt
2022-08-17 16:17:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint809.pt
2022-08-17 16:17:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint809.pt (epoch 809 @ 65529 updates, score 56.85) (writing took 22.808667480945587 seconds)
2022-08-17 16:17:26 | INFO | fairseq_cli.train | end of epoch 809 (average epoch stats below)
2022-08-17 16:17:26 | INFO | train | epoch 809 | loss 3.375 | nll_loss 0.343 | ppl 1.27 | wps 5996.3 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 65529 | lr 0.000247066 | gnorm 0.356 | train_wall 40 | gb_free 10.1 | wall 65142
2022-08-17 16:17:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:17:26 | INFO | fairseq.trainer | begin training epoch 810
2022-08-17 16:17:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:18:03 | INFO | train_inner | epoch 810:     71 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=6618, ups=1.2, wpb=5515.3, bsz=359, num_updates=65600, lr=0.000246932, gnorm=0.379, train_wall=49, gb_free=10.1, wall=65179
2022-08-17 16:18:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:18:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:18:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:18:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:18:16 | INFO | valid | epoch 810 | valid on 'valid' subset | loss 5.173 | nll_loss 2.569 | ppl 5.93 | bleu 56.52 | wps 1933 | wpb 933.5 | bsz 59.6 | num_updates 65610 | best_bleu 57.52
2022-08-17 16:18:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 810 @ 65610 updates
2022-08-17 16:18:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint810.pt
2022-08-17 16:18:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint810.pt
2022-08-17 16:18:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint810.pt (epoch 810 @ 65610 updates, score 56.52) (writing took 13.543920490890741 seconds)
2022-08-17 16:18:30 | INFO | fairseq_cli.train | end of epoch 810 (average epoch stats below)
2022-08-17 16:18:30 | INFO | train | epoch 810 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 6993.4 | ups 1.27 | wpb 5523.2 | bsz 358 | num_updates 65610 | lr 0.000246914 | gnorm 0.385 | train_wall 40 | gb_free 10.2 | wall 65206
2022-08-17 16:18:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:18:30 | INFO | fairseq.trainer | begin training epoch 811
2022-08-17 16:18:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:19:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:19:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:19:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:19:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:19:21 | INFO | valid | epoch 811 | valid on 'valid' subset | loss 5.189 | nll_loss 2.595 | ppl 6.04 | bleu 56.48 | wps 1815.6 | wpb 933.5 | bsz 59.6 | num_updates 65691 | best_bleu 57.52
2022-08-17 16:19:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 811 @ 65691 updates
2022-08-17 16:19:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint811.pt
2022-08-17 16:19:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint811.pt
2022-08-17 16:19:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint811.pt (epoch 811 @ 65691 updates, score 56.48) (writing took 19.283588014543056 seconds)
2022-08-17 16:19:40 | INFO | fairseq_cli.train | end of epoch 811 (average epoch stats below)
2022-08-17 16:19:40 | INFO | train | epoch 811 | loss 3.375 | nll_loss 0.343 | ppl 1.27 | wps 6367.5 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 65691 | lr 0.000246761 | gnorm 0.344 | train_wall 40 | gb_free 10.1 | wall 65277
2022-08-17 16:19:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:19:41 | INFO | fairseq.trainer | begin training epoch 812
2022-08-17 16:19:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:19:46 | INFO | train_inner | epoch 812:      9 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=5328.8, ups=0.97, wpb=5488.4, bsz=356.3, num_updates=65700, lr=0.000246744, gnorm=0.345, train_wall=49, gb_free=10, wall=65282
2022-08-17 16:20:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:20:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:20:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:20:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:20:31 | INFO | valid | epoch 812 | valid on 'valid' subset | loss 5.192 | nll_loss 2.593 | ppl 6.03 | bleu 56.72 | wps 1763.7 | wpb 933.5 | bsz 59.6 | num_updates 65772 | best_bleu 57.52
2022-08-17 16:20:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 812 @ 65772 updates
2022-08-17 16:20:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint812.pt
2022-08-17 16:20:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint812.pt
2022-08-17 16:20:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint812.pt (epoch 812 @ 65772 updates, score 56.72) (writing took 23.802760619670153 seconds)
2022-08-17 16:20:55 | INFO | fairseq_cli.train | end of epoch 812 (average epoch stats below)
2022-08-17 16:20:55 | INFO | train | epoch 812 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 6001.7 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 65772 | lr 0.000246609 | gnorm 0.307 | train_wall 39 | gb_free 10.1 | wall 65351
2022-08-17 16:20:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:20:55 | INFO | fairseq.trainer | begin training epoch 813
2022-08-17 16:20:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:21:11 | INFO | train_inner | epoch 813:     28 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=6520.1, ups=1.17, wpb=5550.8, bsz=360.1, num_updates=65800, lr=0.000246557, gnorm=0.337, train_wall=49, gb_free=10, wall=65367
2022-08-17 16:21:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:21:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:21:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:21:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:21:47 | INFO | valid | epoch 813 | valid on 'valid' subset | loss 5.188 | nll_loss 2.59 | ppl 6.02 | bleu 56.22 | wps 1867.3 | wpb 933.5 | bsz 59.6 | num_updates 65853 | best_bleu 57.52
2022-08-17 16:21:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 813 @ 65853 updates
2022-08-17 16:21:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint813.pt
2022-08-17 16:21:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint813.pt
2022-08-17 16:22:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint813.pt (epoch 813 @ 65853 updates, score 56.22) (writing took 15.502821534872055 seconds)
2022-08-17 16:22:02 | INFO | fairseq_cli.train | end of epoch 813 (average epoch stats below)
2022-08-17 16:22:02 | INFO | train | epoch 813 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 6624.6 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 65853 | lr 0.000246458 | gnorm 0.351 | train_wall 40 | gb_free 10.1 | wall 65419
2022-08-17 16:22:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:22:03 | INFO | fairseq.trainer | begin training epoch 814
2022-08-17 16:22:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:22:31 | INFO | train_inner | epoch 814:     47 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=6949, ups=1.25, wpb=5541.3, bsz=360.6, num_updates=65900, lr=0.00024637, gnorm=0.365, train_wall=49, gb_free=10.1, wall=65447
2022-08-17 16:22:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:22:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:22:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:22:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:22:57 | INFO | valid | epoch 814 | valid on 'valid' subset | loss 5.192 | nll_loss 2.595 | ppl 6.04 | bleu 56.59 | wps 1917.9 | wpb 933.5 | bsz 59.6 | num_updates 65934 | best_bleu 57.52
2022-08-17 16:22:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 814 @ 65934 updates
2022-08-17 16:22:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint814.pt
2022-08-17 16:22:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint814.pt
2022-08-17 16:23:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint814.pt (epoch 814 @ 65934 updates, score 56.59) (writing took 19.119387611746788 seconds)
2022-08-17 16:23:17 | INFO | fairseq_cli.train | end of epoch 814 (average epoch stats below)
2022-08-17 16:23:17 | INFO | train | epoch 814 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 6034.4 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 65934 | lr 0.000246306 | gnorm 0.377 | train_wall 40 | gb_free 10.1 | wall 65493
2022-08-17 16:23:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:23:17 | INFO | fairseq.trainer | begin training epoch 815
2022-08-17 16:23:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:23:51 | INFO | train_inner | epoch 815:     66 / 81 loss=3.375, nll_loss=0.343, ppl=1.27, wps=6918.1, ups=1.25, wpb=5527.5, bsz=359.8, num_updates=66000, lr=0.000246183, gnorm=0.369, train_wall=50, gb_free=10, wall=65527
2022-08-17 16:23:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:24:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:24:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:24:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:24:08 | INFO | valid | epoch 815 | valid on 'valid' subset | loss 5.176 | nll_loss 2.576 | ppl 5.96 | bleu 56.63 | wps 1833 | wpb 933.5 | bsz 59.6 | num_updates 66015 | best_bleu 57.52
2022-08-17 16:24:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 815 @ 66015 updates
2022-08-17 16:24:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint815.pt
2022-08-17 16:24:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint815.pt
2022-08-17 16:24:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint815.pt (epoch 815 @ 66015 updates, score 56.63) (writing took 20.394449662417173 seconds)
2022-08-17 16:24:28 | INFO | fairseq_cli.train | end of epoch 815 (average epoch stats below)
2022-08-17 16:24:28 | INFO | train | epoch 815 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 6244 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 66015 | lr 0.000246155 | gnorm 0.408 | train_wall 40 | gb_free 10.1 | wall 65564
2022-08-17 16:24:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:24:28 | INFO | fairseq.trainer | begin training epoch 816
2022-08-17 16:24:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:25:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:25:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:25:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:25:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:25:21 | INFO | valid | epoch 816 | valid on 'valid' subset | loss 5.172 | nll_loss 2.575 | ppl 5.96 | bleu 57.17 | wps 1939.7 | wpb 933.5 | bsz 59.6 | num_updates 66096 | best_bleu 57.52
2022-08-17 16:25:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 816 @ 66096 updates
2022-08-17 16:25:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint816.pt
2022-08-17 16:25:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint816.pt
2022-08-17 16:25:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint816.pt (epoch 816 @ 66096 updates, score 57.17) (writing took 18.451411299407482 seconds)
2022-08-17 16:25:40 | INFO | fairseq_cli.train | end of epoch 816 (average epoch stats below)
2022-08-17 16:25:40 | INFO | train | epoch 816 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 6231.7 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 66096 | lr 0.000246004 | gnorm 0.287 | train_wall 41 | gb_free 10.2 | wall 65636
2022-08-17 16:25:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:25:40 | INFO | fairseq.trainer | begin training epoch 817
2022-08-17 16:25:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:25:43 | INFO | train_inner | epoch 817:      4 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=4879.9, ups=0.89, wpb=5482.4, bsz=351.6, num_updates=66100, lr=0.000245997, gnorm=0.311, train_wall=51, gb_free=10, wall=65639
2022-08-17 16:26:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:26:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:26:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:26:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:26:31 | INFO | valid | epoch 817 | valid on 'valid' subset | loss 5.181 | nll_loss 2.58 | ppl 5.98 | bleu 56.98 | wps 1736.1 | wpb 933.5 | bsz 59.6 | num_updates 66177 | best_bleu 57.52
2022-08-17 16:26:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 817 @ 66177 updates
2022-08-17 16:26:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint817.pt
2022-08-17 16:26:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint817.pt
2022-08-17 16:26:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint817.pt (epoch 817 @ 66177 updates, score 56.98) (writing took 20.405836261808872 seconds)
2022-08-17 16:26:51 | INFO | fairseq_cli.train | end of epoch 817 (average epoch stats below)
2022-08-17 16:26:51 | INFO | train | epoch 817 | loss 3.375 | nll_loss 0.343 | ppl 1.27 | wps 6276.5 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 66177 | lr 0.000245854 | gnorm 0.325 | train_wall 39 | gb_free 10.1 | wall 65708
2022-08-17 16:26:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:26:51 | INFO | fairseq.trainer | begin training epoch 818
2022-08-17 16:26:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:27:04 | INFO | train_inner | epoch 818:     23 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=6829.8, ups=1.24, wpb=5521.8, bsz=357.5, num_updates=66200, lr=0.000245811, gnorm=0.313, train_wall=48, gb_free=10.1, wall=65720
2022-08-17 16:27:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:27:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:27:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:27:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:27:43 | INFO | valid | epoch 818 | valid on 'valid' subset | loss 5.183 | nll_loss 2.585 | ppl 6 | bleu 56.91 | wps 1915.1 | wpb 933.5 | bsz 59.6 | num_updates 66258 | best_bleu 57.52
2022-08-17 16:27:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 818 @ 66258 updates
2022-08-17 16:27:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint818.pt
2022-08-17 16:27:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint818.pt
2022-08-17 16:27:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint818.pt (epoch 818 @ 66258 updates, score 56.91) (writing took 13.819328546524048 seconds)
2022-08-17 16:27:57 | INFO | fairseq_cli.train | end of epoch 818 (average epoch stats below)
2022-08-17 16:27:57 | INFO | train | epoch 818 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 6811.7 | ups 1.23 | wpb 5523.2 | bsz 358 | num_updates 66258 | lr 0.000245703 | gnorm 0.276 | train_wall 41 | gb_free 10.1 | wall 65773
2022-08-17 16:27:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:27:57 | INFO | fairseq.trainer | begin training epoch 819
2022-08-17 16:27:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:28:19 | INFO | train_inner | epoch 819:     42 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=7362.8, ups=1.33, wpb=5540.8, bsz=358.9, num_updates=66300, lr=0.000245625, gnorm=0.304, train_wall=50, gb_free=10.1, wall=65795
2022-08-17 16:28:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:28:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:28:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:28:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:28:47 | INFO | valid | epoch 819 | valid on 'valid' subset | loss 5.175 | nll_loss 2.568 | ppl 5.93 | bleu 57.2 | wps 1947.2 | wpb 933.5 | bsz 59.6 | num_updates 66339 | best_bleu 57.52
2022-08-17 16:28:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 819 @ 66339 updates
2022-08-17 16:28:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint819.pt
2022-08-17 16:28:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint819.pt
2022-08-17 16:29:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint819.pt (epoch 819 @ 66339 updates, score 57.2) (writing took 16.042971152812243 seconds)
2022-08-17 16:29:04 | INFO | fairseq_cli.train | end of epoch 819 (average epoch stats below)
2022-08-17 16:29:04 | INFO | train | epoch 819 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 6712.8 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 66339 | lr 0.000245553 | gnorm 0.302 | train_wall 40 | gb_free 10.2 | wall 65840
2022-08-17 16:29:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:29:04 | INFO | fairseq.trainer | begin training epoch 820
2022-08-17 16:29:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:29:37 | INFO | train_inner | epoch 820:     61 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=7133.6, ups=1.29, wpb=5530.4, bsz=362.5, num_updates=66400, lr=0.00024544, gnorm=0.294, train_wall=49, gb_free=10, wall=65873
2022-08-17 16:29:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:29:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:29:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:29:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:29:57 | INFO | valid | epoch 820 | valid on 'valid' subset | loss 5.175 | nll_loss 2.574 | ppl 5.96 | bleu 56.76 | wps 1672 | wpb 933.5 | bsz 59.6 | num_updates 66420 | best_bleu 57.52
2022-08-17 16:29:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 820 @ 66420 updates
2022-08-17 16:29:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint820.pt
2022-08-17 16:29:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint820.pt
2022-08-17 16:30:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint820.pt (epoch 820 @ 66420 updates, score 56.76) (writing took 35.54569282382727 seconds)
2022-08-17 16:30:33 | INFO | fairseq_cli.train | end of epoch 820 (average epoch stats below)
2022-08-17 16:30:33 | INFO | train | epoch 820 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 5000.8 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 66420 | lr 0.000245403 | gnorm 0.32 | train_wall 39 | gb_free 10.1 | wall 65929
2022-08-17 16:30:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:30:33 | INFO | fairseq.trainer | begin training epoch 821
2022-08-17 16:30:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:31:16 | INFO | train_inner | epoch 821:     80 / 81 loss=3.376, nll_loss=0.344, ppl=1.27, wps=5554.8, ups=1, wpb=5532.8, bsz=356.9, num_updates=66500, lr=0.000245256, gnorm=0.348, train_wall=50, gb_free=10.1, wall=65973
2022-08-17 16:31:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:31:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:31:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:31:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:31:26 | INFO | valid | epoch 821 | valid on 'valid' subset | loss 5.159 | nll_loss 2.552 | ppl 5.86 | bleu 56.7 | wps 1780.5 | wpb 933.5 | bsz 59.6 | num_updates 66501 | best_bleu 57.52
2022-08-17 16:31:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 821 @ 66501 updates
2022-08-17 16:31:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint821.pt
2022-08-17 16:31:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint821.pt
2022-08-17 16:31:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint821.pt (epoch 821 @ 66501 updates, score 56.7) (writing took 17.22019747272134 seconds)
2022-08-17 16:31:44 | INFO | fairseq_cli.train | end of epoch 821 (average epoch stats below)
2022-08-17 16:31:44 | INFO | train | epoch 821 | loss 3.376 | nll_loss 0.344 | ppl 1.27 | wps 6338 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 66501 | lr 0.000245254 | gnorm 0.35 | train_wall 40 | gb_free 10.3 | wall 66000
2022-08-17 16:31:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:31:44 | INFO | fairseq.trainer | begin training epoch 822
2022-08-17 16:31:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:32:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:32:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:32:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:32:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:32:35 | INFO | valid | epoch 822 | valid on 'valid' subset | loss 5.18 | nll_loss 2.579 | ppl 5.98 | bleu 56.27 | wps 1820.8 | wpb 933.5 | bsz 59.6 | num_updates 66582 | best_bleu 57.52
2022-08-17 16:32:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 822 @ 66582 updates
2022-08-17 16:32:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint822.pt
2022-08-17 16:32:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint822.pt
2022-08-17 16:32:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint822.pt (epoch 822 @ 66582 updates, score 56.27) (writing took 18.796996857970953 seconds)
2022-08-17 16:32:54 | INFO | fairseq_cli.train | end of epoch 822 (average epoch stats below)
2022-08-17 16:32:54 | INFO | train | epoch 822 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 6381.8 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 66582 | lr 0.000245105 | gnorm 0.442 | train_wall 40 | gb_free 10.1 | wall 66070
2022-08-17 16:32:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:32:54 | INFO | fairseq.trainer | begin training epoch 823
2022-08-17 16:32:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:33:04 | INFO | train_inner | epoch 823:     18 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=5128.4, ups=0.93, wpb=5535, bsz=360.6, num_updates=66600, lr=0.000245072, gnorm=0.415, train_wall=49, gb_free=10.1, wall=66081
2022-08-17 16:33:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:33:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:33:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:33:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:33:46 | INFO | valid | epoch 823 | valid on 'valid' subset | loss 5.185 | nll_loss 2.591 | ppl 6.02 | bleu 56.98 | wps 1848.9 | wpb 933.5 | bsz 59.6 | num_updates 66663 | best_bleu 57.52
2022-08-17 16:33:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 823 @ 66663 updates
2022-08-17 16:33:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint823.pt
2022-08-17 16:33:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint823.pt
2022-08-17 16:34:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint823.pt (epoch 823 @ 66663 updates, score 56.98) (writing took 19.662646804004908 seconds)
2022-08-17 16:34:05 | INFO | fairseq_cli.train | end of epoch 823 (average epoch stats below)
2022-08-17 16:34:05 | INFO | train | epoch 823 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 6246 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 66663 | lr 0.000244956 | gnorm 0.298 | train_wall 40 | gb_free 10.3 | wall 66142
2022-08-17 16:34:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:34:06 | INFO | fairseq.trainer | begin training epoch 824
2022-08-17 16:34:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:34:26 | INFO | train_inner | epoch 824:     37 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=6782.1, ups=1.23, wpb=5510.1, bsz=354.2, num_updates=66700, lr=0.000244888, gnorm=0.3, train_wall=50, gb_free=10.1, wall=66162
2022-08-17 16:34:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:34:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:34:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:34:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:34:57 | INFO | valid | epoch 824 | valid on 'valid' subset | loss 5.181 | nll_loss 2.582 | ppl 5.99 | bleu 56.83 | wps 1797.5 | wpb 933.5 | bsz 59.6 | num_updates 66744 | best_bleu 57.52
2022-08-17 16:34:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 824 @ 66744 updates
2022-08-17 16:34:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint824.pt
2022-08-17 16:34:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint824.pt
2022-08-17 16:35:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint824.pt (epoch 824 @ 66744 updates, score 56.83) (writing took 20.33258156850934 seconds)
2022-08-17 16:35:17 | INFO | fairseq_cli.train | end of epoch 824 (average epoch stats below)
2022-08-17 16:35:17 | INFO | train | epoch 824 | loss 3.375 | nll_loss 0.343 | ppl 1.27 | wps 6247.7 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 66744 | lr 0.000244807 | gnorm 0.361 | train_wall 40 | gb_free 10.2 | wall 66213
2022-08-17 16:35:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:35:17 | INFO | fairseq.trainer | begin training epoch 825
2022-08-17 16:35:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:35:46 | INFO | train_inner | epoch 825:     56 / 81 loss=3.375, nll_loss=0.343, ppl=1.27, wps=6797.8, ups=1.24, wpb=5498.8, bsz=357.3, num_updates=66800, lr=0.000244704, gnorm=0.372, train_wall=49, gb_free=10.1, wall=66243
2022-08-17 16:35:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:36:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:36:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:36:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:36:08 | INFO | valid | epoch 825 | valid on 'valid' subset | loss 5.171 | nll_loss 2.569 | ppl 5.93 | bleu 57.01 | wps 1750.2 | wpb 933.5 | bsz 59.6 | num_updates 66825 | best_bleu 57.52
2022-08-17 16:36:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 825 @ 66825 updates
2022-08-17 16:36:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint825.pt
2022-08-17 16:36:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint825.pt
2022-08-17 16:36:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint825.pt (epoch 825 @ 66825 updates, score 57.01) (writing took 23.32661421224475 seconds)
2022-08-17 16:36:32 | INFO | fairseq_cli.train | end of epoch 825 (average epoch stats below)
2022-08-17 16:36:32 | INFO | train | epoch 825 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 5988.6 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 66825 | lr 0.000244659 | gnorm 0.321 | train_wall 40 | gb_free 10 | wall 66288
2022-08-17 16:36:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:36:32 | INFO | fairseq.trainer | begin training epoch 826
2022-08-17 16:36:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:37:12 | INFO | train_inner | epoch 826:     75 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=6528.8, ups=1.18, wpb=5553.9, bsz=359.5, num_updates=66900, lr=0.000244521, gnorm=0.326, train_wall=49, gb_free=10, wall=66328
2022-08-17 16:37:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:37:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:37:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:37:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:37:24 | INFO | valid | epoch 826 | valid on 'valid' subset | loss 5.164 | nll_loss 2.561 | ppl 5.9 | bleu 56.72 | wps 1819.9 | wpb 933.5 | bsz 59.6 | num_updates 66906 | best_bleu 57.52
2022-08-17 16:37:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 826 @ 66906 updates
2022-08-17 16:37:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint826.pt
2022-08-17 16:37:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint826.pt
2022-08-17 16:37:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint826.pt (epoch 826 @ 66906 updates, score 56.72) (writing took 17.459454096853733 seconds)
2022-08-17 16:37:41 | INFO | fairseq_cli.train | end of epoch 826 (average epoch stats below)
2022-08-17 16:37:41 | INFO | train | epoch 826 | loss 3.375 | nll_loss 0.343 | ppl 1.27 | wps 6435.6 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 66906 | lr 0.00024451 | gnorm 0.402 | train_wall 40 | gb_free 10.1 | wall 66357
2022-08-17 16:37:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:37:41 | INFO | fairseq.trainer | begin training epoch 827
2022-08-17 16:37:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:38:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:38:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:38:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:38:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:38:32 | INFO | valid | epoch 827 | valid on 'valid' subset | loss 5.181 | nll_loss 2.582 | ppl 5.99 | bleu 56.57 | wps 1832.2 | wpb 933.5 | bsz 59.6 | num_updates 66987 | best_bleu 57.52
2022-08-17 16:38:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 827 @ 66987 updates
2022-08-17 16:38:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint827.pt
2022-08-17 16:38:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint827.pt
2022-08-17 16:38:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint827.pt (epoch 827 @ 66987 updates, score 56.57) (writing took 20.200410790741444 seconds)
2022-08-17 16:38:53 | INFO | fairseq_cli.train | end of epoch 827 (average epoch stats below)
2022-08-17 16:38:53 | INFO | train | epoch 827 | loss 3.375 | nll_loss 0.343 | ppl 1.27 | wps 6281.5 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 66987 | lr 0.000244363 | gnorm 0.352 | train_wall 39 | gb_free 10.1 | wall 66429
2022-08-17 16:38:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:38:53 | INFO | fairseq.trainer | begin training epoch 828
2022-08-17 16:38:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:39:01 | INFO | train_inner | epoch 828:     13 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=5031.9, ups=0.92, wpb=5487.2, bsz=357.5, num_updates=67000, lr=0.000244339, gnorm=0.412, train_wall=48, gb_free=10.1, wall=66437
2022-08-17 16:39:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:39:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:39:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:39:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:39:44 | INFO | valid | epoch 828 | valid on 'valid' subset | loss 5.175 | nll_loss 2.573 | ppl 5.95 | bleu 56.71 | wps 1847 | wpb 933.5 | bsz 59.6 | num_updates 67068 | best_bleu 57.52
2022-08-17 16:39:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 828 @ 67068 updates
2022-08-17 16:39:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint828.pt
2022-08-17 16:39:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint828.pt
2022-08-17 16:40:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint828.pt (epoch 828 @ 67068 updates, score 56.71) (writing took 34.28426851332188 seconds)
2022-08-17 16:40:18 | INFO | fairseq_cli.train | end of epoch 828 (average epoch stats below)
2022-08-17 16:40:18 | INFO | train | epoch 828 | loss 3.376 | nll_loss 0.344 | ppl 1.27 | wps 5214.9 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 67068 | lr 0.000244215 | gnorm 0.376 | train_wall 40 | gb_free 10.2 | wall 66515
2022-08-17 16:40:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:40:19 | INFO | fairseq.trainer | begin training epoch 829
2022-08-17 16:40:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:40:36 | INFO | train_inner | epoch 829:     32 / 81 loss=3.376, nll_loss=0.344, ppl=1.27, wps=5785.8, ups=1.05, wpb=5528.7, bsz=360, num_updates=67100, lr=0.000244157, gnorm=0.338, train_wall=49, gb_free=10.1, wall=66532
2022-08-17 16:41:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:41:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:41:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:41:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:41:10 | INFO | valid | epoch 829 | valid on 'valid' subset | loss 5.192 | nll_loss 2.593 | ppl 6.03 | bleu 57.04 | wps 1851.4 | wpb 933.5 | bsz 59.6 | num_updates 67149 | best_bleu 57.52
2022-08-17 16:41:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 829 @ 67149 updates
2022-08-17 16:41:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint829.pt
2022-08-17 16:41:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint829.pt
2022-08-17 16:41:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint829.pt (epoch 829 @ 67149 updates, score 57.04) (writing took 2.3819689117372036 seconds)
2022-08-17 16:41:13 | INFO | fairseq_cli.train | end of epoch 829 (average epoch stats below)
2022-08-17 16:41:13 | INFO | train | epoch 829 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 8223.2 | ups 1.49 | wpb 5523.2 | bsz 358 | num_updates 67149 | lr 0.000244068 | gnorm 0.317 | train_wall 40 | gb_free 10.1 | wall 66569
2022-08-17 16:41:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:41:13 | INFO | fairseq.trainer | begin training epoch 830
2022-08-17 16:41:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:41:43 | INFO | train_inner | epoch 830:     51 / 81 loss=3.375, nll_loss=0.343, ppl=1.27, wps=8332.8, ups=1.5, wpb=5552.9, bsz=356.6, num_updates=67200, lr=0.000243975, gnorm=0.334, train_wall=50, gb_free=10.1, wall=66599
2022-08-17 16:41:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:42:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:42:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:42:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:42:08 | INFO | valid | epoch 830 | valid on 'valid' subset | loss 5.179 | nll_loss 2.576 | ppl 5.96 | bleu 57.14 | wps 1823.6 | wpb 933.5 | bsz 59.6 | num_updates 67230 | best_bleu 57.52
2022-08-17 16:42:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 830 @ 67230 updates
2022-08-17 16:42:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint830.pt
2022-08-17 16:42:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint830.pt
2022-08-17 16:42:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint830.pt (epoch 830 @ 67230 updates, score 57.14) (writing took 18.512211967259645 seconds)
2022-08-17 16:42:27 | INFO | fairseq_cli.train | end of epoch 830 (average epoch stats below)
2022-08-17 16:42:27 | INFO | train | epoch 830 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 6052.1 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 67230 | lr 0.000243921 | gnorm 0.304 | train_wall 41 | gb_free 10.2 | wall 66643
2022-08-17 16:42:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:42:27 | INFO | fairseq.trainer | begin training epoch 831
2022-08-17 16:42:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:43:03 | INFO | train_inner | epoch 831:     70 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=6889, ups=1.25, wpb=5522.1, bsz=360.2, num_updates=67300, lr=0.000243794, gnorm=0.296, train_wall=49, gb_free=10.1, wall=66679
2022-08-17 16:43:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:43:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:43:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:43:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:43:17 | INFO | valid | epoch 831 | valid on 'valid' subset | loss 5.182 | nll_loss 2.581 | ppl 5.98 | bleu 57.13 | wps 1904.5 | wpb 933.5 | bsz 59.6 | num_updates 67311 | best_bleu 57.52
2022-08-17 16:43:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 831 @ 67311 updates
2022-08-17 16:43:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint831.pt
2022-08-17 16:43:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint831.pt
2022-08-17 16:43:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint831.pt (epoch 831 @ 67311 updates, score 57.13) (writing took 27.51701847091317 seconds)
2022-08-17 16:43:45 | INFO | fairseq_cli.train | end of epoch 831 (average epoch stats below)
2022-08-17 16:43:45 | INFO | train | epoch 831 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 5713.4 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 67311 | lr 0.000243774 | gnorm 0.321 | train_wall 39 | gb_free 10.1 | wall 66721
2022-08-17 16:43:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:43:45 | INFO | fairseq.trainer | begin training epoch 832
2022-08-17 16:43:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:44:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:44:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:44:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:44:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:44:37 | INFO | valid | epoch 832 | valid on 'valid' subset | loss 5.181 | nll_loss 2.583 | ppl 5.99 | bleu 56.39 | wps 1850.5 | wpb 933.5 | bsz 59.6 | num_updates 67392 | best_bleu 57.52
2022-08-17 16:44:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 832 @ 67392 updates
2022-08-17 16:44:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint832.pt
2022-08-17 16:44:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint832.pt
2022-08-17 16:44:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint832.pt (epoch 832 @ 67392 updates, score 56.39) (writing took 19.138933200389147 seconds)
2022-08-17 16:44:57 | INFO | fairseq_cli.train | end of epoch 832 (average epoch stats below)
2022-08-17 16:44:57 | INFO | train | epoch 832 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 6245.5 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 67392 | lr 0.000243627 | gnorm 0.368 | train_wall 41 | gb_free 10 | wall 66793
2022-08-17 16:44:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:44:57 | INFO | fairseq.trainer | begin training epoch 833
2022-08-17 16:44:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:45:02 | INFO | train_inner | epoch 833:      8 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=4629.5, ups=0.84, wpb=5504.4, bsz=354.1, num_updates=67400, lr=0.000243613, gnorm=0.365, train_wall=50, gb_free=10.1, wall=66798
2022-08-17 16:45:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:45:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:45:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:45:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:45:47 | INFO | valid | epoch 833 | valid on 'valid' subset | loss 5.182 | nll_loss 2.583 | ppl 5.99 | bleu 56.66 | wps 1899.1 | wpb 933.5 | bsz 59.6 | num_updates 67473 | best_bleu 57.52
2022-08-17 16:45:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 833 @ 67473 updates
2022-08-17 16:45:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint833.pt
2022-08-17 16:45:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint833.pt
2022-08-17 16:46:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint833.pt (epoch 833 @ 67473 updates, score 56.66) (writing took 28.086729642003775 seconds)
2022-08-17 16:46:16 | INFO | fairseq_cli.train | end of epoch 833 (average epoch stats below)
2022-08-17 16:46:16 | INFO | train | epoch 833 | loss 3.376 | nll_loss 0.344 | ppl 1.27 | wps 5655.2 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 67473 | lr 0.000243481 | gnorm 0.346 | train_wall 40 | gb_free 10.1 | wall 66872
2022-08-17 16:46:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:46:16 | INFO | fairseq.trainer | begin training epoch 834
2022-08-17 16:46:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:46:31 | INFO | train_inner | epoch 834:     27 / 81 loss=3.376, nll_loss=0.343, ppl=1.27, wps=6215.1, ups=1.12, wpb=5527.2, bsz=361, num_updates=67500, lr=0.000243432, gnorm=0.345, train_wall=50, gb_free=10.1, wall=66887
2022-08-17 16:46:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:46:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:46:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:46:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:47:08 | INFO | valid | epoch 834 | valid on 'valid' subset | loss 5.182 | nll_loss 2.58 | ppl 5.98 | bleu 56.24 | wps 1834.7 | wpb 933.5 | bsz 59.6 | num_updates 67554 | best_bleu 57.52
2022-08-17 16:47:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 834 @ 67554 updates
2022-08-17 16:47:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint834.pt
2022-08-17 16:47:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint834.pt
2022-08-17 16:47:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint834.pt (epoch 834 @ 67554 updates, score 56.24) (writing took 15.786842845380306 seconds)
2022-08-17 16:47:24 | INFO | fairseq_cli.train | end of epoch 834 (average epoch stats below)
2022-08-17 16:47:24 | INFO | train | epoch 834 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 6596.7 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 67554 | lr 0.000243335 | gnorm 0.292 | train_wall 40 | gb_free 10.2 | wall 66940
2022-08-17 16:47:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:47:24 | INFO | fairseq.trainer | begin training epoch 835
2022-08-17 16:47:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:47:48 | INFO | train_inner | epoch 835:     46 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=7123.6, ups=1.29, wpb=5502.8, bsz=356.3, num_updates=67600, lr=0.000243252, gnorm=0.291, train_wall=49, gb_free=10.1, wall=66964
2022-08-17 16:48:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:48:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:48:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:48:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:48:14 | INFO | valid | epoch 835 | valid on 'valid' subset | loss 5.177 | nll_loss 2.574 | ppl 5.95 | bleu 56.7 | wps 1844.3 | wpb 933.5 | bsz 59.6 | num_updates 67635 | best_bleu 57.52
2022-08-17 16:48:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 835 @ 67635 updates
2022-08-17 16:48:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint835.pt
2022-08-17 16:48:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint835.pt
2022-08-17 16:48:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint835.pt (epoch 835 @ 67635 updates, score 56.7) (writing took 43.71628113090992 seconds)
2022-08-17 16:48:58 | INFO | fairseq_cli.train | end of epoch 835 (average epoch stats below)
2022-08-17 16:48:58 | INFO | train | epoch 835 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 4738.2 | ups 0.86 | wpb 5523.2 | bsz 358 | num_updates 67635 | lr 0.000243189 | gnorm 0.304 | train_wall 39 | gb_free 10.1 | wall 67034
2022-08-17 16:48:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:48:58 | INFO | fairseq.trainer | begin training epoch 836
2022-08-17 16:48:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:49:32 | INFO | train_inner | epoch 836:     65 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=5331.8, ups=0.96, wpb=5550.3, bsz=358.1, num_updates=67700, lr=0.000243072, gnorm=0.29, train_wall=48, gb_free=10, wall=67068
2022-08-17 16:49:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:49:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:49:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:49:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:49:50 | INFO | valid | epoch 836 | valid on 'valid' subset | loss 5.171 | nll_loss 2.568 | ppl 5.93 | bleu 56.92 | wps 1826.2 | wpb 933.5 | bsz 59.6 | num_updates 67716 | best_bleu 57.52
2022-08-17 16:49:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 836 @ 67716 updates
2022-08-17 16:49:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint836.pt
2022-08-17 16:49:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint836.pt
2022-08-17 16:50:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint836.pt (epoch 836 @ 67716 updates, score 56.92) (writing took 20.32189166173339 seconds)
2022-08-17 16:50:10 | INFO | fairseq_cli.train | end of epoch 836 (average epoch stats below)
2022-08-17 16:50:10 | INFO | train | epoch 836 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 6197.1 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 67716 | lr 0.000243044 | gnorm 0.297 | train_wall 39 | gb_free 10.1 | wall 67106
2022-08-17 16:50:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:50:10 | INFO | fairseq.trainer | begin training epoch 837
2022-08-17 16:50:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:50:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:50:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:50:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:50:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:51:01 | INFO | valid | epoch 837 | valid on 'valid' subset | loss 5.174 | nll_loss 2.57 | ppl 5.94 | bleu 56.79 | wps 1796.1 | wpb 933.5 | bsz 59.6 | num_updates 67797 | best_bleu 57.52
2022-08-17 16:51:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 837 @ 67797 updates
2022-08-17 16:51:01 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint837.pt
2022-08-17 16:51:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint837.pt
2022-08-17 16:51:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint837.pt (epoch 837 @ 67797 updates, score 56.79) (writing took 19.525479316711426 seconds)
2022-08-17 16:51:21 | INFO | fairseq_cli.train | end of epoch 837 (average epoch stats below)
2022-08-17 16:51:21 | INFO | train | epoch 837 | loss 3.376 | nll_loss 0.344 | ppl 1.27 | wps 6344 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 67797 | lr 0.000242898 | gnorm 0.598 | train_wall 40 | gb_free 10.2 | wall 67177
2022-08-17 16:51:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:51:21 | INFO | fairseq.trainer | begin training epoch 838
2022-08-17 16:51:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:51:23 | INFO | train_inner | epoch 838:      3 / 81 loss=3.376, nll_loss=0.344, ppl=1.27, wps=4948.5, ups=0.9, wpb=5503.1, bsz=357.8, num_updates=67800, lr=0.000242893, gnorm=0.546, train_wall=49, gb_free=10, wall=67180
2022-08-17 16:52:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:52:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:52:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:52:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:52:12 | INFO | valid | epoch 838 | valid on 'valid' subset | loss 5.187 | nll_loss 2.593 | ppl 6.04 | bleu 56.65 | wps 1833.1 | wpb 933.5 | bsz 59.6 | num_updates 67878 | best_bleu 57.52
2022-08-17 16:52:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 838 @ 67878 updates
2022-08-17 16:52:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint838.pt
2022-08-17 16:52:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint838.pt
2022-08-17 16:52:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint838.pt (epoch 838 @ 67878 updates, score 56.65) (writing took 25.71255450323224 seconds)
2022-08-17 16:52:38 | INFO | fairseq_cli.train | end of epoch 838 (average epoch stats below)
2022-08-17 16:52:38 | INFO | train | epoch 838 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 5791.9 | ups 1.05 | wpb 5523.2 | bsz 358 | num_updates 67878 | lr 0.000242753 | gnorm 0.326 | train_wall 40 | gb_free 10.1 | wall 67254
2022-08-17 16:52:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:52:38 | INFO | fairseq.trainer | begin training epoch 839
2022-08-17 16:52:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:52:50 | INFO | train_inner | epoch 839:     22 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=6373.2, ups=1.15, wpb=5541.9, bsz=360.3, num_updates=67900, lr=0.000242714, gnorm=0.341, train_wall=49, gb_free=10.1, wall=67267
2022-08-17 16:53:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:53:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:53:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:53:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:53:29 | INFO | valid | epoch 839 | valid on 'valid' subset | loss 5.197 | nll_loss 2.6 | ppl 6.06 | bleu 56.39 | wps 1934.6 | wpb 933.5 | bsz 59.6 | num_updates 67959 | best_bleu 57.52
2022-08-17 16:53:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 839 @ 67959 updates
2022-08-17 16:53:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint839.pt
2022-08-17 16:53:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint839.pt
2022-08-17 16:53:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint839.pt (epoch 839 @ 67959 updates, score 56.39) (writing took 19.92621959373355 seconds)
2022-08-17 16:53:49 | INFO | fairseq_cli.train | end of epoch 839 (average epoch stats below)
2022-08-17 16:53:49 | INFO | train | epoch 839 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 6319.2 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 67959 | lr 0.000242609 | gnorm 0.346 | train_wall 39 | gb_free 10.1 | wall 67325
2022-08-17 16:53:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:53:49 | INFO | fairseq.trainer | begin training epoch 840
2022-08-17 16:53:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:54:10 | INFO | train_inner | epoch 840:     41 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=6932.6, ups=1.25, wpb=5536.4, bsz=356.3, num_updates=68000, lr=0.000242536, gnorm=0.326, train_wall=48, gb_free=10.1, wall=67346
2022-08-17 16:54:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:54:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:54:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:54:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:54:39 | INFO | valid | epoch 840 | valid on 'valid' subset | loss 5.184 | nll_loss 2.587 | ppl 6.01 | bleu 56.43 | wps 1780.2 | wpb 933.5 | bsz 59.6 | num_updates 68040 | best_bleu 57.52
2022-08-17 16:54:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 840 @ 68040 updates
2022-08-17 16:54:39 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint840.pt
2022-08-17 16:54:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint840.pt
2022-08-17 16:55:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint840.pt (epoch 840 @ 68040 updates, score 56.43) (writing took 29.3269555978477 seconds)
2022-08-17 16:55:09 | INFO | fairseq_cli.train | end of epoch 840 (average epoch stats below)
2022-08-17 16:55:09 | INFO | train | epoch 840 | loss 3.375 | nll_loss 0.343 | ppl 1.27 | wps 5573.8 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 68040 | lr 0.000242464 | gnorm 0.33 | train_wall 39 | gb_free 10.2 | wall 67405
2022-08-17 16:55:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:55:09 | INFO | fairseq.trainer | begin training epoch 841
2022-08-17 16:55:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:55:39 | INFO | train_inner | epoch 841:     60 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=6167.9, ups=1.12, wpb=5491.7, bsz=354.9, num_updates=68100, lr=0.000242357, gnorm=0.318, train_wall=48, gb_free=10.1, wall=67435
2022-08-17 16:55:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:55:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:55:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:55:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:55:59 | INFO | valid | epoch 841 | valid on 'valid' subset | loss 5.192 | nll_loss 2.599 | ppl 6.06 | bleu 56.43 | wps 1769.5 | wpb 933.5 | bsz 59.6 | num_updates 68121 | best_bleu 57.52
2022-08-17 16:55:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 841 @ 68121 updates
2022-08-17 16:55:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint841.pt
2022-08-17 16:56:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint841.pt
2022-08-17 16:56:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint841.pt (epoch 841 @ 68121 updates, score 56.43) (writing took 19.517866551876068 seconds)
2022-08-17 16:56:19 | INFO | fairseq_cli.train | end of epoch 841 (average epoch stats below)
2022-08-17 16:56:19 | INFO | train | epoch 841 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 6400.6 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 68121 | lr 0.00024232 | gnorm 0.285 | train_wall 39 | gb_free 10 | wall 67475
2022-08-17 16:56:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:56:19 | INFO | fairseq.trainer | begin training epoch 842
2022-08-17 16:56:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:57:04 | INFO | train_inner | epoch 842:     79 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=6559.4, ups=1.18, wpb=5547.1, bsz=361.2, num_updates=68200, lr=0.00024218, gnorm=0.324, train_wall=49, gb_free=10, wall=67520
2022-08-17 16:57:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:57:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:57:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:57:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:57:14 | INFO | valid | epoch 842 | valid on 'valid' subset | loss 5.183 | nll_loss 2.586 | ppl 6.01 | bleu 56.21 | wps 1945.1 | wpb 933.5 | bsz 59.6 | num_updates 68202 | best_bleu 57.52
2022-08-17 16:57:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 842 @ 68202 updates
2022-08-17 16:57:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint842.pt
2022-08-17 16:57:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint842.pt
2022-08-17 16:57:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint842.pt (epoch 842 @ 68202 updates, score 56.21) (writing took 16.699294164776802 seconds)
2022-08-17 16:57:30 | INFO | fairseq_cli.train | end of epoch 842 (average epoch stats below)
2022-08-17 16:57:30 | INFO | train | epoch 842 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 6254.4 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 68202 | lr 0.000242176 | gnorm 0.345 | train_wall 40 | gb_free 10.2 | wall 67547
2022-08-17 16:57:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:57:31 | INFO | fairseq.trainer | begin training epoch 843
2022-08-17 16:57:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:58:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:58:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:58:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:58:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:58:23 | INFO | valid | epoch 843 | valid on 'valid' subset | loss 5.193 | nll_loss 2.595 | ppl 6.04 | bleu 56.2 | wps 1819.7 | wpb 933.5 | bsz 59.6 | num_updates 68283 | best_bleu 57.52
2022-08-17 16:58:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 843 @ 68283 updates
2022-08-17 16:58:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint843.pt
2022-08-17 16:58:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint843.pt
2022-08-17 16:58:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint843.pt (epoch 843 @ 68283 updates, score 56.2) (writing took 15.205407034605742 seconds)
2022-08-17 16:58:38 | INFO | fairseq_cli.train | end of epoch 843 (average epoch stats below)
2022-08-17 16:58:38 | INFO | train | epoch 843 | loss 3.375 | nll_loss 0.343 | ppl 1.27 | wps 6623.4 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 68283 | lr 0.000242033 | gnorm 0.331 | train_wall 41 | gb_free 10.2 | wall 67614
2022-08-17 16:58:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:58:38 | INFO | fairseq.trainer | begin training epoch 844
2022-08-17 16:58:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:58:48 | INFO | train_inner | epoch 844:     17 / 81 loss=3.375, nll_loss=0.343, ppl=1.27, wps=5276.5, ups=0.96, wpb=5514.3, bsz=360.2, num_updates=68300, lr=0.000242002, gnorm=0.327, train_wall=50, gb_free=10.1, wall=67625
2022-08-17 16:59:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 16:59:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 16:59:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 16:59:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 16:59:30 | INFO | valid | epoch 844 | valid on 'valid' subset | loss 5.18 | nll_loss 2.58 | ppl 5.98 | bleu 56.46 | wps 1869.4 | wpb 933.5 | bsz 59.6 | num_updates 68364 | best_bleu 57.52
2022-08-17 16:59:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 844 @ 68364 updates
2022-08-17 16:59:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint844.pt
2022-08-17 16:59:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint844.pt
2022-08-17 16:59:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint844.pt (epoch 844 @ 68364 updates, score 56.46) (writing took 2.329143814742565 seconds)
2022-08-17 16:59:32 | INFO | fairseq_cli.train | end of epoch 844 (average epoch stats below)
2022-08-17 16:59:32 | INFO | train | epoch 844 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 8262.9 | ups 1.5 | wpb 5523.2 | bsz 358 | num_updates 68364 | lr 0.000241889 | gnorm 0.285 | train_wall 40 | gb_free 10.1 | wall 67668
2022-08-17 16:59:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 16:59:32 | INFO | fairseq.trainer | begin training epoch 845
2022-08-17 16:59:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 16:59:51 | INFO | train_inner | epoch 845:     36 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=8880.3, ups=1.6, wpb=5536, bsz=355.4, num_updates=68400, lr=0.000241825, gnorm=0.277, train_wall=49, gb_free=10.1, wall=67687
2022-08-17 17:00:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:00:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:00:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:00:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:00:22 | INFO | valid | epoch 845 | valid on 'valid' subset | loss 5.159 | nll_loss 2.561 | ppl 5.9 | bleu 56.95 | wps 1919.6 | wpb 933.5 | bsz 59.6 | num_updates 68445 | best_bleu 57.52
2022-08-17 17:00:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 845 @ 68445 updates
2022-08-17 17:00:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint845.pt
2022-08-17 17:00:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint845.pt
2022-08-17 17:00:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint845.pt (epoch 845 @ 68445 updates, score 56.95) (writing took 14.662462826818228 seconds)
2022-08-17 17:00:37 | INFO | fairseq_cli.train | end of epoch 845 (average epoch stats below)
2022-08-17 17:00:37 | INFO | train | epoch 845 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 6923.1 | ups 1.25 | wpb 5523.2 | bsz 358 | num_updates 68445 | lr 0.000241746 | gnorm 0.321 | train_wall 39 | gb_free 10.1 | wall 67733
2022-08-17 17:00:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:00:37 | INFO | fairseq.trainer | begin training epoch 846
2022-08-17 17:00:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:01:05 | INFO | train_inner | epoch 846:     55 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=7365.8, ups=1.34, wpb=5502.2, bsz=358.3, num_updates=68500, lr=0.000241649, gnorm=0.355, train_wall=49, gb_free=10.1, wall=67762
2022-08-17 17:01:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:01:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:01:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:01:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:01:28 | INFO | valid | epoch 846 | valid on 'valid' subset | loss 5.184 | nll_loss 2.585 | ppl 6 | bleu 56.76 | wps 1851.6 | wpb 933.5 | bsz 59.6 | num_updates 68526 | best_bleu 57.52
2022-08-17 17:01:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 846 @ 68526 updates
2022-08-17 17:01:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint846.pt
2022-08-17 17:01:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint846.pt
2022-08-17 17:01:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint846.pt (epoch 846 @ 68526 updates, score 56.76) (writing took 15.202979773283005 seconds)
2022-08-17 17:01:43 | INFO | fairseq_cli.train | end of epoch 846 (average epoch stats below)
2022-08-17 17:01:43 | INFO | train | epoch 846 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 6749.6 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 68526 | lr 0.000241603 | gnorm 0.327 | train_wall 40 | gb_free 10.1 | wall 67799
2022-08-17 17:01:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:01:43 | INFO | fairseq.trainer | begin training epoch 847
2022-08-17 17:01:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:02:21 | INFO | train_inner | epoch 847:     74 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=7314.5, ups=1.32, wpb=5535, bsz=357.9, num_updates=68600, lr=0.000241473, gnorm=0.306, train_wall=48, gb_free=10.1, wall=67837
2022-08-17 17:02:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:02:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:02:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:02:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:02:34 | INFO | valid | epoch 847 | valid on 'valid' subset | loss 5.179 | nll_loss 2.576 | ppl 5.96 | bleu 56.28 | wps 1895.4 | wpb 933.5 | bsz 59.6 | num_updates 68607 | best_bleu 57.52
2022-08-17 17:02:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 847 @ 68607 updates
2022-08-17 17:02:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint847.pt
2022-08-17 17:02:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint847.pt
2022-08-17 17:02:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint847.pt (epoch 847 @ 68607 updates, score 56.28) (writing took 18.57036292925477 seconds)
2022-08-17 17:02:52 | INFO | fairseq_cli.train | end of epoch 847 (average epoch stats below)
2022-08-17 17:02:52 | INFO | train | epoch 847 | loss 3.375 | nll_loss 0.343 | ppl 1.27 | wps 6458.1 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 68607 | lr 0.00024146 | gnorm 0.313 | train_wall 39 | gb_free 10.2 | wall 67869
2022-08-17 17:02:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:02:52 | INFO | fairseq.trainer | begin training epoch 848
2022-08-17 17:02:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:03:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:03:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:03:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:03:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:03:42 | INFO | valid | epoch 848 | valid on 'valid' subset | loss 5.178 | nll_loss 2.578 | ppl 5.97 | bleu 56.46 | wps 1904.3 | wpb 933.5 | bsz 59.6 | num_updates 68688 | best_bleu 57.52
2022-08-17 17:03:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 848 @ 68688 updates
2022-08-17 17:03:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint848.pt
2022-08-17 17:03:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint848.pt
2022-08-17 17:03:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint848.pt (epoch 848 @ 68688 updates, score 56.46) (writing took 16.886239267885685 seconds)
2022-08-17 17:03:59 | INFO | fairseq_cli.train | end of epoch 848 (average epoch stats below)
2022-08-17 17:03:59 | INFO | train | epoch 848 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 6684.9 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 68688 | lr 0.000241318 | gnorm 0.319 | train_wall 39 | gb_free 10.2 | wall 67935
2022-08-17 17:03:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:03:59 | INFO | fairseq.trainer | begin training epoch 849
2022-08-17 17:03:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:04:07 | INFO | train_inner | epoch 849:     12 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=5199.5, ups=0.95, wpb=5486.8, bsz=357.6, num_updates=68700, lr=0.000241297, gnorm=0.333, train_wall=48, gb_free=10.1, wall=67943
2022-08-17 17:04:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:04:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:04:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:04:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:04:51 | INFO | valid | epoch 849 | valid on 'valid' subset | loss 5.188 | nll_loss 2.59 | ppl 6.02 | bleu 56.45 | wps 1743.9 | wpb 933.5 | bsz 59.6 | num_updates 68769 | best_bleu 57.52
2022-08-17 17:04:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 849 @ 68769 updates
2022-08-17 17:04:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint849.pt
2022-08-17 17:04:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint849.pt
2022-08-17 17:05:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint849.pt (epoch 849 @ 68769 updates, score 56.45) (writing took 17.73004875332117 seconds)
2022-08-17 17:05:09 | INFO | fairseq_cli.train | end of epoch 849 (average epoch stats below)
2022-08-17 17:05:09 | INFO | train | epoch 849 | loss 3.376 | nll_loss 0.344 | ppl 1.27 | wps 6425.5 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 68769 | lr 0.000241176 | gnorm 0.34 | train_wall 40 | gb_free 10.2 | wall 68005
2022-08-17 17:05:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:05:09 | INFO | fairseq.trainer | begin training epoch 850
2022-08-17 17:05:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:05:27 | INFO | train_inner | epoch 850:     31 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=6895.7, ups=1.24, wpb=5559.4, bsz=360.8, num_updates=68800, lr=0.000241121, gnorm=0.304, train_wall=50, gb_free=10.1, wall=68023
2022-08-17 17:05:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:05:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:05:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:05:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:06:02 | INFO | valid | epoch 850 | valid on 'valid' subset | loss 5.173 | nll_loss 2.57 | ppl 5.94 | bleu 56.68 | wps 1904.5 | wpb 933.5 | bsz 59.6 | num_updates 68850 | best_bleu 57.52
2022-08-17 17:06:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 850 @ 68850 updates
2022-08-17 17:06:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint850.pt
2022-08-17 17:06:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint850.pt
2022-08-17 17:06:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint850.pt (epoch 850 @ 68850 updates, score 56.68) (writing took 2.370453830808401 seconds)
2022-08-17 17:06:05 | INFO | fairseq_cli.train | end of epoch 850 (average epoch stats below)
2022-08-17 17:06:05 | INFO | train | epoch 850 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 8004.3 | ups 1.45 | wpb 5523.2 | bsz 358 | num_updates 68850 | lr 0.000241034 | gnorm 0.307 | train_wall 41 | gb_free 10.1 | wall 68061
2022-08-17 17:06:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:06:05 | INFO | fairseq.trainer | begin training epoch 851
2022-08-17 17:06:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:06:31 | INFO | train_inner | epoch 851:     50 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=8612.1, ups=1.56, wpb=5509.4, bsz=356.6, num_updates=68900, lr=0.000240946, gnorm=0.324, train_wall=50, gb_free=10.1, wall=68087
2022-08-17 17:06:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:06:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:06:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:06:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:06:56 | INFO | valid | epoch 851 | valid on 'valid' subset | loss 5.175 | nll_loss 2.572 | ppl 5.95 | bleu 56.79 | wps 1911.1 | wpb 933.5 | bsz 59.6 | num_updates 68931 | best_bleu 57.52
2022-08-17 17:06:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 851 @ 68931 updates
2022-08-17 17:06:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint851.pt
2022-08-17 17:06:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint851.pt
2022-08-17 17:07:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint851.pt (epoch 851 @ 68931 updates, score 56.79) (writing took 29.87971919029951 seconds)
2022-08-17 17:07:26 | INFO | fairseq_cli.train | end of epoch 851 (average epoch stats below)
2022-08-17 17:07:26 | INFO | train | epoch 851 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 5531.1 | ups 1 | wpb 5523.2 | bsz 358 | num_updates 68931 | lr 0.000240892 | gnorm 0.316 | train_wall 40 | gb_free 10.2 | wall 68142
2022-08-17 17:07:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:07:26 | INFO | fairseq.trainer | begin training epoch 852
2022-08-17 17:07:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:08:07 | INFO | train_inner | epoch 852:     69 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=5765.5, ups=1.04, wpb=5531.3, bsz=355.5, num_updates=69000, lr=0.000240772, gnorm=0.322, train_wall=49, gb_free=10.1, wall=68183
2022-08-17 17:08:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:08:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:08:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:08:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:08:23 | INFO | valid | epoch 852 | valid on 'valid' subset | loss 5.162 | nll_loss 2.56 | ppl 5.9 | bleu 57.22 | wps 1897.1 | wpb 933.5 | bsz 59.6 | num_updates 69012 | best_bleu 57.52
2022-08-17 17:08:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 852 @ 69012 updates
2022-08-17 17:08:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint852.pt
2022-08-17 17:08:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint852.pt
2022-08-17 17:08:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint852.pt (epoch 852 @ 69012 updates, score 57.22) (writing took 20.833846516907215 seconds)
2022-08-17 17:08:44 | INFO | fairseq_cli.train | end of epoch 852 (average epoch stats below)
2022-08-17 17:08:44 | INFO | train | epoch 852 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 5681.6 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 69012 | lr 0.000240751 | gnorm 0.327 | train_wall 40 | gb_free 10.2 | wall 68221
2022-08-17 17:08:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:08:45 | INFO | fairseq.trainer | begin training epoch 853
2022-08-17 17:08:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:09:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:09:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:09:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:09:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:09:37 | INFO | valid | epoch 853 | valid on 'valid' subset | loss 5.191 | nll_loss 2.596 | ppl 6.05 | bleu 56.82 | wps 1585.3 | wpb 933.5 | bsz 59.6 | num_updates 69093 | best_bleu 57.52
2022-08-17 17:09:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 853 @ 69093 updates
2022-08-17 17:09:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint853.pt
2022-08-17 17:09:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint853.pt
2022-08-17 17:10:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint853.pt (epoch 853 @ 69093 updates, score 56.82) (writing took 31.971976555883884 seconds)
2022-08-17 17:10:09 | INFO | fairseq_cli.train | end of epoch 853 (average epoch stats below)
2022-08-17 17:10:09 | INFO | train | epoch 853 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 5266.2 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 69093 | lr 0.00024061 | gnorm 0.309 | train_wall 40 | gb_free 10.1 | wall 68306
2022-08-17 17:10:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:10:10 | INFO | fairseq.trainer | begin training epoch 854
2022-08-17 17:10:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:10:14 | INFO | train_inner | epoch 854:      7 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=4349.2, ups=0.79, wpb=5520.1, bsz=357.8, num_updates=69100, lr=0.000240597, gnorm=0.315, train_wall=49, gb_free=10.1, wall=68310
2022-08-17 17:10:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:10:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:10:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:10:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:11:01 | INFO | valid | epoch 854 | valid on 'valid' subset | loss 5.181 | nll_loss 2.579 | ppl 5.97 | bleu 56.79 | wps 1879.8 | wpb 933.5 | bsz 59.6 | num_updates 69174 | best_bleu 57.52
2022-08-17 17:11:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 854 @ 69174 updates
2022-08-17 17:11:01 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint854.pt
2022-08-17 17:11:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint854.pt
2022-08-17 17:11:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint854.pt (epoch 854 @ 69174 updates, score 56.79) (writing took 17.7358156144619 seconds)
2022-08-17 17:11:19 | INFO | fairseq_cli.train | end of epoch 854 (average epoch stats below)
2022-08-17 17:11:19 | INFO | train | epoch 854 | loss 3.376 | nll_loss 0.343 | ppl 1.27 | wps 6461.6 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 69174 | lr 0.000240469 | gnorm 0.606 | train_wall 40 | gb_free 10.3 | wall 68375
2022-08-17 17:11:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:11:19 | INFO | fairseq.trainer | begin training epoch 855
2022-08-17 17:11:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:11:33 | INFO | train_inner | epoch 855:     26 / 81 loss=3.375, nll_loss=0.343, ppl=1.27, wps=7003.2, ups=1.27, wpb=5510.1, bsz=358.3, num_updates=69200, lr=0.000240424, gnorm=0.579, train_wall=49, gb_free=10.1, wall=68389
2022-08-17 17:12:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:12:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:12:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:12:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:12:11 | INFO | valid | epoch 855 | valid on 'valid' subset | loss 5.188 | nll_loss 2.592 | ppl 6.03 | bleu 57.03 | wps 1803.5 | wpb 933.5 | bsz 59.6 | num_updates 69255 | best_bleu 57.52
2022-08-17 17:12:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 855 @ 69255 updates
2022-08-17 17:12:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint855.pt
2022-08-17 17:12:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint855.pt
2022-08-17 17:12:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint855.pt (epoch 855 @ 69255 updates, score 57.03) (writing took 19.345555745065212 seconds)
2022-08-17 17:12:30 | INFO | fairseq_cli.train | end of epoch 855 (average epoch stats below)
2022-08-17 17:12:30 | INFO | train | epoch 855 | loss 3.375 | nll_loss 0.343 | ppl 1.27 | wps 6266.1 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 69255 | lr 0.000240328 | gnorm 0.42 | train_wall 39 | gb_free 10.3 | wall 68446
2022-08-17 17:12:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:12:30 | INFO | fairseq.trainer | begin training epoch 856
2022-08-17 17:12:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:12:53 | INFO | train_inner | epoch 856:     45 / 81 loss=3.375, nll_loss=0.343, ppl=1.27, wps=6907.3, ups=1.24, wpb=5548.1, bsz=362.8, num_updates=69300, lr=0.00024025, gnorm=0.395, train_wall=49, gb_free=10, wall=68469
2022-08-17 17:13:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:13:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:13:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:13:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:13:22 | INFO | valid | epoch 856 | valid on 'valid' subset | loss 5.183 | nll_loss 2.583 | ppl 5.99 | bleu 56.67 | wps 1624.7 | wpb 933.5 | bsz 59.6 | num_updates 69336 | best_bleu 57.52
2022-08-17 17:13:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 856 @ 69336 updates
2022-08-17 17:13:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint856.pt
2022-08-17 17:13:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint856.pt
2022-08-17 17:13:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint856.pt (epoch 856 @ 69336 updates, score 56.67) (writing took 36.57026298344135 seconds)
2022-08-17 17:13:58 | INFO | fairseq_cli.train | end of epoch 856 (average epoch stats below)
2022-08-17 17:13:58 | INFO | train | epoch 856 | loss 3.375 | nll_loss 0.343 | ppl 1.27 | wps 5066.7 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 69336 | lr 0.000240188 | gnorm 0.339 | train_wall 40 | gb_free 10.1 | wall 68535
2022-08-17 17:13:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:13:59 | INFO | fairseq.trainer | begin training epoch 857
2022-08-17 17:13:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:14:32 | INFO | train_inner | epoch 857:     64 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=5562.4, ups=1.01, wpb=5502, bsz=352.9, num_updates=69400, lr=0.000240077, gnorm=0.307, train_wall=50, gb_free=10.1, wall=68568
2022-08-17 17:14:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:14:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:14:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:14:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:14:50 | INFO | valid | epoch 857 | valid on 'valid' subset | loss 5.188 | nll_loss 2.588 | ppl 6.01 | bleu 56.79 | wps 1824.9 | wpb 933.5 | bsz 59.6 | num_updates 69417 | best_bleu 57.52
2022-08-17 17:14:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 857 @ 69417 updates
2022-08-17 17:14:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint857.pt
2022-08-17 17:14:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint857.pt
2022-08-17 17:15:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint857.pt (epoch 857 @ 69417 updates, score 56.79) (writing took 18.014506954699755 seconds)
2022-08-17 17:15:08 | INFO | fairseq_cli.train | end of epoch 857 (average epoch stats below)
2022-08-17 17:15:08 | INFO | train | epoch 857 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 6407.8 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 69417 | lr 0.000240047 | gnorm 0.327 | train_wall 40 | gb_free 10.2 | wall 68604
2022-08-17 17:15:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:15:08 | INFO | fairseq.trainer | begin training epoch 858
2022-08-17 17:15:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:15:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:15:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:15:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:15:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:15:58 | INFO | valid | epoch 858 | valid on 'valid' subset | loss 5.159 | nll_loss 2.554 | ppl 5.87 | bleu 56.71 | wps 1899.4 | wpb 933.5 | bsz 59.6 | num_updates 69498 | best_bleu 57.52
2022-08-17 17:15:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 858 @ 69498 updates
2022-08-17 17:15:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint858.pt
2022-08-17 17:15:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint858.pt
2022-08-17 17:16:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint858.pt (epoch 858 @ 69498 updates, score 56.71) (writing took 22.71980756521225 seconds)
2022-08-17 17:16:21 | INFO | fairseq_cli.train | end of epoch 858 (average epoch stats below)
2022-08-17 17:16:21 | INFO | train | epoch 858 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 6150.3 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 69498 | lr 0.000239908 | gnorm 0.33 | train_wall 39 | gb_free 10.2 | wall 68677
2022-08-17 17:16:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:16:21 | INFO | fairseq.trainer | begin training epoch 859
2022-08-17 17:16:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:16:23 | INFO | train_inner | epoch 859:      2 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=4975.7, ups=0.9, wpb=5524.6, bsz=359, num_updates=69500, lr=0.000239904, gnorm=0.34, train_wall=48, gb_free=10, wall=68679
2022-08-17 17:17:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:17:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:17:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:17:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:17:12 | INFO | valid | epoch 859 | valid on 'valid' subset | loss 5.171 | nll_loss 2.571 | ppl 5.94 | bleu 56.8 | wps 1877.2 | wpb 933.5 | bsz 59.6 | num_updates 69579 | best_bleu 57.52
2022-08-17 17:17:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 859 @ 69579 updates
2022-08-17 17:17:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint859.pt
2022-08-17 17:17:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint859.pt
2022-08-17 17:17:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint859.pt (epoch 859 @ 69579 updates, score 56.8) (writing took 20.822981536388397 seconds)
2022-08-17 17:17:33 | INFO | fairseq_cli.train | end of epoch 859 (average epoch stats below)
2022-08-17 17:17:33 | INFO | train | epoch 859 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 6186.2 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 69579 | lr 0.000239768 | gnorm 0.374 | train_wall 40 | gb_free 10.2 | wall 68749
2022-08-17 17:17:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:17:33 | INFO | fairseq.trainer | begin training epoch 860
2022-08-17 17:17:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:17:45 | INFO | train_inner | epoch 860:     21 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=6745.4, ups=1.21, wpb=5552.7, bsz=357.4, num_updates=69600, lr=0.000239732, gnorm=0.373, train_wall=49, gb_free=10.1, wall=68762
2022-08-17 17:18:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:18:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:18:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:18:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:18:24 | INFO | valid | epoch 860 | valid on 'valid' subset | loss 5.166 | nll_loss 2.563 | ppl 5.91 | bleu 56.52 | wps 1914.9 | wpb 933.5 | bsz 59.6 | num_updates 69660 | best_bleu 57.52
2022-08-17 17:18:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 860 @ 69660 updates
2022-08-17 17:18:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint860.pt
2022-08-17 17:18:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint860.pt
2022-08-17 17:18:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint860.pt (epoch 860 @ 69660 updates, score 56.52) (writing took 16.84203863888979 seconds)
2022-08-17 17:18:41 | INFO | fairseq_cli.train | end of epoch 860 (average epoch stats below)
2022-08-17 17:18:41 | INFO | train | epoch 860 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 6567.1 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 69660 | lr 0.000239628 | gnorm 0.336 | train_wall 40 | gb_free 10.1 | wall 68818
2022-08-17 17:18:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:18:41 | INFO | fairseq.trainer | begin training epoch 861
2022-08-17 17:18:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:19:03 | INFO | train_inner | epoch 861:     40 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=7098, ups=1.3, wpb=5480.8, bsz=360.2, num_updates=69700, lr=0.00023956, gnorm=0.301, train_wall=49, gb_free=10.1, wall=68839
2022-08-17 17:19:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:19:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:19:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:19:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:19:33 | INFO | valid | epoch 861 | valid on 'valid' subset | loss 5.177 | nll_loss 2.577 | ppl 5.97 | bleu 56.31 | wps 1851.4 | wpb 933.5 | bsz 59.6 | num_updates 69741 | best_bleu 57.52
2022-08-17 17:19:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 861 @ 69741 updates
2022-08-17 17:19:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint861.pt
2022-08-17 17:19:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint861.pt
2022-08-17 17:19:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint861.pt (epoch 861 @ 69741 updates, score 56.31) (writing took 22.187741730362177 seconds)
2022-08-17 17:19:55 | INFO | fairseq_cli.train | end of epoch 861 (average epoch stats below)
2022-08-17 17:19:55 | INFO | train | epoch 861 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 6039.7 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 69741 | lr 0.000239489 | gnorm 0.282 | train_wall 41 | gb_free 10.1 | wall 68892
2022-08-17 17:19:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:19:56 | INFO | fairseq.trainer | begin training epoch 862
2022-08-17 17:19:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:20:26 | INFO | train_inner | epoch 862:     59 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=6621.5, ups=1.19, wpb=5554.3, bsz=361.3, num_updates=69800, lr=0.000239388, gnorm=0.53, train_wall=50, gb_free=10.1, wall=68923
2022-08-17 17:20:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:20:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:20:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:20:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:20:47 | INFO | valid | epoch 862 | valid on 'valid' subset | loss 5.173 | nll_loss 2.571 | ppl 5.94 | bleu 56.61 | wps 1835.9 | wpb 933.5 | bsz 59.6 | num_updates 69822 | best_bleu 57.52
2022-08-17 17:20:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 862 @ 69822 updates
2022-08-17 17:20:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint862.pt
2022-08-17 17:20:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint862.pt
2022-08-17 17:21:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint862.pt (epoch 862 @ 69822 updates, score 56.61) (writing took 29.995668578892946 seconds)
2022-08-17 17:21:17 | INFO | fairseq_cli.train | end of epoch 862 (average epoch stats below)
2022-08-17 17:21:17 | INFO | train | epoch 862 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 5479.2 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 69822 | lr 0.00023935 | gnorm 0.595 | train_wall 40 | gb_free 10.2 | wall 68973
2022-08-17 17:21:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:21:17 | INFO | fairseq.trainer | begin training epoch 863
2022-08-17 17:21:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:21:58 | INFO | train_inner | epoch 863:     78 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=6032.8, ups=1.09, wpb=5529.7, bsz=354.2, num_updates=69900, lr=0.000239217, gnorm=0.313, train_wall=50, gb_free=10, wall=69014
2022-08-17 17:21:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:22:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:22:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:22:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:22:09 | INFO | valid | epoch 863 | valid on 'valid' subset | loss 5.161 | nll_loss 2.554 | ppl 5.87 | bleu 57.16 | wps 1735 | wpb 933.5 | bsz 59.6 | num_updates 69903 | best_bleu 57.52
2022-08-17 17:22:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 863 @ 69903 updates
2022-08-17 17:22:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint863.pt
2022-08-17 17:22:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint863.pt
2022-08-17 17:22:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint863.pt (epoch 863 @ 69903 updates, score 57.16) (writing took 38.3176736459136 seconds)
2022-08-17 17:22:48 | INFO | fairseq_cli.train | end of epoch 863 (average epoch stats below)
2022-08-17 17:22:48 | INFO | train | epoch 863 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 4924.2 | ups 0.89 | wpb 5523.2 | bsz 358 | num_updates 69903 | lr 0.000239212 | gnorm 0.33 | train_wall 40 | gb_free 10.1 | wall 69064
2022-08-17 17:22:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:22:48 | INFO | fairseq.trainer | begin training epoch 864
2022-08-17 17:22:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:23:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:23:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:23:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:23:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:23:41 | INFO | valid | epoch 864 | valid on 'valid' subset | loss 5.176 | nll_loss 2.574 | ppl 5.96 | bleu 56.67 | wps 1916.5 | wpb 933.5 | bsz 59.6 | num_updates 69984 | best_bleu 57.52
2022-08-17 17:23:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 864 @ 69984 updates
2022-08-17 17:23:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint864.pt
2022-08-17 17:23:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint864.pt
2022-08-17 17:24:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint864.pt (epoch 864 @ 69984 updates, score 56.67) (writing took 21.51702145114541 seconds)
2022-08-17 17:24:02 | INFO | fairseq_cli.train | end of epoch 864 (average epoch stats below)
2022-08-17 17:24:02 | INFO | train | epoch 864 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 6023 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 69984 | lr 0.000239073 | gnorm 0.393 | train_wall 41 | gb_free 10.3 | wall 69138
2022-08-17 17:24:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:24:02 | INFO | fairseq.trainer | begin training epoch 865
2022-08-17 17:24:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:24:11 | INFO | train_inner | epoch 865:     16 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=4151.7, ups=0.75, wpb=5532.3, bsz=358.7, num_updates=70000, lr=0.000239046, gnorm=0.377, train_wall=50, gb_free=10.1, wall=69148
2022-08-17 17:24:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:24:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:24:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:24:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:24:53 | INFO | valid | epoch 865 | valid on 'valid' subset | loss 5.196 | nll_loss 2.6 | ppl 6.06 | bleu 56.78 | wps 1909.2 | wpb 933.5 | bsz 59.6 | num_updates 70065 | best_bleu 57.52
2022-08-17 17:24:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 865 @ 70065 updates
2022-08-17 17:24:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint865.pt
2022-08-17 17:24:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint865.pt
2022-08-17 17:25:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint865.pt (epoch 865 @ 70065 updates, score 56.78) (writing took 27.11338309571147 seconds)
2022-08-17 17:25:20 | INFO | fairseq_cli.train | end of epoch 865 (average epoch stats below)
2022-08-17 17:25:20 | INFO | train | epoch 865 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 5716.3 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 70065 | lr 0.000238935 | gnorm 0.306 | train_wall 40 | gb_free 10.1 | wall 69217
2022-08-17 17:25:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:25:21 | INFO | fairseq.trainer | begin training epoch 866
2022-08-17 17:25:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:25:40 | INFO | train_inner | epoch 866:     35 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=6187.5, ups=1.13, wpb=5482.3, bsz=353.1, num_updates=70100, lr=0.000238875, gnorm=0.33, train_wall=49, gb_free=10.1, wall=69236
2022-08-17 17:26:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:26:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:26:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:26:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:26:13 | INFO | valid | epoch 866 | valid on 'valid' subset | loss 5.185 | nll_loss 2.59 | ppl 6.02 | bleu 56.67 | wps 1759.1 | wpb 933.5 | bsz 59.6 | num_updates 70146 | best_bleu 57.52
2022-08-17 17:26:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 866 @ 70146 updates
2022-08-17 17:26:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint866.pt
2022-08-17 17:26:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint866.pt
2022-08-17 17:26:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint866.pt (epoch 866 @ 70146 updates, score 56.67) (writing took 29.074776850640774 seconds)
2022-08-17 17:26:42 | INFO | fairseq_cli.train | end of epoch 866 (average epoch stats below)
2022-08-17 17:26:42 | INFO | train | epoch 866 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 5485.2 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 70146 | lr 0.000238797 | gnorm 0.312 | train_wall 39 | gb_free 10.2 | wall 69298
2022-08-17 17:26:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:26:42 | INFO | fairseq.trainer | begin training epoch 867
2022-08-17 17:26:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:27:11 | INFO | train_inner | epoch 867:     54 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=6115.1, ups=1.1, wpb=5535.6, bsz=359.7, num_updates=70200, lr=0.000238705, gnorm=0.322, train_wall=49, gb_free=10.1, wall=69327
2022-08-17 17:27:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:27:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:27:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:27:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:27:33 | INFO | valid | epoch 867 | valid on 'valid' subset | loss 5.181 | nll_loss 2.584 | ppl 6 | bleu 56.93 | wps 1937.8 | wpb 933.5 | bsz 59.6 | num_updates 70227 | best_bleu 57.52
2022-08-17 17:27:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 867 @ 70227 updates
2022-08-17 17:27:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint867.pt
2022-08-17 17:27:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint867.pt
2022-08-17 17:27:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint867.pt (epoch 867 @ 70227 updates, score 56.93) (writing took 22.17273262143135 seconds)
2022-08-17 17:27:55 | INFO | fairseq_cli.train | end of epoch 867 (average epoch stats below)
2022-08-17 17:27:55 | INFO | train | epoch 867 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 6102.7 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 70227 | lr 0.000238659 | gnorm 0.329 | train_wall 40 | gb_free 10.1 | wall 69372
2022-08-17 17:27:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:27:56 | INFO | fairseq.trainer | begin training epoch 868
2022-08-17 17:27:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:28:34 | INFO | train_inner | epoch 868:     73 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=6634.3, ups=1.2, wpb=5538.7, bsz=357.8, num_updates=70300, lr=0.000238535, gnorm=0.329, train_wall=50, gb_free=10.1, wall=69410
2022-08-17 17:28:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:28:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:28:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:28:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:28:47 | INFO | valid | epoch 868 | valid on 'valid' subset | loss 5.174 | nll_loss 2.568 | ppl 5.93 | bleu 56.53 | wps 1825.3 | wpb 933.5 | bsz 59.6 | num_updates 70308 | best_bleu 57.52
2022-08-17 17:28:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 868 @ 70308 updates
2022-08-17 17:28:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint868.pt
2022-08-17 17:28:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint868.pt
2022-08-17 17:29:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint868.pt (epoch 868 @ 70308 updates, score 56.53) (writing took 14.826066337525845 seconds)
2022-08-17 17:29:02 | INFO | fairseq_cli.train | end of epoch 868 (average epoch stats below)
2022-08-17 17:29:02 | INFO | train | epoch 868 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 6672.1 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 70308 | lr 0.000238522 | gnorm 0.335 | train_wall 40 | gb_free 10.1 | wall 69439
2022-08-17 17:29:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:29:03 | INFO | fairseq.trainer | begin training epoch 869
2022-08-17 17:29:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:29:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:29:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:29:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:29:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:29:53 | INFO | valid | epoch 869 | valid on 'valid' subset | loss 5.195 | nll_loss 2.594 | ppl 6.04 | bleu 56.23 | wps 1978 | wpb 933.5 | bsz 59.6 | num_updates 70389 | best_bleu 57.52
2022-08-17 17:29:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 869 @ 70389 updates
2022-08-17 17:29:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint869.pt
2022-08-17 17:29:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint869.pt
2022-08-17 17:30:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint869.pt (epoch 869 @ 70389 updates, score 56.23) (writing took 38.06833231449127 seconds)
2022-08-17 17:30:32 | INFO | fairseq_cli.train | end of epoch 869 (average epoch stats below)
2022-08-17 17:30:32 | INFO | train | epoch 869 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 5010.2 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 70389 | lr 0.000238384 | gnorm 0.401 | train_wall 40 | gb_free 10.1 | wall 69528
2022-08-17 17:30:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:30:32 | INFO | fairseq.trainer | begin training epoch 870
2022-08-17 17:30:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:30:39 | INFO | train_inner | epoch 870:     11 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=4414.1, ups=0.8, wpb=5509.2, bsz=362, num_updates=70400, lr=0.000238366, gnorm=0.374, train_wall=48, gb_free=10, wall=69535
2022-08-17 17:31:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:31:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:31:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:31:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:31:24 | INFO | valid | epoch 870 | valid on 'valid' subset | loss 5.199 | nll_loss 2.602 | ppl 6.07 | bleu 56.67 | wps 1990.8 | wpb 933.5 | bsz 59.6 | num_updates 70470 | best_bleu 57.52
2022-08-17 17:31:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 870 @ 70470 updates
2022-08-17 17:31:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint870.pt
2022-08-17 17:31:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint870.pt
2022-08-17 17:31:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint870.pt (epoch 870 @ 70470 updates, score 56.67) (writing took 20.51460623368621 seconds)
2022-08-17 17:31:45 | INFO | fairseq_cli.train | end of epoch 870 (average epoch stats below)
2022-08-17 17:31:45 | INFO | train | epoch 870 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 6120.3 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 70470 | lr 0.000238247 | gnorm 0.305 | train_wall 41 | gb_free 10.1 | wall 69601
2022-08-17 17:31:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:31:45 | INFO | fairseq.trainer | begin training epoch 871
2022-08-17 17:31:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:32:04 | INFO | train_inner | epoch 871:     30 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=6470.4, ups=1.17, wpb=5523.2, bsz=356, num_updates=70500, lr=0.000238197, gnorm=0.309, train_wall=50, gb_free=10.1, wall=69620
2022-08-17 17:32:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:32:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:32:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:32:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:32:39 | INFO | valid | epoch 871 | valid on 'valid' subset | loss 5.183 | nll_loss 2.587 | ppl 6.01 | bleu 56.54 | wps 1883.8 | wpb 933.5 | bsz 59.6 | num_updates 70551 | best_bleu 57.52
2022-08-17 17:32:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 871 @ 70551 updates
2022-08-17 17:32:39 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint871.pt
2022-08-17 17:32:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint871.pt
2022-08-17 17:32:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint871.pt (epoch 871 @ 70551 updates, score 56.54) (writing took 17.79903193935752 seconds)
2022-08-17 17:32:57 | INFO | fairseq_cli.train | end of epoch 871 (average epoch stats below)
2022-08-17 17:32:57 | INFO | train | epoch 871 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 6158.6 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 70551 | lr 0.00023811 | gnorm 0.433 | train_wall 39 | gb_free 10.1 | wall 69674
2022-08-17 17:32:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:32:58 | INFO | fairseq.trainer | begin training epoch 872
2022-08-17 17:32:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:33:23 | INFO | train_inner | epoch 872:     49 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=6997.9, ups=1.26, wpb=5534.1, bsz=357, num_updates=70600, lr=0.000238028, gnorm=0.422, train_wall=49, gb_free=10.1, wall=69700
2022-08-17 17:33:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:33:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:33:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:33:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:33:48 | INFO | valid | epoch 872 | valid on 'valid' subset | loss 5.176 | nll_loss 2.575 | ppl 5.96 | bleu 56.38 | wps 1915 | wpb 933.5 | bsz 59.6 | num_updates 70632 | best_bleu 57.52
2022-08-17 17:33:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 872 @ 70632 updates
2022-08-17 17:33:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint872.pt
2022-08-17 17:33:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint872.pt
2022-08-17 17:34:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint872.pt (epoch 872 @ 70632 updates, score 56.38) (writing took 35.32261449843645 seconds)
2022-08-17 17:34:24 | INFO | fairseq_cli.train | end of epoch 872 (average epoch stats below)
2022-08-17 17:34:24 | INFO | train | epoch 872 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 5185.1 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 70632 | lr 0.000237974 | gnorm 0.316 | train_wall 39 | gb_free 10.1 | wall 69760
2022-08-17 17:34:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:34:24 | INFO | fairseq.trainer | begin training epoch 873
2022-08-17 17:34:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:35:00 | INFO | train_inner | epoch 873:     68 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=5757.5, ups=1.04, wpb=5548.1, bsz=362.6, num_updates=70700, lr=0.000237859, gnorm=0.305, train_wall=49, gb_free=10.1, wall=69796
2022-08-17 17:35:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:35:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:35:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:35:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:35:16 | INFO | valid | epoch 873 | valid on 'valid' subset | loss 5.182 | nll_loss 2.585 | ppl 6 | bleu 56.47 | wps 1671.1 | wpb 933.5 | bsz 59.6 | num_updates 70713 | best_bleu 57.52
2022-08-17 17:35:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 873 @ 70713 updates
2022-08-17 17:35:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint873.pt
2022-08-17 17:35:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint873.pt
2022-08-17 17:35:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint873.pt (epoch 873 @ 70713 updates, score 56.47) (writing took 29.77390817180276 seconds)
2022-08-17 17:35:46 | INFO | fairseq_cli.train | end of epoch 873 (average epoch stats below)
2022-08-17 17:35:46 | INFO | train | epoch 873 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 5447.5 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 70713 | lr 0.000237838 | gnorm 0.316 | train_wall 40 | gb_free 10.1 | wall 69842
2022-08-17 17:35:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:35:46 | INFO | fairseq.trainer | begin training epoch 874
2022-08-17 17:35:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:36:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:36:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:36:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:36:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:36:40 | INFO | valid | epoch 874 | valid on 'valid' subset | loss 5.181 | nll_loss 2.581 | ppl 5.98 | bleu 56.44 | wps 1847.2 | wpb 933.5 | bsz 59.6 | num_updates 70794 | best_bleu 57.52
2022-08-17 17:36:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 874 @ 70794 updates
2022-08-17 17:36:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint874.pt
2022-08-17 17:36:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint874.pt
2022-08-17 17:37:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint874.pt (epoch 874 @ 70794 updates, score 56.44) (writing took 19.8458566442132 seconds)
2022-08-17 17:37:00 | INFO | fairseq_cli.train | end of epoch 874 (average epoch stats below)
2022-08-17 17:37:00 | INFO | train | epoch 874 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 6000.7 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 70794 | lr 0.000237701 | gnorm 0.349 | train_wall 40 | gb_free 10.1 | wall 69917
2022-08-17 17:37:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:37:01 | INFO | fairseq.trainer | begin training epoch 875
2022-08-17 17:37:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:37:05 | INFO | train_inner | epoch 875:      6 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=4368.1, ups=0.8, wpb=5466.2, bsz=353.8, num_updates=70800, lr=0.000237691, gnorm=0.351, train_wall=49, gb_free=10.1, wall=69921
2022-08-17 17:37:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:37:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:37:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:37:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:37:52 | INFO | valid | epoch 875 | valid on 'valid' subset | loss 5.188 | nll_loss 2.586 | ppl 6.01 | bleu 56.33 | wps 1865.5 | wpb 933.5 | bsz 59.6 | num_updates 70875 | best_bleu 57.52
2022-08-17 17:37:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 875 @ 70875 updates
2022-08-17 17:37:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint875.pt
2022-08-17 17:37:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint875.pt
2022-08-17 17:38:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint875.pt (epoch 875 @ 70875 updates, score 56.33) (writing took 33.59840767458081 seconds)
2022-08-17 17:38:26 | INFO | fairseq_cli.train | end of epoch 875 (average epoch stats below)
2022-08-17 17:38:26 | INFO | train | epoch 875 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 5220.5 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 70875 | lr 0.000237566 | gnorm 0.299 | train_wall 40 | gb_free 10.1 | wall 70002
2022-08-17 17:38:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:38:26 | INFO | fairseq.trainer | begin training epoch 876
2022-08-17 17:38:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:38:40 | INFO | train_inner | epoch 876:     25 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=5783.9, ups=1.05, wpb=5520.6, bsz=354.7, num_updates=70900, lr=0.000237524, gnorm=0.307, train_wall=50, gb_free=10.1, wall=70016
2022-08-17 17:39:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:39:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:39:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:39:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:39:19 | INFO | valid | epoch 876 | valid on 'valid' subset | loss 5.179 | nll_loss 2.579 | ppl 5.98 | bleu 56.65 | wps 1758.9 | wpb 933.5 | bsz 59.6 | num_updates 70956 | best_bleu 57.52
2022-08-17 17:39:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 876 @ 70956 updates
2022-08-17 17:39:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint876.pt
2022-08-17 17:39:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint876.pt
2022-08-17 17:40:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint876.pt (epoch 876 @ 70956 updates, score 56.65) (writing took 41.96167711913586 seconds)
2022-08-17 17:40:01 | INFO | fairseq_cli.train | end of epoch 876 (average epoch stats below)
2022-08-17 17:40:01 | INFO | train | epoch 876 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 4729.7 | ups 0.86 | wpb 5523.2 | bsz 358 | num_updates 70956 | lr 0.00023743 | gnorm 0.329 | train_wall 41 | gb_free 10.1 | wall 70097
2022-08-17 17:40:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:40:01 | INFO | fairseq.trainer | begin training epoch 877
2022-08-17 17:40:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:40:25 | INFO | train_inner | epoch 877:     44 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=5277.5, ups=0.95, wpb=5541.9, bsz=362.2, num_updates=71000, lr=0.000237356, gnorm=0.32, train_wall=50, gb_free=10.1, wall=70121
2022-08-17 17:40:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:40:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:40:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:40:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:40:54 | INFO | valid | epoch 877 | valid on 'valid' subset | loss 5.188 | nll_loss 2.59 | ppl 6.02 | bleu 56.27 | wps 1855 | wpb 933.5 | bsz 59.6 | num_updates 71037 | best_bleu 57.52
2022-08-17 17:40:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 877 @ 71037 updates
2022-08-17 17:40:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint877.pt
2022-08-17 17:40:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint877.pt
2022-08-17 17:40:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint877.pt (epoch 877 @ 71037 updates, score 56.27) (writing took 2.7352214232087135 seconds)
2022-08-17 17:40:57 | INFO | fairseq_cli.train | end of epoch 877 (average epoch stats below)
2022-08-17 17:40:57 | INFO | train | epoch 877 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 7910.2 | ups 1.43 | wpb 5523.2 | bsz 358 | num_updates 71037 | lr 0.000237295 | gnorm 0.325 | train_wall 42 | gb_free 10.1 | wall 70153
2022-08-17 17:40:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:40:57 | INFO | fairseq.trainer | begin training epoch 878
2022-08-17 17:40:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:41:35 | INFO | train_inner | epoch 878:     63 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=7949.9, ups=1.44, wpb=5532.4, bsz=361.2, num_updates=71100, lr=0.000237189, gnorm=0.34, train_wall=52, gb_free=10.1, wall=70191
2022-08-17 17:41:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:41:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:41:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:41:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:41:55 | INFO | valid | epoch 878 | valid on 'valid' subset | loss 5.192 | nll_loss 2.598 | ppl 6.06 | bleu 56.95 | wps 1796.9 | wpb 933.5 | bsz 59.6 | num_updates 71118 | best_bleu 57.52
2022-08-17 17:41:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 878 @ 71118 updates
2022-08-17 17:41:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint878.pt
2022-08-17 17:41:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint878.pt
2022-08-17 17:42:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint878.pt (epoch 878 @ 71118 updates, score 56.95) (writing took 24.966082219034433 seconds)
2022-08-17 17:42:20 | INFO | fairseq_cli.train | end of epoch 878 (average epoch stats below)
2022-08-17 17:42:20 | INFO | train | epoch 878 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 5417.6 | ups 0.98 | wpb 5523.2 | bsz 358 | num_updates 71118 | lr 0.000237159 | gnorm 0.363 | train_wall 41 | gb_free 10.3 | wall 70236
2022-08-17 17:42:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:42:20 | INFO | fairseq.trainer | begin training epoch 879
2022-08-17 17:42:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:43:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:43:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:43:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:43:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:43:12 | INFO | valid | epoch 879 | valid on 'valid' subset | loss 5.195 | nll_loss 2.596 | ppl 6.05 | bleu 56.56 | wps 1805.2 | wpb 933.5 | bsz 59.6 | num_updates 71199 | best_bleu 57.52
2022-08-17 17:43:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 879 @ 71199 updates
2022-08-17 17:43:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint879.pt
2022-08-17 17:43:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint879.pt
2022-08-17 17:43:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint879.pt (epoch 879 @ 71199 updates, score 56.56) (writing took 39.704748176038265 seconds)
2022-08-17 17:43:52 | INFO | fairseq_cli.train | end of epoch 879 (average epoch stats below)
2022-08-17 17:43:52 | INFO | train | epoch 879 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 4877.1 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 71199 | lr 0.000237024 | gnorm 0.34 | train_wall 41 | gb_free 10.1 | wall 70328
2022-08-17 17:43:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:43:52 | INFO | fairseq.trainer | begin training epoch 880
2022-08-17 17:43:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:43:53 | INFO | train_inner | epoch 880:      1 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=3977.3, ups=0.72, wpb=5509.4, bsz=354.4, num_updates=71200, lr=0.000237023, gnorm=0.361, train_wall=50, gb_free=10.1, wall=70330
2022-08-17 17:44:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:44:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:44:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:44:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:44:44 | INFO | valid | epoch 880 | valid on 'valid' subset | loss 5.188 | nll_loss 2.594 | ppl 6.04 | bleu 56.11 | wps 1883.1 | wpb 933.5 | bsz 59.6 | num_updates 71280 | best_bleu 57.52
2022-08-17 17:44:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 880 @ 71280 updates
2022-08-17 17:44:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint880.pt
2022-08-17 17:44:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint880.pt
2022-08-17 17:44:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint880.pt (epoch 880 @ 71280 updates, score 56.11) (writing took 2.5324736312031746 seconds)
2022-08-17 17:44:47 | INFO | fairseq_cli.train | end of epoch 880 (average epoch stats below)
2022-08-17 17:44:47 | INFO | train | epoch 880 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 8129.8 | ups 1.47 | wpb 5523.2 | bsz 358 | num_updates 71280 | lr 0.00023689 | gnorm 0.362 | train_wall 41 | gb_free 10.1 | wall 70383
2022-08-17 17:44:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:44:47 | INFO | fairseq.trainer | begin training epoch 881
2022-08-17 17:44:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:44:59 | INFO | train_inner | epoch 881:     20 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=8398.7, ups=1.52, wpb=5525.3, bsz=357.2, num_updates=71300, lr=0.000236856, gnorm=0.339, train_wall=50, gb_free=10.1, wall=70395
2022-08-17 17:45:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:45:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:45:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:45:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:45:41 | INFO | valid | epoch 881 | valid on 'valid' subset | loss 5.18 | nll_loss 2.583 | ppl 5.99 | bleu 56.94 | wps 1840.8 | wpb 933.5 | bsz 59.6 | num_updates 71361 | best_bleu 57.52
2022-08-17 17:45:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 881 @ 71361 updates
2022-08-17 17:45:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint881.pt
2022-08-17 17:45:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint881.pt
2022-08-17 17:45:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint881.pt (epoch 881 @ 71361 updates, score 56.94) (writing took 14.287793152034283 seconds)
2022-08-17 17:45:56 | INFO | fairseq_cli.train | end of epoch 881 (average epoch stats below)
2022-08-17 17:45:56 | INFO | train | epoch 881 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 6469.6 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 71361 | lr 0.000236755 | gnorm 0.311 | train_wall 41 | gb_free 10.2 | wall 70452
2022-08-17 17:45:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:45:56 | INFO | fairseq.trainer | begin training epoch 882
2022-08-17 17:45:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:46:17 | INFO | train_inner | epoch 882:     39 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=7130.7, ups=1.29, wpb=5539.6, bsz=356.4, num_updates=71400, lr=0.000236691, gnorm=0.338, train_wall=50, gb_free=10.1, wall=70473
2022-08-17 17:46:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:46:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:46:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:46:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:46:47 | INFO | valid | epoch 882 | valid on 'valid' subset | loss 5.203 | nll_loss 2.612 | ppl 6.11 | bleu 56.17 | wps 1931.3 | wpb 933.5 | bsz 59.6 | num_updates 71442 | best_bleu 57.52
2022-08-17 17:46:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 882 @ 71442 updates
2022-08-17 17:46:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint882.pt
2022-08-17 17:46:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint882.pt
2022-08-17 17:46:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint882.pt (epoch 882 @ 71442 updates, score 56.17) (writing took 2.3927623592317104 seconds)
2022-08-17 17:46:49 | INFO | fairseq_cli.train | end of epoch 882 (average epoch stats below)
2022-08-17 17:46:49 | INFO | train | epoch 882 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 8340.4 | ups 1.51 | wpb 5523.2 | bsz 358 | num_updates 71442 | lr 0.000236621 | gnorm 0.339 | train_wall 40 | gb_free 10.2 | wall 70506
2022-08-17 17:46:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:46:50 | INFO | fairseq.trainer | begin training epoch 883
2022-08-17 17:46:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:47:20 | INFO | train_inner | epoch 883:     58 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=8730.5, ups=1.58, wpb=5526.4, bsz=359.7, num_updates=71500, lr=0.000236525, gnorm=0.334, train_wall=50, gb_free=10, wall=70536
2022-08-17 17:47:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:47:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:47:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:47:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:47:41 | INFO | valid | epoch 883 | valid on 'valid' subset | loss 5.196 | nll_loss 2.599 | ppl 6.06 | bleu 56.21 | wps 1945.9 | wpb 933.5 | bsz 59.6 | num_updates 71523 | best_bleu 57.52
2022-08-17 17:47:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 883 @ 71523 updates
2022-08-17 17:47:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint883.pt
2022-08-17 17:47:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint883.pt
2022-08-17 17:48:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint883.pt (epoch 883 @ 71523 updates, score 56.21) (writing took 28.686075128614902 seconds)
2022-08-17 17:48:10 | INFO | fairseq_cli.train | end of epoch 883 (average epoch stats below)
2022-08-17 17:48:10 | INFO | train | epoch 883 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 5574.7 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 71523 | lr 0.000236487 | gnorm 0.345 | train_wall 41 | gb_free 10.2 | wall 70586
2022-08-17 17:48:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:48:10 | INFO | fairseq.trainer | begin training epoch 884
2022-08-17 17:48:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:48:49 | INFO | train_inner | epoch 884:     77 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=6188.3, ups=1.12, wpb=5526.5, bsz=361.5, num_updates=71600, lr=0.00023636, gnorm=0.295, train_wall=50, gb_free=10.1, wall=70626
2022-08-17 17:48:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:48:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:48:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:48:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:49:01 | INFO | valid | epoch 884 | valid on 'valid' subset | loss 5.187 | nll_loss 2.589 | ppl 6.02 | bleu 57.18 | wps 1786.7 | wpb 933.5 | bsz 59.6 | num_updates 71604 | best_bleu 57.52
2022-08-17 17:49:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 884 @ 71604 updates
2022-08-17 17:49:01 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint884.pt
2022-08-17 17:49:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint884.pt
2022-08-17 17:49:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint884.pt (epoch 884 @ 71604 updates, score 57.18) (writing took 27.01500114798546 seconds)
2022-08-17 17:49:28 | INFO | fairseq_cli.train | end of epoch 884 (average epoch stats below)
2022-08-17 17:49:28 | INFO | train | epoch 884 | loss 3.375 | nll_loss 0.343 | ppl 1.27 | wps 5710.1 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 71604 | lr 0.000236353 | gnorm 0.295 | train_wall 40 | gb_free 10.2 | wall 70664
2022-08-17 17:49:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:49:28 | INFO | fairseq.trainer | begin training epoch 885
2022-08-17 17:49:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:50:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:50:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:50:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:50:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:50:21 | INFO | valid | epoch 885 | valid on 'valid' subset | loss 5.194 | nll_loss 2.599 | ppl 6.06 | bleu 56.01 | wps 1889.2 | wpb 933.5 | bsz 59.6 | num_updates 71685 | best_bleu 57.52
2022-08-17 17:50:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 885 @ 71685 updates
2022-08-17 17:50:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint885.pt
2022-08-17 17:50:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint885.pt
2022-08-17 17:50:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint885.pt (epoch 885 @ 71685 updates, score 56.01) (writing took 18.823845997452736 seconds)
2022-08-17 17:50:40 | INFO | fairseq_cli.train | end of epoch 885 (average epoch stats below)
2022-08-17 17:50:40 | INFO | train | epoch 885 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 6244.5 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 71685 | lr 0.00023622 | gnorm 0.339 | train_wall 41 | gb_free 10.1 | wall 70736
2022-08-17 17:50:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:50:40 | INFO | fairseq.trainer | begin training epoch 886
2022-08-17 17:50:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:50:49 | INFO | train_inner | epoch 886:     15 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=4621, ups=0.84, wpb=5514.8, bsz=357.6, num_updates=71700, lr=0.000236195, gnorm=0.349, train_wall=50, gb_free=10.1, wall=70745
2022-08-17 17:51:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:51:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:51:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:51:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:51:34 | INFO | valid | epoch 886 | valid on 'valid' subset | loss 5.197 | nll_loss 2.601 | ppl 6.07 | bleu 55.97 | wps 1637 | wpb 933.5 | bsz 59.6 | num_updates 71766 | best_bleu 57.52
2022-08-17 17:51:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 886 @ 71766 updates
2022-08-17 17:51:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint886.pt
2022-08-17 17:51:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint886.pt
2022-08-17 17:51:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint886.pt (epoch 886 @ 71766 updates, score 55.97) (writing took 22.84782387688756 seconds)
2022-08-17 17:51:57 | INFO | fairseq_cli.train | end of epoch 886 (average epoch stats below)
2022-08-17 17:51:57 | INFO | train | epoch 886 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 5773.2 | ups 1.05 | wpb 5523.2 | bsz 358 | num_updates 71766 | lr 0.000236086 | gnorm 0.336 | train_wall 41 | gb_free 10.1 | wall 70813
2022-08-17 17:51:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:51:57 | INFO | fairseq.trainer | begin training epoch 887
2022-08-17 17:51:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:52:16 | INFO | train_inner | epoch 887:     34 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=6356.3, ups=1.15, wpb=5522.2, bsz=359.6, num_updates=71800, lr=0.00023603, gnorm=0.327, train_wall=50, gb_free=10, wall=70832
2022-08-17 17:52:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:52:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:52:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:52:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:52:48 | INFO | valid | epoch 887 | valid on 'valid' subset | loss 5.183 | nll_loss 2.582 | ppl 5.99 | bleu 56.02 | wps 1772.1 | wpb 933.5 | bsz 59.6 | num_updates 71847 | best_bleu 57.52
2022-08-17 17:52:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 887 @ 71847 updates
2022-08-17 17:52:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint887.pt
2022-08-17 17:52:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint887.pt
2022-08-17 17:53:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint887.pt (epoch 887 @ 71847 updates, score 56.02) (writing took 28.78225139901042 seconds)
2022-08-17 17:53:17 | INFO | fairseq_cli.train | end of epoch 887 (average epoch stats below)
2022-08-17 17:53:17 | INFO | train | epoch 887 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 5608.3 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 71847 | lr 0.000235953 | gnorm 0.308 | train_wall 39 | gb_free 10.1 | wall 70893
2022-08-17 17:53:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:53:17 | INFO | fairseq.trainer | begin training epoch 888
2022-08-17 17:53:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:53:45 | INFO | train_inner | epoch 888:     53 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=6155.5, ups=1.12, wpb=5513.6, bsz=355.2, num_updates=71900, lr=0.000235866, gnorm=0.306, train_wall=48, gb_free=10.2, wall=70922
2022-08-17 17:54:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:54:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:54:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:54:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:54:09 | INFO | valid | epoch 888 | valid on 'valid' subset | loss 5.19 | nll_loss 2.592 | ppl 6.03 | bleu 56.22 | wps 1864.9 | wpb 933.5 | bsz 59.6 | num_updates 71928 | best_bleu 57.52
2022-08-17 17:54:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 888 @ 71928 updates
2022-08-17 17:54:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint888.pt
2022-08-17 17:54:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint888.pt
2022-08-17 17:54:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint888.pt (epoch 888 @ 71928 updates, score 56.22) (writing took 16.934599574655294 seconds)
2022-08-17 17:54:26 | INFO | fairseq_cli.train | end of epoch 888 (average epoch stats below)
2022-08-17 17:54:26 | INFO | train | epoch 888 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 6436.2 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 71928 | lr 0.00023582 | gnorm 0.317 | train_wall 39 | gb_free 10.1 | wall 70963
2022-08-17 17:54:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:54:27 | INFO | fairseq.trainer | begin training epoch 889
2022-08-17 17:54:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:55:03 | INFO | train_inner | epoch 889:     72 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=7155.6, ups=1.29, wpb=5536.3, bsz=358.5, num_updates=72000, lr=0.000235702, gnorm=0.353, train_wall=49, gb_free=10.1, wall=70999
2022-08-17 17:55:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:55:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:55:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:55:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:55:16 | INFO | valid | epoch 889 | valid on 'valid' subset | loss 5.188 | nll_loss 2.591 | ppl 6.03 | bleu 56.19 | wps 1851.2 | wpb 933.5 | bsz 59.6 | num_updates 72009 | best_bleu 57.52
2022-08-17 17:55:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 889 @ 72009 updates
2022-08-17 17:55:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint889.pt
2022-08-17 17:55:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint889.pt
2022-08-17 17:55:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint889.pt (epoch 889 @ 72009 updates, score 56.19) (writing took 34.96541693806648 seconds)
2022-08-17 17:55:51 | INFO | fairseq_cli.train | end of epoch 889 (average epoch stats below)
2022-08-17 17:55:51 | INFO | train | epoch 889 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 5265.7 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 72009 | lr 0.000235688 | gnorm 0.349 | train_wall 39 | gb_free 10.1 | wall 71048
2022-08-17 17:55:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:55:52 | INFO | fairseq.trainer | begin training epoch 890
2022-08-17 17:55:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:56:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:56:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:56:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:56:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:56:43 | INFO | valid | epoch 890 | valid on 'valid' subset | loss 5.181 | nll_loss 2.582 | ppl 5.99 | bleu 57.21 | wps 1744.5 | wpb 933.5 | bsz 59.6 | num_updates 72090 | best_bleu 57.52
2022-08-17 17:56:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 890 @ 72090 updates
2022-08-17 17:56:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint890.pt
2022-08-17 17:56:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint890.pt
2022-08-17 17:57:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint890.pt (epoch 890 @ 72090 updates, score 57.21) (writing took 21.23448769748211 seconds)
2022-08-17 17:57:05 | INFO | fairseq_cli.train | end of epoch 890 (average epoch stats below)
2022-08-17 17:57:05 | INFO | train | epoch 890 | loss 3.375 | nll_loss 0.343 | ppl 1.27 | wps 6115.3 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 72090 | lr 0.000235555 | gnorm 0.315 | train_wall 40 | gb_free 10.2 | wall 71121
2022-08-17 17:57:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:57:05 | INFO | fairseq.trainer | begin training epoch 891
2022-08-17 17:57:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:57:12 | INFO | train_inner | epoch 891:     10 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=4273.6, ups=0.78, wpb=5512.9, bsz=357, num_updates=72100, lr=0.000235539, gnorm=0.319, train_wall=49, gb_free=10.1, wall=71128
2022-08-17 17:57:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:57:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:57:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:57:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:57:57 | INFO | valid | epoch 891 | valid on 'valid' subset | loss 5.196 | nll_loss 2.6 | ppl 6.06 | bleu 56.87 | wps 1829.1 | wpb 933.5 | bsz 59.6 | num_updates 72171 | best_bleu 57.52
2022-08-17 17:57:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 891 @ 72171 updates
2022-08-17 17:57:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint891.pt
2022-08-17 17:57:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint891.pt
2022-08-17 17:58:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint891.pt (epoch 891 @ 72171 updates, score 56.87) (writing took 14.011386562138796 seconds)
2022-08-17 17:58:12 | INFO | fairseq_cli.train | end of epoch 891 (average epoch stats below)
2022-08-17 17:58:12 | INFO | train | epoch 891 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 6684.8 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 72171 | lr 0.000235423 | gnorm 0.309 | train_wall 40 | gb_free 10.1 | wall 71188
2022-08-17 17:58:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:58:12 | INFO | fairseq.trainer | begin training epoch 892
2022-08-17 17:58:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 17:58:29 | INFO | train_inner | epoch 892:     29 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=7180.1, ups=1.3, wpb=5538.1, bsz=357, num_updates=72200, lr=0.000235376, gnorm=0.301, train_wall=50, gb_free=10.1, wall=71205
2022-08-17 17:58:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 17:58:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 17:58:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 17:58:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 17:59:04 | INFO | valid | epoch 892 | valid on 'valid' subset | loss 5.196 | nll_loss 2.598 | ppl 6.06 | bleu 56.27 | wps 1898.7 | wpb 933.5 | bsz 59.6 | num_updates 72252 | best_bleu 57.52
2022-08-17 17:59:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 892 @ 72252 updates
2022-08-17 17:59:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint892.pt
2022-08-17 17:59:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint892.pt
2022-08-17 17:59:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint892.pt (epoch 892 @ 72252 updates, score 56.27) (writing took 37.712797820568085 seconds)
2022-08-17 17:59:42 | INFO | fairseq_cli.train | end of epoch 892 (average epoch stats below)
2022-08-17 17:59:42 | INFO | train | epoch 892 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 4956.6 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 72252 | lr 0.000235291 | gnorm 0.319 | train_wall 39 | gb_free 10.3 | wall 71278
2022-08-17 17:59:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 17:59:42 | INFO | fairseq.trainer | begin training epoch 893
2022-08-17 17:59:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:00:12 | INFO | train_inner | epoch 893:     48 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=5333.7, ups=0.97, wpb=5508.1, bsz=361.4, num_updates=72300, lr=0.000235213, gnorm=0.322, train_wall=49, gb_free=10.1, wall=71308
2022-08-17 18:00:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:00:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:00:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:00:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:00:39 | INFO | valid | epoch 893 | valid on 'valid' subset | loss 5.205 | nll_loss 2.611 | ppl 6.11 | bleu 56.24 | wps 1922.4 | wpb 933.5 | bsz 59.6 | num_updates 72333 | best_bleu 57.52
2022-08-17 18:00:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 893 @ 72333 updates
2022-08-17 18:00:39 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint893.pt
2022-08-17 18:00:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint893.pt
2022-08-17 18:01:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint893.pt (epoch 893 @ 72333 updates, score 56.24) (writing took 28.645326301455498 seconds)
2022-08-17 18:01:08 | INFO | fairseq_cli.train | end of epoch 893 (average epoch stats below)
2022-08-17 18:01:08 | INFO | train | epoch 893 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 5174.9 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 72333 | lr 0.000235159 | gnorm 0.312 | train_wall 40 | gb_free 10.1 | wall 71364
2022-08-17 18:01:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:01:08 | INFO | fairseq.trainer | begin training epoch 894
2022-08-17 18:01:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:01:44 | INFO | train_inner | epoch 894:     67 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=5997.5, ups=1.08, wpb=5533.1, bsz=357.3, num_updates=72400, lr=0.00023505, gnorm=0.324, train_wall=49, gb_free=10.1, wall=71401
2022-08-17 18:01:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:01:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:01:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:01:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:02:00 | INFO | valid | epoch 894 | valid on 'valid' subset | loss 5.179 | nll_loss 2.579 | ppl 5.97 | bleu 56.35 | wps 1942.3 | wpb 933.5 | bsz 59.6 | num_updates 72414 | best_bleu 57.52
2022-08-17 18:02:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 894 @ 72414 updates
2022-08-17 18:02:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint894.pt
2022-08-17 18:02:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint894.pt
2022-08-17 18:02:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint894.pt (epoch 894 @ 72414 updates, score 56.35) (writing took 16.507587131112814 seconds)
2022-08-17 18:02:17 | INFO | fairseq_cli.train | end of epoch 894 (average epoch stats below)
2022-08-17 18:02:17 | INFO | train | epoch 894 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 6527.6 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 72414 | lr 0.000235028 | gnorm 0.322 | train_wall 39 | gb_free 10.1 | wall 71433
2022-08-17 18:02:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:02:17 | INFO | fairseq.trainer | begin training epoch 895
2022-08-17 18:02:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:02:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:03:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:03:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:03:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:03:09 | INFO | valid | epoch 895 | valid on 'valid' subset | loss 5.172 | nll_loss 2.57 | ppl 5.94 | bleu 56.77 | wps 1893.3 | wpb 933.5 | bsz 59.6 | num_updates 72495 | best_bleu 57.52
2022-08-17 18:03:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 895 @ 72495 updates
2022-08-17 18:03:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint895.pt
2022-08-17 18:03:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint895.pt
2022-08-17 18:03:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint895.pt (epoch 895 @ 72495 updates, score 56.77) (writing took 37.76541827246547 seconds)
2022-08-17 18:03:47 | INFO | fairseq_cli.train | end of epoch 895 (average epoch stats below)
2022-08-17 18:03:47 | INFO | train | epoch 895 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 4981.1 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 72495 | lr 0.000234896 | gnorm 0.288 | train_wall 40 | gb_free 10.2 | wall 71523
2022-08-17 18:03:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:03:47 | INFO | fairseq.trainer | begin training epoch 896
2022-08-17 18:03:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:03:51 | INFO | train_inner | epoch 896:      5 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=4353.8, ups=0.79, wpb=5497.6, bsz=357.1, num_updates=72500, lr=0.000234888, gnorm=0.284, train_wall=49, gb_free=10.1, wall=71527
2022-08-17 18:04:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:04:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:04:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:04:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:04:39 | INFO | valid | epoch 896 | valid on 'valid' subset | loss 5.177 | nll_loss 2.579 | ppl 5.98 | bleu 56.69 | wps 1928 | wpb 933.5 | bsz 59.6 | num_updates 72576 | best_bleu 57.52
2022-08-17 18:04:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 896 @ 72576 updates
2022-08-17 18:04:39 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint896.pt
2022-08-17 18:04:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint896.pt
2022-08-17 18:05:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint896.pt (epoch 896 @ 72576 updates, score 56.69) (writing took 21.248330753296614 seconds)
2022-08-17 18:05:00 | INFO | fairseq_cli.train | end of epoch 896 (average epoch stats below)
2022-08-17 18:05:00 | INFO | train | epoch 896 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6071.3 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 72576 | lr 0.000234765 | gnorm 0.314 | train_wall 40 | gb_free 10.1 | wall 71596
2022-08-17 18:05:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:05:00 | INFO | fairseq.trainer | begin training epoch 897
2022-08-17 18:05:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:05:14 | INFO | train_inner | epoch 897:     24 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=6663.9, ups=1.2, wpb=5540.1, bsz=357.9, num_updates=72600, lr=0.000234726, gnorm=0.323, train_wall=50, gb_free=10, wall=71610
2022-08-17 18:05:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:05:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:05:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:05:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:05:51 | INFO | valid | epoch 897 | valid on 'valid' subset | loss 5.178 | nll_loss 2.578 | ppl 5.97 | bleu 56.91 | wps 1973.8 | wpb 933.5 | bsz 59.6 | num_updates 72657 | best_bleu 57.52
2022-08-17 18:05:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 897 @ 72657 updates
2022-08-17 18:05:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint897.pt
2022-08-17 18:05:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint897.pt
2022-08-17 18:06:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint897.pt (epoch 897 @ 72657 updates, score 56.91) (writing took 14.922286592423916 seconds)
2022-08-17 18:06:06 | INFO | fairseq_cli.train | end of epoch 897 (average epoch stats below)
2022-08-17 18:06:06 | INFO | train | epoch 897 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 6807.9 | ups 1.23 | wpb 5523.2 | bsz 358 | num_updates 72657 | lr 0.000234634 | gnorm 0.314 | train_wall 40 | gb_free 10.3 | wall 71662
2022-08-17 18:06:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:06:06 | INFO | fairseq.trainer | begin training epoch 898
2022-08-17 18:06:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:06:30 | INFO | train_inner | epoch 898:     43 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=7233.1, ups=1.3, wpb=5542.7, bsz=356.9, num_updates=72700, lr=0.000234565, gnorm=0.336, train_wall=49, gb_free=10, wall=71687
2022-08-17 18:06:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:06:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:06:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:06:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:07:00 | INFO | valid | epoch 898 | valid on 'valid' subset | loss 5.185 | nll_loss 2.586 | ppl 6.01 | bleu 56.52 | wps 1921 | wpb 933.5 | bsz 59.6 | num_updates 72738 | best_bleu 57.52
2022-08-17 18:07:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 898 @ 72738 updates
2022-08-17 18:07:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint898.pt
2022-08-17 18:07:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint898.pt
2022-08-17 18:07:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint898.pt (epoch 898 @ 72738 updates, score 56.52) (writing took 36.92863253131509 seconds)
2022-08-17 18:07:37 | INFO | fairseq_cli.train | end of epoch 898 (average epoch stats below)
2022-08-17 18:07:37 | INFO | train | epoch 898 | loss 3.375 | nll_loss 0.343 | ppl 1.27 | wps 4931.8 | ups 0.89 | wpb 5523.2 | bsz 358 | num_updates 72738 | lr 0.000234503 | gnorm 0.337 | train_wall 40 | gb_free 10.2 | wall 71753
2022-08-17 18:07:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:07:37 | INFO | fairseq.trainer | begin training epoch 899
2022-08-17 18:07:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:08:09 | INFO | train_inner | epoch 899:     62 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=5550.7, ups=1.01, wpb=5496.3, bsz=360.7, num_updates=72800, lr=0.000234404, gnorm=0.333, train_wall=50, gb_free=10, wall=71786
2022-08-17 18:08:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:08:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:08:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:08:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:08:29 | INFO | valid | epoch 899 | valid on 'valid' subset | loss 5.181 | nll_loss 2.58 | ppl 5.98 | bleu 56.6 | wps 1740 | wpb 933.5 | bsz 59.6 | num_updates 72819 | best_bleu 57.52
2022-08-17 18:08:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 899 @ 72819 updates
2022-08-17 18:08:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint899.pt
2022-08-17 18:08:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint899.pt
2022-08-17 18:08:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint899.pt (epoch 899 @ 72819 updates, score 56.6) (writing took 2.5884628742933273 seconds)
2022-08-17 18:08:32 | INFO | fairseq_cli.train | end of epoch 899 (average epoch stats below)
2022-08-17 18:08:32 | INFO | train | epoch 899 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 8154.6 | ups 1.48 | wpb 5523.2 | bsz 358 | num_updates 72819 | lr 0.000234373 | gnorm 0.372 | train_wall 40 | gb_free 10.2 | wall 71808
2022-08-17 18:08:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:08:32 | INFO | fairseq.trainer | begin training epoch 900
2022-08-17 18:08:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:09:14 | INFO | train_inner | epoch 900:     81 / 81 loss=3.375, nll_loss=0.343, ppl=1.27, wps=8523.8, ups=1.54, wpb=5521.4, bsz=355.4, num_updates=72900, lr=0.000234243, gnorm=0.384, train_wall=50, gb_free=10.2, wall=71850
2022-08-17 18:09:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:09:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:09:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:09:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:09:24 | INFO | valid | epoch 900 | valid on 'valid' subset | loss 5.178 | nll_loss 2.573 | ppl 5.95 | bleu 56.68 | wps 1810.9 | wpb 933.5 | bsz 59.6 | num_updates 72900 | best_bleu 57.52
2022-08-17 18:09:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 900 @ 72900 updates
2022-08-17 18:09:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint900.pt
2022-08-17 18:09:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint900.pt
2022-08-17 18:09:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint900.pt (epoch 900 @ 72900 updates, score 56.68) (writing took 30.480437595397234 seconds)
2022-08-17 18:09:54 | INFO | fairseq_cli.train | end of epoch 900 (average epoch stats below)
2022-08-17 18:09:54 | INFO | train | epoch 900 | loss 3.375 | nll_loss 0.343 | ppl 1.27 | wps 5399.6 | ups 0.98 | wpb 5523.2 | bsz 358 | num_updates 72900 | lr 0.000234243 | gnorm 0.375 | train_wall 41 | gb_free 10.2 | wall 71891
2022-08-17 18:09:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:09:55 | INFO | fairseq.trainer | begin training epoch 901
2022-08-17 18:09:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:10:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:10:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:10:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:10:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:10:45 | INFO | valid | epoch 901 | valid on 'valid' subset | loss 5.169 | nll_loss 2.567 | ppl 5.93 | bleu 56.36 | wps 1857.5 | wpb 933.5 | bsz 59.6 | num_updates 72981 | best_bleu 57.52
2022-08-17 18:10:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 901 @ 72981 updates
2022-08-17 18:10:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint901.pt
2022-08-17 18:10:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint901.pt
2022-08-17 18:11:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint901.pt (epoch 901 @ 72981 updates, score 56.36) (writing took 32.031988345086575 seconds)
2022-08-17 18:11:17 | INFO | fairseq_cli.train | end of epoch 901 (average epoch stats below)
2022-08-17 18:11:17 | INFO | train | epoch 901 | loss 3.375 | nll_loss 0.343 | ppl 1.27 | wps 5405.8 | ups 0.98 | wpb 5523.2 | bsz 358 | num_updates 72981 | lr 0.000234113 | gnorm 0.408 | train_wall 39 | gb_free 10.1 | wall 71973
2022-08-17 18:11:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:11:17 | INFO | fairseq.trainer | begin training epoch 902
2022-08-17 18:11:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:11:29 | INFO | train_inner | epoch 902:     19 / 81 loss=3.375, nll_loss=0.342, ppl=1.27, wps=4115.1, ups=0.74, wpb=5528, bsz=357.6, num_updates=73000, lr=0.000234082, gnorm=0.393, train_wall=49, gb_free=10.1, wall=71985
2022-08-17 18:11:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:12:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:12:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:12:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:12:09 | INFO | valid | epoch 902 | valid on 'valid' subset | loss 5.182 | nll_loss 2.581 | ppl 5.98 | bleu 57.1 | wps 1883.7 | wpb 933.5 | bsz 59.6 | num_updates 73062 | best_bleu 57.52
2022-08-17 18:12:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 902 @ 73062 updates
2022-08-17 18:12:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint902.pt
2022-08-17 18:12:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint902.pt
2022-08-17 18:12:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint902.pt (epoch 902 @ 73062 updates, score 57.1) (writing took 16.328381966799498 seconds)
2022-08-17 18:12:25 | INFO | fairseq_cli.train | end of epoch 902 (average epoch stats below)
2022-08-17 18:12:25 | INFO | train | epoch 902 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 6589 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 73062 | lr 0.000233983 | gnorm 0.392 | train_wall 40 | gb_free 10.1 | wall 72041
2022-08-17 18:12:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:12:25 | INFO | fairseq.trainer | begin training epoch 903
2022-08-17 18:12:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:12:49 | INFO | train_inner | epoch 903:     38 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=6878.7, ups=1.25, wpb=5523.8, bsz=357.3, num_updates=73100, lr=0.000233922, gnorm=0.368, train_wall=49, gb_free=10.1, wall=72065
2022-08-17 18:13:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:13:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:13:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:13:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:13:21 | INFO | valid | epoch 903 | valid on 'valid' subset | loss 5.198 | nll_loss 2.599 | ppl 6.06 | bleu 56.32 | wps 1674.4 | wpb 933.5 | bsz 59.6 | num_updates 73143 | best_bleu 57.52
2022-08-17 18:13:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 903 @ 73143 updates
2022-08-17 18:13:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint903.pt
2022-08-17 18:13:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint903.pt
2022-08-17 18:13:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint903.pt (epoch 903 @ 73143 updates, score 56.32) (writing took 15.120012644678354 seconds)
2022-08-17 18:13:37 | INFO | fairseq_cli.train | end of epoch 903 (average epoch stats below)
2022-08-17 18:13:37 | INFO | train | epoch 903 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 6247.6 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 73143 | lr 0.000233853 | gnorm 0.319 | train_wall 40 | gb_free 10.3 | wall 72113
2022-08-17 18:13:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:13:37 | INFO | fairseq.trainer | begin training epoch 904
2022-08-17 18:13:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:14:12 | INFO | train_inner | epoch 904:     57 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=6682.4, ups=1.21, wpb=5538.5, bsz=364.4, num_updates=73200, lr=0.000233762, gnorm=0.321, train_wall=49, gb_free=10.1, wall=72148
2022-08-17 18:14:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:14:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:14:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:14:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:14:33 | INFO | valid | epoch 904 | valid on 'valid' subset | loss 5.179 | nll_loss 2.579 | ppl 5.98 | bleu 56.79 | wps 1784 | wpb 933.5 | bsz 59.6 | num_updates 73224 | best_bleu 57.52
2022-08-17 18:14:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 904 @ 73224 updates
2022-08-17 18:14:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint904.pt
2022-08-17 18:14:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint904.pt
2022-08-17 18:14:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint904.pt (epoch 904 @ 73224 updates, score 56.79) (writing took 25.224212557077408 seconds)
2022-08-17 18:14:58 | INFO | fairseq_cli.train | end of epoch 904 (average epoch stats below)
2022-08-17 18:14:58 | INFO | train | epoch 904 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 5480.1 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 73224 | lr 0.000233724 | gnorm 0.459 | train_wall 39 | gb_free 10.2 | wall 72195
2022-08-17 18:14:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:14:59 | INFO | fairseq.trainer | begin training epoch 905
2022-08-17 18:14:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:15:38 | INFO | train_inner | epoch 905:     76 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=6392.9, ups=1.16, wpb=5522.6, bsz=354.7, num_updates=73300, lr=0.000233603, gnorm=0.46, train_wall=48, gb_free=10.1, wall=72234
2022-08-17 18:15:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:15:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:15:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:15:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:15:50 | INFO | valid | epoch 905 | valid on 'valid' subset | loss 5.196 | nll_loss 2.599 | ppl 6.06 | bleu 56.45 | wps 1844.5 | wpb 933.5 | bsz 59.6 | num_updates 73305 | best_bleu 57.52
2022-08-17 18:15:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 905 @ 73305 updates
2022-08-17 18:15:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint905.pt
2022-08-17 18:15:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint905.pt
2022-08-17 18:16:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint905.pt (epoch 905 @ 73305 updates, score 56.45) (writing took 15.336433917284012 seconds)
2022-08-17 18:16:05 | INFO | fairseq_cli.train | end of epoch 905 (average epoch stats below)
2022-08-17 18:16:05 | INFO | train | epoch 905 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 6686.3 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 73305 | lr 0.000233595 | gnorm 0.347 | train_wall 39 | gb_free 10.1 | wall 72261
2022-08-17 18:16:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:16:05 | INFO | fairseq.trainer | begin training epoch 906
2022-08-17 18:16:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:16:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:16:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:16:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:16:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:16:57 | INFO | valid | epoch 906 | valid on 'valid' subset | loss 5.19 | nll_loss 2.592 | ppl 6.03 | bleu 57.04 | wps 1708.6 | wpb 933.5 | bsz 59.6 | num_updates 73386 | best_bleu 57.52
2022-08-17 18:16:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 906 @ 73386 updates
2022-08-17 18:16:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint906.pt
2022-08-17 18:16:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint906.pt
2022-08-17 18:17:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint906.pt (epoch 906 @ 73386 updates, score 57.04) (writing took 32.49246806278825 seconds)
2022-08-17 18:17:29 | INFO | fairseq_cli.train | end of epoch 906 (average epoch stats below)
2022-08-17 18:17:29 | INFO | train | epoch 906 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 5319 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 73386 | lr 0.000233466 | gnorm 0.309 | train_wall 39 | gb_free 10.1 | wall 72346
2022-08-17 18:17:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:17:30 | INFO | fairseq.trainer | begin training epoch 907
2022-08-17 18:17:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:17:38 | INFO | train_inner | epoch 907:     14 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=4598.3, ups=0.84, wpb=5503.1, bsz=357.4, num_updates=73400, lr=0.000233444, gnorm=0.32, train_wall=48, gb_free=10.1, wall=72354
2022-08-17 18:18:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:18:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:18:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:18:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:18:21 | INFO | valid | epoch 907 | valid on 'valid' subset | loss 5.187 | nll_loss 2.588 | ppl 6.01 | bleu 56.48 | wps 1693.1 | wpb 933.5 | bsz 59.6 | num_updates 73467 | best_bleu 57.52
2022-08-17 18:18:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 907 @ 73467 updates
2022-08-17 18:18:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint907.pt
2022-08-17 18:18:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint907.pt
2022-08-17 18:18:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint907.pt (epoch 907 @ 73467 updates, score 56.48) (writing took 17.13187462836504 seconds)
2022-08-17 18:18:38 | INFO | fairseq_cli.train | end of epoch 907 (average epoch stats below)
2022-08-17 18:18:38 | INFO | train | epoch 907 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 6507.5 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 73467 | lr 0.000233337 | gnorm 0.442 | train_wall 39 | gb_free 10.1 | wall 72414
2022-08-17 18:18:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:18:38 | INFO | fairseq.trainer | begin training epoch 908
2022-08-17 18:18:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:18:56 | INFO | train_inner | epoch 908:     33 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=7058, ups=1.28, wpb=5517.6, bsz=359.9, num_updates=73500, lr=0.000233285, gnorm=0.426, train_wall=48, gb_free=10.1, wall=72432
2022-08-17 18:19:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:19:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:19:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:19:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:19:30 | INFO | valid | epoch 908 | valid on 'valid' subset | loss 5.212 | nll_loss 2.62 | ppl 6.15 | bleu 55.79 | wps 1800.3 | wpb 933.5 | bsz 59.6 | num_updates 73548 | best_bleu 57.52
2022-08-17 18:19:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 908 @ 73548 updates
2022-08-17 18:19:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint908.pt
2022-08-17 18:19:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint908.pt
2022-08-17 18:19:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint908.pt (epoch 908 @ 73548 updates, score 55.79) (writing took 14.131858065724373 seconds)
2022-08-17 18:19:45 | INFO | fairseq_cli.train | end of epoch 908 (average epoch stats below)
2022-08-17 18:19:45 | INFO | train | epoch 908 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 6712.5 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 73548 | lr 0.000233209 | gnorm 0.337 | train_wall 39 | gb_free 10.2 | wall 72481
2022-08-17 18:19:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:19:45 | INFO | fairseq.trainer | begin training epoch 909
2022-08-17 18:19:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:20:13 | INFO | train_inner | epoch 909:     52 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=7220.4, ups=1.3, wpb=5566.1, bsz=353.6, num_updates=73600, lr=0.000233126, gnorm=0.352, train_wall=50, gb_free=10.2, wall=72509
2022-08-17 18:20:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:20:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:20:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:20:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:20:37 | INFO | valid | epoch 909 | valid on 'valid' subset | loss 5.186 | nll_loss 2.589 | ppl 6.02 | bleu 56.24 | wps 1843 | wpb 933.5 | bsz 59.6 | num_updates 73629 | best_bleu 57.52
2022-08-17 18:20:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 909 @ 73629 updates
2022-08-17 18:20:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint909.pt
2022-08-17 18:20:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint909.pt
2022-08-17 18:21:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint909.pt (epoch 909 @ 73629 updates, score 56.24) (writing took 37.93345080688596 seconds)
2022-08-17 18:21:15 | INFO | fairseq_cli.train | end of epoch 909 (average epoch stats below)
2022-08-17 18:21:15 | INFO | train | epoch 909 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 4972.1 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 73629 | lr 0.00023308 | gnorm 0.355 | train_wall 40 | gb_free 10.1 | wall 72571
2022-08-17 18:21:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:21:15 | INFO | fairseq.trainer | begin training epoch 910
2022-08-17 18:21:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:21:57 | INFO | train_inner | epoch 910:     71 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=5269.8, ups=0.96, wpb=5487.6, bsz=358.8, num_updates=73700, lr=0.000232968, gnorm=0.309, train_wall=49, gb_free=10.2, wall=72613
2022-08-17 18:22:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:22:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:22:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:22:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:22:12 | INFO | valid | epoch 910 | valid on 'valid' subset | loss 5.192 | nll_loss 2.595 | ppl 6.04 | bleu 56.35 | wps 1957.8 | wpb 933.5 | bsz 59.6 | num_updates 73710 | best_bleu 57.52
2022-08-17 18:22:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 910 @ 73710 updates
2022-08-17 18:22:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint910.pt
2022-08-17 18:22:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint910.pt
2022-08-17 18:22:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint910.pt (epoch 910 @ 73710 updates, score 56.35) (writing took 17.471561819314957 seconds)
2022-08-17 18:22:30 | INFO | fairseq_cli.train | end of epoch 910 (average epoch stats below)
2022-08-17 18:22:30 | INFO | train | epoch 910 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 5978.8 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 73710 | lr 0.000232952 | gnorm 0.311 | train_wall 40 | gb_free 10.2 | wall 72646
2022-08-17 18:22:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:22:30 | INFO | fairseq.trainer | begin training epoch 911
2022-08-17 18:22:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:23:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:23:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:23:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:23:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:23:21 | INFO | valid | epoch 911 | valid on 'valid' subset | loss 5.188 | nll_loss 2.594 | ppl 6.04 | bleu 56.76 | wps 1975.4 | wpb 933.5 | bsz 59.6 | num_updates 73791 | best_bleu 57.52
2022-08-17 18:23:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 911 @ 73791 updates
2022-08-17 18:23:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint911.pt
2022-08-17 18:23:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint911.pt
2022-08-17 18:23:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint911.pt (epoch 911 @ 73791 updates, score 56.76) (writing took 15.190855000168085 seconds)
2022-08-17 18:23:37 | INFO | fairseq_cli.train | end of epoch 911 (average epoch stats below)
2022-08-17 18:23:37 | INFO | train | epoch 911 | loss 3.373 | nll_loss 0.34 | ppl 1.27 | wps 6671 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 73791 | lr 0.000232824 | gnorm 0.29 | train_wall 40 | gb_free 10.1 | wall 72713
2022-08-17 18:23:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:23:37 | INFO | fairseq.trainer | begin training epoch 912
2022-08-17 18:23:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:23:43 | INFO | train_inner | epoch 912:      9 / 81 loss=3.373, nll_loss=0.34, ppl=1.27, wps=5243.4, ups=0.95, wpb=5538.5, bsz=360.2, num_updates=73800, lr=0.00023281, gnorm=0.285, train_wall=50, gb_free=10.1, wall=72719
2022-08-17 18:24:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:24:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:24:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:24:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:24:27 | INFO | valid | epoch 912 | valid on 'valid' subset | loss 5.18 | nll_loss 2.581 | ppl 5.98 | bleu 56.53 | wps 1962.4 | wpb 933.5 | bsz 59.6 | num_updates 73872 | best_bleu 57.52
2022-08-17 18:24:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 912 @ 73872 updates
2022-08-17 18:24:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint912.pt
2022-08-17 18:24:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint912.pt
2022-08-17 18:25:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint912.pt (epoch 912 @ 73872 updates, score 56.53) (writing took 37.99460220709443 seconds)
2022-08-17 18:25:05 | INFO | fairseq_cli.train | end of epoch 912 (average epoch stats below)
2022-08-17 18:25:05 | INFO | train | epoch 912 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 5036.6 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 73872 | lr 0.000232697 | gnorm 0.341 | train_wall 40 | gb_free 10 | wall 72802
2022-08-17 18:25:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:25:06 | INFO | fairseq.trainer | begin training epoch 913
2022-08-17 18:25:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:25:22 | INFO | train_inner | epoch 913:     28 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=5554.8, ups=1.01, wpb=5494.2, bsz=355.8, num_updates=73900, lr=0.000232653, gnorm=0.34, train_wall=49, gb_free=10.1, wall=72818
2022-08-17 18:25:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:25:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:25:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:25:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:25:58 | INFO | valid | epoch 913 | valid on 'valid' subset | loss 5.181 | nll_loss 2.583 | ppl 5.99 | bleu 57.15 | wps 1837.1 | wpb 933.5 | bsz 59.6 | num_updates 73953 | best_bleu 57.52
2022-08-17 18:25:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 913 @ 73953 updates
2022-08-17 18:25:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint913.pt
2022-08-17 18:26:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint913.pt
2022-08-17 18:26:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint913.pt (epoch 913 @ 73953 updates, score 57.15) (writing took 19.615544091910124 seconds)
2022-08-17 18:26:18 | INFO | fairseq_cli.train | end of epoch 913 (average epoch stats below)
2022-08-17 18:26:18 | INFO | train | epoch 913 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6165.4 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 73953 | lr 0.000232569 | gnorm 0.301 | train_wall 39 | gb_free 10.1 | wall 72874
2022-08-17 18:26:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:26:18 | INFO | fairseq.trainer | begin training epoch 914
2022-08-17 18:26:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:26:44 | INFO | train_inner | epoch 914:     47 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=6744.3, ups=1.21, wpb=5561.2, bsz=366.2, num_updates=74000, lr=0.000232495, gnorm=0.32, train_wall=49, gb_free=10.1, wall=72900
2022-08-17 18:27:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:27:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:27:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:27:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:27:10 | INFO | valid | epoch 914 | valid on 'valid' subset | loss 5.18 | nll_loss 2.581 | ppl 5.98 | bleu 56.87 | wps 1855.1 | wpb 933.5 | bsz 59.6 | num_updates 74034 | best_bleu 57.52
2022-08-17 18:27:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 914 @ 74034 updates
2022-08-17 18:27:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint914.pt
2022-08-17 18:27:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint914.pt
2022-08-17 18:27:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint914.pt (epoch 914 @ 74034 updates, score 56.87) (writing took 16.77140559628606 seconds)
2022-08-17 18:27:27 | INFO | fairseq_cli.train | end of epoch 914 (average epoch stats below)
2022-08-17 18:27:27 | INFO | train | epoch 914 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 6468.8 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 74034 | lr 0.000232442 | gnorm 0.34 | train_wall 40 | gb_free 10.1 | wall 72943
2022-08-17 18:27:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:27:27 | INFO | fairseq.trainer | begin training epoch 915
2022-08-17 18:27:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:28:02 | INFO | train_inner | epoch 915:     66 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=7102, ups=1.28, wpb=5550.1, bsz=354.3, num_updates=74100, lr=0.000232338, gnorm=0.339, train_wall=50, gb_free=10.1, wall=72979
2022-08-17 18:28:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:28:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:28:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:28:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:28:19 | INFO | valid | epoch 915 | valid on 'valid' subset | loss 5.189 | nll_loss 2.59 | ppl 6.02 | bleu 56.11 | wps 1861.9 | wpb 933.5 | bsz 59.6 | num_updates 74115 | best_bleu 57.52
2022-08-17 18:28:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 915 @ 74115 updates
2022-08-17 18:28:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint915.pt
2022-08-17 18:28:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint915.pt
2022-08-17 18:28:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint915.pt (epoch 915 @ 74115 updates, score 56.11) (writing took 37.38563605025411 seconds)
2022-08-17 18:28:56 | INFO | fairseq_cli.train | end of epoch 915 (average epoch stats below)
2022-08-17 18:28:56 | INFO | train | epoch 915 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 5015.7 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 74115 | lr 0.000232315 | gnorm 0.35 | train_wall 40 | gb_free 10.1 | wall 73033
2022-08-17 18:28:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:28:57 | INFO | fairseq.trainer | begin training epoch 916
2022-08-17 18:28:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:29:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:29:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:29:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:29:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:29:50 | INFO | valid | epoch 916 | valid on 'valid' subset | loss 5.168 | nll_loss 2.566 | ppl 5.92 | bleu 56.97 | wps 1942 | wpb 933.5 | bsz 59.6 | num_updates 74196 | best_bleu 57.52
2022-08-17 18:29:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 916 @ 74196 updates
2022-08-17 18:29:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint916.pt
2022-08-17 18:29:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint916.pt
2022-08-17 18:29:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint916.pt (epoch 916 @ 74196 updates, score 56.97) (writing took 2.6178273037075996 seconds)
2022-08-17 18:29:52 | INFO | fairseq_cli.train | end of epoch 916 (average epoch stats below)
2022-08-17 18:29:52 | INFO | train | epoch 916 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 7993.6 | ups 1.45 | wpb 5523.2 | bsz 358 | num_updates 74196 | lr 0.000232188 | gnorm 0.384 | train_wall 41 | gb_free 10.2 | wall 73089
2022-08-17 18:29:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:29:53 | INFO | fairseq.trainer | begin training epoch 917
2022-08-17 18:29:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:29:56 | INFO | train_inner | epoch 917:      4 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=4834.5, ups=0.88, wpb=5474.1, bsz=355, num_updates=74200, lr=0.000232182, gnorm=0.381, train_wall=50, gb_free=10.2, wall=73092
2022-08-17 18:30:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:30:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:30:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:30:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:30:43 | INFO | valid | epoch 917 | valid on 'valid' subset | loss 5.172 | nll_loss 2.573 | ppl 5.95 | bleu 56.98 | wps 1854.5 | wpb 933.5 | bsz 59.6 | num_updates 74277 | best_bleu 57.52
2022-08-17 18:30:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 917 @ 74277 updates
2022-08-17 18:30:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint917.pt
2022-08-17 18:30:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint917.pt
2022-08-17 18:31:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint917.pt (epoch 917 @ 74277 updates, score 56.98) (writing took 31.414489902555943 seconds)
2022-08-17 18:31:14 | INFO | fairseq_cli.train | end of epoch 917 (average epoch stats below)
2022-08-17 18:31:14 | INFO | train | epoch 917 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5453.4 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 74277 | lr 0.000232061 | gnorm 0.338 | train_wall 39 | gb_free 10.1 | wall 73171
2022-08-17 18:31:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:31:15 | INFO | fairseq.trainer | begin training epoch 918
2022-08-17 18:31:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:31:28 | INFO | train_inner | epoch 918:     23 / 81 loss=3.373, nll_loss=0.34, ppl=1.27, wps=6021, ups=1.08, wpb=5557.7, bsz=361.3, num_updates=74300, lr=0.000232025, gnorm=0.329, train_wall=49, gb_free=10.1, wall=73184
2022-08-17 18:31:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:31:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:31:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:31:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:32:06 | INFO | valid | epoch 918 | valid on 'valid' subset | loss 5.177 | nll_loss 2.574 | ppl 5.96 | bleu 57 | wps 1818.8 | wpb 933.5 | bsz 59.6 | num_updates 74358 | best_bleu 57.52
2022-08-17 18:32:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 918 @ 74358 updates
2022-08-17 18:32:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint918.pt
2022-08-17 18:32:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint918.pt
2022-08-17 18:32:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint918.pt (epoch 918 @ 74358 updates, score 57.0) (writing took 35.94423262402415 seconds)
2022-08-17 18:32:42 | INFO | fairseq_cli.train | end of epoch 918 (average epoch stats below)
2022-08-17 18:32:42 | INFO | train | epoch 918 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 5116.2 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 74358 | lr 0.000231935 | gnorm 0.325 | train_wall 39 | gb_free 10.3 | wall 73258
2022-08-17 18:32:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:32:42 | INFO | fairseq.trainer | begin training epoch 919
2022-08-17 18:32:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:33:05 | INFO | train_inner | epoch 919:     42 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=5684, ups=1.03, wpb=5503.1, bsz=354.5, num_updates=74400, lr=0.000231869, gnorm=0.312, train_wall=48, gb_free=10, wall=73281
2022-08-17 18:33:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:33:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:33:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:33:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:33:34 | INFO | valid | epoch 919 | valid on 'valid' subset | loss 5.175 | nll_loss 2.578 | ppl 5.97 | bleu 57.09 | wps 1811.2 | wpb 933.5 | bsz 59.6 | num_updates 74439 | best_bleu 57.52
2022-08-17 18:33:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 919 @ 74439 updates
2022-08-17 18:33:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint919.pt
2022-08-17 18:33:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint919.pt
2022-08-17 18:33:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint919.pt (epoch 919 @ 74439 updates, score 57.09) (writing took 19.22074907645583 seconds)
2022-08-17 18:33:53 | INFO | fairseq_cli.train | end of epoch 919 (average epoch stats below)
2022-08-17 18:33:53 | INFO | train | epoch 919 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6288.1 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 74439 | lr 0.000231809 | gnorm 0.299 | train_wall 39 | gb_free 10.1 | wall 73329
2022-08-17 18:33:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:33:53 | INFO | fairseq.trainer | begin training epoch 920
2022-08-17 18:33:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:34:25 | INFO | train_inner | epoch 920:     61 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=6874.2, ups=1.24, wpb=5529.4, bsz=359.8, num_updates=74500, lr=0.000231714, gnorm=0.326, train_wall=49, gb_free=10.1, wall=73361
2022-08-17 18:34:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:34:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:34:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:34:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:34:45 | INFO | valid | epoch 920 | valid on 'valid' subset | loss 5.181 | nll_loss 2.583 | ppl 5.99 | bleu 56.48 | wps 1813.4 | wpb 933.5 | bsz 59.6 | num_updates 74520 | best_bleu 57.52
2022-08-17 18:34:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 920 @ 74520 updates
2022-08-17 18:34:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint920.pt
2022-08-17 18:34:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint920.pt
2022-08-17 18:35:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint920.pt (epoch 920 @ 74520 updates, score 56.48) (writing took 17.992017827928066 seconds)
2022-08-17 18:35:04 | INFO | fairseq_cli.train | end of epoch 920 (average epoch stats below)
2022-08-17 18:35:04 | INFO | train | epoch 920 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6332.8 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 74520 | lr 0.000231683 | gnorm 0.383 | train_wall 40 | gb_free 10.2 | wall 73400
2022-08-17 18:35:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:35:04 | INFO | fairseq.trainer | begin training epoch 921
2022-08-17 18:35:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:35:45 | INFO | train_inner | epoch 921:     80 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=6904.1, ups=1.25, wpb=5525.5, bsz=357.4, num_updates=74600, lr=0.000231558, gnorm=0.394, train_wall=48, gb_free=10.1, wall=73441
2022-08-17 18:35:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:35:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:35:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:35:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:35:55 | INFO | valid | epoch 921 | valid on 'valid' subset | loss 5.195 | nll_loss 2.601 | ppl 6.07 | bleu 56.44 | wps 1899.8 | wpb 933.5 | bsz 59.6 | num_updates 74601 | best_bleu 57.52
2022-08-17 18:35:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 921 @ 74601 updates
2022-08-17 18:35:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint921.pt
2022-08-17 18:35:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint921.pt
2022-08-17 18:36:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint921.pt (epoch 921 @ 74601 updates, score 56.44) (writing took 30.026283979415894 seconds)
2022-08-17 18:36:25 | INFO | fairseq_cli.train | end of epoch 921 (average epoch stats below)
2022-08-17 18:36:25 | INFO | train | epoch 921 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 5513.3 | ups 1 | wpb 5523.2 | bsz 358 | num_updates 74601 | lr 0.000231557 | gnorm 0.35 | train_wall 39 | gb_free 10.1 | wall 73481
2022-08-17 18:36:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:36:25 | INFO | fairseq.trainer | begin training epoch 922
2022-08-17 18:36:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:37:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:37:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:37:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:37:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:37:21 | INFO | valid | epoch 922 | valid on 'valid' subset | loss 5.188 | nll_loss 2.591 | ppl 6.02 | bleu 56.87 | wps 1882.3 | wpb 933.5 | bsz 59.6 | num_updates 74682 | best_bleu 57.52
2022-08-17 18:37:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 922 @ 74682 updates
2022-08-17 18:37:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint922.pt
2022-08-17 18:37:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint922.pt
2022-08-17 18:37:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint922.pt (epoch 922 @ 74682 updates, score 56.87) (writing took 20.572249978780746 seconds)
2022-08-17 18:37:42 | INFO | fairseq_cli.train | end of epoch 922 (average epoch stats below)
2022-08-17 18:37:42 | INFO | train | epoch 922 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 5828.6 | ups 1.06 | wpb 5523.2 | bsz 358 | num_updates 74682 | lr 0.000231431 | gnorm 0.312 | train_wall 40 | gb_free 10.3 | wall 73558
2022-08-17 18:37:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:37:42 | INFO | fairseq.trainer | begin training epoch 923
2022-08-17 18:37:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:37:52 | INFO | train_inner | epoch 923:     18 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=4339, ups=0.79, wpb=5509.4, bsz=355.1, num_updates=74700, lr=0.000231403, gnorm=0.318, train_wall=49, gb_free=10.1, wall=73568
2022-08-17 18:38:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:38:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:38:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:38:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:38:35 | INFO | valid | epoch 923 | valid on 'valid' subset | loss 5.2 | nll_loss 2.605 | ppl 6.08 | bleu 56.51 | wps 1802 | wpb 933.5 | bsz 59.6 | num_updates 74763 | best_bleu 57.52
2022-08-17 18:38:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 923 @ 74763 updates
2022-08-17 18:38:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint923.pt
2022-08-17 18:38:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint923.pt
2022-08-17 18:38:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint923.pt (epoch 923 @ 74763 updates, score 56.51) (writing took 22.755347933620214 seconds)
2022-08-17 18:38:58 | INFO | fairseq_cli.train | end of epoch 923 (average epoch stats below)
2022-08-17 18:38:58 | INFO | train | epoch 923 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5891.6 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 74763 | lr 0.000231306 | gnorm 0.32 | train_wall 41 | gb_free 10.1 | wall 73634
2022-08-17 18:38:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:38:58 | INFO | fairseq.trainer | begin training epoch 924
2022-08-17 18:38:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:39:18 | INFO | train_inner | epoch 924:     37 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=6462.9, ups=1.17, wpb=5517.5, bsz=356.6, num_updates=74800, lr=0.000231249, gnorm=0.319, train_wall=51, gb_free=10.1, wall=73654
2022-08-17 18:39:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:39:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:39:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:39:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:39:49 | INFO | valid | epoch 924 | valid on 'valid' subset | loss 5.201 | nll_loss 2.607 | ppl 6.09 | bleu 56.51 | wps 1879.1 | wpb 933.5 | bsz 59.6 | num_updates 74844 | best_bleu 57.52
2022-08-17 18:39:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 924 @ 74844 updates
2022-08-17 18:39:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint924.pt
2022-08-17 18:39:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint924.pt
2022-08-17 18:40:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint924.pt (epoch 924 @ 74844 updates, score 56.51) (writing took 27.887806687504053 seconds)
2022-08-17 18:40:17 | INFO | fairseq_cli.train | end of epoch 924 (average epoch stats below)
2022-08-17 18:40:17 | INFO | train | epoch 924 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5632.1 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 74844 | lr 0.000231181 | gnorm 0.321 | train_wall 41 | gb_free 10.1 | wall 73713
2022-08-17 18:40:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:40:17 | INFO | fairseq.trainer | begin training epoch 925
2022-08-17 18:40:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:40:47 | INFO | train_inner | epoch 925:     56 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=6228.1, ups=1.12, wpb=5545.3, bsz=362.6, num_updates=74900, lr=0.000231094, gnorm=0.308, train_wall=49, gb_free=10.1, wall=73743
2022-08-17 18:40:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:41:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:41:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:41:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:41:09 | INFO | valid | epoch 925 | valid on 'valid' subset | loss 5.188 | nll_loss 2.585 | ppl 6 | bleu 56.93 | wps 1778.7 | wpb 933.5 | bsz 59.6 | num_updates 74925 | best_bleu 57.52
2022-08-17 18:41:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 925 @ 74925 updates
2022-08-17 18:41:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint925.pt
2022-08-17 18:41:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint925.pt
2022-08-17 18:41:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint925.pt (epoch 925 @ 74925 updates, score 56.93) (writing took 16.11810089275241 seconds)
2022-08-17 18:41:25 | INFO | fairseq_cli.train | end of epoch 925 (average epoch stats below)
2022-08-17 18:41:25 | INFO | train | epoch 925 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 6582.8 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 74925 | lr 0.000231056 | gnorm 0.305 | train_wall 40 | gb_free 10.1 | wall 73781
2022-08-17 18:41:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:41:25 | INFO | fairseq.trainer | begin training epoch 926
2022-08-17 18:41:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:42:04 | INFO | train_inner | epoch 926:     75 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=7173.3, ups=1.3, wpb=5514.5, bsz=357.2, num_updates=75000, lr=0.00023094, gnorm=0.338, train_wall=49, gb_free=10.1, wall=73820
2022-08-17 18:42:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:42:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:42:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:42:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:42:16 | INFO | valid | epoch 926 | valid on 'valid' subset | loss 5.201 | nll_loss 2.61 | ppl 6.1 | bleu 56.51 | wps 1849.4 | wpb 933.5 | bsz 59.6 | num_updates 75006 | best_bleu 57.52
2022-08-17 18:42:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 926 @ 75006 updates
2022-08-17 18:42:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint926.pt
2022-08-17 18:42:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint926.pt
2022-08-17 18:42:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint926.pt (epoch 926 @ 75006 updates, score 56.51) (writing took 30.156822457909584 seconds)
2022-08-17 18:42:46 | INFO | fairseq_cli.train | end of epoch 926 (average epoch stats below)
2022-08-17 18:42:46 | INFO | train | epoch 926 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 5492.9 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 75006 | lr 0.000230931 | gnorm 0.364 | train_wall 40 | gb_free 10.1 | wall 73863
2022-08-17 18:42:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:42:47 | INFO | fairseq.trainer | begin training epoch 927
2022-08-17 18:42:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:43:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:43:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:43:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:43:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:43:42 | INFO | valid | epoch 927 | valid on 'valid' subset | loss 5.21 | nll_loss 2.619 | ppl 6.14 | bleu 55.64 | wps 1857.6 | wpb 933.5 | bsz 59.6 | num_updates 75087 | best_bleu 57.52
2022-08-17 18:43:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 927 @ 75087 updates
2022-08-17 18:43:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint927.pt
2022-08-17 18:43:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint927.pt
2022-08-17 18:44:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint927.pt (epoch 927 @ 75087 updates, score 55.64) (writing took 29.2244703322649 seconds)
2022-08-17 18:44:11 | INFO | fairseq_cli.train | end of epoch 927 (average epoch stats below)
2022-08-17 18:44:11 | INFO | train | epoch 927 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 5286.9 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 75087 | lr 0.000230806 | gnorm 0.327 | train_wall 39 | gb_free 10.2 | wall 73947
2022-08-17 18:44:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:44:11 | INFO | fairseq.trainer | begin training epoch 928
2022-08-17 18:44:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:44:19 | INFO | train_inner | epoch 928:     13 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=4068.9, ups=0.74, wpb=5516.6, bsz=357.3, num_updates=75100, lr=0.000230786, gnorm=0.353, train_wall=49, gb_free=10.1, wall=73955
2022-08-17 18:44:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:44:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:44:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:44:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:45:02 | INFO | valid | epoch 928 | valid on 'valid' subset | loss 5.206 | nll_loss 2.607 | ppl 6.09 | bleu 56.34 | wps 1961.6 | wpb 933.5 | bsz 59.6 | num_updates 75168 | best_bleu 57.52
2022-08-17 18:45:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 928 @ 75168 updates
2022-08-17 18:45:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint928.pt
2022-08-17 18:45:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint928.pt
2022-08-17 18:45:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint928.pt (epoch 928 @ 75168 updates, score 56.34) (writing took 16.496932569891214 seconds)
2022-08-17 18:45:19 | INFO | fairseq_cli.train | end of epoch 928 (average epoch stats below)
2022-08-17 18:45:19 | INFO | train | epoch 928 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 6584.3 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 75168 | lr 0.000230682 | gnorm 0.404 | train_wall 40 | gb_free 10.1 | wall 74015
2022-08-17 18:45:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:45:19 | INFO | fairseq.trainer | begin training epoch 929
2022-08-17 18:45:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:45:38 | INFO | train_inner | epoch 929:     32 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=7020.9, ups=1.27, wpb=5508.3, bsz=355.3, num_updates=75200, lr=0.000230633, gnorm=0.435, train_wall=49, gb_free=10.2, wall=74034
2022-08-17 18:46:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:46:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:46:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:46:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:46:11 | INFO | valid | epoch 929 | valid on 'valid' subset | loss 5.2 | nll_loss 2.608 | ppl 6.09 | bleu 56.23 | wps 1879.8 | wpb 933.5 | bsz 59.6 | num_updates 75249 | best_bleu 57.52
2022-08-17 18:46:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 929 @ 75249 updates
2022-08-17 18:46:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint929.pt
2022-08-17 18:46:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint929.pt
2022-08-17 18:46:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint929.pt (epoch 929 @ 75249 updates, score 56.23) (writing took 35.451517671346664 seconds)
2022-08-17 18:46:47 | INFO | fairseq_cli.train | end of epoch 929 (average epoch stats below)
2022-08-17 18:46:47 | INFO | train | epoch 929 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 5097.8 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 75249 | lr 0.000230558 | gnorm 0.467 | train_wall 40 | gb_free 10.2 | wall 74103
2022-08-17 18:46:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:46:47 | INFO | fairseq.trainer | begin training epoch 930
2022-08-17 18:46:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:47:13 | INFO | train_inner | epoch 930:     51 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=5780.9, ups=1.04, wpb=5543.1, bsz=364.1, num_updates=75300, lr=0.00023048, gnorm=1.088, train_wall=49, gb_free=10.1, wall=74130
2022-08-17 18:47:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:47:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:47:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:47:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:47:38 | INFO | valid | epoch 930 | valid on 'valid' subset | loss 5.202 | nll_loss 2.608 | ppl 6.1 | bleu 56.24 | wps 1735.7 | wpb 933.5 | bsz 59.6 | num_updates 75330 | best_bleu 57.52
2022-08-17 18:47:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 930 @ 75330 updates
2022-08-17 18:47:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint930.pt
2022-08-17 18:47:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint930.pt
2022-08-17 18:48:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint930.pt (epoch 930 @ 75330 updates, score 56.24) (writing took 35.88913508877158 seconds)
2022-08-17 18:48:14 | INFO | fairseq_cli.train | end of epoch 930 (average epoch stats below)
2022-08-17 18:48:14 | INFO | train | epoch 930 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 5139.1 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 75330 | lr 0.000230434 | gnorm 1.217 | train_wall 39 | gb_free 10.1 | wall 74190
2022-08-17 18:48:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:48:14 | INFO | fairseq.trainer | begin training epoch 931
2022-08-17 18:48:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:48:51 | INFO | train_inner | epoch 931:     70 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=5696.7, ups=1.03, wpb=5539.1, bsz=354.8, num_updates=75400, lr=0.000230327, gnorm=0.315, train_wall=48, gb_free=10.1, wall=74227
2022-08-17 18:48:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:48:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:48:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:48:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:49:06 | INFO | valid | epoch 931 | valid on 'valid' subset | loss 5.22 | nll_loss 2.631 | ppl 6.19 | bleu 55.56 | wps 1743.4 | wpb 933.5 | bsz 59.6 | num_updates 75411 | best_bleu 57.52
2022-08-17 18:49:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 931 @ 75411 updates
2022-08-17 18:49:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint931.pt
2022-08-17 18:49:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint931.pt
2022-08-17 18:49:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint931.pt (epoch 931 @ 75411 updates, score 55.56) (writing took 33.373853761702776 seconds)
2022-08-17 18:49:39 | INFO | fairseq_cli.train | end of epoch 931 (average epoch stats below)
2022-08-17 18:49:39 | INFO | train | epoch 931 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 5228.4 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 75411 | lr 0.00023031 | gnorm 0.332 | train_wall 39 | gb_free 10.1 | wall 74276
2022-08-17 18:49:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:49:40 | INFO | fairseq.trainer | begin training epoch 932
2022-08-17 18:49:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:50:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:50:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:50:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:50:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:50:31 | INFO | valid | epoch 932 | valid on 'valid' subset | loss 5.205 | nll_loss 2.614 | ppl 6.12 | bleu 56.39 | wps 1801.8 | wpb 933.5 | bsz 59.6 | num_updates 75492 | best_bleu 57.52
2022-08-17 18:50:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 932 @ 75492 updates
2022-08-17 18:50:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint932.pt
2022-08-17 18:50:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint932.pt
2022-08-17 18:50:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint932.pt (epoch 932 @ 75492 updates, score 56.39) (writing took 18.84133166447282 seconds)
2022-08-17 18:50:50 | INFO | fairseq_cli.train | end of epoch 932 (average epoch stats below)
2022-08-17 18:50:50 | INFO | train | epoch 932 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 6350.8 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 75492 | lr 0.000230186 | gnorm 0.337 | train_wall 40 | gb_free 10.2 | wall 74346
2022-08-17 18:50:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:50:50 | INFO | fairseq.trainer | begin training epoch 933
2022-08-17 18:50:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:50:55 | INFO | train_inner | epoch 933:      8 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=4424.9, ups=0.8, wpb=5510.1, bsz=359, num_updates=75500, lr=0.000230174, gnorm=0.362, train_wall=49, gb_free=10.1, wall=74351
2022-08-17 18:51:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:51:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:51:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:51:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:51:41 | INFO | valid | epoch 933 | valid on 'valid' subset | loss 5.216 | nll_loss 2.623 | ppl 6.16 | bleu 56.44 | wps 1919.7 | wpb 933.5 | bsz 59.6 | num_updates 75573 | best_bleu 57.52
2022-08-17 18:51:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 933 @ 75573 updates
2022-08-17 18:51:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint933.pt
2022-08-17 18:51:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint933.pt
2022-08-17 18:51:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint933.pt (epoch 933 @ 75573 updates, score 56.44) (writing took 14.685278452932835 seconds)
2022-08-17 18:51:56 | INFO | fairseq_cli.train | end of epoch 933 (average epoch stats below)
2022-08-17 18:51:56 | INFO | train | epoch 933 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6764.6 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 75573 | lr 0.000230063 | gnorm 0.381 | train_wall 39 | gb_free 10.1 | wall 74412
2022-08-17 18:51:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:51:56 | INFO | fairseq.trainer | begin training epoch 934
2022-08-17 18:51:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:52:11 | INFO | train_inner | epoch 934:     27 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=7300.9, ups=1.32, wpb=5510.6, bsz=358.4, num_updates=75600, lr=0.000230022, gnorm=0.362, train_wall=49, gb_free=10, wall=74427
2022-08-17 18:52:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:52:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:52:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:52:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:52:47 | INFO | valid | epoch 934 | valid on 'valid' subset | loss 5.215 | nll_loss 2.624 | ppl 6.16 | bleu 56.6 | wps 1762.9 | wpb 933.5 | bsz 59.6 | num_updates 75654 | best_bleu 57.52
2022-08-17 18:52:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 934 @ 75654 updates
2022-08-17 18:52:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint934.pt
2022-08-17 18:52:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint934.pt
2022-08-17 18:53:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint934.pt (epoch 934 @ 75654 updates, score 56.6) (writing took 43.546629544347525 seconds)
2022-08-17 18:53:31 | INFO | fairseq_cli.train | end of epoch 934 (average epoch stats below)
2022-08-17 18:53:31 | INFO | train | epoch 934 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 4709.4 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 75654 | lr 0.00022994 | gnorm 0.347 | train_wall 40 | gb_free 10.2 | wall 74507
2022-08-17 18:53:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:53:31 | INFO | fairseq.trainer | begin training epoch 935
2022-08-17 18:53:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:53:56 | INFO | train_inner | epoch 935:     46 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=5233.5, ups=0.95, wpb=5531.4, bsz=356.9, num_updates=75700, lr=0.00022987, gnorm=0.352, train_wall=49, gb_free=10.2, wall=74533
2022-08-17 18:54:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:54:24 | INFO | valid | epoch 935 | valid on 'valid' subset | loss 5.215 | nll_loss 2.625 | ppl 6.17 | bleu 48.69 | wps 1778.1 | wpb 933.5 | bsz 59.6 | num_updates 75735 | best_bleu 57.52
2022-08-17 18:54:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 935 @ 75735 updates
2022-08-17 18:54:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint935.pt
2022-08-17 18:54:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint935.pt
2022-08-17 18:54:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint935.pt (epoch 935 @ 75735 updates, score 48.69) (writing took 20.981097131967545 seconds)
2022-08-17 18:54:45 | INFO | fairseq_cli.train | end of epoch 935 (average epoch stats below)
2022-08-17 18:54:45 | INFO | train | epoch 935 | loss 3.375 | nll_loss 0.343 | ppl 1.27 | wps 6067 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 75735 | lr 0.000229817 | gnorm 0.378 | train_wall 40 | gb_free 10.2 | wall 74581
2022-08-17 18:54:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:54:45 | INFO | fairseq.trainer | begin training epoch 936
2022-08-17 18:54:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:55:19 | INFO | train_inner | epoch 936:     65 / 81 loss=3.376, nll_loss=0.344, ppl=1.27, wps=6721.1, ups=1.22, wpb=5523.7, bsz=356.7, num_updates=75800, lr=0.000229718, gnorm=0.51, train_wall=49, gb_free=10.1, wall=74615
2022-08-17 18:55:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:55:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:55:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:55:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:55:36 | INFO | valid | epoch 936 | valid on 'valid' subset | loss 5.193 | nll_loss 2.594 | ppl 6.04 | bleu 55.79 | wps 1898 | wpb 933.5 | bsz 59.6 | num_updates 75816 | best_bleu 57.52
2022-08-17 18:55:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 936 @ 75816 updates
2022-08-17 18:55:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint936.pt
2022-08-17 18:55:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint936.pt
2022-08-17 18:56:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint936.pt (epoch 936 @ 75816 updates, score 55.79) (writing took 25.187309004366398 seconds)
2022-08-17 18:56:01 | INFO | fairseq_cli.train | end of epoch 936 (average epoch stats below)
2022-08-17 18:56:01 | INFO | train | epoch 936 | loss 3.375 | nll_loss 0.342 | ppl 1.27 | wps 5836 | ups 1.06 | wpb 5523.2 | bsz 358 | num_updates 75816 | lr 0.000229694 | gnorm 0.506 | train_wall 40 | gb_free 10.1 | wall 74658
2022-08-17 18:56:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:56:02 | INFO | fairseq.trainer | begin training epoch 937
2022-08-17 18:56:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:56:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:56:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:56:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:56:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:56:54 | INFO | valid | epoch 937 | valid on 'valid' subset | loss 5.208 | nll_loss 2.612 | ppl 6.11 | bleu 56.75 | wps 1880 | wpb 933.5 | bsz 59.6 | num_updates 75897 | best_bleu 57.52
2022-08-17 18:56:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 937 @ 75897 updates
2022-08-17 18:56:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint937.pt
2022-08-17 18:56:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint937.pt
2022-08-17 18:57:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint937.pt (epoch 937 @ 75897 updates, score 56.75) (writing took 46.49793868884444 seconds)
2022-08-17 18:57:41 | INFO | fairseq_cli.train | end of epoch 937 (average epoch stats below)
2022-08-17 18:57:41 | INFO | train | epoch 937 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 4481.9 | ups 0.81 | wpb 5523.2 | bsz 358 | num_updates 75897 | lr 0.000229571 | gnorm 0.583 | train_wall 38 | gb_free 10.1 | wall 74757
2022-08-17 18:57:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:57:41 | INFO | fairseq.trainer | begin training epoch 938
2022-08-17 18:57:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:57:44 | INFO | train_inner | epoch 938:      3 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=3790.2, ups=0.69, wpb=5506.7, bsz=357.7, num_updates=75900, lr=0.000229567, gnorm=0.543, train_wall=48, gb_free=10, wall=74760
2022-08-17 18:58:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:58:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:58:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:58:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:58:34 | INFO | valid | epoch 938 | valid on 'valid' subset | loss 5.194 | nll_loss 2.598 | ppl 6.05 | bleu 56.85 | wps 1887.4 | wpb 933.5 | bsz 59.6 | num_updates 75978 | best_bleu 57.52
2022-08-17 18:58:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 938 @ 75978 updates
2022-08-17 18:58:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint938.pt
2022-08-17 18:58:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint938.pt
2022-08-17 18:58:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint938.pt (epoch 938 @ 75978 updates, score 56.85) (writing took 17.839038487523794 seconds)
2022-08-17 18:58:52 | INFO | fairseq_cli.train | end of epoch 938 (average epoch stats below)
2022-08-17 18:58:52 | INFO | train | epoch 938 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 6306.6 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 75978 | lr 0.000229449 | gnorm 0.36 | train_wall 41 | gb_free 10.1 | wall 74828
2022-08-17 18:58:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 18:58:52 | INFO | fairseq.trainer | begin training epoch 939
2022-08-17 18:58:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 18:59:04 | INFO | train_inner | epoch 939:     22 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=6906.2, ups=1.24, wpb=5554.9, bsz=361.4, num_updates=76000, lr=0.000229416, gnorm=0.337, train_wall=50, gb_free=10.1, wall=74841
2022-08-17 18:59:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 18:59:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 18:59:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 18:59:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 18:59:43 | INFO | valid | epoch 939 | valid on 'valid' subset | loss 5.187 | nll_loss 2.592 | ppl 6.03 | bleu 56.46 | wps 1733.1 | wpb 933.5 | bsz 59.6 | num_updates 76059 | best_bleu 57.52
2022-08-17 18:59:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 939 @ 76059 updates
2022-08-17 18:59:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint939.pt
2022-08-17 18:59:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint939.pt
2022-08-17 19:00:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint939.pt (epoch 939 @ 76059 updates, score 56.46) (writing took 29.213689424097538 seconds)
2022-08-17 19:00:13 | INFO | fairseq_cli.train | end of epoch 939 (average epoch stats below)
2022-08-17 19:00:13 | INFO | train | epoch 939 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5547.4 | ups 1 | wpb 5523.2 | bsz 358 | num_updates 76059 | lr 0.000229327 | gnorm 0.301 | train_wall 40 | gb_free 10.2 | wall 74909
2022-08-17 19:00:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:00:13 | INFO | fairseq.trainer | begin training epoch 940
2022-08-17 19:00:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:00:34 | INFO | train_inner | epoch 940:     41 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=6102.5, ups=1.11, wpb=5491.5, bsz=353.9, num_updates=76100, lr=0.000229265, gnorm=0.32, train_wall=49, gb_free=10, wall=74931
2022-08-17 19:00:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:00:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:00:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:00:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:01:03 | INFO | valid | epoch 940 | valid on 'valid' subset | loss 5.175 | nll_loss 2.573 | ppl 5.95 | bleu 56.99 | wps 1908.8 | wpb 933.5 | bsz 59.6 | num_updates 76140 | best_bleu 57.52
2022-08-17 19:01:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 940 @ 76140 updates
2022-08-17 19:01:03 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint940.pt
2022-08-17 19:01:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint940.pt
2022-08-17 19:01:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint940.pt (epoch 940 @ 76140 updates, score 56.99) (writing took 33.7883526943624 seconds)
2022-08-17 19:01:37 | INFO | fairseq_cli.train | end of epoch 940 (average epoch stats below)
2022-08-17 19:01:37 | INFO | train | epoch 940 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5311.3 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 76140 | lr 0.000229205 | gnorm 0.33 | train_wall 39 | gb_free 10 | wall 74993
2022-08-17 19:01:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:01:37 | INFO | fairseq.trainer | begin training epoch 941
2022-08-17 19:01:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:02:08 | INFO | train_inner | epoch 941:     60 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=5879.9, ups=1.06, wpb=5535.7, bsz=361.9, num_updates=76200, lr=0.000229114, gnorm=0.307, train_wall=49, gb_free=10.1, wall=75025
2022-08-17 19:02:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:02:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:02:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:02:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:02:28 | INFO | valid | epoch 941 | valid on 'valid' subset | loss 5.203 | nll_loss 2.61 | ppl 6.11 | bleu 56.16 | wps 1864.2 | wpb 933.5 | bsz 59.6 | num_updates 76221 | best_bleu 57.52
2022-08-17 19:02:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 941 @ 76221 updates
2022-08-17 19:02:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint941.pt
2022-08-17 19:02:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint941.pt
2022-08-17 19:02:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint941.pt (epoch 941 @ 76221 updates, score 56.16) (writing took 21.741434685885906 seconds)
2022-08-17 19:02:50 | INFO | fairseq_cli.train | end of epoch 941 (average epoch stats below)
2022-08-17 19:02:50 | INFO | train | epoch 941 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 6132.7 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 76221 | lr 0.000229083 | gnorm 0.31 | train_wall 39 | gb_free 10.2 | wall 75066
2022-08-17 19:02:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:02:50 | INFO | fairseq.trainer | begin training epoch 942
2022-08-17 19:02:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:03:36 | INFO | train_inner | epoch 942:     79 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=6365.6, ups=1.15, wpb=5543.2, bsz=357.8, num_updates=76300, lr=0.000228964, gnorm=0.358, train_wall=49, gb_free=10.1, wall=75112
2022-08-17 19:03:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:03:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:03:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:03:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:03:45 | INFO | valid | epoch 942 | valid on 'valid' subset | loss 5.2 | nll_loss 2.602 | ppl 6.07 | bleu 56.51 | wps 1914.7 | wpb 933.5 | bsz 59.6 | num_updates 76302 | best_bleu 57.52
2022-08-17 19:03:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 942 @ 76302 updates
2022-08-17 19:03:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint942.pt
2022-08-17 19:03:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint942.pt
2022-08-17 19:04:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint942.pt (epoch 942 @ 76302 updates, score 56.51) (writing took 16.137476425617933 seconds)
2022-08-17 19:04:02 | INFO | fairseq_cli.train | end of epoch 942 (average epoch stats below)
2022-08-17 19:04:02 | INFO | train | epoch 942 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6239.8 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 76302 | lr 0.000228961 | gnorm 0.355 | train_wall 40 | gb_free 10.1 | wall 75138
2022-08-17 19:04:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:04:02 | INFO | fairseq.trainer | begin training epoch 943
2022-08-17 19:04:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:04:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:04:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:04:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:04:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:04:52 | INFO | valid | epoch 943 | valid on 'valid' subset | loss 5.191 | nll_loss 2.591 | ppl 6.03 | bleu 57.17 | wps 1897 | wpb 933.5 | bsz 59.6 | num_updates 76383 | best_bleu 57.52
2022-08-17 19:04:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 943 @ 76383 updates
2022-08-17 19:04:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint943.pt
2022-08-17 19:04:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint943.pt
2022-08-17 19:05:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint943.pt (epoch 943 @ 76383 updates, score 57.17) (writing took 34.57706327363849 seconds)
2022-08-17 19:05:27 | INFO | fairseq_cli.train | end of epoch 943 (average epoch stats below)
2022-08-17 19:05:27 | INFO | train | epoch 943 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 5273.2 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 76383 | lr 0.00022884 | gnorm 0.323 | train_wall 39 | gb_free 10.2 | wall 75223
2022-08-17 19:05:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:05:27 | INFO | fairseq.trainer | begin training epoch 944
2022-08-17 19:05:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:05:36 | INFO | train_inner | epoch 944:     17 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=4537.2, ups=0.83, wpb=5472, bsz=354.4, num_updates=76400, lr=0.000228814, gnorm=0.325, train_wall=48, gb_free=10, wall=75232
2022-08-17 19:06:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:06:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:06:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:06:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:06:17 | INFO | valid | epoch 944 | valid on 'valid' subset | loss 5.205 | nll_loss 2.611 | ppl 6.11 | bleu 56.3 | wps 1800.9 | wpb 933.5 | bsz 59.6 | num_updates 76464 | best_bleu 57.52
2022-08-17 19:06:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 944 @ 76464 updates
2022-08-17 19:06:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint944.pt
2022-08-17 19:06:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint944.pt
2022-08-17 19:06:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint944.pt (epoch 944 @ 76464 updates, score 56.3) (writing took 29.473496451973915 seconds)
2022-08-17 19:06:47 | INFO | fairseq_cli.train | end of epoch 944 (average epoch stats below)
2022-08-17 19:06:47 | INFO | train | epoch 944 | loss 3.373 | nll_loss 0.34 | ppl 1.27 | wps 5591 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 76464 | lr 0.000228719 | gnorm 0.311 | train_wall 39 | gb_free 10.1 | wall 75303
2022-08-17 19:06:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:06:47 | INFO | fairseq.trainer | begin training epoch 945
2022-08-17 19:06:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:07:06 | INFO | train_inner | epoch 945:     36 / 81 loss=3.373, nll_loss=0.34, ppl=1.27, wps=6188.6, ups=1.12, wpb=5537, bsz=364.1, num_updates=76500, lr=0.000228665, gnorm=0.311, train_wall=48, gb_free=10, wall=75322
2022-08-17 19:07:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:07:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:07:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:07:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:07:37 | INFO | valid | epoch 945 | valid on 'valid' subset | loss 5.204 | nll_loss 2.614 | ppl 6.12 | bleu 56.23 | wps 1704.5 | wpb 933.5 | bsz 59.6 | num_updates 76545 | best_bleu 57.52
2022-08-17 19:07:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 945 @ 76545 updates
2022-08-17 19:07:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint945.pt
2022-08-17 19:07:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint945.pt
2022-08-17 19:08:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint945.pt (epoch 945 @ 76545 updates, score 56.23) (writing took 32.29353044182062 seconds)
2022-08-17 19:08:10 | INFO | fairseq_cli.train | end of epoch 945 (average epoch stats below)
2022-08-17 19:08:10 | INFO | train | epoch 945 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 5375.2 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 76545 | lr 0.000228598 | gnorm 0.308 | train_wall 39 | gb_free 10.2 | wall 75386
2022-08-17 19:08:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:08:10 | INFO | fairseq.trainer | begin training epoch 946
2022-08-17 19:08:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:08:39 | INFO | train_inner | epoch 946:     55 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=5908, ups=1.07, wpb=5536.8, bsz=354.7, num_updates=76600, lr=0.000228515, gnorm=0.309, train_wall=49, gb_free=10.2, wall=75416
2022-08-17 19:08:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:08:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:08:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:08:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:09:03 | INFO | valid | epoch 946 | valid on 'valid' subset | loss 5.214 | nll_loss 2.622 | ppl 6.16 | bleu 56.22 | wps 1786.3 | wpb 933.5 | bsz 59.6 | num_updates 76626 | best_bleu 57.52
2022-08-17 19:09:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 946 @ 76626 updates
2022-08-17 19:09:03 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint946.pt
2022-08-17 19:09:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint946.pt
2022-08-17 19:09:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint946.pt (epoch 946 @ 76626 updates, score 56.22) (writing took 15.724697213619947 seconds)
2022-08-17 19:09:19 | INFO | fairseq_cli.train | end of epoch 946 (average epoch stats below)
2022-08-17 19:09:19 | INFO | train | epoch 946 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6499.4 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 76626 | lr 0.000228477 | gnorm 0.308 | train_wall 40 | gb_free 10.1 | wall 75455
2022-08-17 19:09:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:09:19 | INFO | fairseq.trainer | begin training epoch 947
2022-08-17 19:09:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:09:57 | INFO | train_inner | epoch 947:     74 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=7118, ups=1.28, wpb=5542.1, bsz=358.6, num_updates=76700, lr=0.000228366, gnorm=0.319, train_wall=50, gb_free=10.1, wall=75493
2022-08-17 19:10:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:10:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:10:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:10:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:10:10 | INFO | valid | epoch 947 | valid on 'valid' subset | loss 5.202 | nll_loss 2.606 | ppl 6.09 | bleu 56.33 | wps 1733.9 | wpb 933.5 | bsz 59.6 | num_updates 76707 | best_bleu 57.52
2022-08-17 19:10:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 947 @ 76707 updates
2022-08-17 19:10:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint947.pt
2022-08-17 19:10:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint947.pt
2022-08-17 19:10:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint947.pt (epoch 947 @ 76707 updates, score 56.33) (writing took 2.6444569416344166 seconds)
2022-08-17 19:10:13 | INFO | fairseq_cli.train | end of epoch 947 (average epoch stats below)
2022-08-17 19:10:13 | INFO | train | epoch 947 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 8181.6 | ups 1.48 | wpb 5523.2 | bsz 358 | num_updates 76707 | lr 0.000228356 | gnorm 0.326 | train_wall 40 | gb_free 10.2 | wall 75510
2022-08-17 19:10:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:10:14 | INFO | fairseq.trainer | begin training epoch 948
2022-08-17 19:10:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:10:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:10:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:10:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:10:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:11:06 | INFO | valid | epoch 948 | valid on 'valid' subset | loss 5.207 | nll_loss 2.612 | ppl 6.11 | bleu 56.33 | wps 1844.6 | wpb 933.5 | bsz 59.6 | num_updates 76788 | best_bleu 57.52
2022-08-17 19:11:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 948 @ 76788 updates
2022-08-17 19:11:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint948.pt
2022-08-17 19:11:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint948.pt
2022-08-17 19:11:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint948.pt (epoch 948 @ 76788 updates, score 56.33) (writing took 24.05546037852764 seconds)
2022-08-17 19:11:30 | INFO | fairseq_cli.train | end of epoch 948 (average epoch stats below)
2022-08-17 19:11:30 | INFO | train | epoch 948 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5800.9 | ups 1.05 | wpb 5523.2 | bsz 358 | num_updates 76788 | lr 0.000228236 | gnorm 0.311 | train_wall 41 | gb_free 10.1 | wall 75587
2022-08-17 19:11:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:11:31 | INFO | fairseq.trainer | begin training epoch 949
2022-08-17 19:11:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:11:38 | INFO | train_inner | epoch 949:     12 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=5415.8, ups=0.99, wpb=5480.3, bsz=352.2, num_updates=76800, lr=0.000228218, gnorm=0.315, train_wall=50, gb_free=10.1, wall=75595
2022-08-17 19:12:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:12:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:12:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:12:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:12:23 | INFO | valid | epoch 949 | valid on 'valid' subset | loss 5.195 | nll_loss 2.6 | ppl 6.06 | bleu 56.08 | wps 1900.1 | wpb 933.5 | bsz 59.6 | num_updates 76869 | best_bleu 57.52
2022-08-17 19:12:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 949 @ 76869 updates
2022-08-17 19:12:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint949.pt
2022-08-17 19:12:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint949.pt
2022-08-17 19:13:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint949.pt (epoch 949 @ 76869 updates, score 56.08) (writing took 42.90428724139929 seconds)
2022-08-17 19:13:06 | INFO | fairseq_cli.train | end of epoch 949 (average epoch stats below)
2022-08-17 19:13:06 | INFO | train | epoch 949 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 4690.1 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 76869 | lr 0.000228115 | gnorm 0.374 | train_wall 39 | gb_free 10.1 | wall 75682
2022-08-17 19:13:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:13:06 | INFO | fairseq.trainer | begin training epoch 950
2022-08-17 19:13:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:13:22 | INFO | train_inner | epoch 950:     31 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=5326, ups=0.96, wpb=5540.5, bsz=363.1, num_updates=76900, lr=0.000228069, gnorm=0.38, train_wall=48, gb_free=10.1, wall=75699
2022-08-17 19:13:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:13:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:13:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:13:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:13:57 | INFO | valid | epoch 950 | valid on 'valid' subset | loss 5.19 | nll_loss 2.591 | ppl 6.03 | bleu 56.72 | wps 1870.9 | wpb 933.5 | bsz 59.6 | num_updates 76950 | best_bleu 57.52
2022-08-17 19:13:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 950 @ 76950 updates
2022-08-17 19:13:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint950.pt
2022-08-17 19:13:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint950.pt
2022-08-17 19:14:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint950.pt (epoch 950 @ 76950 updates, score 56.72) (writing took 18.83382111042738 seconds)
2022-08-17 19:14:16 | INFO | fairseq_cli.train | end of epoch 950 (average epoch stats below)
2022-08-17 19:14:16 | INFO | train | epoch 950 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 6408.1 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 76950 | lr 0.000227995 | gnorm 0.392 | train_wall 40 | gb_free 10.1 | wall 75752
2022-08-17 19:14:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:14:16 | INFO | fairseq.trainer | begin training epoch 951
2022-08-17 19:14:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:14:42 | INFO | train_inner | epoch 951:     50 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=6921.7, ups=1.25, wpb=5541.1, bsz=360.2, num_updates=77000, lr=0.000227921, gnorm=0.347, train_wall=50, gb_free=10.1, wall=75779
2022-08-17 19:14:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:15:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:15:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:15:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:15:08 | INFO | valid | epoch 951 | valid on 'valid' subset | loss 5.185 | nll_loss 2.588 | ppl 6.01 | bleu 56.6 | wps 1896.2 | wpb 933.5 | bsz 59.6 | num_updates 77031 | best_bleu 57.52
2022-08-17 19:15:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 951 @ 77031 updates
2022-08-17 19:15:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint951.pt
2022-08-17 19:15:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint951.pt
2022-08-17 19:15:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint951.pt (epoch 951 @ 77031 updates, score 56.6) (writing took 21.579180233180523 seconds)
2022-08-17 19:15:29 | INFO | fairseq_cli.train | end of epoch 951 (average epoch stats below)
2022-08-17 19:15:29 | INFO | train | epoch 951 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6080.5 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 77031 | lr 0.000227875 | gnorm 0.332 | train_wall 41 | gb_free 10.1 | wall 75825
2022-08-17 19:15:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:15:29 | INFO | fairseq.trainer | begin training epoch 952
2022-08-17 19:15:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:16:05 | INFO | train_inner | epoch 952:     69 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=6690.7, ups=1.21, wpb=5536.6, bsz=356.9, num_updates=77100, lr=0.000227773, gnorm=0.373, train_wall=50, gb_free=10.1, wall=75861
2022-08-17 19:16:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:16:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:16:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:16:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:16:20 | INFO | valid | epoch 952 | valid on 'valid' subset | loss 5.199 | nll_loss 2.607 | ppl 6.09 | bleu 56.75 | wps 1944.3 | wpb 933.5 | bsz 59.6 | num_updates 77112 | best_bleu 57.52
2022-08-17 19:16:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 952 @ 77112 updates
2022-08-17 19:16:20 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint952.pt
2022-08-17 19:16:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint952.pt
2022-08-17 19:16:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint952.pt (epoch 952 @ 77112 updates, score 56.75) (writing took 36.8168626986444 seconds)
2022-08-17 19:16:57 | INFO | fairseq_cli.train | end of epoch 952 (average epoch stats below)
2022-08-17 19:16:57 | INFO | train | epoch 952 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5101.6 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 77112 | lr 0.000227756 | gnorm 0.358 | train_wall 40 | gb_free 10.2 | wall 75913
2022-08-17 19:16:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:16:57 | INFO | fairseq.trainer | begin training epoch 953
2022-08-17 19:16:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:17:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:17:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:17:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:17:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:17:48 | INFO | valid | epoch 953 | valid on 'valid' subset | loss 5.192 | nll_loss 2.593 | ppl 6.04 | bleu 57.3 | wps 1864.2 | wpb 933.5 | bsz 59.6 | num_updates 77193 | best_bleu 57.52
2022-08-17 19:17:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 953 @ 77193 updates
2022-08-17 19:17:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint953.pt
2022-08-17 19:17:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint953.pt
2022-08-17 19:18:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint953.pt (epoch 953 @ 77193 updates, score 57.3) (writing took 21.391553729772568 seconds)
2022-08-17 19:18:10 | INFO | fairseq_cli.train | end of epoch 953 (average epoch stats below)
2022-08-17 19:18:10 | INFO | train | epoch 953 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6146.8 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 77193 | lr 0.000227636 | gnorm 0.317 | train_wall 40 | gb_free 10.2 | wall 75986
2022-08-17 19:18:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:18:10 | INFO | fairseq.trainer | begin training epoch 954
2022-08-17 19:18:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:18:15 | INFO | train_inner | epoch 954:      7 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=4235.3, ups=0.77, wpb=5501.1, bsz=357.1, num_updates=77200, lr=0.000227626, gnorm=0.309, train_wall=49, gb_free=10.1, wall=75991
2022-08-17 19:18:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:18:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:18:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:18:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:19:01 | INFO | valid | epoch 954 | valid on 'valid' subset | loss 5.202 | nll_loss 2.608 | ppl 6.1 | bleu 56.82 | wps 1834.8 | wpb 933.5 | bsz 59.6 | num_updates 77274 | best_bleu 57.52
2022-08-17 19:19:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 954 @ 77274 updates
2022-08-17 19:19:01 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint954.pt
2022-08-17 19:19:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint954.pt
2022-08-17 19:19:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint954.pt (epoch 954 @ 77274 updates, score 56.82) (writing took 15.7015969902277 seconds)
2022-08-17 19:19:17 | INFO | fairseq_cli.train | end of epoch 954 (average epoch stats below)
2022-08-17 19:19:17 | INFO | train | epoch 954 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 6654.5 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 77274 | lr 0.000227517 | gnorm 0.347 | train_wall 39 | gb_free 10.2 | wall 76053
2022-08-17 19:19:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:19:17 | INFO | fairseq.trainer | begin training epoch 955
2022-08-17 19:19:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:19:31 | INFO | train_inner | epoch 955:     26 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=7196.7, ups=1.31, wpb=5493.7, bsz=356.4, num_updates=77300, lr=0.000227478, gnorm=0.351, train_wall=48, gb_free=10.2, wall=76068
2022-08-17 19:19:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:20:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:20:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:20:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:20:08 | INFO | valid | epoch 955 | valid on 'valid' subset | loss 5.18 | nll_loss 2.581 | ppl 5.98 | bleu 56.87 | wps 1867.1 | wpb 933.5 | bsz 59.6 | num_updates 77355 | best_bleu 57.52
2022-08-17 19:20:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 955 @ 77355 updates
2022-08-17 19:20:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint955.pt
2022-08-17 19:20:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint955.pt
2022-08-17 19:20:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint955.pt (epoch 955 @ 77355 updates, score 56.87) (writing took 39.156680420041084 seconds)
2022-08-17 19:20:48 | INFO | fairseq_cli.train | end of epoch 955 (average epoch stats below)
2022-08-17 19:20:48 | INFO | train | epoch 955 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 4930.2 | ups 0.89 | wpb 5523.2 | bsz 358 | num_updates 77355 | lr 0.000227398 | gnorm 0.345 | train_wall 40 | gb_free 10.1 | wall 76144
2022-08-17 19:20:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:20:48 | INFO | fairseq.trainer | begin training epoch 956
2022-08-17 19:20:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:21:11 | INFO | train_inner | epoch 956:     45 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=5586.5, ups=1, wpb=5571.6, bsz=364.3, num_updates=77400, lr=0.000227331, gnorm=0.325, train_wall=49, gb_free=10.1, wall=76167
2022-08-17 19:21:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:21:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:21:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:21:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:21:39 | INFO | valid | epoch 956 | valid on 'valid' subset | loss 5.194 | nll_loss 2.596 | ppl 6.05 | bleu 57.02 | wps 1747.1 | wpb 933.5 | bsz 59.6 | num_updates 77436 | best_bleu 57.52
2022-08-17 19:21:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 956 @ 77436 updates
2022-08-17 19:21:39 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint956.pt
2022-08-17 19:21:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint956.pt
2022-08-17 19:21:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint956.pt (epoch 956 @ 77436 updates, score 57.02) (writing took 18.85651084035635 seconds)
2022-08-17 19:21:58 | INFO | fairseq_cli.train | end of epoch 956 (average epoch stats below)
2022-08-17 19:21:58 | INFO | train | epoch 956 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6401.3 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 77436 | lr 0.000227279 | gnorm 0.306 | train_wall 39 | gb_free 10.2 | wall 76214
2022-08-17 19:21:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:21:58 | INFO | fairseq.trainer | begin training epoch 957
2022-08-17 19:21:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:22:31 | INFO | train_inner | epoch 957:     64 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=6953.5, ups=1.25, wpb=5545.8, bsz=357, num_updates=77500, lr=0.000227185, gnorm=0.331, train_wall=49, gb_free=10.1, wall=76247
2022-08-17 19:22:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:22:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:22:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:22:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:22:48 | INFO | valid | epoch 957 | valid on 'valid' subset | loss 5.189 | nll_loss 2.592 | ppl 6.03 | bleu 57.17 | wps 1954.4 | wpb 933.5 | bsz 59.6 | num_updates 77517 | best_bleu 57.52
2022-08-17 19:22:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 957 @ 77517 updates
2022-08-17 19:22:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint957.pt
2022-08-17 19:22:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint957.pt
2022-08-17 19:23:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint957.pt (epoch 957 @ 77517 updates, score 57.17) (writing took 16.247391384094954 seconds)
2022-08-17 19:23:05 | INFO | fairseq_cli.train | end of epoch 957 (average epoch stats below)
2022-08-17 19:23:05 | INFO | train | epoch 957 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6680.7 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 77517 | lr 0.00022716 | gnorm 0.345 | train_wall 40 | gb_free 10.2 | wall 76281
2022-08-17 19:23:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:23:05 | INFO | fairseq.trainer | begin training epoch 958
2022-08-17 19:23:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:23:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:23:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:23:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:23:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:23:55 | INFO | valid | epoch 958 | valid on 'valid' subset | loss 5.189 | nll_loss 2.593 | ppl 6.04 | bleu 57.36 | wps 1887.5 | wpb 933.5 | bsz 59.6 | num_updates 77598 | best_bleu 57.52
2022-08-17 19:23:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 958 @ 77598 updates
2022-08-17 19:23:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint958.pt
2022-08-17 19:23:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint958.pt
2022-08-17 19:24:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint958.pt (epoch 958 @ 77598 updates, score 57.36) (writing took 35.58190327882767 seconds)
2022-08-17 19:24:31 | INFO | fairseq_cli.train | end of epoch 958 (average epoch stats below)
2022-08-17 19:24:31 | INFO | train | epoch 958 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 5196.3 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 77598 | lr 0.000227041 | gnorm 0.305 | train_wall 39 | gb_free 10.4 | wall 76367
2022-08-17 19:24:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:24:31 | INFO | fairseq.trainer | begin training epoch 959
2022-08-17 19:24:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:24:33 | INFO | train_inner | epoch 959:      2 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=4482.1, ups=0.82, wpb=5469.7, bsz=353.6, num_updates=77600, lr=0.000227038, gnorm=0.315, train_wall=48, gb_free=10.1, wall=76369
2022-08-17 19:25:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:25:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:25:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:25:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:25:23 | INFO | valid | epoch 959 | valid on 'valid' subset | loss 5.213 | nll_loss 2.622 | ppl 6.15 | bleu 56.26 | wps 1825 | wpb 933.5 | bsz 59.6 | num_updates 77679 | best_bleu 57.52
2022-08-17 19:25:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 959 @ 77679 updates
2022-08-17 19:25:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint959.pt
2022-08-17 19:25:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint959.pt
2022-08-17 19:25:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint959.pt (epoch 959 @ 77679 updates, score 56.26) (writing took 26.649482667446136 seconds)
2022-08-17 19:25:50 | INFO | fairseq_cli.train | end of epoch 959 (average epoch stats below)
2022-08-17 19:25:50 | INFO | train | epoch 959 | loss 3.373 | nll_loss 0.34 | ppl 1.27 | wps 5672.5 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 77679 | lr 0.000226923 | gnorm 0.301 | train_wall 40 | gb_free 10.1 | wall 76446
2022-08-17 19:25:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:25:50 | INFO | fairseq.trainer | begin training epoch 960
2022-08-17 19:25:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:26:02 | INFO | train_inner | epoch 960:     21 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=6193.2, ups=1.12, wpb=5517.9, bsz=357.8, num_updates=77700, lr=0.000226892, gnorm=0.294, train_wall=49, gb_free=10.1, wall=76458
2022-08-17 19:26:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:26:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:26:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:26:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:26:42 | INFO | valid | epoch 960 | valid on 'valid' subset | loss 5.213 | nll_loss 2.622 | ppl 6.16 | bleu 56.7 | wps 1935.5 | wpb 933.5 | bsz 59.6 | num_updates 77760 | best_bleu 57.52
2022-08-17 19:26:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 960 @ 77760 updates
2022-08-17 19:26:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint960.pt
2022-08-17 19:26:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint960.pt
2022-08-17 19:26:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint960.pt (epoch 960 @ 77760 updates, score 56.7) (writing took 16.433284915983677 seconds)
2022-08-17 19:26:58 | INFO | fairseq_cli.train | end of epoch 960 (average epoch stats below)
2022-08-17 19:26:58 | INFO | train | epoch 960 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6488.6 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 77760 | lr 0.000226805 | gnorm 0.317 | train_wall 40 | gb_free 10.3 | wall 76515
2022-08-17 19:26:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:26:59 | INFO | fairseq.trainer | begin training epoch 961
2022-08-17 19:26:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:27:20 | INFO | train_inner | epoch 961:     40 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=7131, ups=1.29, wpb=5543.6, bsz=354.1, num_updates=77800, lr=0.000226746, gnorm=0.339, train_wall=49, gb_free=10.1, wall=76536
2022-08-17 19:27:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:27:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:27:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:27:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:27:49 | INFO | valid | epoch 961 | valid on 'valid' subset | loss 5.216 | nll_loss 2.624 | ppl 6.16 | bleu 56.29 | wps 1852.2 | wpb 933.5 | bsz 59.6 | num_updates 77841 | best_bleu 57.52
2022-08-17 19:27:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 961 @ 77841 updates
2022-08-17 19:27:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint961.pt
2022-08-17 19:27:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint961.pt
2022-08-17 19:28:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint961.pt (epoch 961 @ 77841 updates, score 56.29) (writing took 34.68800909817219 seconds)
2022-08-17 19:28:24 | INFO | fairseq_cli.train | end of epoch 961 (average epoch stats below)
2022-08-17 19:28:24 | INFO | train | epoch 961 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 5211 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 77841 | lr 0.000226687 | gnorm 0.375 | train_wall 40 | gb_free 10.1 | wall 76601
2022-08-17 19:28:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:28:25 | INFO | fairseq.trainer | begin training epoch 962
2022-08-17 19:28:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:28:56 | INFO | train_inner | epoch 962:     59 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=5769.3, ups=1.04, wpb=5530.9, bsz=358.9, num_updates=77900, lr=0.000226601, gnorm=0.381, train_wall=49, gb_free=10.1, wall=76632
2022-08-17 19:29:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:29:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:29:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:29:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:29:17 | INFO | valid | epoch 962 | valid on 'valid' subset | loss 5.212 | nll_loss 2.621 | ppl 6.15 | bleu 56.59 | wps 1768.1 | wpb 933.5 | bsz 59.6 | num_updates 77922 | best_bleu 57.52
2022-08-17 19:29:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 962 @ 77922 updates
2022-08-17 19:29:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint962.pt
2022-08-17 19:29:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint962.pt
2022-08-17 19:29:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint962.pt (epoch 962 @ 77922 updates, score 56.59) (writing took 19.218214064836502 seconds)
2022-08-17 19:29:36 | INFO | fairseq_cli.train | end of epoch 962 (average epoch stats below)
2022-08-17 19:29:36 | INFO | train | epoch 962 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6247.2 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 77922 | lr 0.000226569 | gnorm 0.345 | train_wall 41 | gb_free 10.1 | wall 76672
2022-08-17 19:29:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:29:36 | INFO | fairseq.trainer | begin training epoch 963
2022-08-17 19:29:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:30:25 | INFO | train_inner | epoch 963:     78 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=6171, ups=1.12, wpb=5526.9, bsz=362.3, num_updates=78000, lr=0.000226455, gnorm=0.371, train_wall=48, gb_free=10.1, wall=76722
2022-08-17 19:30:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:30:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:30:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:30:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:30:36 | INFO | valid | epoch 963 | valid on 'valid' subset | loss 5.227 | nll_loss 2.643 | ppl 6.24 | bleu 56.6 | wps 1863.1 | wpb 933.5 | bsz 59.6 | num_updates 78003 | best_bleu 57.52
2022-08-17 19:30:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 963 @ 78003 updates
2022-08-17 19:30:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint963.pt
2022-08-17 19:30:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint963.pt
2022-08-17 19:30:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint963.pt (epoch 963 @ 78003 updates, score 56.6) (writing took 16.971802685409784 seconds)
2022-08-17 19:30:53 | INFO | fairseq_cli.train | end of epoch 963 (average epoch stats below)
2022-08-17 19:30:53 | INFO | train | epoch 963 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5797.7 | ups 1.05 | wpb 5523.2 | bsz 358 | num_updates 78003 | lr 0.000226451 | gnorm 0.401 | train_wall 39 | gb_free 10.2 | wall 76749
2022-08-17 19:30:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:30:53 | INFO | fairseq.trainer | begin training epoch 964
2022-08-17 19:30:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:31:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:31:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:31:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:31:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:31:45 | INFO | valid | epoch 964 | valid on 'valid' subset | loss 5.206 | nll_loss 2.617 | ppl 6.13 | bleu 56.44 | wps 1811.2 | wpb 933.5 | bsz 59.6 | num_updates 78084 | best_bleu 57.52
2022-08-17 19:31:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 964 @ 78084 updates
2022-08-17 19:31:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint964.pt
2022-08-17 19:31:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint964.pt
2022-08-17 19:32:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint964.pt (epoch 964 @ 78084 updates, score 56.44) (writing took 35.00915291532874 seconds)
2022-08-17 19:32:20 | INFO | fairseq_cli.train | end of epoch 964 (average epoch stats below)
2022-08-17 19:32:20 | INFO | train | epoch 964 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 5122.9 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 78084 | lr 0.000226334 | gnorm 0.423 | train_wall 40 | gb_free 10.1 | wall 76837
2022-08-17 19:32:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:32:21 | INFO | fairseq.trainer | begin training epoch 965
2022-08-17 19:32:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:32:29 | INFO | train_inner | epoch 965:     16 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=4424.3, ups=0.81, wpb=5491.7, bsz=355.3, num_updates=78100, lr=0.00022631, gnorm=0.408, train_wall=49, gb_free=10.1, wall=76846
2022-08-17 19:33:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:33:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:33:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:33:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:33:13 | INFO | valid | epoch 965 | valid on 'valid' subset | loss 5.198 | nll_loss 2.607 | ppl 6.09 | bleu 56.44 | wps 1901.8 | wpb 933.5 | bsz 59.6 | num_updates 78165 | best_bleu 57.52
2022-08-17 19:33:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 965 @ 78165 updates
2022-08-17 19:33:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint965.pt
2022-08-17 19:33:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint965.pt
2022-08-17 19:33:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint965.pt (epoch 965 @ 78165 updates, score 56.44) (writing took 2.794367030262947 seconds)
2022-08-17 19:33:16 | INFO | fairseq_cli.train | end of epoch 965 (average epoch stats below)
2022-08-17 19:33:16 | INFO | train | epoch 965 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 8102.3 | ups 1.47 | wpb 5523.2 | bsz 358 | num_updates 78165 | lr 0.000226216 | gnorm 0.387 | train_wall 41 | gb_free 10.3 | wall 76892
2022-08-17 19:33:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:33:16 | INFO | fairseq.trainer | begin training epoch 966
2022-08-17 19:33:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:33:37 | INFO | train_inner | epoch 966:     35 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=8216.3, ups=1.48, wpb=5567.4, bsz=360.3, num_updates=78200, lr=0.000226166, gnorm=0.389, train_wall=50, gb_free=10.1, wall=76913
2022-08-17 19:34:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:34:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:34:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:34:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:34:11 | INFO | valid | epoch 966 | valid on 'valid' subset | loss 5.2 | nll_loss 2.607 | ppl 6.09 | bleu 57.1 | wps 1780.5 | wpb 933.5 | bsz 59.6 | num_updates 78246 | best_bleu 57.52
2022-08-17 19:34:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 966 @ 78246 updates
2022-08-17 19:34:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint966.pt
2022-08-17 19:34:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint966.pt
2022-08-17 19:34:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint966.pt (epoch 966 @ 78246 updates, score 57.1) (writing took 24.32735462486744 seconds)
2022-08-17 19:34:36 | INFO | fairseq_cli.train | end of epoch 966 (average epoch stats below)
2022-08-17 19:34:36 | INFO | train | epoch 966 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5578.2 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 78246 | lr 0.000226099 | gnorm 0.336 | train_wall 40 | gb_free 10.2 | wall 76972
2022-08-17 19:34:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:34:36 | INFO | fairseq.trainer | begin training epoch 967
2022-08-17 19:34:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:35:04 | INFO | train_inner | epoch 967:     54 / 81 loss=3.373, nll_loss=0.34, ppl=1.27, wps=6299.9, ups=1.15, wpb=5471.4, bsz=355.8, num_updates=78300, lr=0.000226021, gnorm=0.344, train_wall=49, gb_free=10.1, wall=77000
2022-08-17 19:35:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:35:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:35:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:35:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:35:27 | INFO | valid | epoch 967 | valid on 'valid' subset | loss 5.217 | nll_loss 2.623 | ppl 6.16 | bleu 56.91 | wps 1665.3 | wpb 933.5 | bsz 59.6 | num_updates 78327 | best_bleu 57.52
2022-08-17 19:35:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 967 @ 78327 updates
2022-08-17 19:35:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint967.pt
2022-08-17 19:35:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint967.pt
2022-08-17 19:36:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint967.pt (epoch 967 @ 78327 updates, score 56.91) (writing took 45.00902605801821 seconds)
2022-08-17 19:36:13 | INFO | fairseq_cli.train | end of epoch 967 (average epoch stats below)
2022-08-17 19:36:13 | INFO | train | epoch 967 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 4624.1 | ups 0.84 | wpb 5523.2 | bsz 358 | num_updates 78327 | lr 0.000225982 | gnorm 0.366 | train_wall 39 | gb_free 10.2 | wall 77069
2022-08-17 19:36:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:36:13 | INFO | fairseq.trainer | begin training epoch 968
2022-08-17 19:36:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:36:51 | INFO | train_inner | epoch 968:     73 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=5220, ups=0.94, wpb=5558.2, bsz=355.8, num_updates=78400, lr=0.000225877, gnorm=0.339, train_wall=49, gb_free=10, wall=77107
2022-08-17 19:36:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:36:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:36:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:36:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:37:04 | INFO | valid | epoch 968 | valid on 'valid' subset | loss 5.214 | nll_loss 2.62 | ppl 6.15 | bleu 56.75 | wps 1908.5 | wpb 933.5 | bsz 59.6 | num_updates 78408 | best_bleu 57.52
2022-08-17 19:37:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 968 @ 78408 updates
2022-08-17 19:37:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint968.pt
2022-08-17 19:37:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint968.pt
2022-08-17 19:37:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint968.pt (epoch 968 @ 78408 updates, score 56.75) (writing took 23.297805834561586 seconds)
2022-08-17 19:37:27 | INFO | fairseq_cli.train | end of epoch 968 (average epoch stats below)
2022-08-17 19:37:27 | INFO | train | epoch 968 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5996.3 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 78408 | lr 0.000225865 | gnorm 0.314 | train_wall 40 | gb_free 10 | wall 77143
2022-08-17 19:37:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:37:27 | INFO | fairseq.trainer | begin training epoch 969
2022-08-17 19:37:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:38:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:38:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:38:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:38:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:38:22 | INFO | valid | epoch 969 | valid on 'valid' subset | loss 5.229 | nll_loss 2.638 | ppl 6.22 | bleu 56.52 | wps 1945.2 | wpb 933.5 | bsz 59.6 | num_updates 78489 | best_bleu 57.52
2022-08-17 19:38:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 969 @ 78489 updates
2022-08-17 19:38:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint969.pt
2022-08-17 19:38:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint969.pt
2022-08-17 19:38:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint969.pt (epoch 969 @ 78489 updates, score 56.52) (writing took 15.467557568103075 seconds)
2022-08-17 19:38:37 | INFO | fairseq_cli.train | end of epoch 969 (average epoch stats below)
2022-08-17 19:38:37 | INFO | train | epoch 969 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6386.3 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 78489 | lr 0.000225749 | gnorm 0.293 | train_wall 41 | gb_free 10.1 | wall 77214
2022-08-17 19:38:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:38:38 | INFO | fairseq.trainer | begin training epoch 970
2022-08-17 19:38:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:38:45 | INFO | train_inner | epoch 970:     11 / 81 loss=3.373, nll_loss=0.34, ppl=1.27, wps=4821.6, ups=0.88, wpb=5506.1, bsz=360.2, num_updates=78500, lr=0.000225733, gnorm=0.282, train_wall=50, gb_free=10.1, wall=77221
2022-08-17 19:39:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:39:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:39:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:39:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:39:32 | INFO | valid | epoch 970 | valid on 'valid' subset | loss 5.207 | nll_loss 2.612 | ppl 6.11 | bleu 56.76 | wps 1791.6 | wpb 933.5 | bsz 59.6 | num_updates 78570 | best_bleu 57.52
2022-08-17 19:39:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 970 @ 78570 updates
2022-08-17 19:39:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint970.pt
2022-08-17 19:39:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint970.pt
2022-08-17 19:40:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint970.pt (epoch 970 @ 78570 updates, score 56.76) (writing took 35.18400310352445 seconds)
2022-08-17 19:40:08 | INFO | fairseq_cli.train | end of epoch 970 (average epoch stats below)
2022-08-17 19:40:08 | INFO | train | epoch 970 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 4949.4 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 78570 | lr 0.000225632 | gnorm 0.329 | train_wall 40 | gb_free 10.2 | wall 77304
2022-08-17 19:40:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:40:08 | INFO | fairseq.trainer | begin training epoch 971
2022-08-17 19:40:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:40:27 | INFO | train_inner | epoch 971:     30 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=5420.1, ups=0.98, wpb=5528.3, bsz=361.3, num_updates=78600, lr=0.000225589, gnorm=0.338, train_wall=50, gb_free=10.1, wall=77323
2022-08-17 19:40:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:40:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:40:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:40:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:41:02 | INFO | valid | epoch 971 | valid on 'valid' subset | loss 5.205 | nll_loss 2.611 | ppl 6.11 | bleu 56.93 | wps 1876.5 | wpb 933.5 | bsz 59.6 | num_updates 78651 | best_bleu 57.52
2022-08-17 19:41:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 971 @ 78651 updates
2022-08-17 19:41:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint971.pt
2022-08-17 19:41:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint971.pt
2022-08-17 19:41:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint971.pt (epoch 971 @ 78651 updates, score 56.93) (writing took 27.114472068846226 seconds)
2022-08-17 19:41:30 | INFO | fairseq_cli.train | end of epoch 971 (average epoch stats below)
2022-08-17 19:41:30 | INFO | train | epoch 971 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 5467.5 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 78651 | lr 0.000225516 | gnorm 0.351 | train_wall 40 | gb_free 10.1 | wall 77386
2022-08-17 19:41:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:41:30 | INFO | fairseq.trainer | begin training epoch 972
2022-08-17 19:41:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:41:56 | INFO | train_inner | epoch 972:     49 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=6201.2, ups=1.12, wpb=5527.4, bsz=353.8, num_updates=78700, lr=0.000225446, gnorm=0.345, train_wall=49, gb_free=10, wall=77412
2022-08-17 19:42:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:42:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:42:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:42:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:42:21 | INFO | valid | epoch 972 | valid on 'valid' subset | loss 5.201 | nll_loss 2.605 | ppl 6.09 | bleu 56.68 | wps 1916.3 | wpb 933.5 | bsz 59.6 | num_updates 78732 | best_bleu 57.52
2022-08-17 19:42:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 972 @ 78732 updates
2022-08-17 19:42:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint972.pt
2022-08-17 19:42:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint972.pt
2022-08-17 19:42:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint972.pt (epoch 972 @ 78732 updates, score 56.68) (writing took 16.018220826983452 seconds)
2022-08-17 19:42:38 | INFO | fairseq_cli.train | end of epoch 972 (average epoch stats below)
2022-08-17 19:42:38 | INFO | train | epoch 972 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 6580.1 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 78732 | lr 0.0002254 | gnorm 0.34 | train_wall 39 | gb_free 10.1 | wall 77454
2022-08-17 19:42:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:42:38 | INFO | fairseq.trainer | begin training epoch 973
2022-08-17 19:42:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:43:13 | INFO | train_inner | epoch 973:     68 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=7206.7, ups=1.3, wpb=5535.2, bsz=363.2, num_updates=78800, lr=0.000225303, gnorm=0.335, train_wall=49, gb_free=10.1, wall=77489
2022-08-17 19:43:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:43:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:43:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:43:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:43:29 | INFO | valid | epoch 973 | valid on 'valid' subset | loss 5.21 | nll_loss 2.615 | ppl 6.13 | bleu 56.66 | wps 1796.7 | wpb 933.5 | bsz 59.6 | num_updates 78813 | best_bleu 57.52
2022-08-17 19:43:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 973 @ 78813 updates
2022-08-17 19:43:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint973.pt
2022-08-17 19:43:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint973.pt
2022-08-17 19:44:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint973.pt (epoch 973 @ 78813 updates, score 56.66) (writing took 35.80399443954229 seconds)
2022-08-17 19:44:05 | INFO | fairseq_cli.train | end of epoch 973 (average epoch stats below)
2022-08-17 19:44:05 | INFO | train | epoch 973 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5138.5 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 78813 | lr 0.000225284 | gnorm 0.322 | train_wall 40 | gb_free 10.3 | wall 77541
2022-08-17 19:44:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:44:05 | INFO | fairseq.trainer | begin training epoch 974
2022-08-17 19:44:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:44:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:44:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:44:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:44:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:44:56 | INFO | valid | epoch 974 | valid on 'valid' subset | loss 5.204 | nll_loss 2.608 | ppl 6.09 | bleu 56.99 | wps 1886.5 | wpb 933.5 | bsz 59.6 | num_updates 78894 | best_bleu 57.52
2022-08-17 19:44:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 974 @ 78894 updates
2022-08-17 19:44:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint974.pt
2022-08-17 19:44:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint974.pt
2022-08-17 19:45:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint974.pt (epoch 974 @ 78894 updates, score 56.99) (writing took 24.908614944666624 seconds)
2022-08-17 19:45:21 | INFO | fairseq_cli.train | end of epoch 974 (average epoch stats below)
2022-08-17 19:45:21 | INFO | train | epoch 974 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5826.8 | ups 1.05 | wpb 5523.2 | bsz 358 | num_updates 78894 | lr 0.000225169 | gnorm 0.342 | train_wall 40 | gb_free 10.3 | wall 77618
2022-08-17 19:45:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:45:22 | INFO | fairseq.trainer | begin training epoch 975
2022-08-17 19:45:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:45:26 | INFO | train_inner | epoch 975:      6 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=4118.9, ups=0.75, wpb=5494.4, bsz=354.4, num_updates=78900, lr=0.00022516, gnorm=0.343, train_wall=49, gb_free=10.1, wall=77622
2022-08-17 19:46:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:46:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:46:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:46:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:46:14 | INFO | valid | epoch 975 | valid on 'valid' subset | loss 5.199 | nll_loss 2.602 | ppl 6.07 | bleu 56.84 | wps 1878.4 | wpb 933.5 | bsz 59.6 | num_updates 78975 | best_bleu 57.52
2022-08-17 19:46:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 975 @ 78975 updates
2022-08-17 19:46:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint975.pt
2022-08-17 19:46:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint975.pt
2022-08-17 19:46:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint975.pt (epoch 975 @ 78975 updates, score 56.84) (writing took 19.192840438336134 seconds)
2022-08-17 19:46:34 | INFO | fairseq_cli.train | end of epoch 975 (average epoch stats below)
2022-08-17 19:46:34 | INFO | train | epoch 975 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 6180.2 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 78975 | lr 0.000225053 | gnorm 0.399 | train_wall 41 | gb_free 10.1 | wall 77690
2022-08-17 19:46:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:46:34 | INFO | fairseq.trainer | begin training epoch 976
2022-08-17 19:46:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:46:48 | INFO | train_inner | epoch 976:     25 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=6794.5, ups=1.23, wpb=5541.1, bsz=359.5, num_updates=79000, lr=0.000225018, gnorm=0.382, train_wall=50, gb_free=10.1, wall=77704
2022-08-17 19:47:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:47:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:47:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:47:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:47:26 | INFO | valid | epoch 976 | valid on 'valid' subset | loss 5.197 | nll_loss 2.604 | ppl 6.08 | bleu 56.51 | wps 1850.2 | wpb 933.5 | bsz 59.6 | num_updates 79056 | best_bleu 57.52
2022-08-17 19:47:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 976 @ 79056 updates
2022-08-17 19:47:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint976.pt
2022-08-17 19:47:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint976.pt
2022-08-17 19:47:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint976.pt (epoch 976 @ 79056 updates, score 56.51) (writing took 32.01931557059288 seconds)
2022-08-17 19:47:58 | INFO | fairseq_cli.train | end of epoch 976 (average epoch stats below)
2022-08-17 19:47:58 | INFO | train | epoch 976 | loss 3.374 | nll_loss 0.341 | ppl 1.27 | wps 5328 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 79056 | lr 0.000224938 | gnorm 0.351 | train_wall 40 | gb_free 10.1 | wall 77774
2022-08-17 19:47:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:47:58 | INFO | fairseq.trainer | begin training epoch 977
2022-08-17 19:47:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:48:22 | INFO | train_inner | epoch 977:     44 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=5876.4, ups=1.06, wpb=5521.6, bsz=363.4, num_updates=79100, lr=0.000224875, gnorm=0.365, train_wall=50, gb_free=10.1, wall=77798
2022-08-17 19:48:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:48:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:48:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:48:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:48:49 | INFO | valid | epoch 977 | valid on 'valid' subset | loss 5.217 | nll_loss 2.628 | ppl 6.18 | bleu 56.75 | wps 1878.4 | wpb 933.5 | bsz 59.6 | num_updates 79137 | best_bleu 57.52
2022-08-17 19:48:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 977 @ 79137 updates
2022-08-17 19:48:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint977.pt
2022-08-17 19:48:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint977.pt
2022-08-17 19:49:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint977.pt (epoch 977 @ 79137 updates, score 56.75) (writing took 38.27374600246549 seconds)
2022-08-17 19:49:27 | INFO | fairseq_cli.train | end of epoch 977 (average epoch stats below)
2022-08-17 19:49:27 | INFO | train | epoch 977 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 4997 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 79137 | lr 0.000224823 | gnorm 0.389 | train_wall 40 | gb_free 10.1 | wall 77863
2022-08-17 19:49:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:49:27 | INFO | fairseq.trainer | begin training epoch 978
2022-08-17 19:49:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:50:00 | INFO | train_inner | epoch 978:     63 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=5639, ups=1.02, wpb=5530.4, bsz=352.6, num_updates=79200, lr=0.000224733, gnorm=0.348, train_wall=49, gb_free=10.2, wall=77896
2022-08-17 19:50:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:50:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:50:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:50:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:50:18 | INFO | valid | epoch 978 | valid on 'valid' subset | loss 5.196 | nll_loss 2.598 | ppl 6.05 | bleu 56.62 | wps 1868.9 | wpb 933.5 | bsz 59.6 | num_updates 79218 | best_bleu 57.52
2022-08-17 19:50:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 978 @ 79218 updates
2022-08-17 19:50:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint978.pt
2022-08-17 19:50:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint978.pt
2022-08-17 19:50:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint978.pt (epoch 978 @ 79218 updates, score 56.62) (writing took 20.021332148462534 seconds)
2022-08-17 19:50:38 | INFO | fairseq_cli.train | end of epoch 978 (average epoch stats below)
2022-08-17 19:50:38 | INFO | train | epoch 978 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 6326.2 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 79218 | lr 0.000224708 | gnorm 0.311 | train_wall 40 | gb_free 10.1 | wall 77934
2022-08-17 19:50:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:50:38 | INFO | fairseq.trainer | begin training epoch 979
2022-08-17 19:50:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:51:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:51:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:51:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:51:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:51:30 | INFO | valid | epoch 979 | valid on 'valid' subset | loss 5.201 | nll_loss 2.605 | ppl 6.08 | bleu 56.5 | wps 1761.7 | wpb 933.5 | bsz 59.6 | num_updates 79299 | best_bleu 57.52
2022-08-17 19:51:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 979 @ 79299 updates
2022-08-17 19:51:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint979.pt
2022-08-17 19:51:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint979.pt
2022-08-17 19:51:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint979.pt (epoch 979 @ 79299 updates, score 56.5) (writing took 13.751134634017944 seconds)
2022-08-17 19:51:44 | INFO | fairseq_cli.train | end of epoch 979 (average epoch stats below)
2022-08-17 19:51:44 | INFO | train | epoch 979 | loss 3.373 | nll_loss 0.34 | ppl 1.27 | wps 6751.4 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 79299 | lr 0.000224593 | gnorm 0.34 | train_wall 39 | gb_free 10.1 | wall 78000
2022-08-17 19:51:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:51:44 | INFO | fairseq.trainer | begin training epoch 980
2022-08-17 19:51:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:51:46 | INFO | train_inner | epoch 980:      1 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=5178.5, ups=0.94, wpb=5507.4, bsz=356.9, num_updates=79300, lr=0.000224592, gnorm=0.338, train_wall=48, gb_free=10.1, wall=78002
2022-08-17 19:52:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:52:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:52:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:52:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:52:38 | INFO | valid | epoch 980 | valid on 'valid' subset | loss 5.214 | nll_loss 2.623 | ppl 6.16 | bleu 56.29 | wps 1748.5 | wpb 933.5 | bsz 59.6 | num_updates 79380 | best_bleu 57.52
2022-08-17 19:52:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 980 @ 79380 updates
2022-08-17 19:52:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint980.pt
2022-08-17 19:52:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint980.pt
2022-08-17 19:53:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint980.pt (epoch 980 @ 79380 updates, score 56.29) (writing took 32.33498568460345 seconds)
2022-08-17 19:53:10 | INFO | fairseq_cli.train | end of epoch 980 (average epoch stats below)
2022-08-17 19:53:10 | INFO | train | epoch 980 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5199.7 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 79380 | lr 0.000224478 | gnorm 0.315 | train_wall 41 | gb_free 10.2 | wall 78087
2022-08-17 19:53:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:53:11 | INFO | fairseq.trainer | begin training epoch 981
2022-08-17 19:53:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:53:22 | INFO | train_inner | epoch 981:     20 / 81 loss=3.375, nll_loss=0.343, ppl=1.27, wps=5746.6, ups=1.04, wpb=5508.6, bsz=357.8, num_updates=79400, lr=0.00022445, gnorm=1.293, train_wall=51, gb_free=10.2, wall=78098
2022-08-17 19:53:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:53:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:53:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:53:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:54:01 | INFO | valid | epoch 981 | valid on 'valid' subset | loss 5.211 | nll_loss 2.616 | ppl 6.13 | bleu 55.7 | wps 1900.3 | wpb 933.5 | bsz 59.6 | num_updates 79461 | best_bleu 57.52
2022-08-17 19:54:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 981 @ 79461 updates
2022-08-17 19:54:01 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint981.pt
2022-08-17 19:54:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint981.pt
2022-08-17 19:54:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint981.pt (epoch 981 @ 79461 updates, score 55.7) (writing took 18.860163681209087 seconds)
2022-08-17 19:54:21 | INFO | fairseq_cli.train | end of epoch 981 (average epoch stats below)
2022-08-17 19:54:21 | INFO | train | epoch 981 | loss 3.376 | nll_loss 0.344 | ppl 1.27 | wps 6369.1 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 79461 | lr 0.000224364 | gnorm 1.586 | train_wall 40 | gb_free 10.2 | wall 78157
2022-08-17 19:54:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:54:21 | INFO | fairseq.trainer | begin training epoch 982
2022-08-17 19:54:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:54:42 | INFO | train_inner | epoch 982:     39 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=6876.2, ups=1.24, wpb=5536.9, bsz=358.6, num_updates=79500, lr=0.000224309, gnorm=0.361, train_wall=50, gb_free=10.1, wall=78179
2022-08-17 19:55:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:55:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:55:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:55:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:55:13 | INFO | valid | epoch 982 | valid on 'valid' subset | loss 5.189 | nll_loss 2.592 | ppl 6.03 | bleu 57.11 | wps 1850.2 | wpb 933.5 | bsz 59.6 | num_updates 79542 | best_bleu 57.52
2022-08-17 19:55:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 982 @ 79542 updates
2022-08-17 19:55:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint982.pt
2022-08-17 19:55:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint982.pt
2022-08-17 19:55:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint982.pt (epoch 982 @ 79542 updates, score 57.11) (writing took 16.0999999307096 seconds)
2022-08-17 19:55:29 | INFO | fairseq_cli.train | end of epoch 982 (average epoch stats below)
2022-08-17 19:55:29 | INFO | train | epoch 982 | loss 3.372 | nll_loss 0.339 | ppl 1.27 | wps 6554.1 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 79542 | lr 0.00022425 | gnorm 0.298 | train_wall 40 | gb_free 10.1 | wall 78225
2022-08-17 19:55:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:55:29 | INFO | fairseq.trainer | begin training epoch 983
2022-08-17 19:55:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:55:59 | INFO | train_inner | epoch 983:     58 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=7218.9, ups=1.3, wpb=5561.5, bsz=358.6, num_updates=79600, lr=0.000224168, gnorm=0.327, train_wall=50, gb_free=10.1, wall=78256
2022-08-17 19:56:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:56:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:56:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:56:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:56:19 | INFO | valid | epoch 983 | valid on 'valid' subset | loss 5.224 | nll_loss 2.633 | ppl 6.2 | bleu 56.4 | wps 1976.9 | wpb 933.5 | bsz 59.6 | num_updates 79623 | best_bleu 57.52
2022-08-17 19:56:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 983 @ 79623 updates
2022-08-17 19:56:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint983.pt
2022-08-17 19:56:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint983.pt
2022-08-17 19:56:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint983.pt (epoch 983 @ 79623 updates, score 56.4) (writing took 32.83043509721756 seconds)
2022-08-17 19:56:52 | INFO | fairseq_cli.train | end of epoch 983 (average epoch stats below)
2022-08-17 19:56:52 | INFO | train | epoch 983 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5350.7 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 79623 | lr 0.000224136 | gnorm 0.339 | train_wall 40 | gb_free 10.1 | wall 78309
2022-08-17 19:56:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:56:53 | INFO | fairseq.trainer | begin training epoch 984
2022-08-17 19:56:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:57:38 | INFO | train_inner | epoch 984:     77 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=5586, ups=1.02, wpb=5501.4, bsz=356.3, num_updates=79700, lr=0.000224027, gnorm=0.399, train_wall=55, gb_free=10.1, wall=78354
2022-08-17 19:57:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:57:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:57:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:57:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:57:49 | INFO | valid | epoch 984 | valid on 'valid' subset | loss 5.211 | nll_loss 2.619 | ppl 6.14 | bleu 56.33 | wps 1927.9 | wpb 933.5 | bsz 59.6 | num_updates 79704 | best_bleu 57.52
2022-08-17 19:57:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 984 @ 79704 updates
2022-08-17 19:57:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint984.pt
2022-08-17 19:57:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint984.pt
2022-08-17 19:58:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint984.pt (epoch 984 @ 79704 updates, score 56.33) (writing took 28.42104345560074 seconds)
2022-08-17 19:58:17 | INFO | fairseq_cli.train | end of epoch 984 (average epoch stats below)
2022-08-17 19:58:17 | INFO | train | epoch 984 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 5274.4 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 79704 | lr 0.000224022 | gnorm 0.415 | train_wall 46 | gb_free 10 | wall 78393
2022-08-17 19:58:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:58:17 | INFO | fairseq.trainer | begin training epoch 985
2022-08-17 19:58:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:58:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 19:59:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 19:59:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 19:59:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 19:59:09 | INFO | valid | epoch 985 | valid on 'valid' subset | loss 5.207 | nll_loss 2.613 | ppl 6.12 | bleu 56.84 | wps 1785.6 | wpb 933.5 | bsz 59.6 | num_updates 79785 | best_bleu 57.52
2022-08-17 19:59:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 985 @ 79785 updates
2022-08-17 19:59:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint985.pt
2022-08-17 19:59:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint985.pt
2022-08-17 19:59:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint985.pt (epoch 985 @ 79785 updates, score 56.84) (writing took 18.475856628268957 seconds)
2022-08-17 19:59:27 | INFO | fairseq_cli.train | end of epoch 985 (average epoch stats below)
2022-08-17 19:59:27 | INFO | train | epoch 985 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6392.8 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 79785 | lr 0.000223908 | gnorm 0.373 | train_wall 39 | gb_free 10.2 | wall 78463
2022-08-17 19:59:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 19:59:27 | INFO | fairseq.trainer | begin training epoch 986
2022-08-17 19:59:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 19:59:37 | INFO | train_inner | epoch 986:     15 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=4630.7, ups=0.84, wpb=5515.1, bsz=358.6, num_updates=79800, lr=0.000223887, gnorm=0.368, train_wall=48, gb_free=10, wall=78473
2022-08-17 20:00:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:00:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:00:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:00:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:00:23 | INFO | valid | epoch 986 | valid on 'valid' subset | loss 5.225 | nll_loss 2.637 | ppl 6.22 | bleu 56.1 | wps 1827.7 | wpb 933.5 | bsz 59.6 | num_updates 79866 | best_bleu 57.52
2022-08-17 20:00:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 986 @ 79866 updates
2022-08-17 20:00:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint986.pt
2022-08-17 20:00:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint986.pt
2022-08-17 20:01:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint986.pt (epoch 986 @ 79866 updates, score 56.1) (writing took 40.662620071321726 seconds)
2022-08-17 20:01:04 | INFO | fairseq_cli.train | end of epoch 986 (average epoch stats below)
2022-08-17 20:01:04 | INFO | train | epoch 986 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 4610.8 | ups 0.83 | wpb 5523.2 | bsz 358 | num_updates 79866 | lr 0.000223794 | gnorm 0.335 | train_wall 40 | gb_free 10 | wall 78560
2022-08-17 20:01:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:01:04 | INFO | fairseq.trainer | begin training epoch 987
2022-08-17 20:01:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:01:23 | INFO | train_inner | epoch 987:     34 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=5255.5, ups=0.94, wpb=5568.9, bsz=360.8, num_updates=79900, lr=0.000223747, gnorm=0.31, train_wall=50, gb_free=10, wall=78579
2022-08-17 20:01:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:01:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:01:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:01:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:01:56 | INFO | valid | epoch 987 | valid on 'valid' subset | loss 5.214 | nll_loss 2.623 | ppl 6.16 | bleu 56.58 | wps 1904.9 | wpb 933.5 | bsz 59.6 | num_updates 79947 | best_bleu 57.52
2022-08-17 20:01:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 987 @ 79947 updates
2022-08-17 20:01:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint987.pt
2022-08-17 20:01:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint987.pt
2022-08-17 20:02:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint987.pt (epoch 987 @ 79947 updates, score 56.58) (writing took 18.31237456575036 seconds)
2022-08-17 20:02:14 | INFO | fairseq_cli.train | end of epoch 987 (average epoch stats below)
2022-08-17 20:02:14 | INFO | train | epoch 987 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 6410.3 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 79947 | lr 0.000223681 | gnorm 0.307 | train_wall 40 | gb_free 10.1 | wall 78630
2022-08-17 20:02:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:02:14 | INFO | fairseq.trainer | begin training epoch 988
2022-08-17 20:02:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:02:42 | INFO | train_inner | epoch 988:     53 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=6917, ups=1.26, wpb=5486.4, bsz=353.1, num_updates=80000, lr=0.000223607, gnorm=0.304, train_wall=50, gb_free=10.1, wall=78659
2022-08-17 20:02:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:02:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:02:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:02:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:03:07 | INFO | valid | epoch 988 | valid on 'valid' subset | loss 5.224 | nll_loss 2.637 | ppl 6.22 | bleu 56.6 | wps 1667.4 | wpb 933.5 | bsz 59.6 | num_updates 80028 | best_bleu 57.52
2022-08-17 20:03:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 988 @ 80028 updates
2022-08-17 20:03:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint988.pt
2022-08-17 20:03:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint988.pt
2022-08-17 20:03:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint988.pt (epoch 988 @ 80028 updates, score 56.6) (writing took 16.32641650363803 seconds)
2022-08-17 20:03:23 | INFO | fairseq_cli.train | end of epoch 988 (average epoch stats below)
2022-08-17 20:03:23 | INFO | train | epoch 988 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6452.9 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 80028 | lr 0.000223568 | gnorm 0.33 | train_wall 41 | gb_free 10.1 | wall 78700
2022-08-17 20:03:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:03:24 | INFO | fairseq.trainer | begin training epoch 989
2022-08-17 20:03:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:04:02 | INFO | train_inner | epoch 989:     72 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=6895.7, ups=1.25, wpb=5524.6, bsz=361, num_updates=80100, lr=0.000223467, gnorm=0.382, train_wall=51, gb_free=10.2, wall=78739
2022-08-17 20:04:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:04:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:04:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:04:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:04:16 | INFO | valid | epoch 989 | valid on 'valid' subset | loss 5.218 | nll_loss 2.631 | ppl 6.2 | bleu 56.68 | wps 1829.7 | wpb 933.5 | bsz 59.6 | num_updates 80109 | best_bleu 57.52
2022-08-17 20:04:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 989 @ 80109 updates
2022-08-17 20:04:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint989.pt
2022-08-17 20:04:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint989.pt
2022-08-17 20:04:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint989.pt (epoch 989 @ 80109 updates, score 56.68) (writing took 39.942872915416956 seconds)
2022-08-17 20:04:56 | INFO | fairseq_cli.train | end of epoch 989 (average epoch stats below)
2022-08-17 20:04:56 | INFO | train | epoch 989 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 4806.5 | ups 0.87 | wpb 5523.2 | bsz 358 | num_updates 80109 | lr 0.000223455 | gnorm 0.384 | train_wall 41 | gb_free 10.1 | wall 78793
2022-08-17 20:04:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:04:57 | INFO | fairseq.trainer | begin training epoch 990
2022-08-17 20:04:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:05:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:05:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:05:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:05:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:05:49 | INFO | valid | epoch 990 | valid on 'valid' subset | loss 5.212 | nll_loss 2.619 | ppl 6.14 | bleu 56.66 | wps 1930.9 | wpb 933.5 | bsz 59.6 | num_updates 80190 | best_bleu 57.52
2022-08-17 20:05:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 990 @ 80190 updates
2022-08-17 20:05:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint990.pt
2022-08-17 20:05:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint990.pt
2022-08-17 20:06:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint990.pt (epoch 990 @ 80190 updates, score 56.66) (writing took 18.20193339139223 seconds)
2022-08-17 20:06:07 | INFO | fairseq_cli.train | end of epoch 990 (average epoch stats below)
2022-08-17 20:06:07 | INFO | train | epoch 990 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 6330 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 80190 | lr 0.000223342 | gnorm 0.478 | train_wall 41 | gb_free 10.1 | wall 78863
2022-08-17 20:06:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:06:07 | INFO | fairseq.trainer | begin training epoch 991
2022-08-17 20:06:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:06:13 | INFO | train_inner | epoch 991:     10 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=4198.1, ups=0.76, wpb=5496, bsz=356.8, num_updates=80200, lr=0.000223328, gnorm=0.465, train_wall=50, gb_free=10.1, wall=78870
2022-08-17 20:06:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:06:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:06:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:06:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:06:59 | INFO | valid | epoch 991 | valid on 'valid' subset | loss 5.209 | nll_loss 2.616 | ppl 6.13 | bleu 55.93 | wps 1850.9 | wpb 933.5 | bsz 59.6 | num_updates 80271 | best_bleu 57.52
2022-08-17 20:06:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 991 @ 80271 updates
2022-08-17 20:06:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint991.pt
2022-08-17 20:07:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint991.pt
2022-08-17 20:07:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint991.pt (epoch 991 @ 80271 updates, score 55.93) (writing took 18.036317247897387 seconds)
2022-08-17 20:07:17 | INFO | fairseq_cli.train | end of epoch 991 (average epoch stats below)
2022-08-17 20:07:17 | INFO | train | epoch 991 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6429.9 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 80271 | lr 0.000223229 | gnorm 0.389 | train_wall 40 | gb_free 10.2 | wall 78933
2022-08-17 20:07:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:07:17 | INFO | fairseq.trainer | begin training epoch 992
2022-08-17 20:07:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:07:33 | INFO | train_inner | epoch 992:     29 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=6970.1, ups=1.26, wpb=5545.8, bsz=359.1, num_updates=80300, lr=0.000223189, gnorm=0.379, train_wall=49, gb_free=10.1, wall=78949
2022-08-17 20:07:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:07:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:07:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:07:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:08:08 | INFO | valid | epoch 992 | valid on 'valid' subset | loss 5.206 | nll_loss 2.612 | ppl 6.11 | bleu 56.89 | wps 1766.8 | wpb 933.5 | bsz 59.6 | num_updates 80352 | best_bleu 57.52
2022-08-17 20:08:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 992 @ 80352 updates
2022-08-17 20:08:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint992.pt
2022-08-17 20:08:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint992.pt
2022-08-17 20:08:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint992.pt (epoch 992 @ 80352 updates, score 56.89) (writing took 30.012423165142536 seconds)
2022-08-17 20:08:38 | INFO | fairseq_cli.train | end of epoch 992 (average epoch stats below)
2022-08-17 20:08:38 | INFO | train | epoch 992 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5521 | ups 1 | wpb 5523.2 | bsz 358 | num_updates 80352 | lr 0.000223116 | gnorm 0.328 | train_wall 39 | gb_free 10 | wall 79014
2022-08-17 20:08:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:08:38 | INFO | fairseq.trainer | begin training epoch 993
2022-08-17 20:08:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:09:04 | INFO | train_inner | epoch 993:     48 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=6064.3, ups=1.1, wpb=5537.1, bsz=363.6, num_updates=80400, lr=0.00022305, gnorm=0.323, train_wall=48, gb_free=10.1, wall=79041
2022-08-17 20:09:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:09:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:09:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:09:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:09:30 | INFO | valid | epoch 993 | valid on 'valid' subset | loss 5.224 | nll_loss 2.635 | ppl 6.21 | bleu 56.33 | wps 1813.5 | wpb 933.5 | bsz 59.6 | num_updates 80433 | best_bleu 57.52
2022-08-17 20:09:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 993 @ 80433 updates
2022-08-17 20:09:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint993.pt
2022-08-17 20:09:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint993.pt
2022-08-17 20:09:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint993.pt (epoch 993 @ 80433 updates, score 56.33) (writing took 16.842329759150743 seconds)
2022-08-17 20:09:47 | INFO | fairseq_cli.train | end of epoch 993 (average epoch stats below)
2022-08-17 20:09:47 | INFO | train | epoch 993 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6458.8 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 80433 | lr 0.000223004 | gnorm 0.352 | train_wall 39 | gb_free 10.1 | wall 79083
2022-08-17 20:09:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:09:47 | INFO | fairseq.trainer | begin training epoch 994
2022-08-17 20:09:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:10:23 | INFO | train_inner | epoch 994:     67 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=6990.6, ups=1.26, wpb=5530.7, bsz=352.7, num_updates=80500, lr=0.000222911, gnorm=0.387, train_wall=49, gb_free=10.1, wall=79120
2022-08-17 20:10:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:10:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:10:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:10:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:10:40 | INFO | valid | epoch 994 | valid on 'valid' subset | loss 5.225 | nll_loss 2.634 | ppl 6.21 | bleu 56.61 | wps 1884.5 | wpb 933.5 | bsz 59.6 | num_updates 80514 | best_bleu 57.52
2022-08-17 20:10:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 994 @ 80514 updates
2022-08-17 20:10:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint994.pt
2022-08-17 20:10:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint994.pt
2022-08-17 20:11:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint994.pt (epoch 994 @ 80514 updates, score 56.61) (writing took 30.499493964016438 seconds)
2022-08-17 20:11:11 | INFO | fairseq_cli.train | end of epoch 994 (average epoch stats below)
2022-08-17 20:11:11 | INFO | train | epoch 994 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5344.4 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 80514 | lr 0.000222892 | gnorm 0.384 | train_wall 39 | gb_free 10 | wall 79167
2022-08-17 20:11:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:11:11 | INFO | fairseq.trainer | begin training epoch 995
2022-08-17 20:11:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:11:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:11:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:11:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:11:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:12:02 | INFO | valid | epoch 995 | valid on 'valid' subset | loss 5.224 | nll_loss 2.633 | ppl 6.2 | bleu 56.76 | wps 1869 | wpb 933.5 | bsz 59.6 | num_updates 80595 | best_bleu 57.52
2022-08-17 20:12:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 995 @ 80595 updates
2022-08-17 20:12:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint995.pt
2022-08-17 20:12:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint995.pt
2022-08-17 20:12:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint995.pt (epoch 995 @ 80595 updates, score 56.76) (writing took 33.99646198004484 seconds)
2022-08-17 20:12:36 | INFO | fairseq_cli.train | end of epoch 995 (average epoch stats below)
2022-08-17 20:12:36 | INFO | train | epoch 995 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5222 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 80595 | lr 0.00022278 | gnorm 0.36 | train_wall 40 | gb_free 10.1 | wall 79253
2022-08-17 20:12:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:12:37 | INFO | fairseq.trainer | begin training epoch 996
2022-08-17 20:12:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:12:41 | INFO | train_inner | epoch 996:      5 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=3999.5, ups=0.73, wpb=5493.5, bsz=357.9, num_updates=80600, lr=0.000222773, gnorm=0.365, train_wall=49, gb_free=10.1, wall=79257
2022-08-17 20:13:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:13:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:13:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:13:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:13:33 | INFO | valid | epoch 996 | valid on 'valid' subset | loss 5.228 | nll_loss 2.638 | ppl 6.22 | bleu 56.14 | wps 1846.2 | wpb 933.5 | bsz 59.6 | num_updates 80676 | best_bleu 57.52
2022-08-17 20:13:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 996 @ 80676 updates
2022-08-17 20:13:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint996.pt
2022-08-17 20:13:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint996.pt
2022-08-17 20:13:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint996.pt (epoch 996 @ 80676 updates, score 56.14) (writing took 20.175041913986206 seconds)
2022-08-17 20:13:53 | INFO | fairseq_cli.train | end of epoch 996 (average epoch stats below)
2022-08-17 20:13:53 | INFO | train | epoch 996 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5839.7 | ups 1.06 | wpb 5523.2 | bsz 358 | num_updates 80676 | lr 0.000222668 | gnorm 0.377 | train_wall 40 | gb_free 10.1 | wall 79329
2022-08-17 20:13:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:13:53 | INFO | fairseq.trainer | begin training epoch 997
2022-08-17 20:13:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:14:07 | INFO | train_inner | epoch 997:     24 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=6378.2, ups=1.15, wpb=5527.2, bsz=361.8, num_updates=80700, lr=0.000222635, gnorm=0.365, train_wall=49, gb_free=10.1, wall=79344
2022-08-17 20:14:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:14:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:14:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:14:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:14:46 | INFO | valid | epoch 997 | valid on 'valid' subset | loss 5.217 | nll_loss 2.622 | ppl 6.16 | bleu 57.08 | wps 1852 | wpb 933.5 | bsz 59.6 | num_updates 80757 | best_bleu 57.52
2022-08-17 20:14:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 997 @ 80757 updates
2022-08-17 20:14:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint997.pt
2022-08-17 20:14:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint997.pt
2022-08-17 20:15:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint997.pt (epoch 997 @ 80757 updates, score 57.08) (writing took 45.55673123151064 seconds)
2022-08-17 20:15:32 | INFO | fairseq_cli.train | end of epoch 997 (average epoch stats below)
2022-08-17 20:15:32 | INFO | train | epoch 997 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 4544.2 | ups 0.82 | wpb 5523.2 | bsz 358 | num_updates 80757 | lr 0.000222556 | gnorm 0.307 | train_wall 40 | gb_free 10.1 | wall 79428
2022-08-17 20:15:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:15:32 | INFO | fairseq.trainer | begin training epoch 998
2022-08-17 20:15:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:15:56 | INFO | train_inner | epoch 998:     43 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5105, ups=0.92, wpb=5526.2, bsz=355.7, num_updates=80800, lr=0.000222497, gnorm=0.299, train_wall=51, gb_free=10.1, wall=79452
2022-08-17 20:16:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:16:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:16:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:16:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:16:24 | INFO | valid | epoch 998 | valid on 'valid' subset | loss 5.222 | nll_loss 2.634 | ppl 6.21 | bleu 56.2 | wps 1869.9 | wpb 933.5 | bsz 59.6 | num_updates 80838 | best_bleu 57.52
2022-08-17 20:16:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 998 @ 80838 updates
2022-08-17 20:16:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint998.pt
2022-08-17 20:16:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint998.pt
2022-08-17 20:16:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint998.pt (epoch 998 @ 80838 updates, score 56.2) (writing took 31.78987865895033 seconds)
2022-08-17 20:16:56 | INFO | fairseq_cli.train | end of epoch 998 (average epoch stats below)
2022-08-17 20:16:56 | INFO | train | epoch 998 | loss 3.373 | nll_loss 0.34 | ppl 1.27 | wps 5280.9 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 80838 | lr 0.000222445 | gnorm 0.299 | train_wall 41 | gb_free 10.2 | wall 79512
2022-08-17 20:16:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:16:56 | INFO | fairseq.trainer | begin training epoch 999
2022-08-17 20:16:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:17:30 | INFO | train_inner | epoch 999:     62 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=5863.4, ups=1.06, wpb=5536.3, bsz=355.6, num_updates=80900, lr=0.00022236, gnorm=0.291, train_wall=50, gb_free=10.1, wall=79546
2022-08-17 20:17:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:17:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:17:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:17:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:17:49 | INFO | valid | epoch 999 | valid on 'valid' subset | loss 5.216 | nll_loss 2.624 | ppl 6.17 | bleu 56.61 | wps 1905.1 | wpb 933.5 | bsz 59.6 | num_updates 80919 | best_bleu 57.52
2022-08-17 20:17:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 999 @ 80919 updates
2022-08-17 20:17:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint999.pt
2022-08-17 20:17:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint999.pt
2022-08-17 20:17:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint999.pt (epoch 999 @ 80919 updates, score 56.61) (writing took 2.6057311482727528 seconds)
2022-08-17 20:17:51 | INFO | fairseq_cli.train | end of epoch 999 (average epoch stats below)
2022-08-17 20:17:51 | INFO | train | epoch 999 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 8123.7 | ups 1.47 | wpb 5523.2 | bsz 358 | num_updates 80919 | lr 0.000222333 | gnorm 0.304 | train_wall 40 | gb_free 10.2 | wall 79568
2022-08-17 20:17:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:17:51 | INFO | fairseq.trainer | begin training epoch 1000
2022-08-17 20:17:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:18:33 | INFO | train_inner | epoch 1000:     81 / 81 loss=3.373, nll_loss=0.34, ppl=1.27, wps=8698.8, ups=1.58, wpb=5496.1, bsz=358.6, num_updates=81000, lr=0.000222222, gnorm=0.325, train_wall=49, gb_free=10, wall=79610
2022-08-17 20:18:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:18:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:18:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:18:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:18:43 | INFO | valid | epoch 1000 | valid on 'valid' subset | loss 5.214 | nll_loss 2.622 | ppl 6.16 | bleu 56.7 | wps 1884.1 | wpb 933.5 | bsz 59.6 | num_updates 81000 | best_bleu 57.52
2022-08-17 20:18:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1000 @ 81000 updates
2022-08-17 20:18:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1000.pt
2022-08-17 20:18:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1000.pt
2022-08-17 20:19:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1000.pt (epoch 1000 @ 81000 updates, score 56.7) (writing took 40.38622684031725 seconds)
2022-08-17 20:19:23 | INFO | fairseq_cli.train | end of epoch 1000 (average epoch stats below)
2022-08-17 20:19:23 | INFO | train | epoch 1000 | loss 3.373 | nll_loss 0.34 | ppl 1.27 | wps 4876.6 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 81000 | lr 0.000222222 | gnorm 0.316 | train_wall 40 | gb_free 10 | wall 79659
2022-08-17 20:19:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:19:23 | INFO | fairseq.trainer | begin training epoch 1001
2022-08-17 20:19:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:20:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:20:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:20:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:20:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:20:15 | INFO | valid | epoch 1001 | valid on 'valid' subset | loss 5.191 | nll_loss 2.594 | ppl 6.04 | bleu 57 | wps 1838.3 | wpb 933.5 | bsz 59.6 | num_updates 81081 | best_bleu 57.52
2022-08-17 20:20:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1001 @ 81081 updates
2022-08-17 20:20:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1001.pt
2022-08-17 20:20:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1001.pt
2022-08-17 20:20:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1001.pt (epoch 1001 @ 81081 updates, score 57.0) (writing took 20.416027948260307 seconds)
2022-08-17 20:20:35 | INFO | fairseq_cli.train | end of epoch 1001 (average epoch stats below)
2022-08-17 20:20:35 | INFO | train | epoch 1001 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 6179.1 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 81081 | lr 0.000222111 | gnorm 0.397 | train_wall 40 | gb_free 10.1 | wall 79732
2022-08-17 20:20:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:20:36 | INFO | fairseq.trainer | begin training epoch 1002
2022-08-17 20:20:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:20:47 | INFO | train_inner | epoch 1002:     19 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=4133.7, ups=0.75, wpb=5523.5, bsz=360.9, num_updates=81100, lr=0.000222085, gnorm=0.376, train_wall=50, gb_free=10.1, wall=79743
2022-08-17 20:21:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:21:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:21:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:21:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:21:28 | INFO | valid | epoch 1002 | valid on 'valid' subset | loss 5.213 | nll_loss 2.618 | ppl 6.14 | bleu 57 | wps 1825.4 | wpb 933.5 | bsz 59.6 | num_updates 81162 | best_bleu 57.52
2022-08-17 20:21:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1002 @ 81162 updates
2022-08-17 20:21:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1002.pt
2022-08-17 20:21:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1002.pt
2022-08-17 20:21:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1002.pt (epoch 1002 @ 81162 updates, score 57.0) (writing took 16.26777133345604 seconds)
2022-08-17 20:21:44 | INFO | fairseq_cli.train | end of epoch 1002 (average epoch stats below)
2022-08-17 20:21:44 | INFO | train | epoch 1002 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6507.1 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 81162 | lr 0.000222 | gnorm 0.352 | train_wall 40 | gb_free 10.2 | wall 79800
2022-08-17 20:21:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:21:44 | INFO | fairseq.trainer | begin training epoch 1003
2022-08-17 20:21:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:22:06 | INFO | train_inner | epoch 1003:     38 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=6984.2, ups=1.26, wpb=5528.5, bsz=359.8, num_updates=81200, lr=0.000221948, gnorm=0.352, train_wall=49, gb_free=10.1, wall=79822
2022-08-17 20:22:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:22:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:22:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:22:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:22:37 | INFO | valid | epoch 1003 | valid on 'valid' subset | loss 5.213 | nll_loss 2.618 | ppl 6.14 | bleu 56.65 | wps 1785.2 | wpb 933.5 | bsz 59.6 | num_updates 81243 | best_bleu 57.52
2022-08-17 20:22:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1003 @ 81243 updates
2022-08-17 20:22:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1003.pt
2022-08-17 20:22:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1003.pt
2022-08-17 20:23:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1003.pt (epoch 1003 @ 81243 updates, score 56.65) (writing took 34.285852797329426 seconds)
2022-08-17 20:23:12 | INFO | fairseq_cli.train | end of epoch 1003 (average epoch stats below)
2022-08-17 20:23:12 | INFO | train | epoch 1003 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5119.9 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 81243 | lr 0.00022189 | gnorm 0.312 | train_wall 40 | gb_free 10.3 | wall 79888
2022-08-17 20:23:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:23:12 | INFO | fairseq.trainer | begin training epoch 1004
2022-08-17 20:23:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:23:43 | INFO | train_inner | epoch 1004:     57 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=5739.7, ups=1.04, wpb=5539.4, bsz=356.1, num_updates=81300, lr=0.000221812, gnorm=0.338, train_wall=49, gb_free=10.1, wall=79919
2022-08-17 20:23:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:23:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:23:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:23:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:24:05 | INFO | valid | epoch 1004 | valid on 'valid' subset | loss 5.213 | nll_loss 2.623 | ppl 6.16 | bleu 56.41 | wps 1637.1 | wpb 933.5 | bsz 59.6 | num_updates 81324 | best_bleu 57.52
2022-08-17 20:24:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1004 @ 81324 updates
2022-08-17 20:24:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1004.pt
2022-08-17 20:24:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1004.pt
2022-08-17 20:24:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1004.pt (epoch 1004 @ 81324 updates, score 56.41) (writing took 43.47463462129235 seconds)
2022-08-17 20:24:49 | INFO | fairseq_cli.train | end of epoch 1004 (average epoch stats below)
2022-08-17 20:24:49 | INFO | train | epoch 1004 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 4604.8 | ups 0.83 | wpb 5523.2 | bsz 358 | num_updates 81324 | lr 0.000221779 | gnorm 0.356 | train_wall 40 | gb_free 10.1 | wall 79985
2022-08-17 20:24:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:24:49 | INFO | fairseq.trainer | begin training epoch 1005
2022-08-17 20:24:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:25:29 | INFO | train_inner | epoch 1005:     76 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=5193.4, ups=0.94, wpb=5536.5, bsz=358.2, num_updates=81400, lr=0.000221676, gnorm=0.335, train_wall=49, gb_free=10.1, wall=80025
2022-08-17 20:25:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:25:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:25:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:25:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:25:41 | INFO | valid | epoch 1005 | valid on 'valid' subset | loss 5.218 | nll_loss 2.63 | ppl 6.19 | bleu 56.65 | wps 1866.4 | wpb 933.5 | bsz 59.6 | num_updates 81405 | best_bleu 57.52
2022-08-17 20:25:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1005 @ 81405 updates
2022-08-17 20:25:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1005.pt
2022-08-17 20:25:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1005.pt
2022-08-17 20:26:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1005.pt (epoch 1005 @ 81405 updates, score 56.65) (writing took 22.111782658845186 seconds)
2022-08-17 20:26:03 | INFO | fairseq_cli.train | end of epoch 1005 (average epoch stats below)
2022-08-17 20:26:03 | INFO | train | epoch 1005 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6010.3 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 81405 | lr 0.000221669 | gnorm 0.329 | train_wall 40 | gb_free 10.3 | wall 80059
2022-08-17 20:26:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:26:03 | INFO | fairseq.trainer | begin training epoch 1006
2022-08-17 20:26:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:26:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:26:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:26:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:26:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:26:54 | INFO | valid | epoch 1006 | valid on 'valid' subset | loss 5.213 | nll_loss 2.623 | ppl 6.16 | bleu 56.37 | wps 1947.4 | wpb 933.5 | bsz 59.6 | num_updates 81486 | best_bleu 57.52
2022-08-17 20:26:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1006 @ 81486 updates
2022-08-17 20:26:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1006.pt
2022-08-17 20:26:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1006.pt
2022-08-17 20:27:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1006.pt (epoch 1006 @ 81486 updates, score 56.37) (writing took 23.25880429148674 seconds)
2022-08-17 20:27:17 | INFO | fairseq_cli.train | end of epoch 1006 (average epoch stats below)
2022-08-17 20:27:17 | INFO | train | epoch 1006 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 6063.2 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 81486 | lr 0.000221559 | gnorm 0.444 | train_wall 40 | gb_free 10.2 | wall 80133
2022-08-17 20:27:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:27:17 | INFO | fairseq.trainer | begin training epoch 1007
2022-08-17 20:27:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:27:26 | INFO | train_inner | epoch 1007:     14 / 81 loss=3.374, nll_loss=0.341, ppl=1.27, wps=4702.4, ups=0.86, wpb=5478.7, bsz=352.1, num_updates=81500, lr=0.00022154, gnorm=0.419, train_wall=49, gb_free=10, wall=80142
2022-08-17 20:27:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:28:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:28:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:28:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:28:08 | INFO | valid | epoch 1007 | valid on 'valid' subset | loss 5.221 | nll_loss 2.631 | ppl 6.19 | bleu 56.59 | wps 1893.4 | wpb 933.5 | bsz 59.6 | num_updates 81567 | best_bleu 57.52
2022-08-17 20:28:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1007 @ 81567 updates
2022-08-17 20:28:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1007.pt
2022-08-17 20:28:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1007.pt
2022-08-17 20:28:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1007.pt (epoch 1007 @ 81567 updates, score 56.59) (writing took 40.17093102261424 seconds)
2022-08-17 20:28:49 | INFO | fairseq_cli.train | end of epoch 1007 (average epoch stats below)
2022-08-17 20:28:49 | INFO | train | epoch 1007 | loss 3.373 | nll_loss 0.34 | ppl 1.27 | wps 4872 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 81567 | lr 0.000221449 | gnorm 0.317 | train_wall 40 | gb_free 10.2 | wall 80225
2022-08-17 20:28:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:28:49 | INFO | fairseq.trainer | begin training epoch 1008
2022-08-17 20:28:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:29:09 | INFO | train_inner | epoch 1008:     33 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5326.7, ups=0.96, wpb=5527.4, bsz=360.1, num_updates=81600, lr=0.000221404, gnorm=0.344, train_wall=49, gb_free=10.1, wall=80246
2022-08-17 20:29:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:29:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:29:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:29:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:29:43 | INFO | valid | epoch 1008 | valid on 'valid' subset | loss 5.228 | nll_loss 2.639 | ppl 6.23 | bleu 56.49 | wps 1772.5 | wpb 933.5 | bsz 59.6 | num_updates 81648 | best_bleu 57.52
2022-08-17 20:29:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1008 @ 81648 updates
2022-08-17 20:29:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1008.pt
2022-08-17 20:29:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1008.pt
2022-08-17 20:30:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1008.pt (epoch 1008 @ 81648 updates, score 56.49) (writing took 19.646806340664625 seconds)
2022-08-17 20:30:03 | INFO | fairseq_cli.train | end of epoch 1008 (average epoch stats below)
2022-08-17 20:30:03 | INFO | train | epoch 1008 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6048.3 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 81648 | lr 0.000221339 | gnorm 0.377 | train_wall 40 | gb_free 10.2 | wall 80299
2022-08-17 20:30:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:30:03 | INFO | fairseq.trainer | begin training epoch 1009
2022-08-17 20:30:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:30:32 | INFO | train_inner | epoch 1009:     52 / 81 loss=3.373, nll_loss=0.342, ppl=1.27, wps=6735.6, ups=1.22, wpb=5537.4, bsz=362.7, num_updates=81700, lr=0.000221268, gnorm=0.375, train_wall=49, gb_free=10, wall=80328
2022-08-17 20:30:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:30:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:30:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:30:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:30:55 | INFO | valid | epoch 1009 | valid on 'valid' subset | loss 5.228 | nll_loss 2.638 | ppl 6.23 | bleu 56.36 | wps 2004.2 | wpb 933.5 | bsz 59.6 | num_updates 81729 | best_bleu 57.52
2022-08-17 20:30:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1009 @ 81729 updates
2022-08-17 20:30:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1009.pt
2022-08-17 20:30:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1009.pt
2022-08-17 20:31:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1009.pt (epoch 1009 @ 81729 updates, score 56.36) (writing took 15.56011113524437 seconds)
2022-08-17 20:31:11 | INFO | fairseq_cli.train | end of epoch 1009 (average epoch stats below)
2022-08-17 20:31:11 | INFO | train | epoch 1009 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6578.1 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 81729 | lr 0.000221229 | gnorm 0.391 | train_wall 40 | gb_free 10.2 | wall 80367
2022-08-17 20:31:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:31:11 | INFO | fairseq.trainer | begin training epoch 1010
2022-08-17 20:31:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:31:48 | INFO | train_inner | epoch 1010:     71 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=7261.1, ups=1.31, wpb=5558.1, bsz=358.3, num_updates=81800, lr=0.000221133, gnorm=0.473, train_wall=50, gb_free=10.1, wall=80404
2022-08-17 20:31:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:31:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:31:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:31:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:32:02 | INFO | valid | epoch 1010 | valid on 'valid' subset | loss 5.205 | nll_loss 2.61 | ppl 6.11 | bleu 56.66 | wps 1878.9 | wpb 933.5 | bsz 59.6 | num_updates 81810 | best_bleu 57.52
2022-08-17 20:32:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1010 @ 81810 updates
2022-08-17 20:32:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1010.pt
2022-08-17 20:32:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1010.pt
2022-08-17 20:32:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1010.pt (epoch 1010 @ 81810 updates, score 56.66) (writing took 38.51715188473463 seconds)
2022-08-17 20:32:41 | INFO | fairseq_cli.train | end of epoch 1010 (average epoch stats below)
2022-08-17 20:32:41 | INFO | train | epoch 1010 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 4978.9 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 81810 | lr 0.000221119 | gnorm 0.494 | train_wall 40 | gb_free 10.1 | wall 80457
2022-08-17 20:32:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:32:41 | INFO | fairseq.trainer | begin training epoch 1011
2022-08-17 20:32:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:33:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:33:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:33:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:33:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:33:35 | INFO | valid | epoch 1011 | valid on 'valid' subset | loss 5.218 | nll_loss 2.627 | ppl 6.18 | bleu 56.13 | wps 1858.5 | wpb 933.5 | bsz 59.6 | num_updates 81891 | best_bleu 57.52
2022-08-17 20:33:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1011 @ 81891 updates
2022-08-17 20:33:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1011.pt
2022-08-17 20:33:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1011.pt
2022-08-17 20:33:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1011.pt (epoch 1011 @ 81891 updates, score 56.13) (writing took 21.048598930239677 seconds)
2022-08-17 20:33:56 | INFO | fairseq_cli.train | end of epoch 1011 (average epoch stats below)
2022-08-17 20:33:56 | INFO | train | epoch 1011 | loss 3.373 | nll_loss 0.34 | ppl 1.27 | wps 5913.3 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 81891 | lr 0.00022101 | gnorm 0.341 | train_wall 41 | gb_free 10.2 | wall 80533
2022-08-17 20:33:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:33:57 | INFO | fairseq.trainer | begin training epoch 1012
2022-08-17 20:33:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:34:03 | INFO | train_inner | epoch 1012:      9 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=4073.5, ups=0.74, wpb=5480.2, bsz=355.4, num_updates=81900, lr=0.000220998, gnorm=0.344, train_wall=50, gb_free=10.1, wall=80539
2022-08-17 20:34:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:34:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:34:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:34:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:34:50 | INFO | valid | epoch 1012 | valid on 'valid' subset | loss 5.243 | nll_loss 2.655 | ppl 6.3 | bleu 56.29 | wps 1744.2 | wpb 933.5 | bsz 59.6 | num_updates 81972 | best_bleu 57.52
2022-08-17 20:34:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1012 @ 81972 updates
2022-08-17 20:34:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1012.pt
2022-08-17 20:34:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1012.pt
2022-08-17 20:35:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1012.pt (epoch 1012 @ 81972 updates, score 56.29) (writing took 15.430095583200455 seconds)
2022-08-17 20:35:05 | INFO | fairseq_cli.train | end of epoch 1012 (average epoch stats below)
2022-08-17 20:35:05 | INFO | train | epoch 1012 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6488.1 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 81972 | lr 0.000220901 | gnorm 0.293 | train_wall 41 | gb_free 10.2 | wall 80602
2022-08-17 20:35:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:35:05 | INFO | fairseq.trainer | begin training epoch 1013
2022-08-17 20:35:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:35:21 | INFO | train_inner | epoch 1013:     28 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=7084, ups=1.28, wpb=5544.8, bsz=359.2, num_updates=82000, lr=0.000220863, gnorm=0.287, train_wall=51, gb_free=10.1, wall=80617
2022-08-17 20:35:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:35:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:35:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:35:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:35:58 | INFO | valid | epoch 1013 | valid on 'valid' subset | loss 5.23 | nll_loss 2.641 | ppl 6.24 | bleu 56.07 | wps 1756.4 | wpb 933.5 | bsz 59.6 | num_updates 82053 | best_bleu 57.52
2022-08-17 20:35:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1013 @ 82053 updates
2022-08-17 20:35:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1013.pt
2022-08-17 20:35:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1013.pt
2022-08-17 20:36:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1013.pt (epoch 1013 @ 82053 updates, score 56.07) (writing took 35.952269561588764 seconds)
2022-08-17 20:36:34 | INFO | fairseq_cli.train | end of epoch 1013 (average epoch stats below)
2022-08-17 20:36:34 | INFO | train | epoch 1013 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5057.1 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 82053 | lr 0.000220792 | gnorm 0.368 | train_wall 41 | gb_free 10.2 | wall 80690
2022-08-17 20:36:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:36:34 | INFO | fairseq.trainer | begin training epoch 1014
2022-08-17 20:36:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:37:01 | INFO | train_inner | epoch 1014:     47 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=5553, ups=1, wpb=5531.3, bsz=356.6, num_updates=82100, lr=0.000220729, gnorm=0.384, train_wall=50, gb_free=10, wall=80717
2022-08-17 20:37:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:37:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:37:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:37:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:37:28 | INFO | valid | epoch 1014 | valid on 'valid' subset | loss 5.211 | nll_loss 2.618 | ppl 6.14 | bleu 56.73 | wps 1935.2 | wpb 933.5 | bsz 59.6 | num_updates 82134 | best_bleu 57.52
2022-08-17 20:37:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1014 @ 82134 updates
2022-08-17 20:37:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1014.pt
2022-08-17 20:37:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1014.pt
2022-08-17 20:37:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1014.pt (epoch 1014 @ 82134 updates, score 56.73) (writing took 17.43262629583478 seconds)
2022-08-17 20:37:46 | INFO | fairseq_cli.train | end of epoch 1014 (average epoch stats below)
2022-08-17 20:37:46 | INFO | train | epoch 1014 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 6220.1 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 82134 | lr 0.000220683 | gnorm 0.332 | train_wall 41 | gb_free 10.3 | wall 80762
2022-08-17 20:37:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:37:46 | INFO | fairseq.trainer | begin training epoch 1015
2022-08-17 20:37:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:38:22 | INFO | train_inner | epoch 1015:     66 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=6798.1, ups=1.23, wpb=5509.2, bsz=356, num_updates=82200, lr=0.000220594, gnorm=0.333, train_wall=50, gb_free=10.1, wall=80798
2022-08-17 20:38:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:38:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:38:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:38:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:38:38 | INFO | valid | epoch 1015 | valid on 'valid' subset | loss 5.226 | nll_loss 2.634 | ppl 6.21 | bleu 56.82 | wps 1808.4 | wpb 933.5 | bsz 59.6 | num_updates 82215 | best_bleu 57.52
2022-08-17 20:38:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1015 @ 82215 updates
2022-08-17 20:38:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1015.pt
2022-08-17 20:38:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1015.pt
2022-08-17 20:38:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1015.pt (epoch 1015 @ 82215 updates, score 56.82) (writing took 15.470215782523155 seconds)
2022-08-17 20:38:54 | INFO | fairseq_cli.train | end of epoch 1015 (average epoch stats below)
2022-08-17 20:38:54 | INFO | train | epoch 1015 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6540.6 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 82215 | lr 0.000220574 | gnorm 0.338 | train_wall 40 | gb_free 10.1 | wall 80830
2022-08-17 20:38:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:38:54 | INFO | fairseq.trainer | begin training epoch 1016
2022-08-17 20:38:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:39:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:39:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:39:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:39:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:39:51 | INFO | valid | epoch 1016 | valid on 'valid' subset | loss 5.236 | nll_loss 2.648 | ppl 6.27 | bleu 56.41 | wps 1937.9 | wpb 933.5 | bsz 59.6 | num_updates 82296 | best_bleu 57.52
2022-08-17 20:39:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1016 @ 82296 updates
2022-08-17 20:39:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1016.pt
2022-08-17 20:39:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1016.pt
2022-08-17 20:40:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1016.pt (epoch 1016 @ 82296 updates, score 56.41) (writing took 40.475262731313705 seconds)
2022-08-17 20:40:31 | INFO | fairseq_cli.train | end of epoch 1016 (average epoch stats below)
2022-08-17 20:40:31 | INFO | train | epoch 1016 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 4600.7 | ups 0.83 | wpb 5523.2 | bsz 358 | num_updates 82296 | lr 0.000220465 | gnorm 0.298 | train_wall 40 | gb_free 10.1 | wall 80928
2022-08-17 20:40:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:40:32 | INFO | fairseq.trainer | begin training epoch 1017
2022-08-17 20:40:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:40:35 | INFO | train_inner | epoch 1017:      4 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=4143.6, ups=0.75, wpb=5512.2, bsz=359.6, num_updates=82300, lr=0.00022046, gnorm=0.303, train_wall=49, gb_free=10.1, wall=80931
2022-08-17 20:41:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:41:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:41:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:41:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:41:25 | INFO | valid | epoch 1017 | valid on 'valid' subset | loss 5.228 | nll_loss 2.64 | ppl 6.23 | bleu 55.75 | wps 1722.4 | wpb 933.5 | bsz 59.6 | num_updates 82377 | best_bleu 57.52
2022-08-17 20:41:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1017 @ 82377 updates
2022-08-17 20:41:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1017.pt
2022-08-17 20:41:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1017.pt
2022-08-17 20:41:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1017.pt (epoch 1017 @ 82377 updates, score 55.75) (writing took 2.787123914808035 seconds)
2022-08-17 20:41:28 | INFO | fairseq_cli.train | end of epoch 1017 (average epoch stats below)
2022-08-17 20:41:28 | INFO | train | epoch 1017 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 7922.2 | ups 1.43 | wpb 5523.2 | bsz 358 | num_updates 82377 | lr 0.000220357 | gnorm 0.411 | train_wall 39 | gb_free 10.1 | wall 80984
2022-08-17 20:41:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:41:28 | INFO | fairseq.trainer | begin training epoch 1018
2022-08-17 20:41:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:41:41 | INFO | train_inner | epoch 1018:     23 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=8324.9, ups=1.51, wpb=5515.1, bsz=358.3, num_updates=82400, lr=0.000220326, gnorm=0.4, train_wall=49, gb_free=10, wall=80997
2022-08-17 20:42:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:42:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:42:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:42:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:42:19 | INFO | valid | epoch 1018 | valid on 'valid' subset | loss 5.226 | nll_loss 2.637 | ppl 6.22 | bleu 56.72 | wps 1839.4 | wpb 933.5 | bsz 59.6 | num_updates 82458 | best_bleu 57.52
2022-08-17 20:42:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1018 @ 82458 updates
2022-08-17 20:42:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1018.pt
2022-08-17 20:42:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1018.pt
2022-08-17 20:42:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1018.pt (epoch 1018 @ 82458 updates, score 56.72) (writing took 26.921978149563074 seconds)
2022-08-17 20:42:46 | INFO | fairseq_cli.train | end of epoch 1018 (average epoch stats below)
2022-08-17 20:42:46 | INFO | train | epoch 1018 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5737.3 | ups 1.04 | wpb 5523.2 | bsz 358 | num_updates 82458 | lr 0.000220249 | gnorm 0.379 | train_wall 40 | gb_free 10 | wall 81062
2022-08-17 20:42:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:42:46 | INFO | fairseq.trainer | begin training epoch 1019
2022-08-17 20:42:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:43:08 | INFO | train_inner | epoch 1019:     42 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=6349.2, ups=1.15, wpb=5537.9, bsz=357.3, num_updates=82500, lr=0.000220193, gnorm=0.411, train_wall=49, gb_free=10.1, wall=81084
2022-08-17 20:43:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:43:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:43:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:43:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:43:36 | INFO | valid | epoch 1019 | valid on 'valid' subset | loss 5.239 | nll_loss 2.655 | ppl 6.3 | bleu 55.65 | wps 1970.4 | wpb 933.5 | bsz 59.6 | num_updates 82539 | best_bleu 57.52
2022-08-17 20:43:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1019 @ 82539 updates
2022-08-17 20:43:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1019.pt
2022-08-17 20:43:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1019.pt
2022-08-17 20:44:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1019.pt (epoch 1019 @ 82539 updates, score 55.65) (writing took 29.68844235315919 seconds)
2022-08-17 20:44:06 | INFO | fairseq_cli.train | end of epoch 1019 (average epoch stats below)
2022-08-17 20:44:06 | INFO | train | epoch 1019 | loss 3.373 | nll_loss 0.342 | ppl 1.27 | wps 5591.9 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 82539 | lr 0.000220141 | gnorm 0.416 | train_wall 40 | gb_free 10.2 | wall 81142
2022-08-17 20:44:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:44:06 | INFO | fairseq.trainer | begin training epoch 1020
2022-08-17 20:44:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:44:39 | INFO | train_inner | epoch 1020:     61 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=6093.3, ups=1.1, wpb=5525.4, bsz=359.6, num_updates=82600, lr=0.000220059, gnorm=0.387, train_wall=50, gb_free=10.1, wall=81175
2022-08-17 20:44:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:44:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:44:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:44:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:44:58 | INFO | valid | epoch 1020 | valid on 'valid' subset | loss 5.239 | nll_loss 2.65 | ppl 6.28 | bleu 56.54 | wps 1910.8 | wpb 933.5 | bsz 59.6 | num_updates 82620 | best_bleu 57.52
2022-08-17 20:44:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1020 @ 82620 updates
2022-08-17 20:44:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1020.pt
2022-08-17 20:45:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1020.pt
2022-08-17 20:45:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1020.pt (epoch 1020 @ 82620 updates, score 56.54) (writing took 16.223801143467426 seconds)
2022-08-17 20:45:14 | INFO | fairseq_cli.train | end of epoch 1020 (average epoch stats below)
2022-08-17 20:45:14 | INFO | train | epoch 1020 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6516.3 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 82620 | lr 0.000220033 | gnorm 0.368 | train_wall 41 | gb_free 10.2 | wall 81211
2022-08-17 20:45:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:45:15 | INFO | fairseq.trainer | begin training epoch 1021
2022-08-17 20:45:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:45:56 | INFO | train_inner | epoch 1021:     80 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=7178.7, ups=1.3, wpb=5535, bsz=358, num_updates=82700, lr=0.000219926, gnorm=0.331, train_wall=50, gb_free=10.1, wall=81252
2022-08-17 20:45:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:45:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:45:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:45:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:46:06 | INFO | valid | epoch 1021 | valid on 'valid' subset | loss 5.229 | nll_loss 2.64 | ppl 6.23 | bleu 56.07 | wps 1849.4 | wpb 933.5 | bsz 59.6 | num_updates 82701 | best_bleu 57.52
2022-08-17 20:46:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1021 @ 82701 updates
2022-08-17 20:46:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1021.pt
2022-08-17 20:46:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1021.pt
2022-08-17 20:46:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1021.pt (epoch 1021 @ 82701 updates, score 56.07) (writing took 36.34622548893094 seconds)
2022-08-17 20:46:42 | INFO | fairseq_cli.train | end of epoch 1021 (average epoch stats below)
2022-08-17 20:46:42 | INFO | train | epoch 1021 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5101 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 82701 | lr 0.000219925 | gnorm 0.342 | train_wall 40 | gb_free 10.1 | wall 81298
2022-08-17 20:46:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:46:42 | INFO | fairseq.trainer | begin training epoch 1022
2022-08-17 20:46:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:47:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:47:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:47:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:47:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:47:33 | INFO | valid | epoch 1022 | valid on 'valid' subset | loss 5.226 | nll_loss 2.636 | ppl 6.21 | bleu 56.5 | wps 1934.2 | wpb 933.5 | bsz 59.6 | num_updates 82782 | best_bleu 57.52
2022-08-17 20:47:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1022 @ 82782 updates
2022-08-17 20:47:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1022.pt
2022-08-17 20:47:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1022.pt
2022-08-17 20:48:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1022.pt (epoch 1022 @ 82782 updates, score 56.5) (writing took 31.81331057846546 seconds)
2022-08-17 20:48:05 | INFO | fairseq_cli.train | end of epoch 1022 (average epoch stats below)
2022-08-17 20:48:05 | INFO | train | epoch 1022 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5427.3 | ups 0.98 | wpb 5523.2 | bsz 358 | num_updates 82782 | lr 0.000219817 | gnorm 0.461 | train_wall 40 | gb_free 10 | wall 81381
2022-08-17 20:48:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:48:05 | INFO | fairseq.trainer | begin training epoch 1023
2022-08-17 20:48:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:48:15 | INFO | train_inner | epoch 1023:     18 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=3957.3, ups=0.72, wpb=5502.9, bsz=355, num_updates=82800, lr=0.000219793, gnorm=0.432, train_wall=49, gb_free=10.1, wall=81391
2022-08-17 20:48:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:48:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:48:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:48:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:48:57 | INFO | valid | epoch 1023 | valid on 'valid' subset | loss 5.223 | nll_loss 2.631 | ppl 6.19 | bleu 56.27 | wps 1765.9 | wpb 933.5 | bsz 59.6 | num_updates 82863 | best_bleu 57.52
2022-08-17 20:48:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1023 @ 82863 updates
2022-08-17 20:48:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1023.pt
2022-08-17 20:48:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1023.pt
2022-08-17 20:49:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1023.pt (epoch 1023 @ 82863 updates, score 56.27) (writing took 20.519722264260054 seconds)
2022-08-17 20:49:18 | INFO | fairseq_cli.train | end of epoch 1023 (average epoch stats below)
2022-08-17 20:49:18 | INFO | train | epoch 1023 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6134.5 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 82863 | lr 0.00021971 | gnorm 0.395 | train_wall 40 | gb_free 10.1 | wall 81454
2022-08-17 20:49:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:49:18 | INFO | fairseq.trainer | begin training epoch 1024
2022-08-17 20:49:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:49:39 | INFO | train_inner | epoch 1024:     37 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=6587.2, ups=1.19, wpb=5535.9, bsz=357.8, num_updates=82900, lr=0.000219661, gnorm=0.376, train_wall=50, gb_free=10.1, wall=81475
2022-08-17 20:50:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:50:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:50:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:50:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:50:14 | INFO | valid | epoch 1024 | valid on 'valid' subset | loss 5.205 | nll_loss 2.612 | ppl 6.11 | bleu 57.06 | wps 1599.9 | wpb 933.5 | bsz 59.6 | num_updates 82944 | best_bleu 57.52
2022-08-17 20:50:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1024 @ 82944 updates
2022-08-17 20:50:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1024.pt
2022-08-17 20:50:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1024.pt
2022-08-17 20:50:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1024.pt (epoch 1024 @ 82944 updates, score 57.06) (writing took 24.69646428525448 seconds)
2022-08-17 20:50:39 | INFO | fairseq_cli.train | end of epoch 1024 (average epoch stats below)
2022-08-17 20:50:39 | INFO | train | epoch 1024 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5492.3 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 82944 | lr 0.000219603 | gnorm 0.321 | train_wall 41 | gb_free 10.2 | wall 81535
2022-08-17 20:50:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:50:39 | INFO | fairseq.trainer | begin training epoch 1025
2022-08-17 20:50:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:51:09 | INFO | train_inner | epoch 1025:     56 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=6155.3, ups=1.11, wpb=5524.4, bsz=361.5, num_updates=83000, lr=0.000219529, gnorm=0.318, train_wall=50, gb_free=10.1, wall=81565
2022-08-17 20:51:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:51:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:51:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:51:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:51:31 | INFO | valid | epoch 1025 | valid on 'valid' subset | loss 5.211 | nll_loss 2.617 | ppl 6.13 | bleu 56.57 | wps 1799 | wpb 933.5 | bsz 59.6 | num_updates 83025 | best_bleu 57.52
2022-08-17 20:51:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1025 @ 83025 updates
2022-08-17 20:51:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1025.pt
2022-08-17 20:51:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1025.pt
2022-08-17 20:52:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1025.pt (epoch 1025 @ 83025 updates, score 56.57) (writing took 30.038253635168076 seconds)
2022-08-17 20:52:01 | INFO | fairseq_cli.train | end of epoch 1025 (average epoch stats below)
2022-08-17 20:52:01 | INFO | train | epoch 1025 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5432.9 | ups 0.98 | wpb 5523.2 | bsz 358 | num_updates 83025 | lr 0.000219495 | gnorm 0.311 | train_wall 40 | gb_free 10.1 | wall 81618
2022-08-17 20:52:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:52:02 | INFO | fairseq.trainer | begin training epoch 1026
2022-08-17 20:52:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:52:41 | INFO | train_inner | epoch 1026:     75 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=6029.1, ups=1.09, wpb=5542.1, bsz=357.1, num_updates=83100, lr=0.000219396, gnorm=0.326, train_wall=49, gb_free=10.1, wall=81657
2022-08-17 20:52:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:52:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:52:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:52:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:52:52 | INFO | valid | epoch 1026 | valid on 'valid' subset | loss 5.208 | nll_loss 2.61 | ppl 6.11 | bleu 56.63 | wps 1920.9 | wpb 933.5 | bsz 59.6 | num_updates 83106 | best_bleu 57.52
2022-08-17 20:52:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1026 @ 83106 updates
2022-08-17 20:52:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1026.pt
2022-08-17 20:52:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1026.pt
2022-08-17 20:53:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1026.pt (epoch 1026 @ 83106 updates, score 56.63) (writing took 17.31905895099044 seconds)
2022-08-17 20:53:10 | INFO | fairseq_cli.train | end of epoch 1026 (average epoch stats below)
2022-08-17 20:53:10 | INFO | train | epoch 1026 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6521.5 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 83106 | lr 0.000219388 | gnorm 0.318 | train_wall 39 | gb_free 10.1 | wall 81686
2022-08-17 20:53:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:53:10 | INFO | fairseq.trainer | begin training epoch 1027
2022-08-17 20:53:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:53:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:53:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:53:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:53:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:54:02 | INFO | valid | epoch 1027 | valid on 'valid' subset | loss 5.221 | nll_loss 2.63 | ppl 6.19 | bleu 56.33 | wps 1838.7 | wpb 933.5 | bsz 59.6 | num_updates 83187 | best_bleu 57.52
2022-08-17 20:54:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1027 @ 83187 updates
2022-08-17 20:54:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1027.pt
2022-08-17 20:54:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1027.pt
2022-08-17 20:54:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1027.pt (epoch 1027 @ 83187 updates, score 56.33) (writing took 33.765293419361115 seconds)
2022-08-17 20:54:36 | INFO | fairseq_cli.train | end of epoch 1027 (average epoch stats below)
2022-08-17 20:54:36 | INFO | train | epoch 1027 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5190.4 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 83187 | lr 0.000219282 | gnorm 0.319 | train_wall 40 | gb_free 10.3 | wall 81772
2022-08-17 20:54:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:54:36 | INFO | fairseq.trainer | begin training epoch 1028
2022-08-17 20:54:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:54:45 | INFO | train_inner | epoch 1028:     13 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=4435.1, ups=0.81, wpb=5504.7, bsz=359.9, num_updates=83200, lr=0.000219265, gnorm=0.329, train_wall=49, gb_free=10.1, wall=81781
2022-08-17 20:55:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:55:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:55:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:55:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:55:29 | INFO | valid | epoch 1028 | valid on 'valid' subset | loss 5.228 | nll_loss 2.639 | ppl 6.23 | bleu 56.35 | wps 1835.1 | wpb 933.5 | bsz 59.6 | num_updates 83268 | best_bleu 57.52
2022-08-17 20:55:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1028 @ 83268 updates
2022-08-17 20:55:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1028.pt
2022-08-17 20:55:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1028.pt
2022-08-17 20:55:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1028.pt (epoch 1028 @ 83268 updates, score 56.35) (writing took 25.256532303988934 seconds)
2022-08-17 20:55:54 | INFO | fairseq_cli.train | end of epoch 1028 (average epoch stats below)
2022-08-17 20:55:54 | INFO | train | epoch 1028 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5722.9 | ups 1.04 | wpb 5523.2 | bsz 358 | num_updates 83268 | lr 0.000219175 | gnorm 0.347 | train_wall 40 | gb_free 10.1 | wall 81851
2022-08-17 20:55:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:55:55 | INFO | fairseq.trainer | begin training epoch 1029
2022-08-17 20:55:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:56:12 | INFO | train_inner | epoch 1029:     32 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=6357.2, ups=1.15, wpb=5538, bsz=354.8, num_updates=83300, lr=0.000219133, gnorm=0.349, train_wall=50, gb_free=10.1, wall=81868
2022-08-17 20:56:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:56:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:56:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:56:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:56:47 | INFO | valid | epoch 1029 | valid on 'valid' subset | loss 5.217 | nll_loss 2.625 | ppl 6.17 | bleu 56.37 | wps 1889 | wpb 933.5 | bsz 59.6 | num_updates 83349 | best_bleu 57.52
2022-08-17 20:56:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1029 @ 83349 updates
2022-08-17 20:56:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1029.pt
2022-08-17 20:56:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1029.pt
2022-08-17 20:57:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1029.pt (epoch 1029 @ 83349 updates, score 56.37) (writing took 17.735139559954405 seconds)
2022-08-17 20:57:05 | INFO | fairseq_cli.train | end of epoch 1029 (average epoch stats below)
2022-08-17 20:57:05 | INFO | train | epoch 1029 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6316.4 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 83349 | lr 0.000219068 | gnorm 0.378 | train_wall 41 | gb_free 10.1 | wall 81921
2022-08-17 20:57:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:57:05 | INFO | fairseq.trainer | begin training epoch 1030
2022-08-17 20:57:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:57:32 | INFO | train_inner | epoch 1030:     51 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=6885.9, ups=1.25, wpb=5520.5, bsz=359.8, num_updates=83400, lr=0.000219001, gnorm=0.368, train_wall=50, gb_free=10.1, wall=81948
2022-08-17 20:57:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:57:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:57:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:57:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:57:56 | INFO | valid | epoch 1030 | valid on 'valid' subset | loss 5.211 | nll_loss 2.618 | ppl 6.14 | bleu 56.75 | wps 1904.6 | wpb 933.5 | bsz 59.6 | num_updates 83430 | best_bleu 57.52
2022-08-17 20:57:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1030 @ 83430 updates
2022-08-17 20:57:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1030.pt
2022-08-17 20:57:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1030.pt
2022-08-17 20:58:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1030.pt (epoch 1030 @ 83430 updates, score 56.75) (writing took 34.53441804647446 seconds)
2022-08-17 20:58:31 | INFO | fairseq_cli.train | end of epoch 1030 (average epoch stats below)
2022-08-17 20:58:31 | INFO | train | epoch 1030 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5199.5 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 83430 | lr 0.000218962 | gnorm 0.347 | train_wall 40 | gb_free 10.2 | wall 82007
2022-08-17 20:58:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 20:58:31 | INFO | fairseq.trainer | begin training epoch 1031
2022-08-17 20:58:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 20:59:08 | INFO | train_inner | epoch 1031:     70 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5736.3, ups=1.04, wpb=5517.9, bsz=356.2, num_updates=83500, lr=0.00021887, gnorm=0.306, train_wall=51, gb_free=10.1, wall=82045
2022-08-17 20:59:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 20:59:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 20:59:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 20:59:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 20:59:23 | INFO | valid | epoch 1031 | valid on 'valid' subset | loss 5.219 | nll_loss 2.626 | ppl 6.17 | bleu 56.39 | wps 1873.5 | wpb 933.5 | bsz 59.6 | num_updates 83511 | best_bleu 57.52
2022-08-17 20:59:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1031 @ 83511 updates
2022-08-17 20:59:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1031.pt
2022-08-17 20:59:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1031.pt
2022-08-17 21:00:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1031.pt (epoch 1031 @ 83511 updates, score 56.39) (writing took 39.44727894663811 seconds)
2022-08-17 21:00:02 | INFO | fairseq_cli.train | end of epoch 1031 (average epoch stats below)
2022-08-17 21:00:02 | INFO | train | epoch 1031 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 4903.6 | ups 0.89 | wpb 5523.2 | bsz 358 | num_updates 83511 | lr 0.000218856 | gnorm 0.296 | train_wall 41 | gb_free 10.1 | wall 82099
2022-08-17 21:00:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:00:03 | INFO | fairseq.trainer | begin training epoch 1032
2022-08-17 21:00:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:00:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:00:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:00:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:00:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:00:54 | INFO | valid | epoch 1032 | valid on 'valid' subset | loss 5.218 | nll_loss 2.629 | ppl 6.19 | bleu 56.53 | wps 1811.4 | wpb 933.5 | bsz 59.6 | num_updates 83592 | best_bleu 57.52
2022-08-17 21:00:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1032 @ 83592 updates
2022-08-17 21:00:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1032.pt
2022-08-17 21:00:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1032.pt
2022-08-17 21:01:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1032.pt (epoch 1032 @ 83592 updates, score 56.53) (writing took 20.840703580528498 seconds)
2022-08-17 21:01:15 | INFO | fairseq_cli.train | end of epoch 1032 (average epoch stats below)
2022-08-17 21:01:15 | INFO | train | epoch 1032 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6128.5 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 83592 | lr 0.00021875 | gnorm 2.642 | train_wall 41 | gb_free 10.1 | wall 82172
2022-08-17 21:01:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:01:16 | INFO | fairseq.trainer | begin training epoch 1033
2022-08-17 21:01:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:01:21 | INFO | train_inner | epoch 1033:      8 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=4156.2, ups=0.76, wpb=5504.2, bsz=357.8, num_updates=83600, lr=0.000218739, gnorm=2.195, train_wall=50, gb_free=10.1, wall=82177
2022-08-17 21:02:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:02:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:02:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:02:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:02:14 | INFO | valid | epoch 1033 | valid on 'valid' subset | loss 5.223 | nll_loss 2.633 | ppl 6.2 | bleu 56.13 | wps 1883.6 | wpb 933.5 | bsz 59.6 | num_updates 83673 | best_bleu 57.52
2022-08-17 21:02:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1033 @ 83673 updates
2022-08-17 21:02:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1033.pt
2022-08-17 21:02:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1033.pt
2022-08-17 21:02:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1033.pt (epoch 1033 @ 83673 updates, score 56.13) (writing took 18.051727931946516 seconds)
2022-08-17 21:02:32 | INFO | fairseq_cli.train | end of epoch 1033 (average epoch stats below)
2022-08-17 21:02:32 | INFO | train | epoch 1033 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5827.9 | ups 1.06 | wpb 5523.2 | bsz 358 | num_updates 83673 | lr 0.000218644 | gnorm 0.391 | train_wall 41 | gb_free 10.1 | wall 82248
2022-08-17 21:02:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:02:32 | INFO | fairseq.trainer | begin training epoch 1034
2022-08-17 21:02:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:02:48 | INFO | train_inner | epoch 1034:     27 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=6319.1, ups=1.14, wpb=5527.3, bsz=357.4, num_updates=83700, lr=0.000218609, gnorm=0.392, train_wall=50, gb_free=10.1, wall=82265
2022-08-17 21:03:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:03:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:03:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:03:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:03:25 | INFO | valid | epoch 1034 | valid on 'valid' subset | loss 5.214 | nll_loss 2.62 | ppl 6.15 | bleu 56.63 | wps 1965.6 | wpb 933.5 | bsz 59.6 | num_updates 83754 | best_bleu 57.52
2022-08-17 21:03:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1034 @ 83754 updates
2022-08-17 21:03:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1034.pt
2022-08-17 21:03:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1034.pt
2022-08-17 21:03:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1034.pt (epoch 1034 @ 83754 updates, score 56.63) (writing took 31.978681545704603 seconds)
2022-08-17 21:03:58 | INFO | fairseq_cli.train | end of epoch 1034 (average epoch stats below)
2022-08-17 21:03:58 | INFO | train | epoch 1034 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5243.3 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 83754 | lr 0.000218538 | gnorm 0.35 | train_wall 40 | gb_free 10.1 | wall 82334
2022-08-17 21:03:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:03:58 | INFO | fairseq.trainer | begin training epoch 1035
2022-08-17 21:03:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:04:27 | INFO | train_inner | epoch 1035:     46 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=5598.3, ups=1.01, wpb=5538, bsz=357.4, num_updates=83800, lr=0.000218478, gnorm=0.342, train_wall=49, gb_free=10.1, wall=82364
2022-08-17 21:04:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:04:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:04:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:04:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:04:59 | INFO | valid | epoch 1035 | valid on 'valid' subset | loss 5.217 | nll_loss 2.624 | ppl 6.16 | bleu 56.79 | wps 1771.2 | wpb 933.5 | bsz 59.6 | num_updates 83835 | best_bleu 57.52
2022-08-17 21:04:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1035 @ 83835 updates
2022-08-17 21:04:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1035.pt
2022-08-17 21:05:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1035.pt
2022-08-17 21:05:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1035.pt (epoch 1035 @ 83835 updates, score 56.79) (writing took 42.301737528294325 seconds)
2022-08-17 21:05:41 | INFO | fairseq_cli.train | end of epoch 1035 (average epoch stats below)
2022-08-17 21:05:41 | INFO | train | epoch 1035 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 4321.6 | ups 0.78 | wpb 5523.2 | bsz 358 | num_updates 83835 | lr 0.000218433 | gnorm 0.371 | train_wall 39 | gb_free 10.2 | wall 82437
2022-08-17 21:05:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:05:41 | INFO | fairseq.trainer | begin training epoch 1036
2022-08-17 21:05:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:06:16 | INFO | train_inner | epoch 1036:     65 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5093.6, ups=0.92, wpb=5520.4, bsz=360.8, num_updates=83900, lr=0.000218348, gnorm=0.384, train_wall=50, gb_free=10.1, wall=82472
2022-08-17 21:06:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:06:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:06:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:06:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:06:33 | INFO | valid | epoch 1036 | valid on 'valid' subset | loss 5.219 | nll_loss 2.625 | ppl 6.17 | bleu 56.07 | wps 1862 | wpb 933.5 | bsz 59.6 | num_updates 83916 | best_bleu 57.52
2022-08-17 21:06:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1036 @ 83916 updates
2022-08-17 21:06:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1036.pt
2022-08-17 21:06:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1036.pt
2022-08-17 21:06:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1036.pt (epoch 1036 @ 83916 updates, score 56.07) (writing took 22.55609468743205 seconds)
2022-08-17 21:06:56 | INFO | fairseq_cli.train | end of epoch 1036 (average epoch stats below)
2022-08-17 21:06:56 | INFO | train | epoch 1036 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6007.7 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 83916 | lr 0.000218327 | gnorm 0.359 | train_wall 40 | gb_free 10.1 | wall 82512
2022-08-17 21:06:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:06:56 | INFO | fairseq.trainer | begin training epoch 1037
2022-08-17 21:06:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:07:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:07:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:07:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:07:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:07:47 | INFO | valid | epoch 1037 | valid on 'valid' subset | loss 5.207 | nll_loss 2.612 | ppl 6.11 | bleu 56.86 | wps 1878.6 | wpb 933.5 | bsz 59.6 | num_updates 83997 | best_bleu 57.52
2022-08-17 21:07:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1037 @ 83997 updates
2022-08-17 21:07:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1037.pt
2022-08-17 21:07:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1037.pt
2022-08-17 21:08:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1037.pt (epoch 1037 @ 83997 updates, score 56.86) (writing took 17.306538216769695 seconds)
2022-08-17 21:08:05 | INFO | fairseq_cli.train | end of epoch 1037 (average epoch stats below)
2022-08-17 21:08:05 | INFO | train | epoch 1037 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6458.7 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 83997 | lr 0.000218222 | gnorm 0.477 | train_wall 40 | gb_free 10.1 | wall 82581
2022-08-17 21:08:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:08:05 | INFO | fairseq.trainer | begin training epoch 1038
2022-08-17 21:08:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:08:08 | INFO | train_inner | epoch 1038:      3 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=4919, ups=0.89, wpb=5506.5, bsz=356.6, num_updates=84000, lr=0.000218218, gnorm=0.446, train_wall=49, gb_free=10.1, wall=82584
2022-08-17 21:08:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:08:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:08:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:08:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:08:57 | INFO | valid | epoch 1038 | valid on 'valid' subset | loss 5.213 | nll_loss 2.619 | ppl 6.14 | bleu 56.52 | wps 1845.8 | wpb 933.5 | bsz 59.6 | num_updates 84078 | best_bleu 57.52
2022-08-17 21:08:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1038 @ 84078 updates
2022-08-17 21:08:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1038.pt
2022-08-17 21:08:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1038.pt
2022-08-17 21:09:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1038.pt (epoch 1038 @ 84078 updates, score 56.52) (writing took 34.61158545687795 seconds)
2022-08-17 21:09:32 | INFO | fairseq_cli.train | end of epoch 1038 (average epoch stats below)
2022-08-17 21:09:32 | INFO | train | epoch 1038 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5126.9 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 84078 | lr 0.000218117 | gnorm 0.368 | train_wall 41 | gb_free 10.1 | wall 82668
2022-08-17 21:09:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:09:32 | INFO | fairseq.trainer | begin training epoch 1039
2022-08-17 21:09:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:09:45 | INFO | train_inner | epoch 1039:     22 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5648.3, ups=1.02, wpb=5512.6, bsz=357, num_updates=84100, lr=0.000218088, gnorm=0.371, train_wall=49, gb_free=10.1, wall=82681
2022-08-17 21:10:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:10:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:10:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:10:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:10:26 | INFO | valid | epoch 1039 | valid on 'valid' subset | loss 5.222 | nll_loss 2.626 | ppl 6.17 | bleu 56.72 | wps 1853.4 | wpb 933.5 | bsz 59.6 | num_updates 84159 | best_bleu 57.52
2022-08-17 21:10:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1039 @ 84159 updates
2022-08-17 21:10:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1039.pt
2022-08-17 21:10:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1039.pt
2022-08-17 21:10:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1039.pt (epoch 1039 @ 84159 updates, score 56.72) (writing took 21.54979509115219 seconds)
2022-08-17 21:10:48 | INFO | fairseq_cli.train | end of epoch 1039 (average epoch stats below)
2022-08-17 21:10:48 | INFO | train | epoch 1039 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5905.2 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 84159 | lr 0.000218012 | gnorm 0.365 | train_wall 39 | gb_free 10.2 | wall 82744
2022-08-17 21:10:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:10:48 | INFO | fairseq.trainer | begin training epoch 1040
2022-08-17 21:10:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:11:13 | INFO | train_inner | epoch 1040:     41 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=6370.8, ups=1.14, wpb=5566.4, bsz=363.2, num_updates=84200, lr=0.000217959, gnorm=0.349, train_wall=51, gb_free=10.1, wall=82769
2022-08-17 21:11:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:11:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:11:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:11:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:11:43 | INFO | valid | epoch 1040 | valid on 'valid' subset | loss 5.221 | nll_loss 2.628 | ppl 6.18 | bleu 56.99 | wps 1818.4 | wpb 933.5 | bsz 59.6 | num_updates 84240 | best_bleu 57.52
2022-08-17 21:11:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1040 @ 84240 updates
2022-08-17 21:11:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1040.pt
2022-08-17 21:11:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1040.pt
2022-08-17 21:12:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1040.pt (epoch 1040 @ 84240 updates, score 56.99) (writing took 17.899138160049915 seconds)
2022-08-17 21:12:01 | INFO | fairseq_cli.train | end of epoch 1040 (average epoch stats below)
2022-08-17 21:12:01 | INFO | train | epoch 1040 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6144.2 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 84240 | lr 0.000217907 | gnorm 0.33 | train_wall 41 | gb_free 10.3 | wall 82817
2022-08-17 21:12:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:12:01 | INFO | fairseq.trainer | begin training epoch 1041
2022-08-17 21:12:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:12:34 | INFO | train_inner | epoch 1041:     60 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=6749.1, ups=1.23, wpb=5496.6, bsz=354.2, num_updates=84300, lr=0.000217829, gnorm=0.328, train_wall=51, gb_free=10.1, wall=82850
2022-08-17 21:12:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:12:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:12:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:12:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:12:54 | INFO | valid | epoch 1041 | valid on 'valid' subset | loss 5.211 | nll_loss 2.62 | ppl 6.15 | bleu 56.61 | wps 1870.8 | wpb 933.5 | bsz 59.6 | num_updates 84321 | best_bleu 57.52
2022-08-17 21:12:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1041 @ 84321 updates
2022-08-17 21:12:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1041.pt
2022-08-17 21:12:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1041.pt
2022-08-17 21:13:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1041.pt (epoch 1041 @ 84321 updates, score 56.61) (writing took 35.01697087287903 seconds)
2022-08-17 21:13:29 | INFO | fairseq_cli.train | end of epoch 1041 (average epoch stats below)
2022-08-17 21:13:29 | INFO | train | epoch 1041 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5069.5 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 84321 | lr 0.000217802 | gnorm 0.315 | train_wall 41 | gb_free 10.1 | wall 82905
2022-08-17 21:13:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:13:29 | INFO | fairseq.trainer | begin training epoch 1042
2022-08-17 21:13:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:14:11 | INFO | train_inner | epoch 1042:     79 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5701, ups=1.03, wpb=5534.3, bsz=359.1, num_updates=84400, lr=0.0002177, gnorm=0.391, train_wall=50, gb_free=10, wall=82947
2022-08-17 21:14:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:14:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:14:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:14:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:14:22 | INFO | valid | epoch 1042 | valid on 'valid' subset | loss 5.223 | nll_loss 2.631 | ppl 6.2 | bleu 56.37 | wps 1828.5 | wpb 933.5 | bsz 59.6 | num_updates 84402 | best_bleu 57.52
2022-08-17 21:14:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1042 @ 84402 updates
2022-08-17 21:14:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1042.pt
2022-08-17 21:14:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1042.pt
2022-08-17 21:14:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1042.pt (epoch 1042 @ 84402 updates, score 56.37) (writing took 31.192227076739073 seconds)
2022-08-17 21:14:53 | INFO | fairseq_cli.train | end of epoch 1042 (average epoch stats below)
2022-08-17 21:14:53 | INFO | train | epoch 1042 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5324 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 84402 | lr 0.000217698 | gnorm 0.416 | train_wall 40 | gb_free 10.1 | wall 82989
2022-08-17 21:14:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:14:53 | INFO | fairseq.trainer | begin training epoch 1043
2022-08-17 21:14:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:15:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:15:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:15:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:15:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:15:45 | INFO | valid | epoch 1043 | valid on 'valid' subset | loss 5.236 | nll_loss 2.645 | ppl 6.26 | bleu 56.51 | wps 1765.4 | wpb 933.5 | bsz 59.6 | num_updates 84483 | best_bleu 57.52
2022-08-17 21:15:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1043 @ 84483 updates
2022-08-17 21:15:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1043.pt
2022-08-17 21:15:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1043.pt
2022-08-17 21:16:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1043.pt (epoch 1043 @ 84483 updates, score 56.51) (writing took 40.21021718904376 seconds)
2022-08-17 21:16:26 | INFO | fairseq_cli.train | end of epoch 1043 (average epoch stats below)
2022-08-17 21:16:26 | INFO | train | epoch 1043 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 4831.4 | ups 0.87 | wpb 5523.2 | bsz 358 | num_updates 84483 | lr 0.000217593 | gnorm 0.322 | train_wall 39 | gb_free 10.2 | wall 83082
2022-08-17 21:16:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:16:26 | INFO | fairseq.trainer | begin training epoch 1044
2022-08-17 21:16:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:16:35 | INFO | train_inner | epoch 1044:     17 / 81 loss=3.372, nll_loss=0.339, ppl=1.27, wps=3813.9, ups=0.69, wpb=5505.7, bsz=357.5, num_updates=84500, lr=0.000217571, gnorm=0.332, train_wall=48, gb_free=10.1, wall=83092
2022-08-17 21:17:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:17:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:17:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:17:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:17:17 | INFO | valid | epoch 1044 | valid on 'valid' subset | loss 5.228 | nll_loss 2.636 | ppl 6.22 | bleu 56.64 | wps 1903.2 | wpb 933.5 | bsz 59.6 | num_updates 84564 | best_bleu 57.52
2022-08-17 21:17:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1044 @ 84564 updates
2022-08-17 21:17:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1044.pt
2022-08-17 21:17:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1044.pt
2022-08-17 21:17:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1044.pt (epoch 1044 @ 84564 updates, score 56.64) (writing took 21.637437663972378 seconds)
2022-08-17 21:17:39 | INFO | fairseq_cli.train | end of epoch 1044 (average epoch stats below)
2022-08-17 21:17:39 | INFO | train | epoch 1044 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6070.1 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 84564 | lr 0.000217489 | gnorm 0.322 | train_wall 40 | gb_free 10.1 | wall 83156
2022-08-17 21:17:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:17:39 | INFO | fairseq.trainer | begin training epoch 1045
2022-08-17 21:17:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:17:59 | INFO | train_inner | epoch 1045:     36 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=6612.3, ups=1.19, wpb=5534.9, bsz=358.9, num_updates=84600, lr=0.000217443, gnorm=0.324, train_wall=50, gb_free=10.1, wall=83175
2022-08-17 21:18:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:18:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:18:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:18:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:18:32 | INFO | valid | epoch 1045 | valid on 'valid' subset | loss 5.218 | nll_loss 2.624 | ppl 6.16 | bleu 56.32 | wps 1774.2 | wpb 933.5 | bsz 59.6 | num_updates 84645 | best_bleu 57.52
2022-08-17 21:18:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1045 @ 84645 updates
2022-08-17 21:18:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1045.pt
2022-08-17 21:18:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1045.pt
2022-08-17 21:18:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1045.pt (epoch 1045 @ 84645 updates, score 56.32) (writing took 22.926835916936398 seconds)
2022-08-17 21:18:55 | INFO | fairseq_cli.train | end of epoch 1045 (average epoch stats below)
2022-08-17 21:18:55 | INFO | train | epoch 1045 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5932.4 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 84645 | lr 0.000217385 | gnorm 0.385 | train_wall 40 | gb_free 10.1 | wall 83231
2022-08-17 21:18:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:18:55 | INFO | fairseq.trainer | begin training epoch 1046
2022-08-17 21:18:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:19:23 | INFO | train_inner | epoch 1046:     55 / 81 loss=3.373, nll_loss=0.34, ppl=1.27, wps=6593.4, ups=1.19, wpb=5520.1, bsz=360.9, num_updates=84700, lr=0.000217314, gnorm=0.363, train_wall=49, gb_free=10, wall=83259
2022-08-17 21:19:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:19:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:19:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:19:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:19:45 | INFO | valid | epoch 1046 | valid on 'valid' subset | loss 5.216 | nll_loss 2.625 | ppl 6.17 | bleu 55.97 | wps 1823.3 | wpb 933.5 | bsz 59.6 | num_updates 84726 | best_bleu 57.52
2022-08-17 21:19:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1046 @ 84726 updates
2022-08-17 21:19:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1046.pt
2022-08-17 21:19:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1046.pt
2022-08-17 21:20:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1046.pt (epoch 1046 @ 84726 updates, score 55.97) (writing took 33.835116025060415 seconds)
2022-08-17 21:20:19 | INFO | fairseq_cli.train | end of epoch 1046 (average epoch stats below)
2022-08-17 21:20:19 | INFO | train | epoch 1046 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5322.7 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 84726 | lr 0.000217281 | gnorm 0.322 | train_wall 39 | gb_free 10.2 | wall 83315
2022-08-17 21:20:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:20:19 | INFO | fairseq.trainer | begin training epoch 1047
2022-08-17 21:20:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:20:57 | INFO | train_inner | epoch 1047:     74 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5890.5, ups=1.07, wpb=5524.5, bsz=354.1, num_updates=84800, lr=0.000217186, gnorm=0.335, train_wall=48, gb_free=10.1, wall=83353
2022-08-17 21:21:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:21:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:21:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:21:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:21:09 | INFO | valid | epoch 1047 | valid on 'valid' subset | loss 5.224 | nll_loss 2.631 | ppl 6.2 | bleu 56.43 | wps 1873.6 | wpb 933.5 | bsz 59.6 | num_updates 84807 | best_bleu 57.52
2022-08-17 21:21:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1047 @ 84807 updates
2022-08-17 21:21:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1047.pt
2022-08-17 21:21:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1047.pt
2022-08-17 21:21:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1047.pt (epoch 1047 @ 84807 updates, score 56.43) (writing took 19.36250600591302 seconds)
2022-08-17 21:21:29 | INFO | fairseq_cli.train | end of epoch 1047 (average epoch stats below)
2022-08-17 21:21:29 | INFO | train | epoch 1047 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6394.2 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 84807 | lr 0.000217177 | gnorm 0.354 | train_wall 39 | gb_free 10.3 | wall 83385
2022-08-17 21:21:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:21:29 | INFO | fairseq.trainer | begin training epoch 1048
2022-08-17 21:21:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:22:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:22:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:22:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:22:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:22:19 | INFO | valid | epoch 1048 | valid on 'valid' subset | loss 5.219 | nll_loss 2.627 | ppl 6.18 | bleu 56.22 | wps 1813.3 | wpb 933.5 | bsz 59.6 | num_updates 84888 | best_bleu 57.52
2022-08-17 21:22:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1048 @ 84888 updates
2022-08-17 21:22:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1048.pt
2022-08-17 21:22:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1048.pt
2022-08-17 21:22:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1048.pt (epoch 1048 @ 84888 updates, score 56.22) (writing took 29.69108034670353 seconds)
2022-08-17 21:22:49 | INFO | fairseq_cli.train | end of epoch 1048 (average epoch stats below)
2022-08-17 21:22:49 | INFO | train | epoch 1048 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5561.8 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 84888 | lr 0.000217074 | gnorm 0.354 | train_wall 39 | gb_free 10.3 | wall 83465
2022-08-17 21:22:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:22:49 | INFO | fairseq.trainer | begin training epoch 1049
2022-08-17 21:22:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:22:56 | INFO | train_inner | epoch 1049:     12 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=4595.1, ups=0.84, wpb=5502.2, bsz=360.1, num_updates=84900, lr=0.000217058, gnorm=0.363, train_wall=48, gb_free=10.1, wall=83473
2022-08-17 21:23:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:23:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:23:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:23:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:23:41 | INFO | valid | epoch 1049 | valid on 'valid' subset | loss 5.212 | nll_loss 2.622 | ppl 6.16 | bleu 56.68 | wps 1873.4 | wpb 933.5 | bsz 59.6 | num_updates 84969 | best_bleu 57.52
2022-08-17 21:23:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1049 @ 84969 updates
2022-08-17 21:23:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1049.pt
2022-08-17 21:23:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1049.pt
2022-08-17 21:24:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1049.pt (epoch 1049 @ 84969 updates, score 56.68) (writing took 34.50681660324335 seconds)
2022-08-17 21:24:16 | INFO | fairseq_cli.train | end of epoch 1049 (average epoch stats below)
2022-08-17 21:24:16 | INFO | train | epoch 1049 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5159.9 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 84969 | lr 0.00021697 | gnorm 0.332 | train_wall 41 | gb_free 10.1 | wall 83552
2022-08-17 21:24:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:24:16 | INFO | fairseq.trainer | begin training epoch 1050
2022-08-17 21:24:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:24:34 | INFO | train_inner | epoch 1050:     31 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5684.5, ups=1.03, wpb=5521.3, bsz=352.6, num_updates=85000, lr=0.00021693, gnorm=0.352, train_wall=50, gb_free=10.1, wall=83570
2022-08-17 21:24:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:25:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:25:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:25:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:25:08 | INFO | valid | epoch 1050 | valid on 'valid' subset | loss 5.226 | nll_loss 2.64 | ppl 6.24 | bleu 56.45 | wps 1844.2 | wpb 933.5 | bsz 59.6 | num_updates 85050 | best_bleu 57.52
2022-08-17 21:25:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1050 @ 85050 updates
2022-08-17 21:25:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1050.pt
2022-08-17 21:25:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1050.pt
2022-08-17 21:25:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1050.pt (epoch 1050 @ 85050 updates, score 56.45) (writing took 29.708590637892485 seconds)
2022-08-17 21:25:38 | INFO | fairseq_cli.train | end of epoch 1050 (average epoch stats below)
2022-08-17 21:25:38 | INFO | train | epoch 1050 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5430.3 | ups 0.98 | wpb 5523.2 | bsz 358 | num_updates 85050 | lr 0.000216867 | gnorm 0.368 | train_wall 40 | gb_free 10.2 | wall 83634
2022-08-17 21:25:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:25:38 | INFO | fairseq.trainer | begin training epoch 1051
2022-08-17 21:25:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:26:11 | INFO | train_inner | epoch 1051:     50 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5728.8, ups=1.03, wpb=5551.1, bsz=363, num_updates=85100, lr=0.000216803, gnorm=0.317, train_wall=50, gb_free=10.1, wall=83667
2022-08-17 21:26:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:26:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:26:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:26:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:26:38 | INFO | valid | epoch 1051 | valid on 'valid' subset | loss 5.232 | nll_loss 2.646 | ppl 6.26 | bleu 56.65 | wps 1924.8 | wpb 933.5 | bsz 59.6 | num_updates 85131 | best_bleu 57.52
2022-08-17 21:26:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1051 @ 85131 updates
2022-08-17 21:26:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1051.pt
2022-08-17 21:26:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1051.pt
2022-08-17 21:27:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1051.pt (epoch 1051 @ 85131 updates, score 56.65) (writing took 35.95896278694272 seconds)
2022-08-17 21:27:14 | INFO | fairseq_cli.train | end of epoch 1051 (average epoch stats below)
2022-08-17 21:27:14 | INFO | train | epoch 1051 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 4676.6 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 85131 | lr 0.000216763 | gnorm 0.29 | train_wall 41 | gb_free 10.1 | wall 83730
2022-08-17 21:27:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:27:14 | INFO | fairseq.trainer | begin training epoch 1052
2022-08-17 21:27:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:27:50 | INFO | train_inner | epoch 1052:     69 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5557.2, ups=1, wpb=5541.4, bsz=357.8, num_updates=85200, lr=0.000216676, gnorm=0.316, train_wall=49, gb_free=10.1, wall=83766
2022-08-17 21:27:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:27:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:27:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:27:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:28:05 | INFO | valid | epoch 1052 | valid on 'valid' subset | loss 5.233 | nll_loss 2.641 | ppl 6.24 | bleu 56.06 | wps 1870.3 | wpb 933.5 | bsz 59.6 | num_updates 85212 | best_bleu 57.52
2022-08-17 21:28:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1052 @ 85212 updates
2022-08-17 21:28:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1052.pt
2022-08-17 21:28:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1052.pt
2022-08-17 21:28:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1052.pt (epoch 1052 @ 85212 updates, score 56.06) (writing took 21.67831515520811 seconds)
2022-08-17 21:28:27 | INFO | fairseq_cli.train | end of epoch 1052 (average epoch stats below)
2022-08-17 21:28:27 | INFO | train | epoch 1052 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6108 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 85212 | lr 0.00021666 | gnorm 0.325 | train_wall 40 | gb_free 10.3 | wall 83803
2022-08-17 21:28:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:28:27 | INFO | fairseq.trainer | begin training epoch 1053
2022-08-17 21:28:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:29:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:29:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:29:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:29:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:29:23 | INFO | valid | epoch 1053 | valid on 'valid' subset | loss 5.236 | nll_loss 2.649 | ppl 6.27 | bleu 56.14 | wps 1899.9 | wpb 933.5 | bsz 59.6 | num_updates 85293 | best_bleu 57.52
2022-08-17 21:29:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1053 @ 85293 updates
2022-08-17 21:29:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1053.pt
2022-08-17 21:29:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1053.pt
2022-08-17 21:29:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1053.pt (epoch 1053 @ 85293 updates, score 56.14) (writing took 18.841947719454765 seconds)
2022-08-17 21:29:42 | INFO | fairseq_cli.train | end of epoch 1053 (average epoch stats below)
2022-08-17 21:29:42 | INFO | train | epoch 1053 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5977.9 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 85293 | lr 0.000216558 | gnorm 0.396 | train_wall 39 | gb_free 10.1 | wall 83878
2022-08-17 21:29:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:29:42 | INFO | fairseq.trainer | begin training epoch 1054
2022-08-17 21:29:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:29:47 | INFO | train_inner | epoch 1054:      7 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=4691.2, ups=0.86, wpb=5476.3, bsz=355.4, num_updates=85300, lr=0.000216549, gnorm=0.384, train_wall=49, gb_free=10.1, wall=83883
2022-08-17 21:30:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:30:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:30:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:30:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:30:35 | INFO | valid | epoch 1054 | valid on 'valid' subset | loss 5.235 | nll_loss 2.647 | ppl 6.27 | bleu 55.95 | wps 1818.7 | wpb 933.5 | bsz 59.6 | num_updates 85374 | best_bleu 57.52
2022-08-17 21:30:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1054 @ 85374 updates
2022-08-17 21:30:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1054.pt
2022-08-17 21:30:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1054.pt
2022-08-17 21:30:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1054.pt (epoch 1054 @ 85374 updates, score 55.95) (writing took 22.36995879933238 seconds)
2022-08-17 21:30:58 | INFO | fairseq_cli.train | end of epoch 1054 (average epoch stats below)
2022-08-17 21:30:58 | INFO | train | epoch 1054 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5919.6 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 85374 | lr 0.000216455 | gnorm 0.364 | train_wall 41 | gb_free 10.1 | wall 83954
2022-08-17 21:30:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:30:58 | INFO | fairseq.trainer | begin training epoch 1055
2022-08-17 21:30:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:31:13 | INFO | train_inner | epoch 1055:     26 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=6435.8, ups=1.16, wpb=5534.4, bsz=357.2, num_updates=85400, lr=0.000216422, gnorm=0.353, train_wall=51, gb_free=10.1, wall=83969
2022-08-17 21:31:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:31:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:31:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:31:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:31:51 | INFO | valid | epoch 1055 | valid on 'valid' subset | loss 5.24 | nll_loss 2.655 | ppl 6.3 | bleu 55.81 | wps 1768.9 | wpb 933.5 | bsz 59.6 | num_updates 85455 | best_bleu 57.52
2022-08-17 21:31:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1055 @ 85455 updates
2022-08-17 21:31:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1055.pt
2022-08-17 21:31:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1055.pt
2022-08-17 21:31:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1055.pt (epoch 1055 @ 85455 updates, score 55.81) (writing took 2.514350228011608 seconds)
2022-08-17 21:31:54 | INFO | fairseq_cli.train | end of epoch 1055 (average epoch stats below)
2022-08-17 21:31:54 | INFO | train | epoch 1055 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 7991.5 | ups 1.45 | wpb 5523.2 | bsz 358 | num_updates 85455 | lr 0.000216352 | gnorm 0.337 | train_wall 40 | gb_free 10.1 | wall 84010
2022-08-17 21:31:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:31:54 | INFO | fairseq.trainer | begin training epoch 1056
2022-08-17 21:31:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:32:21 | INFO | train_inner | epoch 1056:     45 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=8164, ups=1.47, wpb=5550.2, bsz=361, num_updates=85500, lr=0.000216295, gnorm=0.351, train_wall=51, gb_free=10.1, wall=84037
2022-08-17 21:32:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:32:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:32:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:32:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:32:51 | INFO | valid | epoch 1056 | valid on 'valid' subset | loss 5.238 | nll_loss 2.647 | ppl 6.27 | bleu 56.4 | wps 1983 | wpb 933.5 | bsz 59.6 | num_updates 85536 | best_bleu 57.52
2022-08-17 21:32:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1056 @ 85536 updates
2022-08-17 21:32:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1056.pt
2022-08-17 21:32:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1056.pt
2022-08-17 21:33:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1056.pt (epoch 1056 @ 85536 updates, score 56.4) (writing took 21.909485563635826 seconds)
2022-08-17 21:33:13 | INFO | fairseq_cli.train | end of epoch 1056 (average epoch stats below)
2022-08-17 21:33:13 | INFO | train | epoch 1056 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5616.1 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 85536 | lr 0.00021625 | gnorm 0.349 | train_wall 43 | gb_free 10.2 | wall 84089
2022-08-17 21:33:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:33:13 | INFO | fairseq.trainer | begin training epoch 1057
2022-08-17 21:33:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:33:49 | INFO | train_inner | epoch 1057:     64 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=6264.3, ups=1.14, wpb=5502.8, bsz=357.4, num_updates=85600, lr=0.000216169, gnorm=0.408, train_wall=51, gb_free=10.2, wall=84125
2022-08-17 21:33:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:33:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:33:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:33:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:34:07 | INFO | valid | epoch 1057 | valid on 'valid' subset | loss 5.227 | nll_loss 2.638 | ppl 6.23 | bleu 55.12 | wps 1836.1 | wpb 933.5 | bsz 59.6 | num_updates 85617 | best_bleu 57.52
2022-08-17 21:34:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1057 @ 85617 updates
2022-08-17 21:34:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1057.pt
2022-08-17 21:34:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1057.pt
2022-08-17 21:34:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1057.pt (epoch 1057 @ 85617 updates, score 55.12) (writing took 37.8722479455173 seconds)
2022-08-17 21:34:45 | INFO | fairseq_cli.train | end of epoch 1057 (average epoch stats below)
2022-08-17 21:34:45 | INFO | train | epoch 1057 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 4890.8 | ups 0.89 | wpb 5523.2 | bsz 358 | num_updates 85617 | lr 0.000216147 | gnorm 0.458 | train_wall 40 | gb_free 10.2 | wall 84181
2022-08-17 21:34:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:34:45 | INFO | fairseq.trainer | begin training epoch 1058
2022-08-17 21:34:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:35:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:35:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:35:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:35:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:35:37 | INFO | valid | epoch 1058 | valid on 'valid' subset | loss 5.234 | nll_loss 2.647 | ppl 6.26 | bleu 56.33 | wps 1906.3 | wpb 933.5 | bsz 59.6 | num_updates 85698 | best_bleu 57.52
2022-08-17 21:35:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1058 @ 85698 updates
2022-08-17 21:35:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1058.pt
2022-08-17 21:35:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1058.pt
2022-08-17 21:35:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1058.pt (epoch 1058 @ 85698 updates, score 56.33) (writing took 17.739464852958918 seconds)
2022-08-17 21:35:54 | INFO | fairseq_cli.train | end of epoch 1058 (average epoch stats below)
2022-08-17 21:35:54 | INFO | train | epoch 1058 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6414.5 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 85698 | lr 0.000216045 | gnorm 0.327 | train_wall 41 | gb_free 10.1 | wall 84251
2022-08-17 21:35:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:35:55 | INFO | fairseq.trainer | begin training epoch 1059
2022-08-17 21:35:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:35:57 | INFO | train_inner | epoch 1059:      2 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=4317, ups=0.78, wpb=5513.7, bsz=356.6, num_updates=85700, lr=0.000216043, gnorm=0.356, train_wall=50, gb_free=10.1, wall=84253
2022-08-17 21:36:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:36:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:36:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:36:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:36:45 | INFO | valid | epoch 1059 | valid on 'valid' subset | loss 5.227 | nll_loss 2.639 | ppl 6.23 | bleu 56.71 | wps 1903 | wpb 933.5 | bsz 59.6 | num_updates 85779 | best_bleu 57.52
2022-08-17 21:36:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1059 @ 85779 updates
2022-08-17 21:36:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1059.pt
2022-08-17 21:36:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1059.pt
2022-08-17 21:37:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1059.pt (epoch 1059 @ 85779 updates, score 56.71) (writing took 32.31986732035875 seconds)
2022-08-17 21:37:18 | INFO | fairseq_cli.train | end of epoch 1059 (average epoch stats below)
2022-08-17 21:37:18 | INFO | train | epoch 1059 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5375.3 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 85779 | lr 0.000215943 | gnorm 0.368 | train_wall 40 | gb_free 10.1 | wall 84334
2022-08-17 21:37:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:37:18 | INFO | fairseq.trainer | begin training epoch 1060
2022-08-17 21:37:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:37:30 | INFO | train_inner | epoch 1060:     21 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5901.1, ups=1.07, wpb=5530.5, bsz=360.5, num_updates=85800, lr=0.000215917, gnorm=0.356, train_wall=50, gb_free=10, wall=84346
2022-08-17 21:38:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:38:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:38:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:38:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:38:09 | INFO | valid | epoch 1060 | valid on 'valid' subset | loss 5.217 | nll_loss 2.626 | ppl 6.17 | bleu 56.81 | wps 1820.1 | wpb 933.5 | bsz 59.6 | num_updates 85860 | best_bleu 57.52
2022-08-17 21:38:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1060 @ 85860 updates
2022-08-17 21:38:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1060.pt
2022-08-17 21:38:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1060.pt
2022-08-17 21:38:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1060.pt (epoch 1060 @ 85860 updates, score 56.81) (writing took 33.47627134621143 seconds)
2022-08-17 21:38:43 | INFO | fairseq_cli.train | end of epoch 1060 (average epoch stats below)
2022-08-17 21:38:43 | INFO | train | epoch 1060 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5244.9 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 85860 | lr 0.000215841 | gnorm 0.362 | train_wall 40 | gb_free 10.1 | wall 84419
2022-08-17 21:38:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:38:43 | INFO | fairseq.trainer | begin training epoch 1061
2022-08-17 21:38:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:39:04 | INFO | train_inner | epoch 1061:     40 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=5865.4, ups=1.06, wpb=5523.6, bsz=355.8, num_updates=85900, lr=0.000215791, gnorm=0.351, train_wall=49, gb_free=10.1, wall=84441
2022-08-17 21:39:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:39:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:39:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:39:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:39:34 | INFO | valid | epoch 1061 | valid on 'valid' subset | loss 5.235 | nll_loss 2.645 | ppl 6.26 | bleu 56.36 | wps 1830.7 | wpb 933.5 | bsz 59.6 | num_updates 85941 | best_bleu 57.52
2022-08-17 21:39:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1061 @ 85941 updates
2022-08-17 21:39:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1061.pt
2022-08-17 21:39:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1061.pt
2022-08-17 21:39:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1061.pt (epoch 1061 @ 85941 updates, score 56.36) (writing took 2.3865573592483997 seconds)
2022-08-17 21:39:37 | INFO | fairseq_cli.train | end of epoch 1061 (average epoch stats below)
2022-08-17 21:39:37 | INFO | train | epoch 1061 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 8354.1 | ups 1.51 | wpb 5523.2 | bsz 358 | num_updates 85941 | lr 0.00021574 | gnorm 0.327 | train_wall 40 | gb_free 10.1 | wall 84473
2022-08-17 21:39:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:39:37 | INFO | fairseq.trainer | begin training epoch 1062
2022-08-17 21:39:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:40:08 | INFO | train_inner | epoch 1062:     59 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=8730.8, ups=1.57, wpb=5544, bsz=361.3, num_updates=86000, lr=0.000215666, gnorm=0.376, train_wall=50, gb_free=10.1, wall=84504
2022-08-17 21:40:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:40:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:40:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:40:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:40:28 | INFO | valid | epoch 1062 | valid on 'valid' subset | loss 5.227 | nll_loss 2.636 | ppl 6.22 | bleu 56.25 | wps 1865.8 | wpb 933.5 | bsz 59.6 | num_updates 86022 | best_bleu 57.52
2022-08-17 21:40:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1062 @ 86022 updates
2022-08-17 21:40:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1062.pt
2022-08-17 21:40:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1062.pt
2022-08-17 21:40:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1062.pt (epoch 1062 @ 86022 updates, score 56.25) (writing took 27.669863522052765 seconds)
2022-08-17 21:40:56 | INFO | fairseq_cli.train | end of epoch 1062 (average epoch stats below)
2022-08-17 21:40:56 | INFO | train | epoch 1062 | loss 3.372 | nll_loss 0.341 | ppl 1.27 | wps 5614.1 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 86022 | lr 0.000215638 | gnorm 0.392 | train_wall 41 | gb_free 10.1 | wall 84552
2022-08-17 21:40:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:40:56 | INFO | fairseq.trainer | begin training epoch 1063
2022-08-17 21:40:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:41:38 | INFO | train_inner | epoch 1063:     78 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=6106.3, ups=1.11, wpb=5521.3, bsz=357.6, num_updates=86100, lr=0.00021554, gnorm=0.324, train_wall=51, gb_free=10, wall=84595
2022-08-17 21:41:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:41:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:41:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:41:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:41:49 | INFO | valid | epoch 1063 | valid on 'valid' subset | loss 5.214 | nll_loss 2.623 | ppl 6.16 | bleu 56.87 | wps 1823.8 | wpb 933.5 | bsz 59.6 | num_updates 86103 | best_bleu 57.52
2022-08-17 21:41:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1063 @ 86103 updates
2022-08-17 21:41:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1063.pt
2022-08-17 21:41:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1063.pt
2022-08-17 21:42:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1063.pt (epoch 1063 @ 86103 updates, score 56.87) (writing took 28.415358927100897 seconds)
2022-08-17 21:42:18 | INFO | fairseq_cli.train | end of epoch 1063 (average epoch stats below)
2022-08-17 21:42:18 | INFO | train | epoch 1063 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5489.9 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 86103 | lr 0.000215537 | gnorm 0.306 | train_wall 41 | gb_free 10.1 | wall 84634
2022-08-17 21:42:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:42:18 | INFO | fairseq.trainer | begin training epoch 1064
2022-08-17 21:42:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:43:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:43:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:43:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:43:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:43:15 | INFO | valid | epoch 1064 | valid on 'valid' subset | loss 5.218 | nll_loss 2.626 | ppl 6.17 | bleu 56.22 | wps 1689 | wpb 933.5 | bsz 59.6 | num_updates 86184 | best_bleu 57.52
2022-08-17 21:43:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1064 @ 86184 updates
2022-08-17 21:43:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1064.pt
2022-08-17 21:43:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1064.pt
2022-08-17 21:43:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1064.pt (epoch 1064 @ 86184 updates, score 56.22) (writing took 21.675113204866648 seconds)
2022-08-17 21:43:37 | INFO | fairseq_cli.train | end of epoch 1064 (average epoch stats below)
2022-08-17 21:43:37 | INFO | train | epoch 1064 | loss 3.372 | nll_loss 0.339 | ppl 1.27 | wps 5643.2 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 86184 | lr 0.000215435 | gnorm 0.328 | train_wall 41 | gb_free 10.2 | wall 84713
2022-08-17 21:43:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:43:37 | INFO | fairseq.trainer | begin training epoch 1065
2022-08-17 21:43:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:43:46 | INFO | train_inner | epoch 1065:     16 / 81 loss=3.371, nll_loss=0.339, ppl=1.26, wps=4301.1, ups=0.78, wpb=5505.6, bsz=357.1, num_updates=86200, lr=0.000215415, gnorm=0.311, train_wall=50, gb_free=10.1, wall=84723
2022-08-17 21:44:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:44:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:44:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:44:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:44:28 | INFO | valid | epoch 1065 | valid on 'valid' subset | loss 5.211 | nll_loss 2.619 | ppl 6.14 | bleu 56.74 | wps 1813.8 | wpb 933.5 | bsz 59.6 | num_updates 86265 | best_bleu 57.52
2022-08-17 21:44:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1065 @ 86265 updates
2022-08-17 21:44:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1065.pt
2022-08-17 21:44:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1065.pt
2022-08-17 21:44:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1065.pt (epoch 1065 @ 86265 updates, score 56.74) (writing took 14.144595496356487 seconds)
2022-08-17 21:44:42 | INFO | fairseq_cli.train | end of epoch 1065 (average epoch stats below)
2022-08-17 21:44:42 | INFO | train | epoch 1065 | loss 3.372 | nll_loss 0.341 | ppl 1.27 | wps 6853.5 | ups 1.24 | wpb 5523.2 | bsz 358 | num_updates 86265 | lr 0.000215334 | gnorm 0.343 | train_wall 40 | gb_free 10.1 | wall 84779
2022-08-17 21:44:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:44:43 | INFO | fairseq.trainer | begin training epoch 1066
2022-08-17 21:44:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:45:02 | INFO | train_inner | epoch 1066:     35 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=7325.5, ups=1.33, wpb=5512.4, bsz=353, num_updates=86300, lr=0.00021529, gnorm=0.363, train_wall=49, gb_free=10, wall=84798
2022-08-17 21:45:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:45:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:45:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:45:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:45:35 | INFO | valid | epoch 1066 | valid on 'valid' subset | loss 5.221 | nll_loss 2.63 | ppl 6.19 | bleu 56.36 | wps 1816.9 | wpb 933.5 | bsz 59.6 | num_updates 86346 | best_bleu 57.52
2022-08-17 21:45:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1066 @ 86346 updates
2022-08-17 21:45:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1066.pt
2022-08-17 21:45:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1066.pt
2022-08-17 21:46:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1066.pt (epoch 1066 @ 86346 updates, score 56.36) (writing took 25.802070699632168 seconds)
2022-08-17 21:46:01 | INFO | fairseq_cli.train | end of epoch 1066 (average epoch stats below)
2022-08-17 21:46:01 | INFO | train | epoch 1066 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5703 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 86346 | lr 0.000215233 | gnorm 0.343 | train_wall 41 | gb_free 10.1 | wall 84857
2022-08-17 21:46:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:46:01 | INFO | fairseq.trainer | begin training epoch 1067
2022-08-17 21:46:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:46:30 | INFO | train_inner | epoch 1067:     54 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=6260.8, ups=1.13, wpb=5552.2, bsz=362.9, num_updates=86400, lr=0.000215166, gnorm=0.336, train_wall=50, gb_free=10, wall=84887
2022-08-17 21:46:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:46:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:46:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:46:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:46:53 | INFO | valid | epoch 1067 | valid on 'valid' subset | loss 5.218 | nll_loss 2.625 | ppl 6.17 | bleu 56.52 | wps 1916.7 | wpb 933.5 | bsz 59.6 | num_updates 86427 | best_bleu 57.52
2022-08-17 21:46:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1067 @ 86427 updates
2022-08-17 21:46:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1067.pt
2022-08-17 21:46:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1067.pt
2022-08-17 21:47:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1067.pt (epoch 1067 @ 86427 updates, score 56.52) (writing took 14.925502635538578 seconds)
2022-08-17 21:47:08 | INFO | fairseq_cli.train | end of epoch 1067 (average epoch stats below)
2022-08-17 21:47:08 | INFO | train | epoch 1067 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6692.9 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 86427 | lr 0.000215132 | gnorm 0.346 | train_wall 40 | gb_free 10.3 | wall 84924
2022-08-17 21:47:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:47:08 | INFO | fairseq.trainer | begin training epoch 1068
2022-08-17 21:47:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:47:46 | INFO | train_inner | epoch 1068:     73 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=7286.4, ups=1.32, wpb=5507.5, bsz=357.4, num_updates=86500, lr=0.000215041, gnorm=0.461, train_wall=49, gb_free=10, wall=84962
2022-08-17 21:47:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:47:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:47:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:47:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:47:59 | INFO | valid | epoch 1068 | valid on 'valid' subset | loss 5.227 | nll_loss 2.636 | ppl 6.22 | bleu 56.49 | wps 1802.6 | wpb 933.5 | bsz 59.6 | num_updates 86508 | best_bleu 57.52
2022-08-17 21:47:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1068 @ 86508 updates
2022-08-17 21:47:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1068.pt
2022-08-17 21:48:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1068.pt
2022-08-17 21:48:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1068.pt (epoch 1068 @ 86508 updates, score 56.49) (writing took 33.912305880337954 seconds)
2022-08-17 21:48:33 | INFO | fairseq_cli.train | end of epoch 1068 (average epoch stats below)
2022-08-17 21:48:33 | INFO | train | epoch 1068 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5228.8 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 86508 | lr 0.000215031 | gnorm 0.48 | train_wall 40 | gb_free 10.2 | wall 85009
2022-08-17 21:48:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:48:33 | INFO | fairseq.trainer | begin training epoch 1069
2022-08-17 21:48:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:49:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:49:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:49:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:49:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:49:24 | INFO | valid | epoch 1069 | valid on 'valid' subset | loss 5.218 | nll_loss 2.626 | ppl 6.17 | bleu 56.4 | wps 1902.6 | wpb 933.5 | bsz 59.6 | num_updates 86589 | best_bleu 57.52
2022-08-17 21:49:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1069 @ 86589 updates
2022-08-17 21:49:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1069.pt
2022-08-17 21:49:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1069.pt
2022-08-17 21:49:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1069.pt (epoch 1069 @ 86589 updates, score 56.4) (writing took 22.275058656930923 seconds)
2022-08-17 21:49:47 | INFO | fairseq_cli.train | end of epoch 1069 (average epoch stats below)
2022-08-17 21:49:47 | INFO | train | epoch 1069 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6073.8 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 86589 | lr 0.000214931 | gnorm 0.372 | train_wall 40 | gb_free 10.2 | wall 85083
2022-08-17 21:49:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:49:47 | INFO | fairseq.trainer | begin training epoch 1070
2022-08-17 21:49:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:49:54 | INFO | train_inner | epoch 1070:     11 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=4320, ups=0.78, wpb=5527.5, bsz=356.9, num_updates=86600, lr=0.000214917, gnorm=0.361, train_wall=49, gb_free=10.1, wall=85090
2022-08-17 21:50:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:50:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:50:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:50:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:50:40 | INFO | valid | epoch 1070 | valid on 'valid' subset | loss 5.224 | nll_loss 2.634 | ppl 6.21 | bleu 56.59 | wps 1682 | wpb 933.5 | bsz 59.6 | num_updates 86670 | best_bleu 57.52
2022-08-17 21:50:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1070 @ 86670 updates
2022-08-17 21:50:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1070.pt
2022-08-17 21:50:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1070.pt
2022-08-17 21:50:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1070.pt (epoch 1070 @ 86670 updates, score 56.59) (writing took 14.546975281089544 seconds)
2022-08-17 21:50:55 | INFO | fairseq_cli.train | end of epoch 1070 (average epoch stats below)
2022-08-17 21:50:55 | INFO | train | epoch 1070 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6610.4 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 86670 | lr 0.00021483 | gnorm 0.312 | train_wall 40 | gb_free 10.2 | wall 85151
2022-08-17 21:50:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:50:55 | INFO | fairseq.trainer | begin training epoch 1071
2022-08-17 21:50:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:51:12 | INFO | train_inner | epoch 1071:     30 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=7088.3, ups=1.28, wpb=5539.1, bsz=360.5, num_updates=86700, lr=0.000214793, gnorm=0.323, train_wall=49, gb_free=10.1, wall=85168
2022-08-17 21:51:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:51:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:51:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:51:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:51:47 | INFO | valid | epoch 1071 | valid on 'valid' subset | loss 5.228 | nll_loss 2.638 | ppl 6.23 | bleu 55.99 | wps 1786.2 | wpb 933.5 | bsz 59.6 | num_updates 86751 | best_bleu 57.52
2022-08-17 21:51:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1071 @ 86751 updates
2022-08-17 21:51:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1071.pt
2022-08-17 21:51:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1071.pt
2022-08-17 21:52:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1071.pt (epoch 1071 @ 86751 updates, score 55.99) (writing took 36.86577030643821 seconds)
2022-08-17 21:52:24 | INFO | fairseq_cli.train | end of epoch 1071 (average epoch stats below)
2022-08-17 21:52:24 | INFO | train | epoch 1071 | loss 3.372 | nll_loss 0.341 | ppl 1.27 | wps 5019.4 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 86751 | lr 0.00021473 | gnorm 0.366 | train_wall 39 | gb_free 10.1 | wall 85240
2022-08-17 21:52:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:52:24 | INFO | fairseq.trainer | begin training epoch 1072
2022-08-17 21:52:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:52:49 | INFO | train_inner | epoch 1072:     49 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5664.9, ups=1.03, wpb=5521.5, bsz=358.2, num_updates=86800, lr=0.000214669, gnorm=0.381, train_wall=49, gb_free=10.1, wall=85266
2022-08-17 21:53:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:53:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:53:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:53:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:53:14 | INFO | valid | epoch 1072 | valid on 'valid' subset | loss 5.223 | nll_loss 2.633 | ppl 6.2 | bleu 55.65 | wps 1895.1 | wpb 933.5 | bsz 59.6 | num_updates 86832 | best_bleu 57.52
2022-08-17 21:53:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1072 @ 86832 updates
2022-08-17 21:53:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1072.pt
2022-08-17 21:53:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1072.pt
2022-08-17 21:53:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1072.pt (epoch 1072 @ 86832 updates, score 55.65) (writing took 25.16985710337758 seconds)
2022-08-17 21:53:39 | INFO | fairseq_cli.train | end of epoch 1072 (average epoch stats below)
2022-08-17 21:53:39 | INFO | train | epoch 1072 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5930.9 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 86832 | lr 0.00021463 | gnorm 0.368 | train_wall 39 | gb_free 10.1 | wall 85315
2022-08-17 21:53:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:53:39 | INFO | fairseq.trainer | begin training epoch 1073
2022-08-17 21:53:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:54:16 | INFO | train_inner | epoch 1073:     68 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=6391.7, ups=1.16, wpb=5517, bsz=358.3, num_updates=86900, lr=0.000214546, gnorm=0.347, train_wall=49, gb_free=10.1, wall=85352
2022-08-17 21:54:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:54:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:54:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:54:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:54:32 | INFO | valid | epoch 1073 | valid on 'valid' subset | loss 5.221 | nll_loss 2.627 | ppl 6.18 | bleu 56.44 | wps 1770.8 | wpb 933.5 | bsz 59.6 | num_updates 86913 | best_bleu 57.52
2022-08-17 21:54:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1073 @ 86913 updates
2022-08-17 21:54:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1073.pt
2022-08-17 21:54:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1073.pt
2022-08-17 21:54:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1073.pt (epoch 1073 @ 86913 updates, score 56.44) (writing took 13.881217017769814 seconds)
2022-08-17 21:54:46 | INFO | fairseq_cli.train | end of epoch 1073 (average epoch stats below)
2022-08-17 21:54:46 | INFO | train | epoch 1073 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6693.8 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 86913 | lr 0.00021453 | gnorm 0.357 | train_wall 41 | gb_free 10.1 | wall 85382
2022-08-17 21:54:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:54:46 | INFO | fairseq.trainer | begin training epoch 1074
2022-08-17 21:54:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:55:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:55:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:55:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:55:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:55:38 | INFO | valid | epoch 1074 | valid on 'valid' subset | loss 5.209 | nll_loss 2.614 | ppl 6.12 | bleu 56.33 | wps 1899.5 | wpb 933.5 | bsz 59.6 | num_updates 86994 | best_bleu 57.52
2022-08-17 21:55:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1074 @ 86994 updates
2022-08-17 21:55:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1074.pt
2022-08-17 21:55:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1074.pt
2022-08-17 21:55:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1074.pt (epoch 1074 @ 86994 updates, score 56.33) (writing took 14.851113013923168 seconds)
2022-08-17 21:55:53 | INFO | fairseq_cli.train | end of epoch 1074 (average epoch stats below)
2022-08-17 21:55:53 | INFO | train | epoch 1074 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6707.9 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 86994 | lr 0.00021443 | gnorm 0.359 | train_wall 40 | gb_free 10.1 | wall 85449
2022-08-17 21:55:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:55:53 | INFO | fairseq.trainer | begin training epoch 1075
2022-08-17 21:55:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:55:57 | INFO | train_inner | epoch 1075:      6 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5439.9, ups=0.99, wpb=5500.5, bsz=355.3, num_updates=87000, lr=0.000214423, gnorm=0.369, train_wall=49, gb_free=10.1, wall=85453
2022-08-17 21:56:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:56:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:56:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:56:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:56:44 | INFO | valid | epoch 1075 | valid on 'valid' subset | loss 5.229 | nll_loss 2.638 | ppl 6.22 | bleu 56.13 | wps 1778 | wpb 933.5 | bsz 59.6 | num_updates 87075 | best_bleu 57.52
2022-08-17 21:56:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1075 @ 87075 updates
2022-08-17 21:56:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1075.pt
2022-08-17 21:56:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1075.pt
2022-08-17 21:56:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1075.pt (epoch 1075 @ 87075 updates, score 56.13) (writing took 15.520601715892553 seconds)
2022-08-17 21:56:59 | INFO | fairseq_cli.train | end of epoch 1075 (average epoch stats below)
2022-08-17 21:56:59 | INFO | train | epoch 1075 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6693.1 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 87075 | lr 0.00021433 | gnorm 0.367 | train_wall 40 | gb_free 10.1 | wall 85516
2022-08-17 21:57:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:57:00 | INFO | fairseq.trainer | begin training epoch 1076
2022-08-17 21:57:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:57:14 | INFO | train_inner | epoch 1076:     25 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=7201.6, ups=1.3, wpb=5521.9, bsz=358.1, num_updates=87100, lr=0.000214299, gnorm=0.347, train_wall=49, gb_free=10.1, wall=85530
2022-08-17 21:57:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:57:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:57:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:57:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:57:52 | INFO | valid | epoch 1076 | valid on 'valid' subset | loss 5.221 | nll_loss 2.629 | ppl 6.18 | bleu 56.21 | wps 1842.8 | wpb 933.5 | bsz 59.6 | num_updates 87156 | best_bleu 57.52
2022-08-17 21:57:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1076 @ 87156 updates
2022-08-17 21:57:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1076.pt
2022-08-17 21:57:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1076.pt
2022-08-17 21:58:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1076.pt (epoch 1076 @ 87156 updates, score 56.21) (writing took 15.548427235335112 seconds)
2022-08-17 21:58:07 | INFO | fairseq_cli.train | end of epoch 1076 (average epoch stats below)
2022-08-17 21:58:07 | INFO | train | epoch 1076 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6592.8 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 87156 | lr 0.000214231 | gnorm 0.342 | train_wall 40 | gb_free 10.1 | wall 85584
2022-08-17 21:58:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:58:08 | INFO | fairseq.trainer | begin training epoch 1077
2022-08-17 21:58:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 21:58:31 | INFO | train_inner | epoch 1077:     44 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=7173.9, ups=1.29, wpb=5548.4, bsz=362.3, num_updates=87200, lr=0.000214176, gnorm=0.376, train_wall=50, gb_free=10.1, wall=85607
2022-08-17 21:58:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 21:58:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 21:58:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 21:58:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 21:58:58 | INFO | valid | epoch 1077 | valid on 'valid' subset | loss 5.208 | nll_loss 2.613 | ppl 6.12 | bleu 56.75 | wps 1947.4 | wpb 933.5 | bsz 59.6 | num_updates 87237 | best_bleu 57.52
2022-08-17 21:58:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1077 @ 87237 updates
2022-08-17 21:58:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1077.pt
2022-08-17 21:58:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1077.pt
2022-08-17 21:59:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1077.pt (epoch 1077 @ 87237 updates, score 56.75) (writing took 37.224013328552246 seconds)
2022-08-17 21:59:35 | INFO | fairseq_cli.train | end of epoch 1077 (average epoch stats below)
2022-08-17 21:59:35 | INFO | train | epoch 1077 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5082 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 87237 | lr 0.000214131 | gnorm 0.394 | train_wall 39 | gb_free 10.1 | wall 85672
2022-08-17 21:59:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 21:59:36 | INFO | fairseq.trainer | begin training epoch 1078
2022-08-17 21:59:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:00:16 | INFO | train_inner | epoch 1078:     63 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=5274.8, ups=0.95, wpb=5538.3, bsz=354.7, num_updates=87300, lr=0.000214054, gnorm=0.412, train_wall=49, gb_free=10.1, wall=85712
2022-08-17 22:00:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:00:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:00:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:00:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:00:36 | INFO | valid | epoch 1078 | valid on 'valid' subset | loss 5.22 | nll_loss 2.628 | ppl 6.18 | bleu 56.89 | wps 1848.5 | wpb 933.5 | bsz 59.6 | num_updates 87318 | best_bleu 57.52
2022-08-17 22:00:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1078 @ 87318 updates
2022-08-17 22:00:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1078.pt
2022-08-17 22:00:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1078.pt
2022-08-17 22:00:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1078.pt (epoch 1078 @ 87318 updates, score 56.89) (writing took 2.445858635008335 seconds)
2022-08-17 22:00:39 | INFO | fairseq_cli.train | end of epoch 1078 (average epoch stats below)
2022-08-17 22:00:39 | INFO | train | epoch 1078 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 7032 | ups 1.27 | wpb 5523.2 | bsz 358 | num_updates 87318 | lr 0.000214032 | gnorm 0.398 | train_wall 40 | gb_free 10.1 | wall 85735
2022-08-17 22:00:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:00:39 | INFO | fairseq.trainer | begin training epoch 1079
2022-08-17 22:00:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:01:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:01:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:01:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:01:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:01:32 | INFO | valid | epoch 1079 | valid on 'valid' subset | loss 5.221 | nll_loss 2.635 | ppl 6.21 | bleu 56.29 | wps 1729.2 | wpb 933.5 | bsz 59.6 | num_updates 87399 | best_bleu 57.52
2022-08-17 22:01:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1079 @ 87399 updates
2022-08-17 22:01:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1079.pt
2022-08-17 22:01:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1079.pt
2022-08-17 22:02:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1079.pt (epoch 1079 @ 87399 updates, score 56.29) (writing took 29.701798886060715 seconds)
2022-08-17 22:02:01 | INFO | fairseq_cli.train | end of epoch 1079 (average epoch stats below)
2022-08-17 22:02:01 | INFO | train | epoch 1079 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5428.2 | ups 0.98 | wpb 5523.2 | bsz 358 | num_updates 87399 | lr 0.000213932 | gnorm 0.341 | train_wall 39 | gb_free 10.2 | wall 85818
2022-08-17 22:02:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:02:02 | INFO | fairseq.trainer | begin training epoch 1080
2022-08-17 22:02:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:02:03 | INFO | train_inner | epoch 1080:      1 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=5115.2, ups=0.93, wpb=5485.3, bsz=357.6, num_updates=87400, lr=0.000213931, gnorm=0.334, train_wall=48, gb_free=10.1, wall=85819
2022-08-17 22:02:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:02:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:02:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:02:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:02:58 | INFO | valid | epoch 1080 | valid on 'valid' subset | loss 5.22 | nll_loss 2.631 | ppl 6.2 | bleu 56.67 | wps 1935.1 | wpb 933.5 | bsz 59.6 | num_updates 87480 | best_bleu 57.52
2022-08-17 22:02:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1080 @ 87480 updates
2022-08-17 22:02:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1080.pt
2022-08-17 22:03:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1080.pt
2022-08-17 22:03:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1080.pt (epoch 1080 @ 87480 updates, score 56.67) (writing took 36.49278234690428 seconds)
2022-08-17 22:03:35 | INFO | fairseq_cli.train | end of epoch 1080 (average epoch stats below)
2022-08-17 22:03:35 | INFO | train | epoch 1080 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 4795 | ups 0.87 | wpb 5523.2 | bsz 358 | num_updates 87480 | lr 0.000213833 | gnorm 0.31 | train_wall 41 | gb_free 10.1 | wall 85911
2022-08-17 22:03:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:03:35 | INFO | fairseq.trainer | begin training epoch 1081
2022-08-17 22:03:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:03:47 | INFO | train_inner | epoch 1081:     20 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5330.4, ups=0.96, wpb=5524.3, bsz=357.8, num_updates=87500, lr=0.000213809, gnorm=0.33, train_wall=51, gb_free=10, wall=85923
2022-08-17 22:04:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:04:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:04:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:04:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:04:27 | INFO | valid | epoch 1081 | valid on 'valid' subset | loss 5.236 | nll_loss 2.647 | ppl 6.27 | bleu 56.29 | wps 1882.7 | wpb 933.5 | bsz 59.6 | num_updates 87561 | best_bleu 57.52
2022-08-17 22:04:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1081 @ 87561 updates
2022-08-17 22:04:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1081.pt
2022-08-17 22:04:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1081.pt
2022-08-17 22:04:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1081.pt (epoch 1081 @ 87561 updates, score 56.29) (writing took 15.705207519233227 seconds)
2022-08-17 22:04:43 | INFO | fairseq_cli.train | end of epoch 1081 (average epoch stats below)
2022-08-17 22:04:43 | INFO | train | epoch 1081 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6535.2 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 87561 | lr 0.000213735 | gnorm 0.382 | train_wall 41 | gb_free 10.2 | wall 85979
2022-08-17 22:04:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:04:43 | INFO | fairseq.trainer | begin training epoch 1082
2022-08-17 22:04:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:05:05 | INFO | train_inner | epoch 1082:     39 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=7079.6, ups=1.28, wpb=5529.7, bsz=356.6, num_updates=87600, lr=0.000213687, gnorm=0.447, train_wall=50, gb_free=10, wall=86001
2022-08-17 22:05:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:05:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:05:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:05:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:05:37 | INFO | valid | epoch 1082 | valid on 'valid' subset | loss 5.222 | nll_loss 2.629 | ppl 6.19 | bleu 56.29 | wps 1845.7 | wpb 933.5 | bsz 59.6 | num_updates 87642 | best_bleu 57.52
2022-08-17 22:05:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1082 @ 87642 updates
2022-08-17 22:05:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1082.pt
2022-08-17 22:05:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1082.pt
2022-08-17 22:05:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1082.pt (epoch 1082 @ 87642 updates, score 56.29) (writing took 20.730419754981995 seconds)
2022-08-17 22:05:58 | INFO | fairseq_cli.train | end of epoch 1082 (average epoch stats below)
2022-08-17 22:05:58 | INFO | train | epoch 1082 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5950.7 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 87642 | lr 0.000213636 | gnorm 0.432 | train_wall 41 | gb_free 10.1 | wall 86055
2022-08-17 22:05:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:05:59 | INFO | fairseq.trainer | begin training epoch 1083
2022-08-17 22:05:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:06:29 | INFO | train_inner | epoch 1083:     58 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=6538, ups=1.18, wpb=5523.3, bsz=357.6, num_updates=87700, lr=0.000213565, gnorm=0.318, train_wall=51, gb_free=10.1, wall=86086
2022-08-17 22:06:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:06:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:06:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:06:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:06:51 | INFO | valid | epoch 1083 | valid on 'valid' subset | loss 5.222 | nll_loss 2.631 | ppl 6.19 | bleu 56.84 | wps 1630.5 | wpb 933.5 | bsz 59.6 | num_updates 87723 | best_bleu 57.52
2022-08-17 22:06:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1083 @ 87723 updates
2022-08-17 22:06:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1083.pt
2022-08-17 22:06:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1083.pt
2022-08-17 22:07:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1083.pt (epoch 1083 @ 87723 updates, score 56.84) (writing took 34.40130756422877 seconds)
2022-08-17 22:07:26 | INFO | fairseq_cli.train | end of epoch 1083 (average epoch stats below)
2022-08-17 22:07:26 | INFO | train | epoch 1083 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5118.6 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 87723 | lr 0.000213537 | gnorm 0.33 | train_wall 41 | gb_free 10.1 | wall 86142
2022-08-17 22:07:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:07:26 | INFO | fairseq.trainer | begin training epoch 1084
2022-08-17 22:07:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:08:04 | INFO | train_inner | epoch 1084:     77 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5854.7, ups=1.06, wpb=5541.1, bsz=362, num_updates=87800, lr=0.000213443, gnorm=0.367, train_wall=48, gb_free=10.1, wall=86180
2022-08-17 22:08:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:08:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:08:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:08:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:08:15 | INFO | valid | epoch 1084 | valid on 'valid' subset | loss 5.232 | nll_loss 2.64 | ppl 6.23 | bleu 56.45 | wps 1942.7 | wpb 933.5 | bsz 59.6 | num_updates 87804 | best_bleu 57.52
2022-08-17 22:08:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1084 @ 87804 updates
2022-08-17 22:08:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1084.pt
2022-08-17 22:08:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1084.pt
2022-08-17 22:08:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1084.pt (epoch 1084 @ 87804 updates, score 56.45) (writing took 19.692338325083256 seconds)
2022-08-17 22:08:34 | INFO | fairseq_cli.train | end of epoch 1084 (average epoch stats below)
2022-08-17 22:08:34 | INFO | train | epoch 1084 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6516.6 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 87804 | lr 0.000213439 | gnorm 0.371 | train_wall 38 | gb_free 10.3 | wall 86211
2022-08-17 22:08:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:08:35 | INFO | fairseq.trainer | begin training epoch 1085
2022-08-17 22:08:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:09:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:09:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:09:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:09:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:09:25 | INFO | valid | epoch 1085 | valid on 'valid' subset | loss 5.225 | nll_loss 2.632 | ppl 6.2 | bleu 55.91 | wps 1829.6 | wpb 933.5 | bsz 59.6 | num_updates 87885 | best_bleu 57.52
2022-08-17 22:09:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1085 @ 87885 updates
2022-08-17 22:09:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1085.pt
2022-08-17 22:09:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1085.pt
2022-08-17 22:10:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1085.pt (epoch 1085 @ 87885 updates, score 55.91) (writing took 39.752495624125004 seconds)
2022-08-17 22:10:05 | INFO | fairseq_cli.train | end of epoch 1085 (average epoch stats below)
2022-08-17 22:10:05 | INFO | train | epoch 1085 | loss 3.372 | nll_loss 0.341 | ppl 1.27 | wps 4950.7 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 87885 | lr 0.00021334 | gnorm 0.34 | train_wall 39 | gb_free 10.1 | wall 86301
2022-08-17 22:10:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:10:05 | INFO | fairseq.trainer | begin training epoch 1086
2022-08-17 22:10:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:10:14 | INFO | train_inner | epoch 1086:     15 / 81 loss=3.372, nll_loss=0.341, ppl=1.27, wps=4257, ups=0.77, wpb=5512.6, bsz=355.1, num_updates=87900, lr=0.000213322, gnorm=0.338, train_wall=48, gb_free=10.2, wall=86310
2022-08-17 22:10:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:10:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:10:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:10:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:10:57 | INFO | valid | epoch 1086 | valid on 'valid' subset | loss 5.213 | nll_loss 2.622 | ppl 6.16 | bleu 55.93 | wps 1800.3 | wpb 933.5 | bsz 59.6 | num_updates 87966 | best_bleu 57.52
2022-08-17 22:10:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1086 @ 87966 updates
2022-08-17 22:10:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1086.pt
2022-08-17 22:10:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1086.pt
2022-08-17 22:11:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1086.pt (epoch 1086 @ 87966 updates, score 55.93) (writing took 33.195681758224964 seconds)
2022-08-17 22:11:31 | INFO | fairseq_cli.train | end of epoch 1086 (average epoch stats below)
2022-08-17 22:11:31 | INFO | train | epoch 1086 | loss 3.371 | nll_loss 0.339 | ppl 1.26 | wps 5221.5 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 87966 | lr 0.000213242 | gnorm 0.315 | train_wall 41 | gb_free 10.1 | wall 86387
2022-08-17 22:11:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:11:31 | INFO | fairseq.trainer | begin training epoch 1087
2022-08-17 22:11:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:11:48 | INFO | train_inner | epoch 1087:     34 / 81 loss=3.372, nll_loss=0.339, ppl=1.27, wps=5828.6, ups=1.06, wpb=5502.3, bsz=359.3, num_updates=88000, lr=0.000213201, gnorm=0.324, train_wall=50, gb_free=10.1, wall=86404
2022-08-17 22:12:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:12:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:12:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:12:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:12:20 | INFO | valid | epoch 1087 | valid on 'valid' subset | loss 5.213 | nll_loss 2.622 | ppl 6.16 | bleu 55.99 | wps 1838.9 | wpb 933.5 | bsz 59.6 | num_updates 88047 | best_bleu 57.52
2022-08-17 22:12:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1087 @ 88047 updates
2022-08-17 22:12:20 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1087.pt
2022-08-17 22:12:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1087.pt
2022-08-17 22:13:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1087.pt (epoch 1087 @ 88047 updates, score 55.99) (writing took 46.56263667717576 seconds)
2022-08-17 22:13:07 | INFO | fairseq_cli.train | end of epoch 1087 (average epoch stats below)
2022-08-17 22:13:07 | INFO | train | epoch 1087 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 4641 | ups 0.84 | wpb 5523.2 | bsz 358 | num_updates 88047 | lr 0.000213144 | gnorm 0.365 | train_wall 39 | gb_free 10.3 | wall 86483
2022-08-17 22:13:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:13:07 | INFO | fairseq.trainer | begin training epoch 1088
2022-08-17 22:13:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:13:40 | INFO | train_inner | epoch 1088:     53 / 81 loss=3.372, nll_loss=0.341, ppl=1.27, wps=4940.1, ups=0.89, wpb=5524.4, bsz=358.6, num_updates=88100, lr=0.00021308, gnorm=0.422, train_wall=48, gb_free=10.1, wall=86516
2022-08-17 22:13:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:13:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:13:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:13:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:14:06 | INFO | valid | epoch 1088 | valid on 'valid' subset | loss 5.223 | nll_loss 2.63 | ppl 6.19 | bleu 56.68 | wps 1883.6 | wpb 933.5 | bsz 59.6 | num_updates 88128 | best_bleu 57.52
2022-08-17 22:14:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1088 @ 88128 updates
2022-08-17 22:14:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1088.pt
2022-08-17 22:14:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1088.pt
2022-08-17 22:14:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1088.pt (epoch 1088 @ 88128 updates, score 56.68) (writing took 20.84133129566908 seconds)
2022-08-17 22:14:27 | INFO | fairseq_cli.train | end of epoch 1088 (average epoch stats below)
2022-08-17 22:14:27 | INFO | train | epoch 1088 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5610.8 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 88128 | lr 0.000213046 | gnorm 0.409 | train_wall 40 | gb_free 10.2 | wall 86563
2022-08-17 22:14:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:14:27 | INFO | fairseq.trainer | begin training epoch 1089
2022-08-17 22:14:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:15:04 | INFO | train_inner | epoch 1089:     72 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=6648.6, ups=1.19, wpb=5567.8, bsz=358.4, num_updates=88200, lr=0.000212959, gnorm=0.365, train_wall=49, gb_free=10.1, wall=86600
2022-08-17 22:15:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:15:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:15:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:15:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:15:17 | INFO | valid | epoch 1089 | valid on 'valid' subset | loss 5.205 | nll_loss 2.608 | ppl 6.1 | bleu 56.18 | wps 1849.1 | wpb 933.5 | bsz 59.6 | num_updates 88209 | best_bleu 57.52
2022-08-17 22:15:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1089 @ 88209 updates
2022-08-17 22:15:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1089.pt
2022-08-17 22:15:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1089.pt
2022-08-17 22:15:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1089.pt (epoch 1089 @ 88209 updates, score 56.18) (writing took 17.76960315927863 seconds)
2022-08-17 22:15:35 | INFO | fairseq_cli.train | end of epoch 1089 (average epoch stats below)
2022-08-17 22:15:35 | INFO | train | epoch 1089 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6507.2 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 88209 | lr 0.000212948 | gnorm 0.378 | train_wall 40 | gb_free 10.1 | wall 86632
2022-08-17 22:15:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:15:36 | INFO | fairseq.trainer | begin training epoch 1090
2022-08-17 22:15:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:16:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:16:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:16:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:16:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:16:27 | INFO | valid | epoch 1090 | valid on 'valid' subset | loss 5.199 | nll_loss 2.6 | ppl 6.06 | bleu 57 | wps 1888.4 | wpb 933.5 | bsz 59.6 | num_updates 88290 | best_bleu 57.52
2022-08-17 22:16:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1090 @ 88290 updates
2022-08-17 22:16:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1090.pt
2022-08-17 22:16:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1090.pt
2022-08-17 22:17:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1090.pt (epoch 1090 @ 88290 updates, score 57.0) (writing took 34.98839258775115 seconds)
2022-08-17 22:17:03 | INFO | fairseq_cli.train | end of epoch 1090 (average epoch stats below)
2022-08-17 22:17:03 | INFO | train | epoch 1090 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 5131.8 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 88290 | lr 0.00021285 | gnorm 0.364 | train_wall 40 | gb_free 10.1 | wall 86719
2022-08-17 22:17:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:17:03 | INFO | fairseq.trainer | begin training epoch 1091
2022-08-17 22:17:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:17:09 | INFO | train_inner | epoch 1091:     10 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=4372.6, ups=0.8, wpb=5497.3, bsz=354.6, num_updates=88300, lr=0.000212838, gnorm=0.351, train_wall=50, gb_free=10.1, wall=86725
2022-08-17 22:17:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:17:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:17:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:17:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:17:54 | INFO | valid | epoch 1091 | valid on 'valid' subset | loss 5.209 | nll_loss 2.618 | ppl 6.14 | bleu 56.25 | wps 1918.5 | wpb 933.5 | bsz 59.6 | num_updates 88371 | best_bleu 57.52
2022-08-17 22:17:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1091 @ 88371 updates
2022-08-17 22:17:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1091.pt
2022-08-17 22:17:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1091.pt
2022-08-17 22:18:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1091.pt (epoch 1091 @ 88371 updates, score 56.25) (writing took 17.464122522622347 seconds)
2022-08-17 22:18:11 | INFO | fairseq_cli.train | end of epoch 1091 (average epoch stats below)
2022-08-17 22:18:11 | INFO | train | epoch 1091 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6518.6 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 88371 | lr 0.000212753 | gnorm 0.314 | train_wall 40 | gb_free 10.1 | wall 86787
2022-08-17 22:18:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:18:11 | INFO | fairseq.trainer | begin training epoch 1092
2022-08-17 22:18:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:18:27 | INFO | train_inner | epoch 1092:     29 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=7074.1, ups=1.28, wpb=5512.7, bsz=360.2, num_updates=88400, lr=0.000212718, gnorm=0.335, train_wall=49, gb_free=10.1, wall=86803
2022-08-17 22:18:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:18:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:18:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:18:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:19:04 | INFO | valid | epoch 1092 | valid on 'valid' subset | loss 5.211 | nll_loss 2.613 | ppl 6.12 | bleu 56.65 | wps 1808.4 | wpb 933.5 | bsz 59.6 | num_updates 88452 | best_bleu 57.52
2022-08-17 22:19:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1092 @ 88452 updates
2022-08-17 22:19:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1092.pt
2022-08-17 22:19:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1092.pt
2022-08-17 22:19:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1092.pt (epoch 1092 @ 88452 updates, score 56.65) (writing took 14.925140004605055 seconds)
2022-08-17 22:19:19 | INFO | fairseq_cli.train | end of epoch 1092 (average epoch stats below)
2022-08-17 22:19:19 | INFO | train | epoch 1092 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6626.4 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 88452 | lr 0.000212655 | gnorm 0.421 | train_wall 40 | gb_free 10.1 | wall 86855
2022-08-17 22:19:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:19:19 | INFO | fairseq.trainer | begin training epoch 1093
2022-08-17 22:19:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:19:45 | INFO | train_inner | epoch 1093:     48 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=7072.4, ups=1.28, wpb=5509.2, bsz=355.4, num_updates=88500, lr=0.000212598, gnorm=0.397, train_wall=51, gb_free=10.1, wall=86881
2022-08-17 22:20:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:20:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:20:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:20:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:20:11 | INFO | valid | epoch 1093 | valid on 'valid' subset | loss 5.224 | nll_loss 2.633 | ppl 6.2 | bleu 55.97 | wps 1931.2 | wpb 933.5 | bsz 59.6 | num_updates 88533 | best_bleu 57.52
2022-08-17 22:20:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1093 @ 88533 updates
2022-08-17 22:20:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1093.pt
2022-08-17 22:20:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1093.pt
2022-08-17 22:20:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1093.pt (epoch 1093 @ 88533 updates, score 55.97) (writing took 36.42109214514494 seconds)
2022-08-17 22:20:47 | INFO | fairseq_cli.train | end of epoch 1093 (average epoch stats below)
2022-08-17 22:20:47 | INFO | train | epoch 1093 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5045.8 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 88533 | lr 0.000212558 | gnorm 0.336 | train_wall 41 | gb_free 10.1 | wall 86944
2022-08-17 22:20:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:20:48 | INFO | fairseq.trainer | begin training epoch 1094
2022-08-17 22:20:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:21:23 | INFO | train_inner | epoch 1094:     67 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5631.5, ups=1.02, wpb=5539.7, bsz=359.2, num_updates=88600, lr=0.000212478, gnorm=0.319, train_wall=50, gb_free=10.1, wall=86980
2022-08-17 22:21:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:21:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:21:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:21:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:21:40 | INFO | valid | epoch 1094 | valid on 'valid' subset | loss 5.217 | nll_loss 2.626 | ppl 6.17 | bleu 56.1 | wps 1811.9 | wpb 933.5 | bsz 59.6 | num_updates 88614 | best_bleu 57.52
2022-08-17 22:21:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1094 @ 88614 updates
2022-08-17 22:21:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1094.pt
2022-08-17 22:21:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1094.pt
2022-08-17 22:22:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1094.pt (epoch 1094 @ 88614 updates, score 56.1) (writing took 22.781636606901884 seconds)
2022-08-17 22:22:03 | INFO | fairseq_cli.train | end of epoch 1094 (average epoch stats below)
2022-08-17 22:22:03 | INFO | train | epoch 1094 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5931.1 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 88614 | lr 0.000212461 | gnorm 0.318 | train_wall 40 | gb_free 10.2 | wall 87019
2022-08-17 22:22:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:22:03 | INFO | fairseq.trainer | begin training epoch 1095
2022-08-17 22:22:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:22:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:22:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:22:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:22:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:22:55 | INFO | valid | epoch 1095 | valid on 'valid' subset | loss 5.234 | nll_loss 2.644 | ppl 6.25 | bleu 56.34 | wps 1926.8 | wpb 933.5 | bsz 59.6 | num_updates 88695 | best_bleu 57.52
2022-08-17 22:22:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1095 @ 88695 updates
2022-08-17 22:22:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1095.pt
2022-08-17 22:22:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1095.pt
2022-08-17 22:23:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1095.pt (epoch 1095 @ 88695 updates, score 56.34) (writing took 41.3159564062953 seconds)
2022-08-17 22:23:36 | INFO | fairseq_cli.train | end of epoch 1095 (average epoch stats below)
2022-08-17 22:23:36 | INFO | train | epoch 1095 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 4782.6 | ups 0.87 | wpb 5523.2 | bsz 358 | num_updates 88695 | lr 0.000212364 | gnorm 0.362 | train_wall 40 | gb_free 10.2 | wall 87113
2022-08-17 22:23:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:23:37 | INFO | fairseq.trainer | begin training epoch 1096
2022-08-17 22:23:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:23:40 | INFO | train_inner | epoch 1096:      5 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=4047.1, ups=0.73, wpb=5525.4, bsz=357.6, num_updates=88700, lr=0.000212358, gnorm=0.362, train_wall=49, gb_free=10.1, wall=87116
2022-08-17 22:24:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:24:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:24:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:24:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:24:28 | INFO | valid | epoch 1096 | valid on 'valid' subset | loss 5.218 | nll_loss 2.626 | ppl 6.17 | bleu 56.15 | wps 1879.9 | wpb 933.5 | bsz 59.6 | num_updates 88776 | best_bleu 57.52
2022-08-17 22:24:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1096 @ 88776 updates
2022-08-17 22:24:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1096.pt
2022-08-17 22:24:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1096.pt
2022-08-17 22:24:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1096.pt (epoch 1096 @ 88776 updates, score 56.15) (writing took 17.558424703776836 seconds)
2022-08-17 22:24:46 | INFO | fairseq_cli.train | end of epoch 1096 (average epoch stats below)
2022-08-17 22:24:46 | INFO | train | epoch 1096 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 6446.9 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 88776 | lr 0.000212267 | gnorm 0.351 | train_wall 40 | gb_free 10.1 | wall 87182
2022-08-17 22:24:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:24:46 | INFO | fairseq.trainer | begin training epoch 1097
2022-08-17 22:24:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:25:00 | INFO | train_inner | epoch 1097:     24 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=6947.4, ups=1.25, wpb=5545.3, bsz=359.8, num_updates=88800, lr=0.000212238, gnorm=0.362, train_wall=50, gb_free=10.1, wall=87196
2022-08-17 22:25:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:25:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:25:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:25:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:25:38 | INFO | valid | epoch 1097 | valid on 'valid' subset | loss 5.226 | nll_loss 2.634 | ppl 6.21 | bleu 56.19 | wps 1829.5 | wpb 933.5 | bsz 59.6 | num_updates 88857 | best_bleu 57.52
2022-08-17 22:25:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1097 @ 88857 updates
2022-08-17 22:25:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1097.pt
2022-08-17 22:25:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1097.pt
2022-08-17 22:26:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1097.pt (epoch 1097 @ 88857 updates, score 56.19) (writing took 48.25963731110096 seconds)
2022-08-17 22:26:26 | INFO | fairseq_cli.train | end of epoch 1097 (average epoch stats below)
2022-08-17 22:26:27 | INFO | train | epoch 1097 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 4442.5 | ups 0.8 | wpb 5523.2 | bsz 358 | num_updates 88857 | lr 0.00021217 | gnorm 0.381 | train_wall 41 | gb_free 10.1 | wall 87283
2022-08-17 22:26:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:26:27 | INFO | fairseq.trainer | begin training epoch 1098
2022-08-17 22:26:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:26:50 | INFO | train_inner | epoch 1098:     43 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=4993.1, ups=0.91, wpb=5488, bsz=357.4, num_updates=88900, lr=0.000212119, gnorm=0.372, train_wall=50, gb_free=10, wall=87306
2022-08-17 22:27:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:27:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:27:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:27:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:27:19 | INFO | valid | epoch 1098 | valid on 'valid' subset | loss 5.224 | nll_loss 2.633 | ppl 6.2 | bleu 56.31 | wps 1836.9 | wpb 933.5 | bsz 59.6 | num_updates 88938 | best_bleu 57.52
2022-08-17 22:27:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1098 @ 88938 updates
2022-08-17 22:27:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1098.pt
2022-08-17 22:27:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1098.pt
2022-08-17 22:27:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1098.pt (epoch 1098 @ 88938 updates, score 56.31) (writing took 24.033090583980083 seconds)
2022-08-17 22:27:43 | INFO | fairseq_cli.train | end of epoch 1098 (average epoch stats below)
2022-08-17 22:27:43 | INFO | train | epoch 1098 | loss 3.371 | nll_loss 0.339 | ppl 1.26 | wps 5864.8 | ups 1.06 | wpb 5523.2 | bsz 358 | num_updates 88938 | lr 0.000212073 | gnorm 0.363 | train_wall 40 | gb_free 10.1 | wall 87359
2022-08-17 22:27:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:27:43 | INFO | fairseq.trainer | begin training epoch 1099
2022-08-17 22:27:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:28:15 | INFO | train_inner | epoch 1099:     62 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=6520.3, ups=1.17, wpb=5555.4, bsz=360.6, num_updates=89000, lr=0.000212, gnorm=0.375, train_wall=49, gb_free=10.1, wall=87391
2022-08-17 22:28:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:28:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:28:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:28:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:28:34 | INFO | valid | epoch 1099 | valid on 'valid' subset | loss 5.206 | nll_loss 2.61 | ppl 6.11 | bleu 56.38 | wps 1892.8 | wpb 933.5 | bsz 59.6 | num_updates 89019 | best_bleu 57.52
2022-08-17 22:28:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1099 @ 89019 updates
2022-08-17 22:28:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1099.pt
2022-08-17 22:28:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1099.pt
2022-08-17 22:28:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1099.pt (epoch 1099 @ 89019 updates, score 56.38) (writing took 15.122796889394522 seconds)
2022-08-17 22:28:49 | INFO | fairseq_cli.train | end of epoch 1099 (average epoch stats below)
2022-08-17 22:28:49 | INFO | train | epoch 1099 | loss 3.372 | nll_loss 0.341 | ppl 1.27 | wps 6736 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 89019 | lr 0.000211977 | gnorm 0.394 | train_wall 40 | gb_free 10 | wall 87425
2022-08-17 22:28:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:28:49 | INFO | fairseq.trainer | begin training epoch 1100
2022-08-17 22:28:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:29:31 | INFO | train_inner | epoch 1100:     81 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=7237.2, ups=1.32, wpb=5496.1, bsz=356.2, num_updates=89100, lr=0.000211881, gnorm=0.362, train_wall=50, gb_free=10.2, wall=87467
2022-08-17 22:29:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:29:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:29:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:29:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:29:40 | INFO | valid | epoch 1100 | valid on 'valid' subset | loss 5.218 | nll_loss 2.629 | ppl 6.19 | bleu 56.12 | wps 1844.2 | wpb 933.5 | bsz 59.6 | num_updates 89100 | best_bleu 57.52
2022-08-17 22:29:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1100 @ 89100 updates
2022-08-17 22:29:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1100.pt
2022-08-17 22:29:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1100.pt
2022-08-17 22:30:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1100.pt (epoch 1100 @ 89100 updates, score 56.12) (writing took 39.95039199665189 seconds)
2022-08-17 22:30:20 | INFO | fairseq_cli.train | end of epoch 1100 (average epoch stats below)
2022-08-17 22:30:20 | INFO | train | epoch 1100 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 4902.2 | ups 0.89 | wpb 5523.2 | bsz 358 | num_updates 89100 | lr 0.000211881 | gnorm 0.353 | train_wall 40 | gb_free 10.2 | wall 87517
2022-08-17 22:30:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:30:21 | INFO | fairseq.trainer | begin training epoch 1101
2022-08-17 22:30:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:31:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:31:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:31:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:31:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:31:12 | INFO | valid | epoch 1101 | valid on 'valid' subset | loss 5.213 | nll_loss 2.618 | ppl 6.14 | bleu 56.62 | wps 1886.1 | wpb 933.5 | bsz 59.6 | num_updates 89181 | best_bleu 57.52
2022-08-17 22:31:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1101 @ 89181 updates
2022-08-17 22:31:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1101.pt
2022-08-17 22:31:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1101.pt
2022-08-17 22:31:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1101.pt (epoch 1101 @ 89181 updates, score 56.62) (writing took 19.830073077231646 seconds)
2022-08-17 22:31:32 | INFO | fairseq_cli.train | end of epoch 1101 (average epoch stats below)
2022-08-17 22:31:32 | INFO | train | epoch 1101 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6283.8 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 89181 | lr 0.000211784 | gnorm 0.306 | train_wall 39 | gb_free 10 | wall 87588
2022-08-17 22:31:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:31:32 | INFO | fairseq.trainer | begin training epoch 1102
2022-08-17 22:31:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:31:42 | INFO | train_inner | epoch 1102:     19 / 81 loss=3.371, nll_loss=0.339, ppl=1.26, wps=4223.9, ups=0.76, wpb=5551.2, bsz=360.4, num_updates=89200, lr=0.000211762, gnorm=0.31, train_wall=48, gb_free=10.2, wall=87599
2022-08-17 22:32:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:32:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:32:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:32:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:32:22 | INFO | valid | epoch 1102 | valid on 'valid' subset | loss 5.215 | nll_loss 2.62 | ppl 6.15 | bleu 56.71 | wps 1878.4 | wpb 933.5 | bsz 59.6 | num_updates 89262 | best_bleu 57.52
2022-08-17 22:32:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1102 @ 89262 updates
2022-08-17 22:32:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1102.pt
2022-08-17 22:32:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1102.pt
2022-08-17 22:32:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1102.pt (epoch 1102 @ 89262 updates, score 56.71) (writing took 20.046812161803246 seconds)
2022-08-17 22:32:42 | INFO | fairseq_cli.train | end of epoch 1102 (average epoch stats below)
2022-08-17 22:32:42 | INFO | train | epoch 1102 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6366.2 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 89262 | lr 0.000211688 | gnorm 0.398 | train_wall 39 | gb_free 10.2 | wall 87658
2022-08-17 22:32:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:32:42 | INFO | fairseq.trainer | begin training epoch 1103
2022-08-17 22:32:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:33:02 | INFO | train_inner | epoch 1103:     38 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=6879, ups=1.25, wpb=5500.8, bsz=354.6, num_updates=89300, lr=0.000211643, gnorm=0.403, train_wall=48, gb_free=10.1, wall=87679
2022-08-17 22:33:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:33:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:33:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:33:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:33:33 | INFO | valid | epoch 1103 | valid on 'valid' subset | loss 5.211 | nll_loss 2.616 | ppl 6.13 | bleu 56.23 | wps 1804.3 | wpb 933.5 | bsz 59.6 | num_updates 89343 | best_bleu 57.52
2022-08-17 22:33:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1103 @ 89343 updates
2022-08-17 22:33:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1103.pt
2022-08-17 22:33:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1103.pt
2022-08-17 22:34:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1103.pt (epoch 1103 @ 89343 updates, score 56.23) (writing took 43.08760364726186 seconds)
2022-08-17 22:34:17 | INFO | fairseq_cli.train | end of epoch 1103 (average epoch stats below)
2022-08-17 22:34:17 | INFO | train | epoch 1103 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 4723.4 | ups 0.86 | wpb 5523.2 | bsz 358 | num_updates 89343 | lr 0.000211592 | gnorm 0.36 | train_wall 40 | gb_free 10.2 | wall 87753
2022-08-17 22:34:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:34:17 | INFO | fairseq.trainer | begin training epoch 1104
2022-08-17 22:34:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:34:46 | INFO | train_inner | epoch 1104:     57 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=5306.2, ups=0.96, wpb=5526.7, bsz=361.2, num_updates=89400, lr=0.000211525, gnorm=0.367, train_wall=50, gb_free=10.2, wall=87783
2022-08-17 22:34:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:35:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:35:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:35:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:35:08 | INFO | valid | epoch 1104 | valid on 'valid' subset | loss 5.211 | nll_loss 2.616 | ppl 6.13 | bleu 56.17 | wps 1894.5 | wpb 933.5 | bsz 59.6 | num_updates 89424 | best_bleu 57.52
2022-08-17 22:35:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1104 @ 89424 updates
2022-08-17 22:35:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1104.pt
2022-08-17 22:35:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1104.pt
2022-08-17 22:35:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1104.pt (epoch 1104 @ 89424 updates, score 56.17) (writing took 23.087903439998627 seconds)
2022-08-17 22:35:31 | INFO | fairseq_cli.train | end of epoch 1104 (average epoch stats below)
2022-08-17 22:35:31 | INFO | train | epoch 1104 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6006.1 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 89424 | lr 0.000211496 | gnorm 0.369 | train_wall 40 | gb_free 10.2 | wall 87827
2022-08-17 22:35:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:35:31 | INFO | fairseq.trainer | begin training epoch 1105
2022-08-17 22:35:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:36:11 | INFO | train_inner | epoch 1105:     76 / 81 loss=3.373, nll_loss=0.342, ppl=1.27, wps=6567.4, ups=1.19, wpb=5538.9, bsz=357, num_updates=89500, lr=0.000211407, gnorm=1.172, train_wall=50, gb_free=10.1, wall=87867
2022-08-17 22:36:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:36:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:36:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:36:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:36:23 | INFO | valid | epoch 1105 | valid on 'valid' subset | loss 5.206 | nll_loss 2.614 | ppl 6.12 | bleu 56.11 | wps 1776.2 | wpb 933.5 | bsz 59.6 | num_updates 89505 | best_bleu 57.52
2022-08-17 22:36:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1105 @ 89505 updates
2022-08-17 22:36:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1105.pt
2022-08-17 22:36:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1105.pt
2022-08-17 22:36:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1105.pt (epoch 1105 @ 89505 updates, score 56.11) (writing took 23.367822904139757 seconds)
2022-08-17 22:36:46 | INFO | fairseq_cli.train | end of epoch 1105 (average epoch stats below)
2022-08-17 22:36:46 | INFO | train | epoch 1105 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5956.5 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 89505 | lr 0.000211401 | gnorm 1.377 | train_wall 40 | gb_free 10.2 | wall 87902
2022-08-17 22:36:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:36:46 | INFO | fairseq.trainer | begin training epoch 1106
2022-08-17 22:36:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:37:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:37:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:37:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:37:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:37:43 | INFO | valid | epoch 1106 | valid on 'valid' subset | loss 5.213 | nll_loss 2.617 | ppl 6.13 | bleu 56.62 | wps 1867.9 | wpb 933.5 | bsz 59.6 | num_updates 89586 | best_bleu 57.52
2022-08-17 22:37:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1106 @ 89586 updates
2022-08-17 22:37:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1106.pt
2022-08-17 22:37:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1106.pt
2022-08-17 22:38:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1106.pt (epoch 1106 @ 89586 updates, score 56.62) (writing took 37.29624292254448 seconds)
2022-08-17 22:38:20 | INFO | fairseq_cli.train | end of epoch 1106 (average epoch stats below)
2022-08-17 22:38:20 | INFO | train | epoch 1106 | loss 3.371 | nll_loss 0.338 | ppl 1.26 | wps 4763.2 | ups 0.86 | wpb 5523.2 | bsz 358 | num_updates 89586 | lr 0.000211305 | gnorm 0.28 | train_wall 39 | gb_free 10.3 | wall 87996
2022-08-17 22:38:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:38:20 | INFO | fairseq.trainer | begin training epoch 1107
2022-08-17 22:38:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:38:28 | INFO | train_inner | epoch 1107:     14 / 81 loss=3.371, nll_loss=0.338, ppl=1.26, wps=3999.9, ups=0.73, wpb=5505.2, bsz=356.3, num_updates=89600, lr=0.000211289, gnorm=0.302, train_wall=48, gb_free=10, wall=88005
2022-08-17 22:39:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:39:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:39:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:39:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:39:12 | INFO | valid | epoch 1107 | valid on 'valid' subset | loss 5.214 | nll_loss 2.62 | ppl 6.15 | bleu 56.35 | wps 1809.3 | wpb 933.5 | bsz 59.6 | num_updates 89667 | best_bleu 57.52
2022-08-17 22:39:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1107 @ 89667 updates
2022-08-17 22:39:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1107.pt
2022-08-17 22:39:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1107.pt
2022-08-17 22:39:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1107.pt (epoch 1107 @ 89667 updates, score 56.35) (writing took 20.458805929869413 seconds)
2022-08-17 22:39:33 | INFO | fairseq_cli.train | end of epoch 1107 (average epoch stats below)
2022-08-17 22:39:33 | INFO | train | epoch 1107 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6164.8 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 89667 | lr 0.00021121 | gnorm 0.307 | train_wall 40 | gb_free 10.1 | wall 88069
2022-08-17 22:39:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:39:33 | INFO | fairseq.trainer | begin training epoch 1108
2022-08-17 22:39:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:39:51 | INFO | train_inner | epoch 1108:     33 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=6680.5, ups=1.21, wpb=5539, bsz=357.8, num_updates=89700, lr=0.000211171, gnorm=0.304, train_wall=49, gb_free=10.1, wall=88088
2022-08-17 22:40:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:40:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:40:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:40:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:40:25 | INFO | valid | epoch 1108 | valid on 'valid' subset | loss 5.221 | nll_loss 2.629 | ppl 6.19 | bleu 55.75 | wps 1792.5 | wpb 933.5 | bsz 59.6 | num_updates 89748 | best_bleu 57.52
2022-08-17 22:40:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1108 @ 89748 updates
2022-08-17 22:40:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1108.pt
2022-08-17 22:40:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1108.pt
2022-08-17 22:40:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1108.pt (epoch 1108 @ 89748 updates, score 55.75) (writing took 17.406178176403046 seconds)
2022-08-17 22:40:43 | INFO | fairseq_cli.train | end of epoch 1108 (average epoch stats below)
2022-08-17 22:40:43 | INFO | train | epoch 1108 | loss 3.371 | nll_loss 0.339 | ppl 1.26 | wps 6406.6 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 89748 | lr 0.000211114 | gnorm 0.29 | train_wall 40 | gb_free 10.1 | wall 88139
2022-08-17 22:40:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:40:43 | INFO | fairseq.trainer | begin training epoch 1109
2022-08-17 22:40:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:41:12 | INFO | train_inner | epoch 1109:     52 / 81 loss=3.371, nll_loss=0.339, ppl=1.26, wps=6866.7, ups=1.25, wpb=5508.6, bsz=360.7, num_updates=89800, lr=0.000211053, gnorm=0.325, train_wall=50, gb_free=10.1, wall=88168
2022-08-17 22:41:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:41:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:41:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:41:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:41:37 | INFO | valid | epoch 1109 | valid on 'valid' subset | loss 5.235 | nll_loss 2.645 | ppl 6.25 | bleu 55.72 | wps 1927 | wpb 933.5 | bsz 59.6 | num_updates 89829 | best_bleu 57.52
2022-08-17 22:41:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1109 @ 89829 updates
2022-08-17 22:41:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1109.pt
2022-08-17 22:41:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1109.pt
2022-08-17 22:42:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1109.pt (epoch 1109 @ 89829 updates, score 55.72) (writing took 37.63269382715225 seconds)
2022-08-17 22:42:15 | INFO | fairseq_cli.train | end of epoch 1109 (average epoch stats below)
2022-08-17 22:42:15 | INFO | train | epoch 1109 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 4866.4 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 89829 | lr 0.000211019 | gnorm 1.674 | train_wall 40 | gb_free 10.2 | wall 88231
2022-08-17 22:42:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:42:15 | INFO | fairseq.trainer | begin training epoch 1110
2022-08-17 22:42:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:42:52 | INFO | train_inner | epoch 1110:     71 / 81 loss=3.372, nll_loss=0.341, ppl=1.27, wps=5482.6, ups=0.99, wpb=5526.6, bsz=357.9, num_updates=89900, lr=0.000210936, gnorm=1.429, train_wall=49, gb_free=10, wall=88269
2022-08-17 22:42:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:42:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:42:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:42:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:43:07 | INFO | valid | epoch 1110 | valid on 'valid' subset | loss 5.228 | nll_loss 2.639 | ppl 6.23 | bleu 56.42 | wps 1767.4 | wpb 933.5 | bsz 59.6 | num_updates 89910 | best_bleu 57.52
2022-08-17 22:43:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1110 @ 89910 updates
2022-08-17 22:43:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1110.pt
2022-08-17 22:43:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1110.pt
2022-08-17 22:43:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1110.pt (epoch 1110 @ 89910 updates, score 56.42) (writing took 28.66924289613962 seconds)
2022-08-17 22:43:36 | INFO | fairseq_cli.train | end of epoch 1110 (average epoch stats below)
2022-08-17 22:43:36 | INFO | train | epoch 1110 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5496.4 | ups 1 | wpb 5523.2 | bsz 358 | num_updates 89910 | lr 0.000210924 | gnorm 0.395 | train_wall 40 | gb_free 10.2 | wall 88312
2022-08-17 22:43:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:43:36 | INFO | fairseq.trainer | begin training epoch 1111
2022-08-17 22:43:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:44:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:44:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:44:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:44:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:44:30 | INFO | valid | epoch 1111 | valid on 'valid' subset | loss 5.225 | nll_loss 2.637 | ppl 6.22 | bleu 56.14 | wps 1834.3 | wpb 933.5 | bsz 59.6 | num_updates 89991 | best_bleu 57.52
2022-08-17 22:44:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1111 @ 89991 updates
2022-08-17 22:44:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1111.pt
2022-08-17 22:44:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1111.pt
2022-08-17 22:44:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1111.pt (epoch 1111 @ 89991 updates, score 56.14) (writing took 21.761590871959925 seconds)
2022-08-17 22:44:51 | INFO | fairseq_cli.train | end of epoch 1111 (average epoch stats below)
2022-08-17 22:44:51 | INFO | train | epoch 1111 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5925.4 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 89991 | lr 0.000210829 | gnorm 0.363 | train_wall 41 | gb_free 10 | wall 88388
2022-08-17 22:44:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:44:52 | INFO | fairseq.trainer | begin training epoch 1112
2022-08-17 22:44:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:44:57 | INFO | train_inner | epoch 1112:      9 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=4416.9, ups=0.8, wpb=5524.3, bsz=358.2, num_updates=90000, lr=0.000210819, gnorm=0.375, train_wall=50, gb_free=10.1, wall=88394
2022-08-17 22:45:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:45:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:45:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:45:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:45:43 | INFO | valid | epoch 1112 | valid on 'valid' subset | loss 5.219 | nll_loss 2.628 | ppl 6.18 | bleu 55.73 | wps 1820 | wpb 933.5 | bsz 59.6 | num_updates 90072 | best_bleu 57.52
2022-08-17 22:45:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1112 @ 90072 updates
2022-08-17 22:45:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1112.pt
2022-08-17 22:45:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1112.pt
2022-08-17 22:46:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1112.pt (epoch 1112 @ 90072 updates, score 55.73) (writing took 39.80474549904466 seconds)
2022-08-17 22:46:23 | INFO | fairseq_cli.train | end of epoch 1112 (average epoch stats below)
2022-08-17 22:46:23 | INFO | train | epoch 1112 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 4877.8 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 90072 | lr 0.000210734 | gnorm 0.358 | train_wall 40 | gb_free 10.2 | wall 88479
2022-08-17 22:46:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:46:23 | INFO | fairseq.trainer | begin training epoch 1113
2022-08-17 22:46:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:46:39 | INFO | train_inner | epoch 1113:     28 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5448.3, ups=0.99, wpb=5518.9, bsz=357.2, num_updates=90100, lr=0.000210701, gnorm=0.355, train_wall=49, gb_free=10.1, wall=88495
2022-08-17 22:47:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:47:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:47:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:47:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:47:14 | INFO | valid | epoch 1113 | valid on 'valid' subset | loss 5.219 | nll_loss 2.628 | ppl 6.18 | bleu 56.05 | wps 1891.8 | wpb 933.5 | bsz 59.6 | num_updates 90153 | best_bleu 57.52
2022-08-17 22:47:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1113 @ 90153 updates
2022-08-17 22:47:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1113.pt
2022-08-17 22:47:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1113.pt
2022-08-17 22:47:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1113.pt (epoch 1113 @ 90153 updates, score 56.05) (writing took 2.3489067889750004 seconds)
2022-08-17 22:47:17 | INFO | fairseq_cli.train | end of epoch 1113 (average epoch stats below)
2022-08-17 22:47:17 | INFO | train | epoch 1113 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 8312.2 | ups 1.5 | wpb 5523.2 | bsz 358 | num_updates 90153 | lr 0.00021064 | gnorm 0.346 | train_wall 39 | gb_free 10.1 | wall 88533
2022-08-17 22:47:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:47:17 | INFO | fairseq.trainer | begin training epoch 1114
2022-08-17 22:47:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:47:43 | INFO | train_inner | epoch 1114:     47 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=8574, ups=1.55, wpb=5532.1, bsz=362.3, num_updates=90200, lr=0.000210585, gnorm=0.345, train_wall=49, gb_free=10.1, wall=88560
2022-08-17 22:48:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:48:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:48:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:48:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:48:11 | INFO | valid | epoch 1114 | valid on 'valid' subset | loss 5.211 | nll_loss 2.619 | ppl 6.14 | bleu 56.44 | wps 1783.9 | wpb 933.5 | bsz 59.6 | num_updates 90234 | best_bleu 57.52
2022-08-17 22:48:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1114 @ 90234 updates
2022-08-17 22:48:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1114.pt
2022-08-17 22:48:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1114.pt
2022-08-17 22:48:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1114.pt (epoch 1114 @ 90234 updates, score 56.44) (writing took 26.894099563360214 seconds)
2022-08-17 22:48:38 | INFO | fairseq_cli.train | end of epoch 1114 (average epoch stats below)
2022-08-17 22:48:38 | INFO | train | epoch 1114 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5532.8 | ups 1 | wpb 5523.2 | bsz 358 | num_updates 90234 | lr 0.000210545 | gnorm 0.326 | train_wall 41 | gb_free 10.2 | wall 88614
2022-08-17 22:48:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:48:38 | INFO | fairseq.trainer | begin training epoch 1115
2022-08-17 22:48:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:49:13 | INFO | train_inner | epoch 1115:     66 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=6165.7, ups=1.11, wpb=5540.3, bsz=353, num_updates=90300, lr=0.000210468, gnorm=0.301, train_wall=51, gb_free=10.1, wall=88649
2022-08-17 22:49:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:49:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:49:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:49:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:49:31 | INFO | valid | epoch 1115 | valid on 'valid' subset | loss 5.226 | nll_loss 2.636 | ppl 6.22 | bleu 55.85 | wps 1702.9 | wpb 933.5 | bsz 59.6 | num_updates 90315 | best_bleu 57.52
2022-08-17 22:49:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1115 @ 90315 updates
2022-08-17 22:49:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1115.pt
2022-08-17 22:49:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1115.pt
2022-08-17 22:49:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1115.pt (epoch 1115 @ 90315 updates, score 55.85) (writing took 2.7277603559195995 seconds)
2022-08-17 22:49:34 | INFO | fairseq_cli.train | end of epoch 1115 (average epoch stats below)
2022-08-17 22:49:34 | INFO | train | epoch 1115 | loss 3.372 | nll_loss 0.341 | ppl 1.27 | wps 8029.8 | ups 1.45 | wpb 5523.2 | bsz 358 | num_updates 90315 | lr 0.000210451 | gnorm 0.311 | train_wall 41 | gb_free 10.2 | wall 88670
2022-08-17 22:49:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:49:34 | INFO | fairseq.trainer | begin training epoch 1116
2022-08-17 22:49:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:50:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:50:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:50:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:50:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:50:25 | INFO | valid | epoch 1116 | valid on 'valid' subset | loss 5.216 | nll_loss 2.626 | ppl 6.17 | bleu 56.27 | wps 1920.1 | wpb 933.5 | bsz 59.6 | num_updates 90396 | best_bleu 57.52
2022-08-17 22:50:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1116 @ 90396 updates
2022-08-17 22:50:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1116.pt
2022-08-17 22:50:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1116.pt
2022-08-17 22:50:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1116.pt (epoch 1116 @ 90396 updates, score 56.27) (writing took 23.479986485093832 seconds)
2022-08-17 22:50:49 | INFO | fairseq_cli.train | end of epoch 1116 (average epoch stats below)
2022-08-17 22:50:49 | INFO | train | epoch 1116 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5930.2 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 90396 | lr 0.000210356 | gnorm 0.459 | train_wall 41 | gb_free 10.1 | wall 88745
2022-08-17 22:50:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:50:49 | INFO | fairseq.trainer | begin training epoch 1117
2022-08-17 22:50:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:50:52 | INFO | train_inner | epoch 1117:      4 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5539.4, ups=1.01, wpb=5492.5, bsz=358.4, num_updates=90400, lr=0.000210352, gnorm=0.442, train_wall=50, gb_free=10.1, wall=88749
2022-08-17 22:51:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:51:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:51:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:51:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:51:40 | INFO | valid | epoch 1117 | valid on 'valid' subset | loss 5.221 | nll_loss 2.631 | ppl 6.19 | bleu 56.42 | wps 1879.8 | wpb 933.5 | bsz 59.6 | num_updates 90477 | best_bleu 57.52
2022-08-17 22:51:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1117 @ 90477 updates
2022-08-17 22:51:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1117.pt
2022-08-17 22:51:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1117.pt
2022-08-17 22:52:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1117.pt (epoch 1117 @ 90477 updates, score 56.42) (writing took 36.57536061108112 seconds)
2022-08-17 22:52:16 | INFO | fairseq_cli.train | end of epoch 1117 (average epoch stats below)
2022-08-17 22:52:16 | INFO | train | epoch 1117 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 5118.2 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 90477 | lr 0.000210262 | gnorm 0.311 | train_wall 40 | gb_free 10.2 | wall 88833
2022-08-17 22:52:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:52:17 | INFO | fairseq.trainer | begin training epoch 1118
2022-08-17 22:52:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:52:30 | INFO | train_inner | epoch 1118:     23 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5672.6, ups=1.03, wpb=5522, bsz=357.4, num_updates=90500, lr=0.000210235, gnorm=0.509, train_wall=49, gb_free=10, wall=88846
2022-08-17 22:52:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:53:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:53:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:53:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:53:09 | INFO | valid | epoch 1118 | valid on 'valid' subset | loss 5.218 | nll_loss 2.627 | ppl 6.18 | bleu 56.68 | wps 1849.1 | wpb 933.5 | bsz 59.6 | num_updates 90558 | best_bleu 57.52
2022-08-17 22:53:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1118 @ 90558 updates
2022-08-17 22:53:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1118.pt
2022-08-17 22:53:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1118.pt
2022-08-17 22:53:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1118.pt (epoch 1118 @ 90558 updates, score 56.68) (writing took 20.049193121492863 seconds)
2022-08-17 22:53:29 | INFO | fairseq_cli.train | end of epoch 1118 (average epoch stats below)
2022-08-17 22:53:29 | INFO | train | epoch 1118 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6171.1 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 90558 | lr 0.000210168 | gnorm 0.662 | train_wall 40 | gb_free 10.1 | wall 88905
2022-08-17 22:53:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:53:29 | INFO | fairseq.trainer | begin training epoch 1119
2022-08-17 22:53:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:53:52 | INFO | train_inner | epoch 1119:     42 / 81 loss=3.373, nll_loss=0.342, ppl=1.27, wps=6760.1, ups=1.21, wpb=5571.6, bsz=362.6, num_updates=90600, lr=0.000210119, gnorm=0.432, train_wall=49, gb_free=10.1, wall=88928
2022-08-17 22:54:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:54:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:54:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:54:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:54:21 | INFO | valid | epoch 1119 | valid on 'valid' subset | loss 5.224 | nll_loss 2.635 | ppl 6.21 | bleu 56.06 | wps 1943.3 | wpb 933.5 | bsz 59.6 | num_updates 90639 | best_bleu 57.52
2022-08-17 22:54:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1119 @ 90639 updates
2022-08-17 22:54:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1119.pt
2022-08-17 22:54:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1119.pt
2022-08-17 22:54:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1119.pt (epoch 1119 @ 90639 updates, score 56.06) (writing took 17.745388332754374 seconds)
2022-08-17 22:54:39 | INFO | fairseq_cli.train | end of epoch 1119 (average epoch stats below)
2022-08-17 22:54:39 | INFO | train | epoch 1119 | loss 3.373 | nll_loss 0.342 | ppl 1.27 | wps 6391.9 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 90639 | lr 0.000210074 | gnorm 0.423 | train_wall 40 | gb_free 10.1 | wall 88975
2022-08-17 22:54:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:54:39 | INFO | fairseq.trainer | begin training epoch 1120
2022-08-17 22:54:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:55:11 | INFO | train_inner | epoch 1120:     61 / 81 loss=3.373, nll_loss=0.342, ppl=1.27, wps=6957.4, ups=1.27, wpb=5466.7, bsz=353.7, num_updates=90700, lr=0.000210003, gnorm=0.419, train_wall=50, gb_free=10, wall=89007
2022-08-17 22:55:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:55:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:55:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:55:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:55:29 | INFO | valid | epoch 1120 | valid on 'valid' subset | loss 5.226 | nll_loss 2.635 | ppl 6.21 | bleu 56.43 | wps 1830.2 | wpb 933.5 | bsz 59.6 | num_updates 90720 | best_bleu 57.52
2022-08-17 22:55:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1120 @ 90720 updates
2022-08-17 22:55:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1120.pt
2022-08-17 22:55:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1120.pt
2022-08-17 22:56:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1120.pt (epoch 1120 @ 90720 updates, score 56.43) (writing took 34.30596365407109 seconds)
2022-08-17 22:56:04 | INFO | fairseq_cli.train | end of epoch 1120 (average epoch stats below)
2022-08-17 22:56:04 | INFO | train | epoch 1120 | loss 3.373 | nll_loss 0.342 | ppl 1.27 | wps 5263.2 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 90720 | lr 0.00020998 | gnorm 0.388 | train_wall 39 | gb_free 10.2 | wall 89060
2022-08-17 22:56:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:56:04 | INFO | fairseq.trainer | begin training epoch 1121
2022-08-17 22:56:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:56:47 | INFO | train_inner | epoch 1121:     80 / 81 loss=3.374, nll_loss=0.342, ppl=1.27, wps=5776.1, ups=1.04, wpb=5556.1, bsz=359.4, num_updates=90800, lr=0.000209888, gnorm=0.562, train_wall=48, gb_free=10.1, wall=89103
2022-08-17 22:56:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:56:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:56:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:56:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:56:56 | INFO | valid | epoch 1121 | valid on 'valid' subset | loss 5.229 | nll_loss 2.638 | ppl 6.22 | bleu 56.33 | wps 1854.1 | wpb 933.5 | bsz 59.6 | num_updates 90801 | best_bleu 57.52
2022-08-17 22:56:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1121 @ 90801 updates
2022-08-17 22:56:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1121.pt
2022-08-17 22:56:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1121.pt
2022-08-17 22:56:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1121.pt (epoch 1121 @ 90801 updates, score 56.33) (writing took 2.3105270452797413 seconds)
2022-08-17 22:56:59 | INFO | fairseq_cli.train | end of epoch 1121 (average epoch stats below)
2022-08-17 22:56:59 | INFO | train | epoch 1121 | loss 3.374 | nll_loss 0.342 | ppl 1.27 | wps 8136.5 | ups 1.47 | wpb 5523.2 | bsz 358 | num_updates 90801 | lr 0.000209887 | gnorm 0.611 | train_wall 39 | gb_free 10.1 | wall 89115
2022-08-17 22:56:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:56:59 | INFO | fairseq.trainer | begin training epoch 1122
2022-08-17 22:56:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:57:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:57:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:57:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:57:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:57:50 | INFO | valid | epoch 1122 | valid on 'valid' subset | loss 5.222 | nll_loss 2.631 | ppl 6.19 | bleu 56.14 | wps 1831.3 | wpb 933.5 | bsz 59.6 | num_updates 90882 | best_bleu 57.52
2022-08-17 22:57:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1122 @ 90882 updates
2022-08-17 22:57:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1122.pt
2022-08-17 22:57:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1122.pt
2022-08-17 22:58:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1122.pt (epoch 1122 @ 90882 updates, score 56.14) (writing took 33.3523989059031 seconds)
2022-08-17 22:58:24 | INFO | fairseq_cli.train | end of epoch 1122 (average epoch stats below)
2022-08-17 22:58:24 | INFO | train | epoch 1122 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5267.4 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 90882 | lr 0.000209793 | gnorm 0.414 | train_wall 40 | gb_free 10 | wall 89200
2022-08-17 22:58:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:58:24 | INFO | fairseq.trainer | begin training epoch 1123
2022-08-17 22:58:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 22:58:35 | INFO | train_inner | epoch 1123:     18 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5118.3, ups=0.93, wpb=5515.9, bsz=357.8, num_updates=90900, lr=0.000209772, gnorm=0.404, train_wall=49, gb_free=10.1, wall=89211
2022-08-17 22:59:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 22:59:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 22:59:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 22:59:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 22:59:15 | INFO | valid | epoch 1123 | valid on 'valid' subset | loss 5.231 | nll_loss 2.644 | ppl 6.25 | bleu 55.88 | wps 1819.6 | wpb 933.5 | bsz 59.6 | num_updates 90963 | best_bleu 57.52
2022-08-17 22:59:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1123 @ 90963 updates
2022-08-17 22:59:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1123.pt
2022-08-17 22:59:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1123.pt
2022-08-17 22:59:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1123.pt (epoch 1123 @ 90963 updates, score 55.88) (writing took 32.931046798825264 seconds)
2022-08-17 22:59:49 | INFO | fairseq_cli.train | end of epoch 1123 (average epoch stats below)
2022-08-17 22:59:49 | INFO | train | epoch 1123 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5283.3 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 90963 | lr 0.0002097 | gnorm 0.361 | train_wall 40 | gb_free 10.1 | wall 89285
2022-08-17 22:59:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 22:59:49 | INFO | fairseq.trainer | begin training epoch 1124
2022-08-17 22:59:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:00:09 | INFO | train_inner | epoch 1124:     37 / 81 loss=3.372, nll_loss=0.341, ppl=1.27, wps=5843.7, ups=1.06, wpb=5505.5, bsz=356, num_updates=91000, lr=0.000209657, gnorm=0.347, train_wall=49, gb_free=10.1, wall=89305
2022-08-17 23:00:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:00:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:00:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:00:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:00:40 | INFO | valid | epoch 1124 | valid on 'valid' subset | loss 5.22 | nll_loss 2.63 | ppl 6.19 | bleu 55.94 | wps 1931.5 | wpb 933.5 | bsz 59.6 | num_updates 91044 | best_bleu 57.52
2022-08-17 23:00:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1124 @ 91044 updates
2022-08-17 23:00:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1124.pt
2022-08-17 23:00:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1124.pt
2022-08-17 23:00:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1124.pt (epoch 1124 @ 91044 updates, score 55.94) (writing took 18.18775787204504 seconds)
2022-08-17 23:00:58 | INFO | fairseq_cli.train | end of epoch 1124 (average epoch stats below)
2022-08-17 23:00:58 | INFO | train | epoch 1124 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6441.2 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 91044 | lr 0.000209606 | gnorm 0.344 | train_wall 40 | gb_free 10.2 | wall 89354
2022-08-17 23:00:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:00:58 | INFO | fairseq.trainer | begin training epoch 1125
2022-08-17 23:00:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:01:27 | INFO | train_inner | epoch 1125:     56 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=7071.2, ups=1.28, wpb=5527.4, bsz=359.4, num_updates=91100, lr=0.000209542, gnorm=0.357, train_wall=49, gb_free=10.2, wall=89383
2022-08-17 23:01:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:01:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:01:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:01:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:01:49 | INFO | valid | epoch 1125 | valid on 'valid' subset | loss 5.233 | nll_loss 2.645 | ppl 6.26 | bleu 56.13 | wps 1787 | wpb 933.5 | bsz 59.6 | num_updates 91125 | best_bleu 57.52
2022-08-17 23:01:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1125 @ 91125 updates
2022-08-17 23:01:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1125.pt
2022-08-17 23:01:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1125.pt
2022-08-17 23:02:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1125.pt (epoch 1125 @ 91125 updates, score 56.13) (writing took 18.67296862974763 seconds)
2022-08-17 23:02:08 | INFO | fairseq_cli.train | end of epoch 1125 (average epoch stats below)
2022-08-17 23:02:08 | INFO | train | epoch 1125 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6406.6 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 91125 | lr 0.000209513 | gnorm 0.364 | train_wall 39 | gb_free 10.1 | wall 89424
2022-08-17 23:02:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:02:08 | INFO | fairseq.trainer | begin training epoch 1126
2022-08-17 23:02:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:02:48 | INFO | train_inner | epoch 1126:     75 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=6856.7, ups=1.24, wpb=5528.1, bsz=355.1, num_updates=91200, lr=0.000209427, gnorm=0.381, train_wall=49, gb_free=10.1, wall=89464
2022-08-17 23:02:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:02:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:02:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:02:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:03:00 | INFO | valid | epoch 1126 | valid on 'valid' subset | loss 5.243 | nll_loss 2.658 | ppl 6.31 | bleu 56.13 | wps 1825.4 | wpb 933.5 | bsz 59.6 | num_updates 91206 | best_bleu 57.52
2022-08-17 23:03:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1126 @ 91206 updates
2022-08-17 23:03:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1126.pt
2022-08-17 23:03:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1126.pt
2022-08-17 23:03:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1126.pt (epoch 1126 @ 91206 updates, score 56.13) (writing took 22.262979596853256 seconds)
2022-08-17 23:03:22 | INFO | fairseq_cli.train | end of epoch 1126 (average epoch stats below)
2022-08-17 23:03:22 | INFO | train | epoch 1126 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 6008.7 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 91206 | lr 0.00020942 | gnorm 0.376 | train_wall 40 | gb_free 10.2 | wall 89499
2022-08-17 23:03:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:03:23 | INFO | fairseq.trainer | begin training epoch 1127
2022-08-17 23:03:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:04:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:04:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:04:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:04:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:04:15 | INFO | valid | epoch 1127 | valid on 'valid' subset | loss 5.237 | nll_loss 2.646 | ppl 6.26 | bleu 56.27 | wps 1816.9 | wpb 933.5 | bsz 59.6 | num_updates 91287 | best_bleu 57.52
2022-08-17 23:04:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1127 @ 91287 updates
2022-08-17 23:04:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1127.pt
2022-08-17 23:04:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1127.pt
2022-08-17 23:04:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1127.pt (epoch 1127 @ 91287 updates, score 56.27) (writing took 15.241826210170984 seconds)
2022-08-17 23:04:30 | INFO | fairseq_cli.train | end of epoch 1127 (average epoch stats below)
2022-08-17 23:04:30 | INFO | train | epoch 1127 | loss 3.372 | nll_loss 0.341 | ppl 1.27 | wps 6586.1 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 91287 | lr 0.000209327 | gnorm 0.375 | train_wall 39 | gb_free 10.1 | wall 89566
2022-08-17 23:04:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:04:30 | INFO | fairseq.trainer | begin training epoch 1128
2022-08-17 23:04:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:04:38 | INFO | train_inner | epoch 1128:     13 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5002.4, ups=0.91, wpb=5515.8, bsz=360.7, num_updates=91300, lr=0.000209312, gnorm=0.367, train_wall=48, gb_free=10, wall=89574
2022-08-17 23:05:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:05:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:05:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:05:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:05:21 | INFO | valid | epoch 1128 | valid on 'valid' subset | loss 5.232 | nll_loss 2.642 | ppl 6.24 | bleu 56.13 | wps 1946.4 | wpb 933.5 | bsz 59.6 | num_updates 91368 | best_bleu 57.52
2022-08-17 23:05:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1128 @ 91368 updates
2022-08-17 23:05:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1128.pt
2022-08-17 23:05:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1128.pt
2022-08-17 23:05:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1128.pt (epoch 1128 @ 91368 updates, score 56.13) (writing took 35.18320982903242 seconds)
2022-08-17 23:05:57 | INFO | fairseq_cli.train | end of epoch 1128 (average epoch stats below)
2022-08-17 23:05:57 | INFO | train | epoch 1128 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5177.5 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 91368 | lr 0.000209234 | gnorm 0.395 | train_wall 40 | gb_free 10.1 | wall 89653
2022-08-17 23:05:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:05:57 | INFO | fairseq.trainer | begin training epoch 1129
2022-08-17 23:05:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:06:16 | INFO | train_inner | epoch 1129:     32 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5647, ups=1.02, wpb=5560.4, bsz=360.2, num_updates=91400, lr=0.000209198, gnorm=0.387, train_wall=49, gb_free=10, wall=89673
2022-08-17 23:06:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:06:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:06:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:06:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:06:54 | INFO | valid | epoch 1129 | valid on 'valid' subset | loss 5.225 | nll_loss 2.634 | ppl 6.21 | bleu 56.15 | wps 1936.1 | wpb 933.5 | bsz 59.6 | num_updates 91449 | best_bleu 57.52
2022-08-17 23:06:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1129 @ 91449 updates
2022-08-17 23:06:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1129.pt
2022-08-17 23:06:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1129.pt
2022-08-17 23:07:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1129.pt (epoch 1129 @ 91449 updates, score 56.15) (writing took 23.270557940006256 seconds)
2022-08-17 23:07:17 | INFO | fairseq_cli.train | end of epoch 1129 (average epoch stats below)
2022-08-17 23:07:17 | INFO | train | epoch 1129 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 5568.7 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 91449 | lr 0.000209142 | gnorm 0.366 | train_wall 40 | gb_free 10.1 | wall 89733
2022-08-17 23:07:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:07:17 | INFO | fairseq.trainer | begin training epoch 1130
2022-08-17 23:07:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:07:44 | INFO | train_inner | epoch 1130:     51 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=6333.1, ups=1.15, wpb=5522.6, bsz=358, num_updates=91500, lr=0.000209083, gnorm=0.334, train_wall=50, gb_free=10.1, wall=89760
2022-08-17 23:07:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:08:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:08:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:08:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:08:08 | INFO | valid | epoch 1130 | valid on 'valid' subset | loss 5.222 | nll_loss 2.632 | ppl 6.2 | bleu 55.6 | wps 1973.5 | wpb 933.5 | bsz 59.6 | num_updates 91530 | best_bleu 57.52
2022-08-17 23:08:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1130 @ 91530 updates
2022-08-17 23:08:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1130.pt
2022-08-17 23:08:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1130.pt
2022-08-17 23:08:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1130.pt (epoch 1130 @ 91530 updates, score 55.6) (writing took 15.157054007053375 seconds)
2022-08-17 23:08:23 | INFO | fairseq_cli.train | end of epoch 1130 (average epoch stats below)
2022-08-17 23:08:23 | INFO | train | epoch 1130 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 6767.8 | ups 1.23 | wpb 5523.2 | bsz 358 | num_updates 91530 | lr 0.000209049 | gnorm 0.302 | train_wall 40 | gb_free 10 | wall 89799
2022-08-17 23:08:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:08:23 | INFO | fairseq.trainer | begin training epoch 1131
2022-08-17 23:08:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:09:00 | INFO | train_inner | epoch 1131:     70 / 81 loss=3.372, nll_loss=0.341, ppl=1.27, wps=7224.5, ups=1.31, wpb=5525.7, bsz=360.2, num_updates=91600, lr=0.000208969, gnorm=0.389, train_wall=50, gb_free=10.1, wall=89836
2022-08-17 23:09:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:09:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:09:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:09:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:09:15 | INFO | valid | epoch 1131 | valid on 'valid' subset | loss 5.228 | nll_loss 2.638 | ppl 6.22 | bleu 56.41 | wps 1866.1 | wpb 933.5 | bsz 59.6 | num_updates 91611 | best_bleu 57.52
2022-08-17 23:09:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1131 @ 91611 updates
2022-08-17 23:09:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1131.pt
2022-08-17 23:09:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1131.pt
2022-08-17 23:09:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1131.pt (epoch 1131 @ 91611 updates, score 56.41) (writing took 41.326959785073996 seconds)
2022-08-17 23:09:56 | INFO | fairseq_cli.train | end of epoch 1131 (average epoch stats below)
2022-08-17 23:09:56 | INFO | train | epoch 1131 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 4809.7 | ups 0.87 | wpb 5523.2 | bsz 358 | num_updates 91611 | lr 0.000208957 | gnorm 0.442 | train_wall 40 | gb_free 10.1 | wall 89892
2022-08-17 23:09:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:09:56 | INFO | fairseq.trainer | begin training epoch 1132
2022-08-17 23:09:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:10:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:10:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:10:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:10:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:10:48 | INFO | valid | epoch 1132 | valid on 'valid' subset | loss 5.211 | nll_loss 2.615 | ppl 6.13 | bleu 56.71 | wps 1893.4 | wpb 933.5 | bsz 59.6 | num_updates 91692 | best_bleu 57.52
2022-08-17 23:10:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1132 @ 91692 updates
2022-08-17 23:10:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1132.pt
2022-08-17 23:10:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1132.pt
2022-08-17 23:11:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1132.pt (epoch 1132 @ 91692 updates, score 56.71) (writing took 24.029857262969017 seconds)
2022-08-17 23:11:13 | INFO | fairseq_cli.train | end of epoch 1132 (average epoch stats below)
2022-08-17 23:11:13 | INFO | train | epoch 1132 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 5845.8 | ups 1.06 | wpb 5523.2 | bsz 358 | num_updates 91692 | lr 0.000208864 | gnorm 0.395 | train_wall 41 | gb_free 10.1 | wall 89969
2022-08-17 23:11:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:11:13 | INFO | fairseq.trainer | begin training epoch 1133
2022-08-17 23:11:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:11:18 | INFO | train_inner | epoch 1133:      8 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=3966.3, ups=0.72, wpb=5473.2, bsz=353.6, num_updates=91700, lr=0.000208855, gnorm=0.416, train_wall=50, gb_free=10.1, wall=89974
2022-08-17 23:11:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:11:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:11:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:11:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:12:05 | INFO | valid | epoch 1133 | valid on 'valid' subset | loss 5.213 | nll_loss 2.618 | ppl 6.14 | bleu 56.48 | wps 1886.4 | wpb 933.5 | bsz 59.6 | num_updates 91773 | best_bleu 57.52
2022-08-17 23:12:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1133 @ 91773 updates
2022-08-17 23:12:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1133.pt
2022-08-17 23:12:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1133.pt
2022-08-17 23:12:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1133.pt (epoch 1133 @ 91773 updates, score 56.48) (writing took 21.105726793408394 seconds)
2022-08-17 23:12:26 | INFO | fairseq_cli.train | end of epoch 1133 (average epoch stats below)
2022-08-17 23:12:26 | INFO | train | epoch 1133 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6084.6 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 91773 | lr 0.000208772 | gnorm 0.414 | train_wall 40 | gb_free 10.1 | wall 90042
2022-08-17 23:12:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:12:26 | INFO | fairseq.trainer | begin training epoch 1134
2022-08-17 23:12:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:12:41 | INFO | train_inner | epoch 1134:     27 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=6680.9, ups=1.21, wpb=5536.4, bsz=361.1, num_updates=91800, lr=0.000208741, gnorm=0.397, train_wall=49, gb_free=10.1, wall=90057
2022-08-17 23:13:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:13:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:13:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:13:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:13:17 | INFO | valid | epoch 1134 | valid on 'valid' subset | loss 5.246 | nll_loss 2.659 | ppl 6.32 | bleu 55.78 | wps 1829.9 | wpb 933.5 | bsz 59.6 | num_updates 91854 | best_bleu 57.52
2022-08-17 23:13:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1134 @ 91854 updates
2022-08-17 23:13:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1134.pt
2022-08-17 23:13:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1134.pt
2022-08-17 23:13:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1134.pt (epoch 1134 @ 91854 updates, score 55.78) (writing took 34.70266401395202 seconds)
2022-08-17 23:13:52 | INFO | fairseq_cli.train | end of epoch 1134 (average epoch stats below)
2022-08-17 23:13:52 | INFO | train | epoch 1134 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5197.5 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 91854 | lr 0.00020868 | gnorm 0.33 | train_wall 40 | gb_free 10.2 | wall 90128
2022-08-17 23:13:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:13:52 | INFO | fairseq.trainer | begin training epoch 1135
2022-08-17 23:13:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:14:17 | INFO | train_inner | epoch 1135:     46 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5763, ups=1.04, wpb=5543.3, bsz=357.4, num_updates=91900, lr=0.000208628, gnorm=0.349, train_wall=49, gb_free=10.1, wall=90153
2022-08-17 23:14:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:14:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:14:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:14:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:14:44 | INFO | valid | epoch 1135 | valid on 'valid' subset | loss 5.228 | nll_loss 2.638 | ppl 6.22 | bleu 56.44 | wps 1837.7 | wpb 933.5 | bsz 59.6 | num_updates 91935 | best_bleu 57.52
2022-08-17 23:14:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1135 @ 91935 updates
2022-08-17 23:14:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1135.pt
2022-08-17 23:14:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1135.pt
2022-08-17 23:14:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1135.pt (epoch 1135 @ 91935 updates, score 56.44) (writing took 2.4123232886195183 seconds)
2022-08-17 23:14:47 | INFO | fairseq_cli.train | end of epoch 1135 (average epoch stats below)
2022-08-17 23:14:47 | INFO | train | epoch 1135 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 8217.1 | ups 1.49 | wpb 5523.2 | bsz 358 | num_updates 91935 | lr 0.000208588 | gnorm 0.341 | train_wall 40 | gb_free 10.1 | wall 90183
2022-08-17 23:14:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:14:47 | INFO | fairseq.trainer | begin training epoch 1136
2022-08-17 23:14:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:15:20 | INFO | train_inner | epoch 1136:     65 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=8740.5, ups=1.58, wpb=5521.8, bsz=356, num_updates=92000, lr=0.000208514, gnorm=0.339, train_wall=49, gb_free=10.1, wall=90217
2022-08-17 23:15:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:15:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:15:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:15:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:15:38 | INFO | valid | epoch 1136 | valid on 'valid' subset | loss 5.22 | nll_loss 2.628 | ppl 6.18 | bleu 56.41 | wps 1855 | wpb 933.5 | bsz 59.6 | num_updates 92016 | best_bleu 57.52
2022-08-17 23:15:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1136 @ 92016 updates
2022-08-17 23:15:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1136.pt
2022-08-17 23:15:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1136.pt
2022-08-17 23:16:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1136.pt (epoch 1136 @ 92016 updates, score 56.41) (writing took 31.750386971980333 seconds)
2022-08-17 23:16:10 | INFO | fairseq_cli.train | end of epoch 1136 (average epoch stats below)
2022-08-17 23:16:10 | INFO | train | epoch 1136 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 5381.9 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 92016 | lr 0.000208496 | gnorm 0.347 | train_wall 40 | gb_free 10.1 | wall 90266
2022-08-17 23:16:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:16:10 | INFO | fairseq.trainer | begin training epoch 1137
2022-08-17 23:16:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:16:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:16:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:16:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:16:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:17:01 | INFO | valid | epoch 1137 | valid on 'valid' subset | loss 5.233 | nll_loss 2.645 | ppl 6.26 | bleu 56.46 | wps 1800.1 | wpb 933.5 | bsz 59.6 | num_updates 92097 | best_bleu 57.52
2022-08-17 23:17:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1137 @ 92097 updates
2022-08-17 23:17:01 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1137.pt
2022-08-17 23:17:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1137.pt
2022-08-17 23:17:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1137.pt (epoch 1137 @ 92097 updates, score 56.46) (writing took 20.980205602943897 seconds)
2022-08-17 23:17:22 | INFO | fairseq_cli.train | end of epoch 1137 (average epoch stats below)
2022-08-17 23:17:22 | INFO | train | epoch 1137 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6158.5 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 92097 | lr 0.000208405 | gnorm 0.44 | train_wall 40 | gb_free 10.2 | wall 90339
2022-08-17 23:17:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:17:23 | INFO | fairseq.trainer | begin training epoch 1138
2022-08-17 23:17:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:17:26 | INFO | train_inner | epoch 1138:      3 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=4398.2, ups=0.8, wpb=5507.8, bsz=358.3, num_updates=92100, lr=0.000208401, gnorm=0.409, train_wall=49, gb_free=10.1, wall=90342
2022-08-17 23:18:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:18:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:18:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:18:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:18:20 | INFO | valid | epoch 1138 | valid on 'valid' subset | loss 5.227 | nll_loss 2.637 | ppl 6.22 | bleu 56.73 | wps 1818.6 | wpb 933.5 | bsz 59.6 | num_updates 92178 | best_bleu 57.52
2022-08-17 23:18:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1138 @ 92178 updates
2022-08-17 23:18:20 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1138.pt
2022-08-17 23:18:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1138.pt
2022-08-17 23:18:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1138.pt (epoch 1138 @ 92178 updates, score 56.73) (writing took 23.989471271634102 seconds)
2022-08-17 23:18:44 | INFO | fairseq_cli.train | end of epoch 1138 (average epoch stats below)
2022-08-17 23:18:44 | INFO | train | epoch 1138 | loss 3.372 | nll_loss 0.341 | ppl 1.27 | wps 5464.4 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 92178 | lr 0.000208313 | gnorm 0.765 | train_wall 40 | gb_free 10.1 | wall 90421
2022-08-17 23:18:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:18:45 | INFO | fairseq.trainer | begin training epoch 1139
2022-08-17 23:18:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:18:56 | INFO | train_inner | epoch 1139:     22 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=6078.1, ups=1.1, wpb=5514.8, bsz=354.8, num_updates=92200, lr=0.000208288, gnorm=0.723, train_wall=49, gb_free=10, wall=90433
2022-08-17 23:19:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:19:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:19:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:19:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:19:36 | INFO | valid | epoch 1139 | valid on 'valid' subset | loss 5.242 | nll_loss 2.659 | ppl 6.31 | bleu 56.24 | wps 1848.2 | wpb 933.5 | bsz 59.6 | num_updates 92259 | best_bleu 57.52
2022-08-17 23:19:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1139 @ 92259 updates
2022-08-17 23:19:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1139.pt
2022-08-17 23:19:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1139.pt
2022-08-17 23:19:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1139.pt (epoch 1139 @ 92259 updates, score 56.24) (writing took 20.67607318609953 seconds)
2022-08-17 23:19:57 | INFO | fairseq_cli.train | end of epoch 1139 (average epoch stats below)
2022-08-17 23:19:57 | INFO | train | epoch 1139 | loss 3.372 | nll_loss 0.341 | ppl 1.27 | wps 6199.8 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 92259 | lr 0.000208222 | gnorm 0.581 | train_wall 40 | gb_free 10.1 | wall 90493
2022-08-17 23:19:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:19:57 | INFO | fairseq.trainer | begin training epoch 1140
2022-08-17 23:19:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:20:20 | INFO | train_inner | epoch 1140:     41 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=6652.3, ups=1.2, wpb=5538.6, bsz=360, num_updates=92300, lr=0.000208175, gnorm=0.493, train_wall=50, gb_free=10.1, wall=90516
2022-08-17 23:20:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:20:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:20:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:20:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:20:49 | INFO | valid | epoch 1140 | valid on 'valid' subset | loss 5.243 | nll_loss 2.656 | ppl 6.3 | bleu 55.9 | wps 1851.2 | wpb 933.5 | bsz 59.6 | num_updates 92340 | best_bleu 57.52
2022-08-17 23:20:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1140 @ 92340 updates
2022-08-17 23:20:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1140.pt
2022-08-17 23:20:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1140.pt
2022-08-17 23:21:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1140.pt (epoch 1140 @ 92340 updates, score 55.9) (writing took 34.71968046575785 seconds)
2022-08-17 23:21:24 | INFO | fairseq_cli.train | end of epoch 1140 (average epoch stats below)
2022-08-17 23:21:24 | INFO | train | epoch 1140 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5092.5 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 92340 | lr 0.00020813 | gnorm 0.365 | train_wall 40 | gb_free 10 | wall 90581
2022-08-17 23:21:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:21:25 | INFO | fairseq.trainer | begin training epoch 1141
2022-08-17 23:21:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:21:59 | INFO | train_inner | epoch 1141:     60 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5582.8, ups=1.01, wpb=5547.9, bsz=363.7, num_updates=92400, lr=0.000208063, gnorm=0.378, train_wall=49, gb_free=10, wall=90615
2022-08-17 23:22:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:22:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:22:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:22:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:22:19 | INFO | valid | epoch 1141 | valid on 'valid' subset | loss 5.232 | nll_loss 2.647 | ppl 6.26 | bleu 56.4 | wps 1897 | wpb 933.5 | bsz 59.6 | num_updates 92421 | best_bleu 57.52
2022-08-17 23:22:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1141 @ 92421 updates
2022-08-17 23:22:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1141.pt
2022-08-17 23:22:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1141.pt
2022-08-17 23:22:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1141.pt (epoch 1141 @ 92421 updates, score 56.4) (writing took 16.24273321032524 seconds)
2022-08-17 23:22:36 | INFO | fairseq_cli.train | end of epoch 1141 (average epoch stats below)
2022-08-17 23:22:36 | INFO | train | epoch 1141 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6283 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 92421 | lr 0.000208039 | gnorm 0.362 | train_wall 40 | gb_free 10.2 | wall 90652
2022-08-17 23:22:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:22:36 | INFO | fairseq.trainer | begin training epoch 1142
2022-08-17 23:22:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:23:17 | INFO | train_inner | epoch 1142:     79 / 81 loss=3.373, nll_loss=0.341, ppl=1.27, wps=7046.4, ups=1.28, wpb=5518.8, bsz=352.9, num_updates=92500, lr=0.00020795, gnorm=0.375, train_wall=51, gb_free=10.1, wall=90693
2022-08-17 23:23:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:23:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:23:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:23:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:23:27 | INFO | valid | epoch 1142 | valid on 'valid' subset | loss 5.233 | nll_loss 2.644 | ppl 6.25 | bleu 56.56 | wps 1895.3 | wpb 933.5 | bsz 59.6 | num_updates 92502 | best_bleu 57.52
2022-08-17 23:23:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1142 @ 92502 updates
2022-08-17 23:23:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1142.pt
2022-08-17 23:23:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1142.pt
2022-08-17 23:23:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1142.pt (epoch 1142 @ 92502 updates, score 56.56) (writing took 27.629743590950966 seconds)
2022-08-17 23:23:55 | INFO | fairseq_cli.train | end of epoch 1142 (average epoch stats below)
2022-08-17 23:23:55 | INFO | train | epoch 1142 | loss 3.372 | nll_loss 0.341 | ppl 1.27 | wps 5634.6 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 92502 | lr 0.000207948 | gnorm 0.381 | train_wall 41 | gb_free 10 | wall 90731
2022-08-17 23:23:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:23:55 | INFO | fairseq.trainer | begin training epoch 1143
2022-08-17 23:23:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:24:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:24:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:24:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:24:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:24:46 | INFO | valid | epoch 1143 | valid on 'valid' subset | loss 5.245 | nll_loss 2.657 | ppl 6.31 | bleu 56.18 | wps 1815 | wpb 933.5 | bsz 59.6 | num_updates 92583 | best_bleu 57.52
2022-08-17 23:24:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1143 @ 92583 updates
2022-08-17 23:24:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1143.pt
2022-08-17 23:24:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1143.pt
2022-08-17 23:24:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1143.pt (epoch 1143 @ 92583 updates, score 56.18) (writing took 2.5665042251348495 seconds)
2022-08-17 23:24:49 | INFO | fairseq_cli.train | end of epoch 1143 (average epoch stats below)
2022-08-17 23:24:49 | INFO | train | epoch 1143 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 8322.3 | ups 1.51 | wpb 5523.2 | bsz 358 | num_updates 92583 | lr 0.000207857 | gnorm 0.381 | train_wall 39 | gb_free 10.2 | wall 90785
2022-08-17 23:24:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:24:49 | INFO | fairseq.trainer | begin training epoch 1144
2022-08-17 23:24:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:24:59 | INFO | train_inner | epoch 1144:     17 / 81 loss=3.371, nll_loss=0.339, ppl=1.26, wps=5393.4, ups=0.98, wpb=5499.7, bsz=360.6, num_updates=92600, lr=0.000207838, gnorm=0.377, train_wall=49, gb_free=10.1, wall=90795
2022-08-17 23:25:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:25:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:25:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:25:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:25:44 | INFO | valid | epoch 1144 | valid on 'valid' subset | loss 5.233 | nll_loss 2.642 | ppl 6.24 | bleu 56.63 | wps 1639.8 | wpb 933.5 | bsz 59.6 | num_updates 92664 | best_bleu 57.52
2022-08-17 23:25:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1144 @ 92664 updates
2022-08-17 23:25:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1144.pt
2022-08-17 23:25:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1144.pt
2022-08-17 23:26:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1144.pt (epoch 1144 @ 92664 updates, score 56.63) (writing took 23.614544469863176 seconds)
2022-08-17 23:26:07 | INFO | fairseq_cli.train | end of epoch 1144 (average epoch stats below)
2022-08-17 23:26:07 | INFO | train | epoch 1144 | loss 3.372 | nll_loss 0.341 | ppl 1.27 | wps 5689.8 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 92664 | lr 0.000207766 | gnorm 0.348 | train_wall 41 | gb_free 10.1 | wall 90864
2022-08-17 23:26:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:26:08 | INFO | fairseq.trainer | begin training epoch 1145
2022-08-17 23:26:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:26:28 | INFO | train_inner | epoch 1145:     36 / 81 loss=3.372, nll_loss=0.341, ppl=1.27, wps=6285.1, ups=1.13, wpb=5553.2, bsz=355.7, num_updates=92700, lr=0.000207726, gnorm=0.388, train_wall=51, gb_free=10, wall=90884
2022-08-17 23:26:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:26:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:26:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:26:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:26:59 | INFO | valid | epoch 1145 | valid on 'valid' subset | loss 5.224 | nll_loss 2.635 | ppl 6.21 | bleu 56.13 | wps 1842.3 | wpb 933.5 | bsz 59.6 | num_updates 92745 | best_bleu 57.52
2022-08-17 23:26:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1145 @ 92745 updates
2022-08-17 23:26:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1145.pt
2022-08-17 23:27:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1145.pt
2022-08-17 23:27:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1145.pt (epoch 1145 @ 92745 updates, score 56.13) (writing took 43.103063482791185 seconds)
2022-08-17 23:27:42 | INFO | fairseq_cli.train | end of epoch 1145 (average epoch stats below)
2022-08-17 23:27:42 | INFO | train | epoch 1145 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 4705.5 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 92745 | lr 0.000207675 | gnorm 0.383 | train_wall 40 | gb_free 10.3 | wall 90959
2022-08-17 23:27:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:27:43 | INFO | fairseq.trainer | begin training epoch 1146
2022-08-17 23:27:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:28:12 | INFO | train_inner | epoch 1146:     55 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5244.2, ups=0.96, wpb=5489, bsz=357, num_updates=92800, lr=0.000207614, gnorm=0.387, train_wall=50, gb_free=10.1, wall=90988
2022-08-17 23:28:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:28:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:28:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:28:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:28:34 | INFO | valid | epoch 1146 | valid on 'valid' subset | loss 5.251 | nll_loss 2.666 | ppl 6.35 | bleu 56.47 | wps 1887 | wpb 933.5 | bsz 59.6 | num_updates 92826 | best_bleu 57.52
2022-08-17 23:28:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1146 @ 92826 updates
2022-08-17 23:28:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1146.pt
2022-08-17 23:28:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1146.pt
2022-08-17 23:28:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1146.pt (epoch 1146 @ 92826 updates, score 56.47) (writing took 15.689758438616991 seconds)
2022-08-17 23:28:50 | INFO | fairseq_cli.train | end of epoch 1146 (average epoch stats below)
2022-08-17 23:28:50 | INFO | train | epoch 1146 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6613.8 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 92826 | lr 0.000207585 | gnorm 0.411 | train_wall 41 | gb_free 10 | wall 91026
2022-08-17 23:28:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:28:50 | INFO | fairseq.trainer | begin training epoch 1147
2022-08-17 23:28:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:29:28 | INFO | train_inner | epoch 1147:     74 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=7303.6, ups=1.32, wpb=5551.8, bsz=361.8, num_updates=92900, lr=0.000207502, gnorm=0.344, train_wall=49, gb_free=10.1, wall=91065
2022-08-17 23:29:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:29:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:29:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:29:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:29:41 | INFO | valid | epoch 1147 | valid on 'valid' subset | loss 5.241 | nll_loss 2.651 | ppl 6.28 | bleu 56.17 | wps 1939.4 | wpb 933.5 | bsz 59.6 | num_updates 92907 | best_bleu 57.52
2022-08-17 23:29:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1147 @ 92907 updates
2022-08-17 23:29:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1147.pt
2022-08-17 23:29:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1147.pt
2022-08-17 23:30:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1147.pt (epoch 1147 @ 92907 updates, score 56.17) (writing took 28.04084986448288 seconds)
2022-08-17 23:30:09 | INFO | fairseq_cli.train | end of epoch 1147 (average epoch stats below)
2022-08-17 23:30:09 | INFO | train | epoch 1147 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5685.8 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 92907 | lr 0.000207494 | gnorm 0.349 | train_wall 40 | gb_free 10.1 | wall 91105
2022-08-17 23:30:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:30:09 | INFO | fairseq.trainer | begin training epoch 1148
2022-08-17 23:30:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:30:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:30:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:30:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:30:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:30:59 | INFO | valid | epoch 1148 | valid on 'valid' subset | loss 5.238 | nll_loss 2.652 | ppl 6.29 | bleu 56.11 | wps 1851.3 | wpb 933.5 | bsz 59.6 | num_updates 92988 | best_bleu 57.52
2022-08-17 23:30:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1148 @ 92988 updates
2022-08-17 23:30:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1148.pt
2022-08-17 23:31:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1148.pt
2022-08-17 23:31:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1148.pt (epoch 1148 @ 92988 updates, score 56.11) (writing took 31.466359574347734 seconds)
2022-08-17 23:31:31 | INFO | fairseq_cli.train | end of epoch 1148 (average epoch stats below)
2022-08-17 23:31:31 | INFO | train | epoch 1148 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5435 | ups 0.98 | wpb 5523.2 | bsz 358 | num_updates 92988 | lr 0.000207404 | gnorm 0.344 | train_wall 39 | gb_free 10.3 | wall 91187
2022-08-17 23:31:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:31:31 | INFO | fairseq.trainer | begin training epoch 1149
2022-08-17 23:31:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:31:38 | INFO | train_inner | epoch 1149:     12 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=4235.7, ups=0.77, wpb=5490.7, bsz=351.9, num_updates=93000, lr=0.00020739, gnorm=0.361, train_wall=48, gb_free=10, wall=91194
2022-08-17 23:32:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:32:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:32:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:32:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:32:22 | INFO | valid | epoch 1149 | valid on 'valid' subset | loss 5.239 | nll_loss 2.648 | ppl 6.27 | bleu 56.49 | wps 1683.1 | wpb 933.5 | bsz 59.6 | num_updates 93069 | best_bleu 57.52
2022-08-17 23:32:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1149 @ 93069 updates
2022-08-17 23:32:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1149.pt
2022-08-17 23:32:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1149.pt
2022-08-17 23:32:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1149.pt (epoch 1149 @ 93069 updates, score 56.49) (writing took 14.702393800020218 seconds)
2022-08-17 23:32:37 | INFO | fairseq_cli.train | end of epoch 1149 (average epoch stats below)
2022-08-17 23:32:37 | INFO | train | epoch 1149 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6776.2 | ups 1.23 | wpb 5523.2 | bsz 358 | num_updates 93069 | lr 0.000207313 | gnorm 0.338 | train_wall 39 | gb_free 10.2 | wall 91253
2022-08-17 23:32:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:32:37 | INFO | fairseq.trainer | begin training epoch 1150
2022-08-17 23:32:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:32:55 | INFO | train_inner | epoch 1150:     31 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=7205.8, ups=1.3, wpb=5554.2, bsz=363.4, num_updates=93100, lr=0.000207279, gnorm=0.326, train_wall=48, gb_free=10.1, wall=91271
2022-08-17 23:33:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:33:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:33:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:33:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:33:29 | INFO | valid | epoch 1150 | valid on 'valid' subset | loss 5.226 | nll_loss 2.635 | ppl 6.21 | bleu 56.44 | wps 1986.6 | wpb 933.5 | bsz 59.6 | num_updates 93150 | best_bleu 57.52
2022-08-17 23:33:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1150 @ 93150 updates
2022-08-17 23:33:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1150.pt
2022-08-17 23:33:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1150.pt
2022-08-17 23:34:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1150.pt (epoch 1150 @ 93150 updates, score 56.44) (writing took 40.61019033193588 seconds)
2022-08-17 23:34:10 | INFO | fairseq_cli.train | end of epoch 1150 (average epoch stats below)
2022-08-17 23:34:10 | INFO | train | epoch 1150 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 4818.1 | ups 0.87 | wpb 5523.2 | bsz 358 | num_updates 93150 | lr 0.000207223 | gnorm 0.367 | train_wall 40 | gb_free 10 | wall 91346
2022-08-17 23:34:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:34:10 | INFO | fairseq.trainer | begin training epoch 1151
2022-08-17 23:34:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:34:36 | INFO | train_inner | epoch 1151:     50 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5458.8, ups=0.99, wpb=5490.6, bsz=353.5, num_updates=93200, lr=0.000207168, gnorm=0.345, train_wall=49, gb_free=10.1, wall=91372
2022-08-17 23:34:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:34:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:34:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:34:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:35:00 | INFO | valid | epoch 1151 | valid on 'valid' subset | loss 5.244 | nll_loss 2.657 | ppl 6.31 | bleu 56.21 | wps 1926.1 | wpb 933.5 | bsz 59.6 | num_updates 93231 | best_bleu 57.52
2022-08-17 23:35:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1151 @ 93231 updates
2022-08-17 23:35:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1151.pt
2022-08-17 23:35:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1151.pt
2022-08-17 23:35:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1151.pt (epoch 1151 @ 93231 updates, score 56.21) (writing took 19.327037811279297 seconds)
2022-08-17 23:35:20 | INFO | fairseq_cli.train | end of epoch 1151 (average epoch stats below)
2022-08-17 23:35:20 | INFO | train | epoch 1151 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6412.6 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 93231 | lr 0.000207133 | gnorm 0.38 | train_wall 39 | gb_free 10.3 | wall 91416
2022-08-17 23:35:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:35:20 | INFO | fairseq.trainer | begin training epoch 1152
2022-08-17 23:35:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:35:56 | INFO | train_inner | epoch 1152:     69 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=6889.5, ups=1.24, wpb=5541, bsz=360.8, num_updates=93300, lr=0.000207057, gnorm=0.432, train_wall=50, gb_free=10, wall=91452
2022-08-17 23:36:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:36:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:36:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:36:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:36:11 | INFO | valid | epoch 1152 | valid on 'valid' subset | loss 5.235 | nll_loss 2.645 | ppl 6.26 | bleu 55.56 | wps 1940.3 | wpb 933.5 | bsz 59.6 | num_updates 93312 | best_bleu 57.52
2022-08-17 23:36:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1152 @ 93312 updates
2022-08-17 23:36:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1152.pt
2022-08-17 23:36:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1152.pt
2022-08-17 23:36:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1152.pt (epoch 1152 @ 93312 updates, score 55.56) (writing took 14.626139272004366 seconds)
2022-08-17 23:36:26 | INFO | fairseq_cli.train | end of epoch 1152 (average epoch stats below)
2022-08-17 23:36:26 | INFO | train | epoch 1152 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6765.8 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 93312 | lr 0.000207043 | gnorm 0.385 | train_wall 40 | gb_free 10.2 | wall 91482
2022-08-17 23:36:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:36:26 | INFO | fairseq.trainer | begin training epoch 1153
2022-08-17 23:36:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:37:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:37:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:37:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:37:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:37:19 | INFO | valid | epoch 1153 | valid on 'valid' subset | loss 5.229 | nll_loss 2.64 | ppl 6.23 | bleu 55.59 | wps 1840.6 | wpb 933.5 | bsz 59.6 | num_updates 93393 | best_bleu 57.52
2022-08-17 23:37:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1153 @ 93393 updates
2022-08-17 23:37:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1153.pt
2022-08-17 23:37:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1153.pt
2022-08-17 23:37:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1153.pt (epoch 1153 @ 93393 updates, score 55.59) (writing took 32.45112230628729 seconds)
2022-08-17 23:37:51 | INFO | fairseq_cli.train | end of epoch 1153 (average epoch stats below)
2022-08-17 23:37:51 | INFO | train | epoch 1153 | loss 3.372 | nll_loss 0.341 | ppl 1.27 | wps 5241.7 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 93393 | lr 0.000206954 | gnorm 0.365 | train_wall 42 | gb_free 10.2 | wall 91567
2022-08-17 23:37:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:37:51 | INFO | fairseq.trainer | begin training epoch 1154
2022-08-17 23:37:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:37:56 | INFO | train_inner | epoch 1154:      7 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=4597.9, ups=0.83, wpb=5523.5, bsz=356.9, num_updates=93400, lr=0.000206946, gnorm=0.35, train_wall=51, gb_free=10.1, wall=91572
2022-08-17 23:38:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:38:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:38:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:38:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:38:43 | INFO | valid | epoch 1154 | valid on 'valid' subset | loss 5.223 | nll_loss 2.632 | ppl 6.2 | bleu 56.12 | wps 1845.2 | wpb 933.5 | bsz 59.6 | num_updates 93474 | best_bleu 57.52
2022-08-17 23:38:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1154 @ 93474 updates
2022-08-17 23:38:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1154.pt
2022-08-17 23:38:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1154.pt
2022-08-17 23:39:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1154.pt (epoch 1154 @ 93474 updates, score 56.12) (writing took 18.778444796800613 seconds)
2022-08-17 23:39:02 | INFO | fairseq_cli.train | end of epoch 1154 (average epoch stats below)
2022-08-17 23:39:02 | INFO | train | epoch 1154 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6353.4 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 93474 | lr 0.000206864 | gnorm 0.312 | train_wall 40 | gb_free 10.1 | wall 91638
2022-08-17 23:39:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:39:02 | INFO | fairseq.trainer | begin training epoch 1155
2022-08-17 23:39:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:39:18 | INFO | train_inner | epoch 1155:     26 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=6762.8, ups=1.23, wpb=5508, bsz=356.9, num_updates=93500, lr=0.000206835, gnorm=0.347, train_wall=50, gb_free=10.1, wall=91654
2022-08-17 23:39:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:39:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:39:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:39:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:39:54 | INFO | valid | epoch 1155 | valid on 'valid' subset | loss 5.22 | nll_loss 2.63 | ppl 6.19 | bleu 56.42 | wps 1963.3 | wpb 933.5 | bsz 59.6 | num_updates 93555 | best_bleu 57.52
2022-08-17 23:39:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1155 @ 93555 updates
2022-08-17 23:39:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1155.pt
2022-08-17 23:39:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1155.pt
2022-08-17 23:40:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1155.pt (epoch 1155 @ 93555 updates, score 56.42) (writing took 15.968297999352217 seconds)
2022-08-17 23:40:10 | INFO | fairseq_cli.train | end of epoch 1155 (average epoch stats below)
2022-08-17 23:40:10 | INFO | train | epoch 1155 | loss 3.373 | nll_loss 0.341 | ppl 1.27 | wps 6530.3 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 93555 | lr 0.000206774 | gnorm 0.431 | train_wall 40 | gb_free 10.1 | wall 91706
2022-08-17 23:40:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:40:10 | INFO | fairseq.trainer | begin training epoch 1156
2022-08-17 23:40:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:40:33 | INFO | train_inner | epoch 1156:     45 / 81 loss=3.372, nll_loss=0.341, ppl=1.27, wps=7327.2, ups=1.32, wpb=5538, bsz=357.7, num_updates=93600, lr=0.000206725, gnorm=0.378, train_wall=49, gb_free=10.1, wall=91729
2022-08-17 23:40:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:40:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:40:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:40:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:41:00 | INFO | valid | epoch 1156 | valid on 'valid' subset | loss 5.233 | nll_loss 2.643 | ppl 6.24 | bleu 56.09 | wps 1887.1 | wpb 933.5 | bsz 59.6 | num_updates 93636 | best_bleu 57.52
2022-08-17 23:41:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1156 @ 93636 updates
2022-08-17 23:41:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1156.pt
2022-08-17 23:41:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1156.pt
2022-08-17 23:41:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1156.pt (epoch 1156 @ 93636 updates, score 56.09) (writing took 37.51121687889099 seconds)
2022-08-17 23:41:38 | INFO | fairseq_cli.train | end of epoch 1156 (average epoch stats below)
2022-08-17 23:41:38 | INFO | train | epoch 1156 | loss 3.371 | nll_loss 0.339 | ppl 1.26 | wps 5114.1 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 93636 | lr 0.000206685 | gnorm 0.315 | train_wall 39 | gb_free 10.1 | wall 91794
2022-08-17 23:41:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:41:38 | INFO | fairseq.trainer | begin training epoch 1157
2022-08-17 23:41:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:42:12 | INFO | train_inner | epoch 1157:     64 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=5619.6, ups=1.02, wpb=5524.8, bsz=363.1, num_updates=93700, lr=0.000206614, gnorm=0.337, train_wall=49, gb_free=10.1, wall=91828
2022-08-17 23:42:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:42:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:42:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:42:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:42:29 | INFO | valid | epoch 1157 | valid on 'valid' subset | loss 5.248 | nll_loss 2.661 | ppl 6.32 | bleu 55.46 | wps 1855 | wpb 933.5 | bsz 59.6 | num_updates 93717 | best_bleu 57.52
2022-08-17 23:42:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1157 @ 93717 updates
2022-08-17 23:42:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1157.pt
2022-08-17 23:42:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1157.pt
2022-08-17 23:42:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1157.pt (epoch 1157 @ 93717 updates, score 55.46) (writing took 21.733720391988754 seconds)
2022-08-17 23:42:51 | INFO | fairseq_cli.train | end of epoch 1157 (average epoch stats below)
2022-08-17 23:42:51 | INFO | train | epoch 1157 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 6108.3 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 93717 | lr 0.000206595 | gnorm 0.363 | train_wall 40 | gb_free 10.2 | wall 91867
2022-08-17 23:42:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:42:51 | INFO | fairseq.trainer | begin training epoch 1158
2022-08-17 23:42:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:43:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:43:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:43:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:43:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:43:42 | INFO | valid | epoch 1158 | valid on 'valid' subset | loss 5.23 | nll_loss 2.639 | ppl 6.23 | bleu 56.2 | wps 1889.1 | wpb 933.5 | bsz 59.6 | num_updates 93798 | best_bleu 57.52
2022-08-17 23:43:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1158 @ 93798 updates
2022-08-17 23:43:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1158.pt
2022-08-17 23:43:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1158.pt
2022-08-17 23:44:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1158.pt (epoch 1158 @ 93798 updates, score 56.2) (writing took 18.734768521040678 seconds)
2022-08-17 23:44:01 | INFO | fairseq_cli.train | end of epoch 1158 (average epoch stats below)
2022-08-17 23:44:01 | INFO | train | epoch 1158 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 6388.3 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 93798 | lr 0.000206506 | gnorm 0.364 | train_wall 40 | gb_free 10.2 | wall 91937
2022-08-17 23:44:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:44:01 | INFO | fairseq.trainer | begin training epoch 1159
2022-08-17 23:44:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:44:03 | INFO | train_inner | epoch 1159:      2 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=4932.2, ups=0.9, wpb=5501.5, bsz=353.8, num_updates=93800, lr=0.000206504, gnorm=0.371, train_wall=49, gb_free=10.1, wall=91939
2022-08-17 23:44:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:44:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:44:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:44:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:44:53 | INFO | valid | epoch 1159 | valid on 'valid' subset | loss 5.234 | nll_loss 2.643 | ppl 6.25 | bleu 55.7 | wps 1709.3 | wpb 933.5 | bsz 59.6 | num_updates 93879 | best_bleu 57.52
2022-08-17 23:44:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1159 @ 93879 updates
2022-08-17 23:44:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1159.pt
2022-08-17 23:44:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1159.pt
2022-08-17 23:45:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1159.pt (epoch 1159 @ 93879 updates, score 55.7) (writing took 30.45388061925769 seconds)
2022-08-17 23:45:24 | INFO | fairseq_cli.train | end of epoch 1159 (average epoch stats below)
2022-08-17 23:45:24 | INFO | train | epoch 1159 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5381.2 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 93879 | lr 0.000206417 | gnorm 0.627 | train_wall 40 | gb_free 10.1 | wall 92020
2022-08-17 23:45:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:45:24 | INFO | fairseq.trainer | begin training epoch 1160
2022-08-17 23:45:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:45:37 | INFO | train_inner | epoch 1160:     21 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=5926, ups=1.07, wpb=5544.7, bsz=358.6, num_updates=93900, lr=0.000206394, gnorm=0.566, train_wall=49, gb_free=10, wall=92033
2022-08-17 23:46:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:46:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:46:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:46:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:46:17 | INFO | valid | epoch 1160 | valid on 'valid' subset | loss 5.234 | nll_loss 2.647 | ppl 6.27 | bleu 56.37 | wps 1845.4 | wpb 933.5 | bsz 59.6 | num_updates 93960 | best_bleu 57.52
2022-08-17 23:46:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1160 @ 93960 updates
2022-08-17 23:46:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1160.pt
2022-08-17 23:46:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1160.pt
2022-08-17 23:46:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1160.pt (epoch 1160 @ 93960 updates, score 56.37) (writing took 36.641104839742184 seconds)
2022-08-17 23:46:54 | INFO | fairseq_cli.train | end of epoch 1160 (average epoch stats below)
2022-08-17 23:46:54 | INFO | train | epoch 1160 | loss 3.371 | nll_loss 0.339 | ppl 1.26 | wps 4957.3 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 93960 | lr 0.000206328 | gnorm 0.343 | train_wall 41 | gb_free 10.2 | wall 92111
2022-08-17 23:46:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:46:55 | INFO | fairseq.trainer | begin training epoch 1161
2022-08-17 23:46:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:47:17 | INFO | train_inner | epoch 1161:     40 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5497.4, ups=1, wpb=5507.8, bsz=358.2, num_updates=94000, lr=0.000206284, gnorm=0.382, train_wall=50, gb_free=10, wall=92133
2022-08-17 23:47:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:47:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:47:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:47:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:47:48 | INFO | valid | epoch 1161 | valid on 'valid' subset | loss 5.243 | nll_loss 2.66 | ppl 6.32 | bleu 56.16 | wps 1852.1 | wpb 933.5 | bsz 59.6 | num_updates 94041 | best_bleu 57.52
2022-08-17 23:47:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1161 @ 94041 updates
2022-08-17 23:47:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1161.pt
2022-08-17 23:47:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1161.pt
2022-08-17 23:47:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1161.pt (epoch 1161 @ 94041 updates, score 56.16) (writing took 2.506004933267832 seconds)
2022-08-17 23:47:51 | INFO | fairseq_cli.train | end of epoch 1161 (average epoch stats below)
2022-08-17 23:47:51 | INFO | train | epoch 1161 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 7913.6 | ups 1.43 | wpb 5523.2 | bsz 358 | num_updates 94041 | lr 0.000206239 | gnorm 0.375 | train_wall 42 | gb_free 10 | wall 92167
2022-08-17 23:47:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:47:51 | INFO | fairseq.trainer | begin training epoch 1162
2022-08-17 23:47:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:48:24 | INFO | train_inner | epoch 1162:     59 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=8277.9, ups=1.49, wpb=5543.6, bsz=360.6, num_updates=94100, lr=0.000206175, gnorm=0.487, train_wall=52, gb_free=10.1, wall=92200
2022-08-17 23:48:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:48:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:48:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:48:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:48:45 | INFO | valid | epoch 1162 | valid on 'valid' subset | loss 5.215 | nll_loss 2.621 | ppl 6.15 | bleu 56.52 | wps 1680.9 | wpb 933.5 | bsz 59.6 | num_updates 94122 | best_bleu 57.52
2022-08-17 23:48:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1162 @ 94122 updates
2022-08-17 23:48:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1162.pt
2022-08-17 23:48:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1162.pt
2022-08-17 23:49:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1162.pt (epoch 1162 @ 94122 updates, score 56.52) (writing took 15.580808725208044 seconds)
2022-08-17 23:49:01 | INFO | fairseq_cli.train | end of epoch 1162 (average epoch stats below)
2022-08-17 23:49:01 | INFO | train | epoch 1162 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 6385.8 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 94122 | lr 0.000206151 | gnorm 0.578 | train_wall 41 | gb_free 10.1 | wall 92237
2022-08-17 23:49:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:49:01 | INFO | fairseq.trainer | begin training epoch 1163
2022-08-17 23:49:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:49:41 | INFO | train_inner | epoch 1163:     78 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=7129, ups=1.29, wpb=5536.7, bsz=357.8, num_updates=94200, lr=0.000206065, gnorm=0.463, train_wall=49, gb_free=10.1, wall=92278
2022-08-17 23:49:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:49:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:49:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:49:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:49:52 | INFO | valid | epoch 1163 | valid on 'valid' subset | loss 5.243 | nll_loss 2.657 | ppl 6.31 | bleu 56.27 | wps 1914.5 | wpb 933.5 | bsz 59.6 | num_updates 94203 | best_bleu 57.52
2022-08-17 23:49:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1163 @ 94203 updates
2022-08-17 23:49:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1163.pt
2022-08-17 23:49:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1163.pt
2022-08-17 23:50:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1163.pt (epoch 1163 @ 94203 updates, score 56.27) (writing took 19.97429595515132 seconds)
2022-08-17 23:50:12 | INFO | fairseq_cli.train | end of epoch 1163 (average epoch stats below)
2022-08-17 23:50:12 | INFO | train | epoch 1163 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6292.9 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 94203 | lr 0.000206062 | gnorm 0.439 | train_wall 40 | gb_free 10.3 | wall 92308
2022-08-17 23:50:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:50:12 | INFO | fairseq.trainer | begin training epoch 1164
2022-08-17 23:50:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:51:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:51:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:51:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:51:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:51:10 | INFO | valid | epoch 1164 | valid on 'valid' subset | loss 5.228 | nll_loss 2.639 | ppl 6.23 | bleu 56.38 | wps 1686.2 | wpb 933.5 | bsz 59.6 | num_updates 94284 | best_bleu 57.52
2022-08-17 23:51:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1164 @ 94284 updates
2022-08-17 23:51:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1164.pt
2022-08-17 23:51:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1164.pt
2022-08-17 23:51:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1164.pt (epoch 1164 @ 94284 updates, score 56.38) (writing took 16.804022181779146 seconds)
2022-08-17 23:51:27 | INFO | fairseq_cli.train | end of epoch 1164 (average epoch stats below)
2022-08-17 23:51:27 | INFO | train | epoch 1164 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5988.3 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 94284 | lr 0.000205973 | gnorm 0.393 | train_wall 41 | gb_free 10.1 | wall 92383
2022-08-17 23:51:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:51:27 | INFO | fairseq.trainer | begin training epoch 1165
2022-08-17 23:51:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:51:36 | INFO | train_inner | epoch 1165:     16 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=4790.1, ups=0.87, wpb=5480.5, bsz=351.9, num_updates=94300, lr=0.000205956, gnorm=0.378, train_wall=50, gb_free=10.1, wall=92392
2022-08-17 23:52:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:52:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:52:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:52:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:52:19 | INFO | valid | epoch 1165 | valid on 'valid' subset | loss 5.226 | nll_loss 2.635 | ppl 6.21 | bleu 56.56 | wps 1812 | wpb 933.5 | bsz 59.6 | num_updates 94365 | best_bleu 57.52
2022-08-17 23:52:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1165 @ 94365 updates
2022-08-17 23:52:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1165.pt
2022-08-17 23:52:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1165.pt
2022-08-17 23:52:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1165.pt (epoch 1165 @ 94365 updates, score 56.56) (writing took 21.774826392531395 seconds)
2022-08-17 23:52:41 | INFO | fairseq_cli.train | end of epoch 1165 (average epoch stats below)
2022-08-17 23:52:41 | INFO | train | epoch 1165 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6000.5 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 94365 | lr 0.000205885 | gnorm 0.324 | train_wall 41 | gb_free 10.1 | wall 92458
2022-08-17 23:52:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:52:41 | INFO | fairseq.trainer | begin training epoch 1166
2022-08-17 23:52:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:53:01 | INFO | train_inner | epoch 1166:     35 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=6496, ups=1.17, wpb=5538.1, bsz=363.4, num_updates=94400, lr=0.000205847, gnorm=0.315, train_wall=51, gb_free=10.1, wall=92477
2022-08-17 23:53:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:53:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:53:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:53:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:53:33 | INFO | valid | epoch 1166 | valid on 'valid' subset | loss 5.234 | nll_loss 2.644 | ppl 6.25 | bleu 56.01 | wps 1911.4 | wpb 933.5 | bsz 59.6 | num_updates 94446 | best_bleu 57.52
2022-08-17 23:53:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1166 @ 94446 updates
2022-08-17 23:53:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1166.pt
2022-08-17 23:53:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1166.pt
2022-08-17 23:53:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1166.pt (epoch 1166 @ 94446 updates, score 56.01) (writing took 19.684506479650736 seconds)
2022-08-17 23:53:53 | INFO | fairseq_cli.train | end of epoch 1166 (average epoch stats below)
2022-08-17 23:53:53 | INFO | train | epoch 1166 | loss 3.371 | nll_loss 0.339 | ppl 1.26 | wps 6223.6 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 94446 | lr 0.000205797 | gnorm 0.322 | train_wall 41 | gb_free 10.1 | wall 92529
2022-08-17 23:53:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:53:53 | INFO | fairseq.trainer | begin training epoch 1167
2022-08-17 23:53:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:54:21 | INFO | train_inner | epoch 1167:     54 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=6923.9, ups=1.25, wpb=5554.7, bsz=359.5, num_updates=94500, lr=0.000205738, gnorm=0.38, train_wall=49, gb_free=10.1, wall=92558
2022-08-17 23:54:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:54:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:54:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:54:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:54:45 | INFO | valid | epoch 1167 | valid on 'valid' subset | loss 5.226 | nll_loss 2.634 | ppl 6.21 | bleu 56 | wps 1767.5 | wpb 933.5 | bsz 59.6 | num_updates 94527 | best_bleu 57.52
2022-08-17 23:54:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1167 @ 94527 updates
2022-08-17 23:54:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1167.pt
2022-08-17 23:54:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1167.pt
2022-08-17 23:55:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1167.pt (epoch 1167 @ 94527 updates, score 56.0) (writing took 15.919347520917654 seconds)
2022-08-17 23:55:01 | INFO | fairseq_cli.train | end of epoch 1167 (average epoch stats below)
2022-08-17 23:55:01 | INFO | train | epoch 1167 | loss 3.372 | nll_loss 0.341 | ppl 1.27 | wps 6601.2 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 94527 | lr 0.000205708 | gnorm 0.393 | train_wall 40 | gb_free 10.3 | wall 92597
2022-08-17 23:55:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:55:01 | INFO | fairseq.trainer | begin training epoch 1168
2022-08-17 23:55:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:55:40 | INFO | train_inner | epoch 1168:     73 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=7013.8, ups=1.27, wpb=5514.8, bsz=355.5, num_updates=94600, lr=0.000205629, gnorm=0.361, train_wall=51, gb_free=10.1, wall=92636
2022-08-17 23:55:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:55:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:55:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:55:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:55:54 | INFO | valid | epoch 1168 | valid on 'valid' subset | loss 5.23 | nll_loss 2.638 | ppl 6.23 | bleu 56.61 | wps 1725.7 | wpb 933.5 | bsz 59.6 | num_updates 94608 | best_bleu 57.52
2022-08-17 23:55:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1168 @ 94608 updates
2022-08-17 23:55:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1168.pt
2022-08-17 23:55:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1168.pt
2022-08-17 23:56:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1168.pt (epoch 1168 @ 94608 updates, score 56.61) (writing took 32.00471907109022 seconds)
2022-08-17 23:56:26 | INFO | fairseq_cli.train | end of epoch 1168 (average epoch stats below)
2022-08-17 23:56:26 | INFO | train | epoch 1168 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 5272.8 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 94608 | lr 0.00020562 | gnorm 0.363 | train_wall 41 | gb_free 10.1 | wall 92682
2022-08-17 23:56:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:56:26 | INFO | fairseq.trainer | begin training epoch 1169
2022-08-17 23:56:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:57:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:57:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:57:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:57:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:57:19 | INFO | valid | epoch 1169 | valid on 'valid' subset | loss 5.243 | nll_loss 2.655 | ppl 6.3 | bleu 56 | wps 1702.5 | wpb 933.5 | bsz 59.6 | num_updates 94689 | best_bleu 57.52
2022-08-17 23:57:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1169 @ 94689 updates
2022-08-17 23:57:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1169.pt
2022-08-17 23:57:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1169.pt
2022-08-17 23:57:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1169.pt (epoch 1169 @ 94689 updates, score 56.0) (writing took 2.5329545475542545 seconds)
2022-08-17 23:57:21 | INFO | fairseq_cli.train | end of epoch 1169 (average epoch stats below)
2022-08-17 23:57:21 | INFO | train | epoch 1169 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 8059.1 | ups 1.46 | wpb 5523.2 | bsz 358 | num_updates 94689 | lr 0.000205532 | gnorm 0.426 | train_wall 40 | gb_free 10.3 | wall 92738
2022-08-17 23:57:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:57:22 | INFO | fairseq.trainer | begin training epoch 1170
2022-08-17 23:57:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:57:28 | INFO | train_inner | epoch 1170:     11 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5074, ups=0.92, wpb=5486.6, bsz=355.9, num_updates=94700, lr=0.00020552, gnorm=0.453, train_wall=49, gb_free=10, wall=92744
2022-08-17 23:58:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:58:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:58:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:58:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:58:13 | INFO | valid | epoch 1170 | valid on 'valid' subset | loss 5.243 | nll_loss 2.654 | ppl 6.29 | bleu 56.4 | wps 1843.9 | wpb 933.5 | bsz 59.6 | num_updates 94770 | best_bleu 57.52
2022-08-17 23:58:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1170 @ 94770 updates
2022-08-17 23:58:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1170.pt
2022-08-17 23:58:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1170.pt
2022-08-17 23:58:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1170.pt (epoch 1170 @ 94770 updates, score 56.4) (writing took 24.663861270993948 seconds)
2022-08-17 23:58:37 | INFO | fairseq_cli.train | end of epoch 1170 (average epoch stats below)
2022-08-17 23:58:37 | INFO | train | epoch 1170 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5878.6 | ups 1.06 | wpb 5523.2 | bsz 358 | num_updates 94770 | lr 0.000205445 | gnorm 0.459 | train_wall 40 | gb_free 10.1 | wall 92814
2022-08-17 23:58:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:58:38 | INFO | fairseq.trainer | begin training epoch 1171
2022-08-17 23:58:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-17 23:58:54 | INFO | train_inner | epoch 1171:     30 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=6458.2, ups=1.16, wpb=5547.2, bsz=361, num_updates=94800, lr=0.000205412, gnorm=0.4, train_wall=49, gb_free=10.1, wall=92830
2022-08-17 23:59:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-17 23:59:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-17 23:59:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-17 23:59:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-17 23:59:29 | INFO | valid | epoch 1171 | valid on 'valid' subset | loss 5.227 | nll_loss 2.635 | ppl 6.21 | bleu 56.13 | wps 1898.5 | wpb 933.5 | bsz 59.6 | num_updates 94851 | best_bleu 57.52
2022-08-17 23:59:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1171 @ 94851 updates
2022-08-17 23:59:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1171.pt
2022-08-17 23:59:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1171.pt
2022-08-17 23:59:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1171.pt (epoch 1171 @ 94851 updates, score 56.13) (writing took 30.13371630012989 seconds)
2022-08-17 23:59:59 | INFO | fairseq_cli.train | end of epoch 1171 (average epoch stats below)
2022-08-17 23:59:59 | INFO | train | epoch 1171 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5478.1 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 94851 | lr 0.000205357 | gnorm 0.35 | train_wall 40 | gb_free 10.1 | wall 92895
2022-08-17 23:59:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-17 23:59:59 | INFO | fairseq.trainer | begin training epoch 1172
2022-08-17 23:59:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:00:28 | INFO | train_inner | epoch 1172:     49 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=5897.8, ups=1.07, wpb=5521.8, bsz=360.1, num_updates=94900, lr=0.000205304, gnorm=0.329, train_wall=50, gb_free=10.1, wall=92924
2022-08-18 00:00:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:00:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:00:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:00:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:00:53 | INFO | valid | epoch 1172 | valid on 'valid' subset | loss 5.231 | nll_loss 2.643 | ppl 6.25 | bleu 56.25 | wps 1893.6 | wpb 933.5 | bsz 59.6 | num_updates 94932 | best_bleu 57.52
2022-08-18 00:00:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1172 @ 94932 updates
2022-08-18 00:00:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1172.pt
2022-08-18 00:00:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1172.pt
2022-08-18 00:01:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1172.pt (epoch 1172 @ 94932 updates, score 56.25) (writing took 16.548275753855705 seconds)
2022-08-18 00:01:10 | INFO | fairseq_cli.train | end of epoch 1172 (average epoch stats below)
2022-08-18 00:01:10 | INFO | train | epoch 1172 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 6330.6 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 94932 | lr 0.000205269 | gnorm 0.331 | train_wall 40 | gb_free 10 | wall 92966
2022-08-18 00:01:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:01:10 | INFO | fairseq.trainer | begin training epoch 1173
2022-08-18 00:01:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:01:45 | INFO | train_inner | epoch 1173:     68 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=7169.6, ups=1.3, wpb=5523.6, bsz=356.5, num_updates=95000, lr=0.000205196, gnorm=0.354, train_wall=49, gb_free=10.1, wall=93001
2022-08-18 00:01:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:01:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:01:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:01:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:02:00 | INFO | valid | epoch 1173 | valid on 'valid' subset | loss 5.237 | nll_loss 2.65 | ppl 6.28 | bleu 56.9 | wps 1833.1 | wpb 933.5 | bsz 59.6 | num_updates 95013 | best_bleu 57.52
2022-08-18 00:02:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1173 @ 95013 updates
2022-08-18 00:02:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1173.pt
2022-08-18 00:02:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1173.pt
2022-08-18 00:02:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1173.pt (epoch 1173 @ 95013 updates, score 56.9) (writing took 34.44045401737094 seconds)
2022-08-18 00:02:35 | INFO | fairseq_cli.train | end of epoch 1173 (average epoch stats below)
2022-08-18 00:02:35 | INFO | train | epoch 1173 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 5258.4 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 95013 | lr 0.000205182 | gnorm 0.377 | train_wall 39 | gb_free 10.2 | wall 93051
2022-08-18 00:02:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:02:35 | INFO | fairseq.trainer | begin training epoch 1174
2022-08-18 00:02:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:03:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:03:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:03:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:03:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:03:31 | INFO | valid | epoch 1174 | valid on 'valid' subset | loss 5.233 | nll_loss 2.642 | ppl 6.24 | bleu 56.32 | wps 1900.7 | wpb 933.5 | bsz 59.6 | num_updates 95094 | best_bleu 57.52
2022-08-18 00:03:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1174 @ 95094 updates
2022-08-18 00:03:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1174.pt
2022-08-18 00:03:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1174.pt
2022-08-18 00:04:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1174.pt (epoch 1174 @ 95094 updates, score 56.32) (writing took 35.87446324899793 seconds)
2022-08-18 00:04:07 | INFO | fairseq_cli.train | end of epoch 1174 (average epoch stats below)
2022-08-18 00:04:07 | INFO | train | epoch 1174 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 4867.5 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 95094 | lr 0.000205094 | gnorm 0.362 | train_wall 41 | gb_free 10.2 | wall 93143
2022-08-18 00:04:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:04:07 | INFO | fairseq.trainer | begin training epoch 1175
2022-08-18 00:04:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:04:11 | INFO | train_inner | epoch 1175:      6 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=3763.8, ups=0.68, wpb=5505.7, bsz=354.6, num_updates=95100, lr=0.000205088, gnorm=0.394, train_wall=50, gb_free=10.1, wall=93147
2022-08-18 00:04:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:04:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:04:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:04:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:04:57 | INFO | valid | epoch 1175 | valid on 'valid' subset | loss 5.238 | nll_loss 2.65 | ppl 6.28 | bleu 56.55 | wps 1839.4 | wpb 933.5 | bsz 59.6 | num_updates 95175 | best_bleu 57.52
2022-08-18 00:04:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1175 @ 95175 updates
2022-08-18 00:04:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1175.pt
2022-08-18 00:04:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1175.pt
2022-08-18 00:05:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1175.pt (epoch 1175 @ 95175 updates, score 56.55) (writing took 16.3631427064538 seconds)
2022-08-18 00:05:14 | INFO | fairseq_cli.train | end of epoch 1175 (average epoch stats below)
2022-08-18 00:05:14 | INFO | train | epoch 1175 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6663.9 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 95175 | lr 0.000205007 | gnorm 0.335 | train_wall 40 | gb_free 10.2 | wall 93210
2022-08-18 00:05:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:05:14 | INFO | fairseq.trainer | begin training epoch 1176
2022-08-18 00:05:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:05:28 | INFO | train_inner | epoch 1176:     25 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=7137.4, ups=1.29, wpb=5520.1, bsz=357.6, num_updates=95200, lr=0.00020498, gnorm=0.331, train_wall=49, gb_free=10.2, wall=93225
2022-08-18 00:05:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:06:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:06:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:06:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:06:07 | INFO | valid | epoch 1176 | valid on 'valid' subset | loss 5.235 | nll_loss 2.646 | ppl 6.26 | bleu 56.42 | wps 1889.5 | wpb 933.5 | bsz 59.6 | num_updates 95256 | best_bleu 57.52
2022-08-18 00:06:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1176 @ 95256 updates
2022-08-18 00:06:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1176.pt
2022-08-18 00:06:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1176.pt
2022-08-18 00:06:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1176.pt (epoch 1176 @ 95256 updates, score 56.42) (writing took 25.98480274155736 seconds)
2022-08-18 00:06:34 | INFO | fairseq_cli.train | end of epoch 1176 (average epoch stats below)
2022-08-18 00:06:34 | INFO | train | epoch 1176 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5612.4 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 95256 | lr 0.00020492 | gnorm 0.391 | train_wall 40 | gb_free 10 | wall 93290
2022-08-18 00:06:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:06:34 | INFO | fairseq.trainer | begin training epoch 1177
2022-08-18 00:06:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:06:57 | INFO | train_inner | epoch 1177:     44 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=6237.4, ups=1.13, wpb=5529.2, bsz=364.2, num_updates=95300, lr=0.000204872, gnorm=0.404, train_wall=50, gb_free=10, wall=93313
2022-08-18 00:07:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:07:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:07:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:07:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:07:25 | INFO | valid | epoch 1177 | valid on 'valid' subset | loss 5.238 | nll_loss 2.651 | ppl 6.28 | bleu 56.36 | wps 1792.8 | wpb 933.5 | bsz 59.6 | num_updates 95337 | best_bleu 57.52
2022-08-18 00:07:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1177 @ 95337 updates
2022-08-18 00:07:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1177.pt
2022-08-18 00:07:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1177.pt
2022-08-18 00:07:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1177.pt (epoch 1177 @ 95337 updates, score 56.36) (writing took 27.802050847560167 seconds)
2022-08-18 00:07:53 | INFO | fairseq_cli.train | end of epoch 1177 (average epoch stats below)
2022-08-18 00:07:53 | INFO | train | epoch 1177 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5624.5 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 95337 | lr 0.000204833 | gnorm 0.411 | train_wall 40 | gb_free 10.1 | wall 93369
2022-08-18 00:07:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:07:53 | INFO | fairseq.trainer | begin training epoch 1178
2022-08-18 00:07:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:08:28 | INFO | train_inner | epoch 1178:     63 / 81 loss=3.372, nll_loss=0.341, ppl=1.27, wps=6072.3, ups=1.09, wpb=5550.7, bsz=358.6, num_updates=95400, lr=0.000204765, gnorm=0.358, train_wall=51, gb_free=10.1, wall=93405
2022-08-18 00:08:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:08:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:08:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:08:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:08:46 | INFO | valid | epoch 1178 | valid on 'valid' subset | loss 5.226 | nll_loss 2.638 | ppl 6.23 | bleu 57 | wps 1892.4 | wpb 933.5 | bsz 59.6 | num_updates 95418 | best_bleu 57.52
2022-08-18 00:08:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1178 @ 95418 updates
2022-08-18 00:08:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1178.pt
2022-08-18 00:08:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1178.pt
2022-08-18 00:09:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1178.pt (epoch 1178 @ 95418 updates, score 57.0) (writing took 17.66633950918913 seconds)
2022-08-18 00:09:04 | INFO | fairseq_cli.train | end of epoch 1178 (average epoch stats below)
2022-08-18 00:09:04 | INFO | train | epoch 1178 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6308.6 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 95418 | lr 0.000204746 | gnorm 0.338 | train_wall 41 | gb_free 10.1 | wall 93440
2022-08-18 00:09:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:09:04 | INFO | fairseq.trainer | begin training epoch 1179
2022-08-18 00:09:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:09:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:09:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:09:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:09:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:09:57 | INFO | valid | epoch 1179 | valid on 'valid' subset | loss 5.23 | nll_loss 2.64 | ppl 6.23 | bleu 56.19 | wps 1931.4 | wpb 933.5 | bsz 59.6 | num_updates 95499 | best_bleu 57.52
2022-08-18 00:09:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1179 @ 95499 updates
2022-08-18 00:09:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1179.pt
2022-08-18 00:09:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1179.pt
2022-08-18 00:10:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1179.pt (epoch 1179 @ 95499 updates, score 56.19) (writing took 35.934863679111004 seconds)
2022-08-18 00:10:33 | INFO | fairseq_cli.train | end of epoch 1179 (average epoch stats below)
2022-08-18 00:10:33 | INFO | train | epoch 1179 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 5053.5 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 95499 | lr 0.000204659 | gnorm 0.364 | train_wall 40 | gb_free 10.1 | wall 93529
2022-08-18 00:10:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:10:33 | INFO | fairseq.trainer | begin training epoch 1180
2022-08-18 00:10:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:10:34 | INFO | train_inner | epoch 1180:      1 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=4362.5, ups=0.79, wpb=5493.9, bsz=354.6, num_updates=95500, lr=0.000204658, gnorm=0.368, train_wall=48, gb_free=10, wall=93531
2022-08-18 00:11:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:11:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:11:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:11:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:11:26 | INFO | valid | epoch 1180 | valid on 'valid' subset | loss 5.253 | nll_loss 2.669 | ppl 6.36 | bleu 56.22 | wps 1901.2 | wpb 933.5 | bsz 59.6 | num_updates 95580 | best_bleu 57.52
2022-08-18 00:11:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1180 @ 95580 updates
2022-08-18 00:11:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1180.pt
2022-08-18 00:11:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1180.pt
2022-08-18 00:11:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1180.pt (epoch 1180 @ 95580 updates, score 56.22) (writing took 23.421908356249332 seconds)
2022-08-18 00:11:50 | INFO | fairseq_cli.train | end of epoch 1180 (average epoch stats below)
2022-08-18 00:11:50 | INFO | train | epoch 1180 | loss 3.371 | nll_loss 0.339 | ppl 1.26 | wps 5806.1 | ups 1.05 | wpb 5523.2 | bsz 358 | num_updates 95580 | lr 0.000204572 | gnorm 0.352 | train_wall 39 | gb_free 10.1 | wall 93606
2022-08-18 00:11:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:11:50 | INFO | fairseq.trainer | begin training epoch 1181
2022-08-18 00:11:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:12:02 | INFO | train_inner | epoch 1181:     20 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=6283.2, ups=1.14, wpb=5534.3, bsz=358, num_updates=95600, lr=0.000204551, gnorm=0.363, train_wall=49, gb_free=10, wall=93619
2022-08-18 00:12:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:12:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:12:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:12:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:12:44 | INFO | valid | epoch 1181 | valid on 'valid' subset | loss 5.235 | nll_loss 2.646 | ppl 6.26 | bleu 56.7 | wps 1965.4 | wpb 933.5 | bsz 59.6 | num_updates 95661 | best_bleu 57.52
2022-08-18 00:12:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1181 @ 95661 updates
2022-08-18 00:12:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1181.pt
2022-08-18 00:12:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1181.pt
2022-08-18 00:13:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1181.pt (epoch 1181 @ 95661 updates, score 56.7) (writing took 17.695499140769243 seconds)
2022-08-18 00:13:02 | INFO | fairseq_cli.train | end of epoch 1181 (average epoch stats below)
2022-08-18 00:13:02 | INFO | train | epoch 1181 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 6228.6 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 95661 | lr 0.000204486 | gnorm 0.378 | train_wall 41 | gb_free 10.1 | wall 93678
2022-08-18 00:13:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:13:02 | INFO | fairseq.trainer | begin training epoch 1182
2022-08-18 00:13:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:13:23 | INFO | train_inner | epoch 1182:     39 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=6889.7, ups=1.24, wpb=5540, bsz=360.5, num_updates=95700, lr=0.000204444, gnorm=0.382, train_wall=51, gb_free=10.1, wall=93699
2022-08-18 00:13:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:13:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:13:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:13:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:13:53 | INFO | valid | epoch 1182 | valid on 'valid' subset | loss 5.253 | nll_loss 2.666 | ppl 6.35 | bleu 56.13 | wps 1901.2 | wpb 933.5 | bsz 59.6 | num_updates 95742 | best_bleu 57.52
2022-08-18 00:13:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1182 @ 95742 updates
2022-08-18 00:13:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1182.pt
2022-08-18 00:13:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1182.pt
2022-08-18 00:14:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1182.pt (epoch 1182 @ 95742 updates, score 56.13) (writing took 26.000772137194872 seconds)
2022-08-18 00:14:19 | INFO | fairseq_cli.train | end of epoch 1182 (average epoch stats below)
2022-08-18 00:14:19 | INFO | train | epoch 1182 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5780.5 | ups 1.05 | wpb 5523.2 | bsz 358 | num_updates 95742 | lr 0.000204399 | gnorm 0.382 | train_wall 40 | gb_free 10.2 | wall 93755
2022-08-18 00:14:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:14:19 | INFO | fairseq.trainer | begin training epoch 1183
2022-08-18 00:14:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:14:50 | INFO | train_inner | epoch 1183:     58 / 81 loss=3.372, nll_loss=0.341, ppl=1.27, wps=6314.7, ups=1.14, wpb=5530.1, bsz=357.4, num_updates=95800, lr=0.000204337, gnorm=0.378, train_wall=50, gb_free=10.1, wall=93787
2022-08-18 00:15:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:15:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:15:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:15:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:15:11 | INFO | valid | epoch 1183 | valid on 'valid' subset | loss 5.236 | nll_loss 2.647 | ppl 6.26 | bleu 56.41 | wps 1829.6 | wpb 933.5 | bsz 59.6 | num_updates 95823 | best_bleu 57.52
2022-08-18 00:15:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1183 @ 95823 updates
2022-08-18 00:15:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1183.pt
2022-08-18 00:15:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1183.pt
2022-08-18 00:15:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1183.pt (epoch 1183 @ 95823 updates, score 56.41) (writing took 45.685969695448875 seconds)
2022-08-18 00:15:57 | INFO | fairseq_cli.train | end of epoch 1183 (average epoch stats below)
2022-08-18 00:15:57 | INFO | train | epoch 1183 | loss 3.372 | nll_loss 0.341 | ppl 1.27 | wps 4557.9 | ups 0.83 | wpb 5523.2 | bsz 358 | num_updates 95823 | lr 0.000204313 | gnorm 0.432 | train_wall 40 | gb_free 10.1 | wall 93853
2022-08-18 00:15:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:15:57 | INFO | fairseq.trainer | begin training epoch 1184
2022-08-18 00:15:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:16:38 | INFO | train_inner | epoch 1184:     77 / 81 loss=3.372, nll_loss=0.341, ppl=1.27, wps=5143.8, ups=0.93, wpb=5517.2, bsz=355.8, num_updates=95900, lr=0.000204231, gnorm=0.447, train_wall=50, gb_free=10.1, wall=93894
2022-08-18 00:16:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:16:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:16:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:16:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:16:48 | INFO | valid | epoch 1184 | valid on 'valid' subset | loss 5.251 | nll_loss 2.667 | ppl 6.35 | bleu 56.09 | wps 1906.9 | wpb 933.5 | bsz 59.6 | num_updates 95904 | best_bleu 57.52
2022-08-18 00:16:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1184 @ 95904 updates
2022-08-18 00:16:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1184.pt
2022-08-18 00:16:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1184.pt
2022-08-18 00:17:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1184.pt (epoch 1184 @ 95904 updates, score 56.09) (writing took 21.09530685469508 seconds)
2022-08-18 00:17:10 | INFO | fairseq_cli.train | end of epoch 1184 (average epoch stats below)
2022-08-18 00:17:10 | INFO | train | epoch 1184 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6157.3 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 95904 | lr 0.000204226 | gnorm 0.42 | train_wall 41 | gb_free 10.1 | wall 93926
2022-08-18 00:17:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:17:10 | INFO | fairseq.trainer | begin training epoch 1185
2022-08-18 00:17:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:17:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:17:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:17:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:17:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:18:03 | INFO | valid | epoch 1185 | valid on 'valid' subset | loss 5.23 | nll_loss 2.641 | ppl 6.24 | bleu 56.03 | wps 1903.5 | wpb 933.5 | bsz 59.6 | num_updates 95985 | best_bleu 57.52
2022-08-18 00:18:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1185 @ 95985 updates
2022-08-18 00:18:03 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1185.pt
2022-08-18 00:18:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1185.pt
2022-08-18 00:18:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1185.pt (epoch 1185 @ 95985 updates, score 56.03) (writing took 20.511368677020073 seconds)
2022-08-18 00:18:24 | INFO | fairseq_cli.train | end of epoch 1185 (average epoch stats below)
2022-08-18 00:18:24 | INFO | train | epoch 1185 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6064.2 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 95985 | lr 0.00020414 | gnorm 0.428 | train_wall 40 | gb_free 10.1 | wall 94000
2022-08-18 00:18:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:18:24 | INFO | fairseq.trainer | begin training epoch 1186
2022-08-18 00:18:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:18:32 | INFO | train_inner | epoch 1186:     15 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=4793, ups=0.87, wpb=5489.9, bsz=359.3, num_updates=96000, lr=0.000204124, gnorm=0.429, train_wall=49, gb_free=10.1, wall=94008
2022-08-18 00:19:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:19:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:19:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:19:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:19:14 | INFO | valid | epoch 1186 | valid on 'valid' subset | loss 5.22 | nll_loss 2.629 | ppl 6.18 | bleu 57.13 | wps 1940.1 | wpb 933.5 | bsz 59.6 | num_updates 96066 | best_bleu 57.52
2022-08-18 00:19:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1186 @ 96066 updates
2022-08-18 00:19:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1186.pt
2022-08-18 00:19:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1186.pt
2022-08-18 00:19:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1186.pt (epoch 1186 @ 96066 updates, score 57.13) (writing took 32.06525228172541 seconds)
2022-08-18 00:19:47 | INFO | fairseq_cli.train | end of epoch 1186 (average epoch stats below)
2022-08-18 00:19:47 | INFO | train | epoch 1186 | loss 3.371 | nll_loss 0.339 | ppl 1.26 | wps 5380.3 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 96066 | lr 0.000204054 | gnorm 0.326 | train_wall 40 | gb_free 10.2 | wall 94083
2022-08-18 00:19:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:19:47 | INFO | fairseq.trainer | begin training epoch 1187
2022-08-18 00:19:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:20:07 | INFO | train_inner | epoch 1187:     34 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=5858.8, ups=1.06, wpb=5547.7, bsz=360.6, num_updates=96100, lr=0.000204018, gnorm=0.327, train_wall=50, gb_free=10.1, wall=94103
2022-08-18 00:20:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:20:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:20:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:20:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:20:40 | INFO | valid | epoch 1187 | valid on 'valid' subset | loss 5.252 | nll_loss 2.666 | ppl 6.35 | bleu 56.45 | wps 1827.3 | wpb 933.5 | bsz 59.6 | num_updates 96147 | best_bleu 57.52
2022-08-18 00:20:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1187 @ 96147 updates
2022-08-18 00:20:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1187.pt
2022-08-18 00:20:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1187.pt
2022-08-18 00:20:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1187.pt (epoch 1187 @ 96147 updates, score 56.45) (writing took 17.062461469322443 seconds)
2022-08-18 00:20:57 | INFO | fairseq_cli.train | end of epoch 1187 (average epoch stats below)
2022-08-18 00:20:57 | INFO | train | epoch 1187 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 6350.6 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 96147 | lr 0.000203968 | gnorm 0.37 | train_wall 40 | gb_free 10.2 | wall 94153
2022-08-18 00:20:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:20:57 | INFO | fairseq.trainer | begin training epoch 1188
2022-08-18 00:20:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:21:28 | INFO | train_inner | epoch 1188:     53 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=6843.5, ups=1.24, wpb=5513.6, bsz=358.2, num_updates=96200, lr=0.000203912, gnorm=0.337, train_wall=50, gb_free=10.1, wall=94184
2022-08-18 00:21:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:21:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:21:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:21:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:21:51 | INFO | valid | epoch 1188 | valid on 'valid' subset | loss 5.257 | nll_loss 2.67 | ppl 6.37 | bleu 56.18 | wps 1891.5 | wpb 933.5 | bsz 59.6 | num_updates 96228 | best_bleu 57.52
2022-08-18 00:21:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1188 @ 96228 updates
2022-08-18 00:21:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1188.pt
2022-08-18 00:21:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1188.pt
2022-08-18 00:22:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1188.pt (epoch 1188 @ 96228 updates, score 56.18) (writing took 14.274201024323702 seconds)
2022-08-18 00:22:05 | INFO | fairseq_cli.train | end of epoch 1188 (average epoch stats below)
2022-08-18 00:22:05 | INFO | train | epoch 1188 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6562.9 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 96228 | lr 0.000203882 | gnorm 0.356 | train_wall 41 | gb_free 10.1 | wall 94222
2022-08-18 00:22:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:22:05 | INFO | fairseq.trainer | begin training epoch 1189
2022-08-18 00:22:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:22:45 | INFO | train_inner | epoch 1189:     72 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=7101.6, ups=1.29, wpb=5522.1, bsz=354.5, num_updates=96300, lr=0.000203806, gnorm=0.411, train_wall=50, gb_free=10, wall=94262
2022-08-18 00:22:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:22:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:22:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:22:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:22:59 | INFO | valid | epoch 1189 | valid on 'valid' subset | loss 5.237 | nll_loss 2.65 | ppl 6.28 | bleu 56.53 | wps 1928.2 | wpb 933.5 | bsz 59.6 | num_updates 96309 | best_bleu 57.52
2022-08-18 00:22:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1189 @ 96309 updates
2022-08-18 00:22:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1189.pt
2022-08-18 00:23:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1189.pt
2022-08-18 00:23:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1189.pt (epoch 1189 @ 96309 updates, score 56.53) (writing took 38.70870456844568 seconds)
2022-08-18 00:23:38 | INFO | fairseq_cli.train | end of epoch 1189 (average epoch stats below)
2022-08-18 00:23:38 | INFO | train | epoch 1189 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 4845.7 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 96309 | lr 0.000203796 | gnorm 0.398 | train_wall 41 | gb_free 10.1 | wall 94314
2022-08-18 00:23:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:23:38 | INFO | fairseq.trainer | begin training epoch 1190
2022-08-18 00:23:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:24:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:24:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:24:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:24:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:24:29 | INFO | valid | epoch 1190 | valid on 'valid' subset | loss 5.229 | nll_loss 2.641 | ppl 6.24 | bleu 57.17 | wps 1838.5 | wpb 933.5 | bsz 59.6 | num_updates 96390 | best_bleu 57.52
2022-08-18 00:24:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1190 @ 96390 updates
2022-08-18 00:24:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1190.pt
2022-08-18 00:24:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1190.pt
2022-08-18 00:24:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1190.pt (epoch 1190 @ 96390 updates, score 57.17) (writing took 20.091356210410595 seconds)
2022-08-18 00:24:50 | INFO | fairseq_cli.train | end of epoch 1190 (average epoch stats below)
2022-08-18 00:24:50 | INFO | train | epoch 1190 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 6217.4 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 96390 | lr 0.000203711 | gnorm 0.35 | train_wall 40 | gb_free 10.1 | wall 94386
2022-08-18 00:24:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:24:50 | INFO | fairseq.trainer | begin training epoch 1191
2022-08-18 00:24:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:24:56 | INFO | train_inner | epoch 1191:     10 / 81 loss=3.371, nll_loss=0.339, ppl=1.26, wps=4223.7, ups=0.76, wpb=5523.3, bsz=359, num_updates=96400, lr=0.0002037, gnorm=0.34, train_wall=49, gb_free=10, wall=94392
2022-08-18 00:25:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:25:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:25:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:25:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:25:41 | INFO | valid | epoch 1191 | valid on 'valid' subset | loss 5.235 | nll_loss 2.644 | ppl 6.25 | bleu 56.28 | wps 1894.3 | wpb 933.5 | bsz 59.6 | num_updates 96471 | best_bleu 57.52
2022-08-18 00:25:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1191 @ 96471 updates
2022-08-18 00:25:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1191.pt
2022-08-18 00:25:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1191.pt
2022-08-18 00:26:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1191.pt (epoch 1191 @ 96471 updates, score 56.28) (writing took 19.84175892546773 seconds)
2022-08-18 00:26:01 | INFO | fairseq_cli.train | end of epoch 1191 (average epoch stats below)
2022-08-18 00:26:01 | INFO | train | epoch 1191 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6235 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 96471 | lr 0.000203625 | gnorm 0.392 | train_wall 41 | gb_free 10.1 | wall 94458
2022-08-18 00:26:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:26:02 | INFO | fairseq.trainer | begin training epoch 1192
2022-08-18 00:26:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:26:17 | INFO | train_inner | epoch 1192:     29 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=6829.4, ups=1.23, wpb=5542.7, bsz=359.4, num_updates=96500, lr=0.000203595, gnorm=0.385, train_wall=50, gb_free=10, wall=94473
2022-08-18 00:26:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:26:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:26:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:26:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:26:52 | INFO | valid | epoch 1192 | valid on 'valid' subset | loss 5.239 | nll_loss 2.65 | ppl 6.28 | bleu 56.35 | wps 1864 | wpb 933.5 | bsz 59.6 | num_updates 96552 | best_bleu 57.52
2022-08-18 00:26:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1192 @ 96552 updates
2022-08-18 00:26:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1192.pt
2022-08-18 00:26:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1192.pt
2022-08-18 00:27:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1192.pt (epoch 1192 @ 96552 updates, score 56.35) (writing took 22.372186310589314 seconds)
2022-08-18 00:27:15 | INFO | fairseq_cli.train | end of epoch 1192 (average epoch stats below)
2022-08-18 00:27:15 | INFO | train | epoch 1192 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6076.5 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 96552 | lr 0.00020354 | gnorm 0.429 | train_wall 40 | gb_free 10.2 | wall 94531
2022-08-18 00:27:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:27:15 | INFO | fairseq.trainer | begin training epoch 1193
2022-08-18 00:27:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:27:41 | INFO | train_inner | epoch 1193:     48 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=6569.5, ups=1.2, wpb=5492.1, bsz=354.2, num_updates=96600, lr=0.000203489, gnorm=0.437, train_wall=49, gb_free=10, wall=94557
2022-08-18 00:27:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:27:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:27:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:27:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:28:08 | INFO | valid | epoch 1193 | valid on 'valid' subset | loss 5.237 | nll_loss 2.65 | ppl 6.28 | bleu 56.24 | wps 1763 | wpb 933.5 | bsz 59.6 | num_updates 96633 | best_bleu 57.52
2022-08-18 00:28:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1193 @ 96633 updates
2022-08-18 00:28:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1193.pt
2022-08-18 00:28:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1193.pt
2022-08-18 00:28:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1193.pt (epoch 1193 @ 96633 updates, score 56.24) (writing took 2.386592824012041 seconds)
2022-08-18 00:28:10 | INFO | fairseq_cli.train | end of epoch 1193 (average epoch stats below)
2022-08-18 00:28:10 | INFO | train | epoch 1193 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 8092.2 | ups 1.47 | wpb 5523.2 | bsz 358 | num_updates 96633 | lr 0.000203454 | gnorm 0.337 | train_wall 41 | gb_free 10.1 | wall 94586
2022-08-18 00:28:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:28:10 | INFO | fairseq.trainer | begin training epoch 1194
2022-08-18 00:28:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:28:46 | INFO | train_inner | epoch 1194:     67 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=8451.7, ups=1.52, wpb=5542.5, bsz=357, num_updates=96700, lr=0.000203384, gnorm=0.384, train_wall=50, gb_free=10.1, wall=94623
2022-08-18 00:28:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:28:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:28:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:28:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:29:03 | INFO | valid | epoch 1194 | valid on 'valid' subset | loss 5.235 | nll_loss 2.646 | ppl 6.26 | bleu 56.69 | wps 1764.1 | wpb 933.5 | bsz 59.6 | num_updates 96714 | best_bleu 57.52
2022-08-18 00:29:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1194 @ 96714 updates
2022-08-18 00:29:03 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1194.pt
2022-08-18 00:29:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1194.pt
2022-08-18 00:29:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1194.pt (epoch 1194 @ 96714 updates, score 56.69) (writing took 27.599298663437366 seconds)
2022-08-18 00:29:31 | INFO | fairseq_cli.train | end of epoch 1194 (average epoch stats below)
2022-08-18 00:29:31 | INFO | train | epoch 1194 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 5562.6 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 96714 | lr 0.000203369 | gnorm 0.408 | train_wall 40 | gb_free 10.1 | wall 94667
2022-08-18 00:29:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:29:31 | INFO | fairseq.trainer | begin training epoch 1195
2022-08-18 00:29:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:30:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:30:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:30:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:30:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:30:21 | INFO | valid | epoch 1195 | valid on 'valid' subset | loss 5.241 | nll_loss 2.653 | ppl 6.29 | bleu 56.58 | wps 1731.6 | wpb 933.5 | bsz 59.6 | num_updates 96795 | best_bleu 57.52
2022-08-18 00:30:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1195 @ 96795 updates
2022-08-18 00:30:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1195.pt
2022-08-18 00:30:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1195.pt
2022-08-18 00:30:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1195.pt (epoch 1195 @ 96795 updates, score 56.58) (writing took 32.664075929671526 seconds)
2022-08-18 00:30:54 | INFO | fairseq_cli.train | end of epoch 1195 (average epoch stats below)
2022-08-18 00:30:54 | INFO | train | epoch 1195 | loss 3.371 | nll_loss 0.339 | ppl 1.26 | wps 5359.3 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 96795 | lr 0.000203284 | gnorm 0.334 | train_wall 39 | gb_free 10 | wall 94750
2022-08-18 00:30:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:30:54 | INFO | fairseq.trainer | begin training epoch 1196
2022-08-18 00:30:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:30:58 | INFO | train_inner | epoch 1196:      5 / 81 loss=3.371, nll_loss=0.339, ppl=1.26, wps=4186, ups=0.76, wpb=5512.3, bsz=360.8, num_updates=96800, lr=0.000203279, gnorm=0.328, train_wall=48, gb_free=10.1, wall=94754
2022-08-18 00:31:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:31:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:31:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:31:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:31:47 | INFO | valid | epoch 1196 | valid on 'valid' subset | loss 5.253 | nll_loss 2.665 | ppl 6.34 | bleu 55.94 | wps 1827.1 | wpb 933.5 | bsz 59.6 | num_updates 96876 | best_bleu 57.52
2022-08-18 00:31:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1196 @ 96876 updates
2022-08-18 00:31:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1196.pt
2022-08-18 00:31:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1196.pt
2022-08-18 00:32:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1196.pt (epoch 1196 @ 96876 updates, score 55.94) (writing took 32.572299003601074 seconds)
2022-08-18 00:32:20 | INFO | fairseq_cli.train | end of epoch 1196 (average epoch stats below)
2022-08-18 00:32:20 | INFO | train | epoch 1196 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 5203.1 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 96876 | lr 0.000203199 | gnorm 0.319 | train_wall 41 | gb_free 10.1 | wall 94836
2022-08-18 00:32:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:32:20 | INFO | fairseq.trainer | begin training epoch 1197
2022-08-18 00:32:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:32:34 | INFO | train_inner | epoch 1197:     24 / 81 loss=3.371, nll_loss=0.339, ppl=1.26, wps=5729.2, ups=1.04, wpb=5515, bsz=361.4, num_updates=96900, lr=0.000203174, gnorm=0.335, train_wall=50, gb_free=10.1, wall=94851
2022-08-18 00:33:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:33:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:33:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:33:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:33:14 | INFO | valid | epoch 1197 | valid on 'valid' subset | loss 5.235 | nll_loss 2.647 | ppl 6.26 | bleu 56.32 | wps 1803.2 | wpb 933.5 | bsz 59.6 | num_updates 96957 | best_bleu 57.52
2022-08-18 00:33:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1197 @ 96957 updates
2022-08-18 00:33:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1197.pt
2022-08-18 00:33:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1197.pt
2022-08-18 00:33:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1197.pt (epoch 1197 @ 96957 updates, score 56.32) (writing took 14.281048659235239 seconds)
2022-08-18 00:33:29 | INFO | fairseq_cli.train | end of epoch 1197 (average epoch stats below)
2022-08-18 00:33:29 | INFO | train | epoch 1197 | loss 3.371 | nll_loss 0.339 | ppl 1.26 | wps 6528.2 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 96957 | lr 0.000203114 | gnorm 0.381 | train_wall 40 | gb_free 10.1 | wall 94905
2022-08-18 00:33:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:33:29 | INFO | fairseq.trainer | begin training epoch 1198
2022-08-18 00:33:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:33:54 | INFO | train_inner | epoch 1198:     43 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=6966.5, ups=1.26, wpb=5526.6, bsz=357.2, num_updates=97000, lr=0.000203069, gnorm=0.354, train_wall=50, gb_free=10.1, wall=94930
2022-08-18 00:34:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:34:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:34:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:34:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:34:23 | INFO | valid | epoch 1198 | valid on 'valid' subset | loss 5.244 | nll_loss 2.655 | ppl 6.3 | bleu 56.13 | wps 1798.1 | wpb 933.5 | bsz 59.6 | num_updates 97038 | best_bleu 57.52
2022-08-18 00:34:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1198 @ 97038 updates
2022-08-18 00:34:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1198.pt
2022-08-18 00:34:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1198.pt
2022-08-18 00:34:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1198.pt (epoch 1198 @ 97038 updates, score 56.13) (writing took 23.036158863455057 seconds)
2022-08-18 00:34:46 | INFO | fairseq_cli.train | end of epoch 1198 (average epoch stats below)
2022-08-18 00:34:46 | INFO | train | epoch 1198 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 5804.5 | ups 1.05 | wpb 5523.2 | bsz 358 | num_updates 97038 | lr 0.000203029 | gnorm 0.355 | train_wall 40 | gb_free 10.1 | wall 94982
2022-08-18 00:34:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:34:46 | INFO | fairseq.trainer | begin training epoch 1199
2022-08-18 00:34:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:35:18 | INFO | train_inner | epoch 1199:     62 / 81 loss=3.371, nll_loss=0.339, ppl=1.26, wps=6637.5, ups=1.19, wpb=5582.5, bsz=359.4, num_updates=97100, lr=0.000202965, gnorm=0.37, train_wall=48, gb_free=10, wall=95014
2022-08-18 00:35:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:35:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:35:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:35:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:35:37 | INFO | valid | epoch 1199 | valid on 'valid' subset | loss 5.233 | nll_loss 2.642 | ppl 6.24 | bleu 56.36 | wps 1965.6 | wpb 933.5 | bsz 59.6 | num_updates 97119 | best_bleu 57.52
2022-08-18 00:35:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1199 @ 97119 updates
2022-08-18 00:35:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1199.pt
2022-08-18 00:35:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1199.pt
2022-08-18 00:35:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1199.pt (epoch 1199 @ 97119 updates, score 56.36) (writing took 16.355451855808496 seconds)
2022-08-18 00:35:53 | INFO | fairseq_cli.train | end of epoch 1199 (average epoch stats below)
2022-08-18 00:35:53 | INFO | train | epoch 1199 | loss 3.371 | nll_loss 0.339 | ppl 1.26 | wps 6643.5 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 97119 | lr 0.000202945 | gnorm 0.389 | train_wall 40 | gb_free 10.1 | wall 95049
2022-08-18 00:35:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:35:53 | INFO | fairseq.trainer | begin training epoch 1200
2022-08-18 00:35:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:36:36 | INFO | train_inner | epoch 1200:     81 / 81 loss=3.372, nll_loss=0.341, ppl=1.27, wps=6946, ups=1.27, wpb=5460.1, bsz=352.3, num_updates=97200, lr=0.00020286, gnorm=0.438, train_wall=51, gb_free=10.1, wall=95093
2022-08-18 00:36:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:36:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:36:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:36:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:36:46 | INFO | valid | epoch 1200 | valid on 'valid' subset | loss 5.24 | nll_loss 2.649 | ppl 6.27 | bleu 56.59 | wps 1873.6 | wpb 933.5 | bsz 59.6 | num_updates 97200 | best_bleu 57.52
2022-08-18 00:36:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1200 @ 97200 updates
2022-08-18 00:36:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1200.pt
2022-08-18 00:36:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1200.pt
2022-08-18 00:37:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1200.pt (epoch 1200 @ 97200 updates, score 56.59) (writing took 14.011446792632341 seconds)
2022-08-18 00:37:00 | INFO | fairseq_cli.train | end of epoch 1200 (average epoch stats below)
2022-08-18 00:37:00 | INFO | train | epoch 1200 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6702.1 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 97200 | lr 0.00020286 | gnorm 0.424 | train_wall 41 | gb_free 10.1 | wall 95116
2022-08-18 00:37:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:37:00 | INFO | fairseq.trainer | begin training epoch 1201
2022-08-18 00:37:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:37:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:37:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:37:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:37:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:37:53 | INFO | valid | epoch 1201 | valid on 'valid' subset | loss 5.237 | nll_loss 2.644 | ppl 6.25 | bleu 56.53 | wps 1878.7 | wpb 933.5 | bsz 59.6 | num_updates 97281 | best_bleu 57.52
2022-08-18 00:37:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1201 @ 97281 updates
2022-08-18 00:37:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1201.pt
2022-08-18 00:37:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1201.pt
2022-08-18 00:38:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1201.pt (epoch 1201 @ 97281 updates, score 56.53) (writing took 35.03012302517891 seconds)
2022-08-18 00:38:29 | INFO | fairseq_cli.train | end of epoch 1201 (average epoch stats below)
2022-08-18 00:38:29 | INFO | train | epoch 1201 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 5044 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 97281 | lr 0.000202776 | gnorm 0.393 | train_wall 41 | gb_free 10.1 | wall 95205
2022-08-18 00:38:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:38:29 | INFO | fairseq.trainer | begin training epoch 1202
2022-08-18 00:38:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:38:39 | INFO | train_inner | epoch 1202:     19 / 81 loss=3.371, nll_loss=0.339, ppl=1.26, wps=4505.7, ups=0.82, wpb=5525.8, bsz=360.6, num_updates=97300, lr=0.000202756, gnorm=0.364, train_wall=50, gb_free=10.1, wall=95215
2022-08-18 00:39:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:39:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:39:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:39:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:39:18 | INFO | valid | epoch 1202 | valid on 'valid' subset | loss 5.243 | nll_loss 2.655 | ppl 6.3 | bleu 56.12 | wps 1981.2 | wpb 933.5 | bsz 59.6 | num_updates 97362 | best_bleu 57.52
2022-08-18 00:39:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1202 @ 97362 updates
2022-08-18 00:39:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1202.pt
2022-08-18 00:39:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1202.pt
2022-08-18 00:39:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1202.pt (epoch 1202 @ 97362 updates, score 56.12) (writing took 17.18315366655588 seconds)
2022-08-18 00:39:35 | INFO | fairseq_cli.train | end of epoch 1202 (average epoch stats below)
2022-08-18 00:39:35 | INFO | train | epoch 1202 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6705.4 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 97362 | lr 0.000202691 | gnorm 0.326 | train_wall 39 | gb_free 10.1 | wall 95272
2022-08-18 00:39:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:39:36 | INFO | fairseq.trainer | begin training epoch 1203
2022-08-18 00:39:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:39:57 | INFO | train_inner | epoch 1203:     38 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=7161.3, ups=1.29, wpb=5566.7, bsz=355.5, num_updates=97400, lr=0.000202652, gnorm=0.326, train_wall=49, gb_free=10.1, wall=95293
2022-08-18 00:40:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:40:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:40:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:40:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:40:29 | INFO | valid | epoch 1203 | valid on 'valid' subset | loss 5.239 | nll_loss 2.65 | ppl 6.28 | bleu 56.27 | wps 1930.1 | wpb 933.5 | bsz 59.6 | num_updates 97443 | best_bleu 57.52
2022-08-18 00:40:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1203 @ 97443 updates
2022-08-18 00:40:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1203.pt
2022-08-18 00:40:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1203.pt
2022-08-18 00:40:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1203.pt (epoch 1203 @ 97443 updates, score 56.27) (writing took 15.99343615397811 seconds)
2022-08-18 00:40:45 | INFO | fairseq_cli.train | end of epoch 1203 (average epoch stats below)
2022-08-18 00:40:45 | INFO | train | epoch 1203 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 6439.1 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 97443 | lr 0.000202607 | gnorm 0.375 | train_wall 41 | gb_free 10.2 | wall 95341
2022-08-18 00:40:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:40:45 | INFO | fairseq.trainer | begin training epoch 1204
2022-08-18 00:40:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:41:17 | INFO | train_inner | epoch 1204:     57 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=6879.6, ups=1.25, wpb=5483.1, bsz=360.3, num_updates=97500, lr=0.000202548, gnorm=0.771, train_wall=52, gb_free=10, wall=95373
2022-08-18 00:41:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:41:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:41:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:41:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:41:37 | INFO | valid | epoch 1204 | valid on 'valid' subset | loss 5.228 | nll_loss 2.632 | ppl 6.2 | bleu 56.88 | wps 1945.4 | wpb 933.5 | bsz 59.6 | num_updates 97524 | best_bleu 57.52
2022-08-18 00:41:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1204 @ 97524 updates
2022-08-18 00:41:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1204.pt
2022-08-18 00:41:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1204.pt
2022-08-18 00:42:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1204.pt (epoch 1204 @ 97524 updates, score 56.88) (writing took 33.506574243307114 seconds)
2022-08-18 00:42:11 | INFO | fairseq_cli.train | end of epoch 1204 (average epoch stats below)
2022-08-18 00:42:11 | INFO | train | epoch 1204 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 5191.9 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 97524 | lr 0.000202523 | gnorm 0.813 | train_wall 41 | gb_free 10.1 | wall 95427
2022-08-18 00:42:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:42:11 | INFO | fairseq.trainer | begin training epoch 1205
2022-08-18 00:42:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:42:51 | INFO | train_inner | epoch 1205:     76 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=5846.4, ups=1.05, wpb=5545.1, bsz=357, num_updates=97600, lr=0.000202444, gnorm=0.392, train_wall=50, gb_free=10.1, wall=95468
2022-08-18 00:42:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:42:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:42:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:42:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:43:04 | INFO | valid | epoch 1205 | valid on 'valid' subset | loss 5.235 | nll_loss 2.647 | ppl 6.26 | bleu 56.28 | wps 1790.2 | wpb 933.5 | bsz 59.6 | num_updates 97605 | best_bleu 57.52
2022-08-18 00:43:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1205 @ 97605 updates
2022-08-18 00:43:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1205.pt
2022-08-18 00:43:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1205.pt
2022-08-18 00:43:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1205.pt (epoch 1205 @ 97605 updates, score 56.28) (writing took 22.32819651067257 seconds)
2022-08-18 00:43:26 | INFO | fairseq_cli.train | end of epoch 1205 (average epoch stats below)
2022-08-18 00:43:26 | INFO | train | epoch 1205 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 5960.2 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 97605 | lr 0.000202439 | gnorm 0.407 | train_wall 41 | gb_free 10.1 | wall 95502
2022-08-18 00:43:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:43:26 | INFO | fairseq.trainer | begin training epoch 1206
2022-08-18 00:43:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:44:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:44:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:44:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:44:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:44:17 | INFO | valid | epoch 1206 | valid on 'valid' subset | loss 5.237 | nll_loss 2.651 | ppl 6.28 | bleu 56.19 | wps 1898.2 | wpb 933.5 | bsz 59.6 | num_updates 97686 | best_bleu 57.52
2022-08-18 00:44:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1206 @ 97686 updates
2022-08-18 00:44:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1206.pt
2022-08-18 00:44:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1206.pt
2022-08-18 00:44:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1206.pt (epoch 1206 @ 97686 updates, score 56.19) (writing took 15.542742658406496 seconds)
2022-08-18 00:44:32 | INFO | fairseq_cli.train | end of epoch 1206 (average epoch stats below)
2022-08-18 00:44:32 | INFO | train | epoch 1206 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 6735.9 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 97686 | lr 0.000202355 | gnorm 0.375 | train_wall 39 | gb_free 10.1 | wall 95569
2022-08-18 00:44:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:44:33 | INFO | fairseq.trainer | begin training epoch 1207
2022-08-18 00:44:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:44:42 | INFO | train_inner | epoch 1207:     14 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=4989.2, ups=0.91, wpb=5499.6, bsz=357, num_updates=97700, lr=0.00020234, gnorm=0.382, train_wall=48, gb_free=10.1, wall=95578
2022-08-18 00:45:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:45:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:45:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:45:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:45:25 | INFO | valid | epoch 1207 | valid on 'valid' subset | loss 5.24 | nll_loss 2.652 | ppl 6.28 | bleu 56.43 | wps 1956.1 | wpb 933.5 | bsz 59.6 | num_updates 97767 | best_bleu 57.52
2022-08-18 00:45:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1207 @ 97767 updates
2022-08-18 00:45:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1207.pt
2022-08-18 00:45:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1207.pt
2022-08-18 00:46:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1207.pt (epoch 1207 @ 97767 updates, score 56.43) (writing took 37.437530513852835 seconds)
2022-08-18 00:46:02 | INFO | fairseq_cli.train | end of epoch 1207 (average epoch stats below)
2022-08-18 00:46:02 | INFO | train | epoch 1207 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 4977.7 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 97767 | lr 0.000202271 | gnorm 0.36 | train_wall 40 | gb_free 10.3 | wall 95659
2022-08-18 00:46:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:46:03 | INFO | fairseq.trainer | begin training epoch 1208
2022-08-18 00:46:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:46:21 | INFO | train_inner | epoch 1208:     33 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=5574.9, ups=1.01, wpb=5536.1, bsz=357.9, num_updates=97800, lr=0.000202237, gnorm=0.359, train_wall=50, gb_free=10.1, wall=95677
2022-08-18 00:46:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:46:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:46:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:46:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:46:55 | INFO | valid | epoch 1208 | valid on 'valid' subset | loss 5.22 | nll_loss 2.626 | ppl 6.17 | bleu 56.79 | wps 1992.4 | wpb 933.5 | bsz 59.6 | num_updates 97848 | best_bleu 57.52
2022-08-18 00:46:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1208 @ 97848 updates
2022-08-18 00:46:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1208.pt
2022-08-18 00:46:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1208.pt
2022-08-18 00:47:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1208.pt (epoch 1208 @ 97848 updates, score 56.79) (writing took 21.83830988034606 seconds)
2022-08-18 00:47:17 | INFO | fairseq_cli.train | end of epoch 1208 (average epoch stats below)
2022-08-18 00:47:17 | INFO | train | epoch 1208 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 5993.2 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 97848 | lr 0.000202187 | gnorm 0.378 | train_wall 42 | gb_free 10.1 | wall 95733
2022-08-18 00:47:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:47:17 | INFO | fairseq.trainer | begin training epoch 1209
2022-08-18 00:47:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:47:47 | INFO | train_inner | epoch 1209:     52 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=6453.1, ups=1.17, wpb=5537.4, bsz=359.6, num_updates=97900, lr=0.000202134, gnorm=0.372, train_wall=50, gb_free=10, wall=95763
2022-08-18 00:48:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:48:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:48:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:48:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:48:10 | INFO | valid | epoch 1209 | valid on 'valid' subset | loss 5.25 | nll_loss 2.665 | ppl 6.34 | bleu 55.98 | wps 1931.8 | wpb 933.5 | bsz 59.6 | num_updates 97929 | best_bleu 57.52
2022-08-18 00:48:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1209 @ 97929 updates
2022-08-18 00:48:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1209.pt
2022-08-18 00:48:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1209.pt
2022-08-18 00:48:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1209.pt (epoch 1209 @ 97929 updates, score 55.98) (writing took 14.935244016349316 seconds)
2022-08-18 00:48:25 | INFO | fairseq_cli.train | end of epoch 1209 (average epoch stats below)
2022-08-18 00:48:25 | INFO | train | epoch 1209 | loss 3.371 | nll_loss 0.339 | ppl 1.26 | wps 6548.1 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 97929 | lr 0.000202104 | gnorm 0.337 | train_wall 39 | gb_free 10.1 | wall 95802
2022-08-18 00:48:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:48:26 | INFO | fairseq.trainer | begin training epoch 1210
2022-08-18 00:48:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:49:04 | INFO | train_inner | epoch 1210:     71 / 81 loss=3.371, nll_loss=0.339, ppl=1.26, wps=7186.1, ups=1.3, wpb=5531.5, bsz=357.5, num_updates=98000, lr=0.000202031, gnorm=0.307, train_wall=50, gb_free=10, wall=95840
2022-08-18 00:49:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:49:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:49:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:49:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:49:17 | INFO | valid | epoch 1210 | valid on 'valid' subset | loss 5.247 | nll_loss 2.662 | ppl 6.33 | bleu 56.5 | wps 1927.5 | wpb 933.5 | bsz 59.6 | num_updates 98010 | best_bleu 57.52
2022-08-18 00:49:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1210 @ 98010 updates
2022-08-18 00:49:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1210.pt
2022-08-18 00:49:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1210.pt
2022-08-18 00:49:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1210.pt (epoch 1210 @ 98010 updates, score 56.5) (writing took 35.05306653305888 seconds)
2022-08-18 00:49:53 | INFO | fairseq_cli.train | end of epoch 1210 (average epoch stats below)
2022-08-18 00:49:53 | INFO | train | epoch 1210 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5119.9 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 98010 | lr 0.00020202 | gnorm 0.318 | train_wall 41 | gb_free 10.2 | wall 95889
2022-08-18 00:49:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:49:53 | INFO | fairseq.trainer | begin training epoch 1211
2022-08-18 00:49:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:50:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:50:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:50:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:50:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:50:46 | INFO | valid | epoch 1211 | valid on 'valid' subset | loss 5.262 | nll_loss 2.679 | ppl 6.4 | bleu 56 | wps 2031.5 | wpb 933.5 | bsz 59.6 | num_updates 98091 | best_bleu 57.52
2022-08-18 00:50:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1211 @ 98091 updates
2022-08-18 00:50:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1211.pt
2022-08-18 00:50:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1211.pt
2022-08-18 00:51:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1211.pt (epoch 1211 @ 98091 updates, score 56.0) (writing took 20.739353332668543 seconds)
2022-08-18 00:51:07 | INFO | fairseq_cli.train | end of epoch 1211 (average epoch stats below)
2022-08-18 00:51:07 | INFO | train | epoch 1211 | loss 3.371 | nll_loss 0.339 | ppl 1.26 | wps 5999 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 98091 | lr 0.000201937 | gnorm 0.311 | train_wall 40 | gb_free 10.1 | wall 95963
2022-08-18 00:51:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:51:07 | INFO | fairseq.trainer | begin training epoch 1212
2022-08-18 00:51:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:51:13 | INFO | train_inner | epoch 1212:      9 / 81 loss=3.371, nll_loss=0.339, ppl=1.26, wps=4214.9, ups=0.77, wpb=5469.3, bsz=356.2, num_updates=98100, lr=0.000201928, gnorm=0.319, train_wall=49, gb_free=10.2, wall=95970
2022-08-18 00:51:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:51:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:51:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:51:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:52:00 | INFO | valid | epoch 1212 | valid on 'valid' subset | loss 5.246 | nll_loss 2.662 | ppl 6.33 | bleu 56.29 | wps 1921.1 | wpb 933.5 | bsz 59.6 | num_updates 98172 | best_bleu 57.52
2022-08-18 00:52:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1212 @ 98172 updates
2022-08-18 00:52:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1212.pt
2022-08-18 00:52:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1212.pt
2022-08-18 00:52:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1212.pt (epoch 1212 @ 98172 updates, score 56.29) (writing took 14.702386483550072 seconds)
2022-08-18 00:52:15 | INFO | fairseq_cli.train | end of epoch 1212 (average epoch stats below)
2022-08-18 00:52:15 | INFO | train | epoch 1212 | loss 3.371 | nll_loss 0.339 | ppl 1.26 | wps 6637.9 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 98172 | lr 0.000201853 | gnorm 0.317 | train_wall 41 | gb_free 10.2 | wall 96031
2022-08-18 00:52:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:52:15 | INFO | fairseq.trainer | begin training epoch 1213
2022-08-18 00:52:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:52:31 | INFO | train_inner | epoch 1213:     28 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=7181.1, ups=1.3, wpb=5536.7, bsz=359.2, num_updates=98200, lr=0.000201825, gnorm=0.326, train_wall=50, gb_free=10.1, wall=96047
2022-08-18 00:52:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:52:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:52:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:52:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:53:07 | INFO | valid | epoch 1213 | valid on 'valid' subset | loss 5.247 | nll_loss 2.662 | ppl 6.33 | bleu 56.28 | wps 1829.2 | wpb 933.5 | bsz 59.6 | num_updates 98253 | best_bleu 57.52
2022-08-18 00:53:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1213 @ 98253 updates
2022-08-18 00:53:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1213.pt
2022-08-18 00:53:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1213.pt
2022-08-18 00:53:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1213.pt (epoch 1213 @ 98253 updates, score 56.28) (writing took 39.212463568896055 seconds)
2022-08-18 00:53:47 | INFO | fairseq_cli.train | end of epoch 1213 (average epoch stats below)
2022-08-18 00:53:47 | INFO | train | epoch 1213 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 4865.8 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 98253 | lr 0.00020177 | gnorm 0.373 | train_wall 40 | gb_free 10.1 | wall 96123
2022-08-18 00:53:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:53:47 | INFO | fairseq.trainer | begin training epoch 1214
2022-08-18 00:53:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:54:13 | INFO | train_inner | epoch 1214:     47 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=5411.4, ups=0.97, wpb=5551.6, bsz=361.1, num_updates=98300, lr=0.000201722, gnorm=0.33, train_wall=51, gb_free=10.1, wall=96149
2022-08-18 00:54:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:54:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:54:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:54:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:54:40 | INFO | valid | epoch 1214 | valid on 'valid' subset | loss 5.253 | nll_loss 2.67 | ppl 6.36 | bleu 56.32 | wps 1813 | wpb 933.5 | bsz 59.6 | num_updates 98334 | best_bleu 57.52
2022-08-18 00:54:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1214 @ 98334 updates
2022-08-18 00:54:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1214.pt
2022-08-18 00:54:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1214.pt
2022-08-18 00:55:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1214.pt (epoch 1214 @ 98334 updates, score 56.32) (writing took 22.8622928224504 seconds)
2022-08-18 00:55:03 | INFO | fairseq_cli.train | end of epoch 1214 (average epoch stats below)
2022-08-18 00:55:03 | INFO | train | epoch 1214 | loss 3.371 | nll_loss 0.339 | ppl 1.26 | wps 5876 | ups 1.06 | wpb 5523.2 | bsz 358 | num_updates 98334 | lr 0.000201687 | gnorm 0.308 | train_wall 41 | gb_free 10.1 | wall 96199
2022-08-18 00:55:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:55:03 | INFO | fairseq.trainer | begin training epoch 1215
2022-08-18 00:55:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:55:40 | INFO | train_inner | epoch 1215:     66 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=6316, ups=1.15, wpb=5503.1, bsz=357.6, num_updates=98400, lr=0.000201619, gnorm=0.427, train_wall=49, gb_free=10, wall=96237
2022-08-18 00:55:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:55:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:55:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:55:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:55:58 | INFO | valid | epoch 1215 | valid on 'valid' subset | loss 5.239 | nll_loss 2.65 | ppl 6.28 | bleu 57.1 | wps 1801.1 | wpb 933.5 | bsz 59.6 | num_updates 98415 | best_bleu 57.52
2022-08-18 00:55:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1215 @ 98415 updates
2022-08-18 00:55:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1215.pt
2022-08-18 00:56:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1215.pt
2022-08-18 00:56:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1215.pt (epoch 1215 @ 98415 updates, score 57.1) (writing took 17.31350003927946 seconds)
2022-08-18 00:56:16 | INFO | fairseq_cli.train | end of epoch 1215 (average epoch stats below)
2022-08-18 00:56:16 | INFO | train | epoch 1215 | loss 3.371 | nll_loss 0.339 | ppl 1.26 | wps 6138.4 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 98415 | lr 0.000201604 | gnorm 0.444 | train_wall 40 | gb_free 10.2 | wall 96272
2022-08-18 00:56:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:56:16 | INFO | fairseq.trainer | begin training epoch 1216
2022-08-18 00:56:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:56:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:56:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:56:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:56:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:57:07 | INFO | valid | epoch 1216 | valid on 'valid' subset | loss 5.253 | nll_loss 2.666 | ppl 6.35 | bleu 56.41 | wps 2013.2 | wpb 933.5 | bsz 59.6 | num_updates 98496 | best_bleu 57.52
2022-08-18 00:57:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1216 @ 98496 updates
2022-08-18 00:57:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1216.pt
2022-08-18 00:57:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1216.pt
2022-08-18 00:57:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1216.pt (epoch 1216 @ 98496 updates, score 56.41) (writing took 35.10105087980628 seconds)
2022-08-18 00:57:42 | INFO | fairseq_cli.train | end of epoch 1216 (average epoch stats below)
2022-08-18 00:57:42 | INFO | train | epoch 1216 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 5180.2 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 98496 | lr 0.000201521 | gnorm 0.358 | train_wall 40 | gb_free 10.1 | wall 96358
2022-08-18 00:57:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:57:42 | INFO | fairseq.trainer | begin training epoch 1217
2022-08-18 00:57:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:57:46 | INFO | train_inner | epoch 1217:      4 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=4402.2, ups=0.8, wpb=5512, bsz=354.8, num_updates=98500, lr=0.000201517, gnorm=0.357, train_wall=50, gb_free=10, wall=96362
2022-08-18 00:58:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:58:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:58:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:58:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:58:35 | INFO | valid | epoch 1217 | valid on 'valid' subset | loss 5.272 | nll_loss 2.691 | ppl 6.46 | bleu 55.79 | wps 1781.5 | wpb 933.5 | bsz 59.6 | num_updates 98577 | best_bleu 57.52
2022-08-18 00:58:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1217 @ 98577 updates
2022-08-18 00:58:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1217.pt
2022-08-18 00:58:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1217.pt
2022-08-18 00:59:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1217.pt (epoch 1217 @ 98577 updates, score 55.79) (writing took 25.654627446085215 seconds)
2022-08-18 00:59:01 | INFO | fairseq_cli.train | end of epoch 1217 (average epoch stats below)
2022-08-18 00:59:01 | INFO | train | epoch 1217 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 5683.4 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 98577 | lr 0.000201438 | gnorm 0.384 | train_wall 41 | gb_free 10.1 | wall 96437
2022-08-18 00:59:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 00:59:01 | INFO | fairseq.trainer | begin training epoch 1218
2022-08-18 00:59:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 00:59:14 | INFO | train_inner | epoch 1218:     23 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=6258.9, ups=1.13, wpb=5544.6, bsz=359.6, num_updates=98600, lr=0.000201415, gnorm=0.372, train_wall=50, gb_free=10.1, wall=96450
2022-08-18 00:59:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 00:59:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 00:59:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 00:59:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 00:59:54 | INFO | valid | epoch 1218 | valid on 'valid' subset | loss 5.261 | nll_loss 2.68 | ppl 6.41 | bleu 56.7 | wps 1815 | wpb 933.5 | bsz 59.6 | num_updates 98658 | best_bleu 57.52
2022-08-18 00:59:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1218 @ 98658 updates
2022-08-18 00:59:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1218.pt
2022-08-18 00:59:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1218.pt
2022-08-18 01:00:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1218.pt (epoch 1218 @ 98658 updates, score 56.7) (writing took 16.881318349391222 seconds)
2022-08-18 01:00:11 | INFO | fairseq_cli.train | end of epoch 1218 (average epoch stats below)
2022-08-18 01:00:11 | INFO | train | epoch 1218 | loss 3.371 | nll_loss 0.339 | ppl 1.26 | wps 6386.1 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 98658 | lr 0.000201356 | gnorm 0.37 | train_wall 40 | gb_free 10.1 | wall 96507
2022-08-18 01:00:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:00:11 | INFO | fairseq.trainer | begin training epoch 1219
2022-08-18 01:00:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:00:34 | INFO | train_inner | epoch 1219:     42 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=6936.1, ups=1.25, wpb=5539.2, bsz=355.4, num_updates=98700, lr=0.000201313, gnorm=0.371, train_wall=51, gb_free=10.1, wall=96530
2022-08-18 01:00:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:00:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:00:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:00:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:01:03 | INFO | valid | epoch 1219 | valid on 'valid' subset | loss 5.25 | nll_loss 2.662 | ppl 6.33 | bleu 56.51 | wps 1891.4 | wpb 933.5 | bsz 59.6 | num_updates 98739 | best_bleu 57.52
2022-08-18 01:01:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1219 @ 98739 updates
2022-08-18 01:01:03 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1219.pt
2022-08-18 01:01:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1219.pt
2022-08-18 01:01:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1219.pt (epoch 1219 @ 98739 updates, score 56.51) (writing took 37.14460124075413 seconds)
2022-08-18 01:01:41 | INFO | fairseq_cli.train | end of epoch 1219 (average epoch stats below)
2022-08-18 01:01:41 | INFO | train | epoch 1219 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 4977.8 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 98739 | lr 0.000201273 | gnorm 0.35 | train_wall 41 | gb_free 10.2 | wall 96597
2022-08-18 01:01:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:01:41 | INFO | fairseq.trainer | begin training epoch 1220
2022-08-18 01:01:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:02:14 | INFO | train_inner | epoch 1220:     61 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=5518.8, ups=1, wpb=5503.5, bsz=361.4, num_updates=98800, lr=0.000201211, gnorm=0.408, train_wall=51, gb_free=10.2, wall=96630
2022-08-18 01:02:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:02:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:02:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:02:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:02:33 | INFO | valid | epoch 1220 | valid on 'valid' subset | loss 5.253 | nll_loss 2.668 | ppl 6.36 | bleu 56.33 | wps 1921.6 | wpb 933.5 | bsz 59.6 | num_updates 98820 | best_bleu 57.52
2022-08-18 01:02:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1220 @ 98820 updates
2022-08-18 01:02:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1220.pt
2022-08-18 01:02:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1220.pt
2022-08-18 01:02:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1220.pt (epoch 1220 @ 98820 updates, score 56.33) (writing took 2.3565243743360043 seconds)
2022-08-18 01:02:35 | INFO | fairseq_cli.train | end of epoch 1220 (average epoch stats below)
2022-08-18 01:02:35 | INFO | train | epoch 1220 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 8167.1 | ups 1.48 | wpb 5523.2 | bsz 358 | num_updates 98820 | lr 0.000201191 | gnorm 0.411 | train_wall 40 | gb_free 10.1 | wall 96652
2022-08-18 01:02:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:02:36 | INFO | fairseq.trainer | begin training epoch 1221
2022-08-18 01:02:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:03:23 | INFO | train_inner | epoch 1221:     80 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=8058.5, ups=1.45, wpb=5542, bsz=357.9, num_updates=98900, lr=0.000201109, gnorm=0.371, train_wall=50, gb_free=10.1, wall=96699
2022-08-18 01:03:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:03:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:03:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:03:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:03:32 | INFO | valid | epoch 1221 | valid on 'valid' subset | loss 5.275 | nll_loss 2.694 | ppl 6.47 | bleu 56.29 | wps 1904.9 | wpb 933.5 | bsz 59.6 | num_updates 98901 | best_bleu 57.52
2022-08-18 01:03:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1221 @ 98901 updates
2022-08-18 01:03:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1221.pt
2022-08-18 01:03:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1221.pt
2022-08-18 01:03:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1221.pt (epoch 1221 @ 98901 updates, score 56.29) (writing took 26.761456362903118 seconds)
2022-08-18 01:03:59 | INFO | fairseq_cli.train | end of epoch 1221 (average epoch stats below)
2022-08-18 01:03:59 | INFO | train | epoch 1221 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 5362.6 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 98901 | lr 0.000201108 | gnorm 0.392 | train_wall 41 | gb_free 10.1 | wall 96735
2022-08-18 01:03:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:03:59 | INFO | fairseq.trainer | begin training epoch 1222
2022-08-18 01:03:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:04:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:04:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:04:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:04:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:04:52 | INFO | valid | epoch 1222 | valid on 'valid' subset | loss 5.259 | nll_loss 2.677 | ppl 6.39 | bleu 55.47 | wps 1862.1 | wpb 933.5 | bsz 59.6 | num_updates 98982 | best_bleu 57.52
2022-08-18 01:04:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1222 @ 98982 updates
2022-08-18 01:04:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1222.pt
2022-08-18 01:04:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1222.pt
2022-08-18 01:05:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1222.pt (epoch 1222 @ 98982 updates, score 55.47) (writing took 40.98569494485855 seconds)
2022-08-18 01:05:34 | INFO | fairseq_cli.train | end of epoch 1222 (average epoch stats below)
2022-08-18 01:05:34 | INFO | train | epoch 1222 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 4720.7 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 98982 | lr 0.000201026 | gnorm 0.342 | train_wall 41 | gb_free 10.2 | wall 96830
2022-08-18 01:05:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:05:34 | INFO | fairseq.trainer | begin training epoch 1223
2022-08-18 01:05:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:05:46 | INFO | train_inner | epoch 1223:     18 / 81 loss=3.371, nll_loss=0.339, ppl=1.26, wps=3830.5, ups=0.7, wpb=5486.3, bsz=355.9, num_updates=99000, lr=0.000201008, gnorm=0.346, train_wall=51, gb_free=10, wall=96842
2022-08-18 01:06:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:06:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:06:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:06:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:06:31 | INFO | valid | epoch 1223 | valid on 'valid' subset | loss 5.266 | nll_loss 2.685 | ppl 6.43 | bleu 55.92 | wps 1983.3 | wpb 933.5 | bsz 59.6 | num_updates 99063 | best_bleu 57.52
2022-08-18 01:06:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1223 @ 99063 updates
2022-08-18 01:06:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1223.pt
2022-08-18 01:06:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1223.pt
2022-08-18 01:06:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1223.pt (epoch 1223 @ 99063 updates, score 55.92) (writing took 18.549237567931414 seconds)
2022-08-18 01:06:50 | INFO | fairseq_cli.train | end of epoch 1223 (average epoch stats below)
2022-08-18 01:06:50 | INFO | train | epoch 1223 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 5886.7 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 99063 | lr 0.000200944 | gnorm 0.356 | train_wall 40 | gb_free 10.1 | wall 96906
2022-08-18 01:06:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:06:50 | INFO | fairseq.trainer | begin training epoch 1224
2022-08-18 01:06:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:07:10 | INFO | train_inner | epoch 1224:     37 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=6576.6, ups=1.19, wpb=5546.9, bsz=356.2, num_updates=99100, lr=0.000200906, gnorm=0.365, train_wall=50, gb_free=10.1, wall=96926
2022-08-18 01:07:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:07:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:07:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:07:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:07:43 | INFO | valid | epoch 1224 | valid on 'valid' subset | loss 5.254 | nll_loss 2.671 | ppl 6.37 | bleu 55.97 | wps 1827.6 | wpb 933.5 | bsz 59.6 | num_updates 99144 | best_bleu 57.52
2022-08-18 01:07:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1224 @ 99144 updates
2022-08-18 01:07:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1224.pt
2022-08-18 01:07:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1224.pt
2022-08-18 01:08:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1224.pt (epoch 1224 @ 99144 updates, score 55.97) (writing took 17.54996807128191 seconds)
2022-08-18 01:08:00 | INFO | fairseq_cli.train | end of epoch 1224 (average epoch stats below)
2022-08-18 01:08:00 | INFO | train | epoch 1224 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6337.8 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 99144 | lr 0.000200862 | gnorm 0.356 | train_wall 41 | gb_free 10.1 | wall 96976
2022-08-18 01:08:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:08:00 | INFO | fairseq.trainer | begin training epoch 1225
2022-08-18 01:08:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:08:30 | INFO | train_inner | epoch 1225:     56 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=6892.9, ups=1.25, wpb=5512.6, bsz=357.6, num_updates=99200, lr=0.000200805, gnorm=0.318, train_wall=49, gb_free=10.1, wall=97006
2022-08-18 01:08:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:08:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:08:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:08:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:08:52 | INFO | valid | epoch 1225 | valid on 'valid' subset | loss 5.228 | nll_loss 2.636 | ppl 6.22 | bleu 57.05 | wps 1873.5 | wpb 933.5 | bsz 59.6 | num_updates 99225 | best_bleu 57.52
2022-08-18 01:08:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1225 @ 99225 updates
2022-08-18 01:08:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1225.pt
2022-08-18 01:08:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1225.pt
2022-08-18 01:09:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1225.pt (epoch 1225 @ 99225 updates, score 57.05) (writing took 35.536198414862156 seconds)
2022-08-18 01:09:28 | INFO | fairseq_cli.train | end of epoch 1225 (average epoch stats below)
2022-08-18 01:09:28 | INFO | train | epoch 1225 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 5101.7 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 99225 | lr 0.00020078 | gnorm 0.302 | train_wall 40 | gb_free 10.1 | wall 97064
2022-08-18 01:09:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:09:28 | INFO | fairseq.trainer | begin training epoch 1226
2022-08-18 01:09:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:10:08 | INFO | train_inner | epoch 1226:     75 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=5624.6, ups=1.02, wpb=5534.2, bsz=363, num_updates=99300, lr=0.000200704, gnorm=0.315, train_wall=50, gb_free=10, wall=97105
2022-08-18 01:10:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:10:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:10:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:10:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:10:20 | INFO | valid | epoch 1226 | valid on 'valid' subset | loss 5.259 | nll_loss 2.673 | ppl 6.38 | bleu 56.24 | wps 1919.8 | wpb 933.5 | bsz 59.6 | num_updates 99306 | best_bleu 57.52
2022-08-18 01:10:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1226 @ 99306 updates
2022-08-18 01:10:20 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1226.pt
2022-08-18 01:10:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1226.pt
2022-08-18 01:10:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1226.pt (epoch 1226 @ 99306 updates, score 56.24) (writing took 22.582223884761333 seconds)
2022-08-18 01:10:43 | INFO | fairseq_cli.train | end of epoch 1226 (average epoch stats below)
2022-08-18 01:10:43 | INFO | train | epoch 1226 | loss 3.371 | nll_loss 0.339 | ppl 1.26 | wps 5961 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 99306 | lr 0.000200698 | gnorm 0.325 | train_wall 40 | gb_free 10.1 | wall 97139
2022-08-18 01:10:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:10:43 | INFO | fairseq.trainer | begin training epoch 1227
2022-08-18 01:10:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:11:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:11:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:11:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:11:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:11:36 | INFO | valid | epoch 1227 | valid on 'valid' subset | loss 5.256 | nll_loss 2.669 | ppl 6.36 | bleu 55.82 | wps 1629.5 | wpb 933.5 | bsz 59.6 | num_updates 99387 | best_bleu 57.52
2022-08-18 01:11:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1227 @ 99387 updates
2022-08-18 01:11:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1227.pt
2022-08-18 01:11:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1227.pt
2022-08-18 01:11:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1227.pt (epoch 1227 @ 99387 updates, score 55.82) (writing took 15.052699267864227 seconds)
2022-08-18 01:11:51 | INFO | fairseq_cli.train | end of epoch 1227 (average epoch stats below)
2022-08-18 01:11:51 | INFO | train | epoch 1227 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 6585.2 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 99387 | lr 0.000200616 | gnorm 0.424 | train_wall 40 | gb_free 10.2 | wall 97207
2022-08-18 01:11:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:11:51 | INFO | fairseq.trainer | begin training epoch 1228
2022-08-18 01:11:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:12:00 | INFO | train_inner | epoch 1228:     13 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=4935.1, ups=0.9, wpb=5491.1, bsz=356.1, num_updates=99400, lr=0.000200603, gnorm=0.42, train_wall=49, gb_free=10.1, wall=97216
2022-08-18 01:12:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:12:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:12:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:12:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:12:45 | INFO | valid | epoch 1228 | valid on 'valid' subset | loss 5.249 | nll_loss 2.665 | ppl 6.34 | bleu 55.72 | wps 1981 | wpb 933.5 | bsz 59.6 | num_updates 99468 | best_bleu 57.52
2022-08-18 01:12:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1228 @ 99468 updates
2022-08-18 01:12:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1228.pt
2022-08-18 01:12:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1228.pt
2022-08-18 01:13:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1228.pt (epoch 1228 @ 99468 updates, score 55.72) (writing took 33.262379962950945 seconds)
2022-08-18 01:13:19 | INFO | fairseq_cli.train | end of epoch 1228 (average epoch stats below)
2022-08-18 01:13:19 | INFO | train | epoch 1228 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 5088 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 99468 | lr 0.000200534 | gnorm 0.405 | train_wall 40 | gb_free 10 | wall 97295
2022-08-18 01:13:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:13:19 | INFO | fairseq.trainer | begin training epoch 1229
2022-08-18 01:13:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:13:38 | INFO | train_inner | epoch 1229:     32 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=5611.5, ups=1.01, wpb=5539.7, bsz=355.7, num_updates=99500, lr=0.000200502, gnorm=0.393, train_wall=50, gb_free=10, wall=97315
2022-08-18 01:14:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:14:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:14:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:14:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:14:14 | INFO | valid | epoch 1229 | valid on 'valid' subset | loss 5.24 | nll_loss 2.653 | ppl 6.29 | bleu 56.36 | wps 1907.3 | wpb 933.5 | bsz 59.6 | num_updates 99549 | best_bleu 57.52
2022-08-18 01:14:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1229 @ 99549 updates
2022-08-18 01:14:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1229.pt
2022-08-18 01:14:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1229.pt
2022-08-18 01:14:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1229.pt (epoch 1229 @ 99549 updates, score 56.36) (writing took 26.572013214230537 seconds)
2022-08-18 01:14:41 | INFO | fairseq_cli.train | end of epoch 1229 (average epoch stats below)
2022-08-18 01:14:41 | INFO | train | epoch 1229 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 5457.7 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 99549 | lr 0.000200453 | gnorm 0.407 | train_wall 41 | gb_free 10.2 | wall 97377
2022-08-18 01:14:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:14:41 | INFO | fairseq.trainer | begin training epoch 1230
2022-08-18 01:14:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:15:10 | INFO | train_inner | epoch 1230:     51 / 81 loss=3.372, nll_loss=0.341, ppl=1.27, wps=6030.8, ups=1.09, wpb=5516.8, bsz=362, num_updates=99600, lr=0.000200401, gnorm=0.481, train_wall=50, gb_free=10.1, wall=97406
2022-08-18 01:15:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:15:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:15:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:15:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:15:36 | INFO | valid | epoch 1230 | valid on 'valid' subset | loss 5.241 | nll_loss 2.653 | ppl 6.29 | bleu 56.56 | wps 1930.4 | wpb 933.5 | bsz 59.6 | num_updates 99630 | best_bleu 57.52
2022-08-18 01:15:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1230 @ 99630 updates
2022-08-18 01:15:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1230.pt
2022-08-18 01:15:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1230.pt
2022-08-18 01:15:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1230.pt (epoch 1230 @ 99630 updates, score 56.56) (writing took 17.953564811497927 seconds)
2022-08-18 01:15:54 | INFO | fairseq_cli.train | end of epoch 1230 (average epoch stats below)
2022-08-18 01:15:54 | INFO | train | epoch 1230 | loss 3.372 | nll_loss 0.341 | ppl 1.27 | wps 6142.9 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 99630 | lr 0.000200371 | gnorm 0.448 | train_wall 41 | gb_free 10.1 | wall 97450
2022-08-18 01:15:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:15:54 | INFO | fairseq.trainer | begin training epoch 1231
2022-08-18 01:15:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:16:32 | INFO | train_inner | epoch 1231:     70 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=6801.3, ups=1.22, wpb=5555.2, bsz=358.8, num_updates=99700, lr=0.000200301, gnorm=0.325, train_wall=51, gb_free=10.1, wall=97488
2022-08-18 01:16:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:16:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:16:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:16:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:16:46 | INFO | valid | epoch 1231 | valid on 'valid' subset | loss 5.229 | nll_loss 2.639 | ppl 6.23 | bleu 56.12 | wps 1964.1 | wpb 933.5 | bsz 59.6 | num_updates 99711 | best_bleu 57.52
2022-08-18 01:16:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1231 @ 99711 updates
2022-08-18 01:16:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1231.pt
2022-08-18 01:16:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1231.pt
2022-08-18 01:17:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1231.pt (epoch 1231 @ 99711 updates, score 56.12) (writing took 36.52455320954323 seconds)
2022-08-18 01:17:23 | INFO | fairseq_cli.train | end of epoch 1231 (average epoch stats below)
2022-08-18 01:17:23 | INFO | train | epoch 1231 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 5035.8 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 99711 | lr 0.00020029 | gnorm 0.334 | train_wall 40 | gb_free 10.1 | wall 97539
2022-08-18 01:17:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:17:23 | INFO | fairseq.trainer | begin training epoch 1232
2022-08-18 01:17:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:18:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:18:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:18:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:18:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:18:16 | INFO | valid | epoch 1232 | valid on 'valid' subset | loss 5.249 | nll_loss 2.661 | ppl 6.32 | bleu 56.5 | wps 1808 | wpb 933.5 | bsz 59.6 | num_updates 99792 | best_bleu 57.52
2022-08-18 01:18:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1232 @ 99792 updates
2022-08-18 01:18:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1232.pt
2022-08-18 01:18:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1232.pt
2022-08-18 01:18:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1232.pt (epoch 1232 @ 99792 updates, score 56.5) (writing took 19.885131537914276 seconds)
2022-08-18 01:18:36 | INFO | fairseq_cli.train | end of epoch 1232 (average epoch stats below)
2022-08-18 01:18:36 | INFO | train | epoch 1232 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 6101.5 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 99792 | lr 0.000200208 | gnorm 0.5 | train_wall 41 | gb_free 10.2 | wall 97612
2022-08-18 01:18:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:18:36 | INFO | fairseq.trainer | begin training epoch 1233
2022-08-18 01:18:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:18:41 | INFO | train_inner | epoch 1233:      8 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=4234.7, ups=0.77, wpb=5486.6, bsz=353.8, num_updates=99800, lr=0.0002002, gnorm=0.488, train_wall=50, gb_free=10, wall=97617
2022-08-18 01:19:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:19:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:19:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:19:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:19:29 | INFO | valid | epoch 1233 | valid on 'valid' subset | loss 5.247 | nll_loss 2.662 | ppl 6.33 | bleu 56.87 | wps 1904 | wpb 933.5 | bsz 59.6 | num_updates 99873 | best_bleu 57.52
2022-08-18 01:19:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1233 @ 99873 updates
2022-08-18 01:19:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1233.pt
2022-08-18 01:19:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1233.pt
2022-08-18 01:19:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1233.pt (epoch 1233 @ 99873 updates, score 56.87) (writing took 14.549985438585281 seconds)
2022-08-18 01:19:44 | INFO | fairseq_cli.train | end of epoch 1233 (average epoch stats below)
2022-08-18 01:19:44 | INFO | train | epoch 1233 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 6605.3 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 99873 | lr 0.000200127 | gnorm 0.362 | train_wall 41 | gb_free 10.2 | wall 97680
2022-08-18 01:19:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:19:44 | INFO | fairseq.trainer | begin training epoch 1234
2022-08-18 01:19:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:20:00 | INFO | train_inner | epoch 1234:     27 / 81 loss=3.371, nll_loss=0.339, ppl=1.26, wps=7092.1, ups=1.27, wpb=5565.6, bsz=364.4, num_updates=99900, lr=0.0002001, gnorm=0.398, train_wall=51, gb_free=10, wall=97696
2022-08-18 01:20:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:20:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:20:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:20:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:20:37 | INFO | valid | epoch 1234 | valid on 'valid' subset | loss 5.235 | nll_loss 2.647 | ppl 6.26 | bleu 56.88 | wps 1843.1 | wpb 933.5 | bsz 59.6 | num_updates 99954 | best_bleu 57.52
2022-08-18 01:20:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1234 @ 99954 updates
2022-08-18 01:20:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1234.pt
2022-08-18 01:20:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1234.pt
2022-08-18 01:21:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1234.pt (epoch 1234 @ 99954 updates, score 56.88) (writing took 39.405244044959545 seconds)
2022-08-18 01:21:17 | INFO | fairseq_cli.train | end of epoch 1234 (average epoch stats below)
2022-08-18 01:21:17 | INFO | train | epoch 1234 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 4801.5 | ups 0.87 | wpb 5523.2 | bsz 358 | num_updates 99954 | lr 0.000200046 | gnorm 0.441 | train_wall 42 | gb_free 10.1 | wall 97773
2022-08-18 01:21:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:21:17 | INFO | fairseq.trainer | begin training epoch 1235
2022-08-18 01:21:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:21:47 | INFO | train_inner | epoch 1235:     46 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=5137.3, ups=0.93, wpb=5502.6, bsz=354.2, num_updates=100000, lr=0.0002, gnorm=0.567, train_wall=51, gb_free=10.1, wall=97803
2022-08-18 01:22:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:22:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:22:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:22:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:22:14 | INFO | valid | epoch 1235 | valid on 'valid' subset | loss 5.232 | nll_loss 2.641 | ppl 6.24 | bleu 56.15 | wps 1778.1 | wpb 933.5 | bsz 59.6 | num_updates 100035 | best_bleu 57.52
2022-08-18 01:22:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1235 @ 100035 updates
2022-08-18 01:22:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1235.pt
2022-08-18 01:22:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1235.pt
2022-08-18 01:22:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1235.pt (epoch 1235 @ 100035 updates, score 56.15) (writing took 36.84498245641589 seconds)
2022-08-18 01:22:51 | INFO | fairseq_cli.train | end of epoch 1235 (average epoch stats below)
2022-08-18 01:22:51 | INFO | train | epoch 1235 | loss 3.371 | nll_loss 0.339 | ppl 1.26 | wps 4741.8 | ups 0.86 | wpb 5523.2 | bsz 358 | num_updates 100035 | lr 0.000199965 | gnorm 0.6 | train_wall 40 | gb_free 10.2 | wall 97867
2022-08-18 01:22:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:22:51 | INFO | fairseq.trainer | begin training epoch 1236
2022-08-18 01:22:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:23:26 | INFO | train_inner | epoch 1236:     65 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=5548.9, ups=1.01, wpb=5517, bsz=356.1, num_updates=100100, lr=0.0001999, gnorm=0.401, train_wall=50, gb_free=10.1, wall=97902
2022-08-18 01:23:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:23:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:23:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:23:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:23:43 | INFO | valid | epoch 1236 | valid on 'valid' subset | loss 5.248 | nll_loss 2.659 | ppl 6.32 | bleu 56.51 | wps 1880.8 | wpb 933.5 | bsz 59.6 | num_updates 100116 | best_bleu 57.52
2022-08-18 01:23:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1236 @ 100116 updates
2022-08-18 01:23:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1236.pt
2022-08-18 01:23:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1236.pt
2022-08-18 01:23:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1236.pt (epoch 1236 @ 100116 updates, score 56.51) (writing took 15.104310892522335 seconds)
2022-08-18 01:23:58 | INFO | fairseq_cli.train | end of epoch 1236 (average epoch stats below)
2022-08-18 01:23:58 | INFO | train | epoch 1236 | loss 3.372 | nll_loss 0.34 | ppl 1.27 | wps 6642.2 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 100116 | lr 0.000199884 | gnorm 0.408 | train_wall 40 | gb_free 10.2 | wall 97935
2022-08-18 01:23:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:23:59 | INFO | fairseq.trainer | begin training epoch 1237
2022-08-18 01:23:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:24:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:24:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:24:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:24:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:24:51 | INFO | valid | epoch 1237 | valid on 'valid' subset | loss 5.256 | nll_loss 2.672 | ppl 6.37 | bleu 55.8 | wps 1832.3 | wpb 933.5 | bsz 59.6 | num_updates 100197 | best_bleu 57.52
2022-08-18 01:24:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1237 @ 100197 updates
2022-08-18 01:24:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1237.pt
2022-08-18 01:24:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1237.pt
2022-08-18 01:25:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1237.pt (epoch 1237 @ 100197 updates, score 55.8) (writing took 43.63082491979003 seconds)
2022-08-18 01:25:34 | INFO | fairseq_cli.train | end of epoch 1237 (average epoch stats below)
2022-08-18 01:25:34 | INFO | train | epoch 1237 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 4664.2 | ups 0.84 | wpb 5523.2 | bsz 358 | num_updates 100197 | lr 0.000199803 | gnorm 0.363 | train_wall 40 | gb_free 10.1 | wall 98031
2022-08-18 01:25:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:25:35 | INFO | fairseq.trainer | begin training epoch 1238
2022-08-18 01:25:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:25:38 | INFO | train_inner | epoch 1238:      3 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=4203, ups=0.76, wpb=5519.7, bsz=357.5, num_updates=100200, lr=0.0001998, gnorm=0.368, train_wall=49, gb_free=10.1, wall=98034
2022-08-18 01:26:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:26:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:26:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:26:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:26:27 | INFO | valid | epoch 1238 | valid on 'valid' subset | loss 5.243 | nll_loss 2.656 | ppl 6.3 | bleu 56.36 | wps 1762.6 | wpb 933.5 | bsz 59.6 | num_updates 100278 | best_bleu 57.52
2022-08-18 01:26:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1238 @ 100278 updates
2022-08-18 01:26:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1238.pt
2022-08-18 01:26:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1238.pt
2022-08-18 01:26:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1238.pt (epoch 1238 @ 100278 updates, score 56.36) (writing took 27.636715181171894 seconds)
2022-08-18 01:26:55 | INFO | fairseq_cli.train | end of epoch 1238 (average epoch stats below)
2022-08-18 01:26:55 | INFO | train | epoch 1238 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 5576.1 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 100278 | lr 0.000199723 | gnorm 0.37 | train_wall 39 | gb_free 10.2 | wall 98111
2022-08-18 01:26:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:26:55 | INFO | fairseq.trainer | begin training epoch 1239
2022-08-18 01:26:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:27:06 | INFO | train_inner | epoch 1239:     22 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=6213.4, ups=1.12, wpb=5525.1, bsz=359.6, num_updates=100300, lr=0.000199701, gnorm=0.371, train_wall=48, gb_free=10.1, wall=98123
2022-08-18 01:27:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:27:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:27:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:27:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:27:46 | INFO | valid | epoch 1239 | valid on 'valid' subset | loss 5.244 | nll_loss 2.656 | ppl 6.3 | bleu 56.5 | wps 1895.4 | wpb 933.5 | bsz 59.6 | num_updates 100359 | best_bleu 57.52
2022-08-18 01:27:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1239 @ 100359 updates
2022-08-18 01:27:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1239.pt
2022-08-18 01:27:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1239.pt
2022-08-18 01:28:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1239.pt (epoch 1239 @ 100359 updates, score 56.5) (writing took 15.998670671135187 seconds)
2022-08-18 01:28:02 | INFO | fairseq_cli.train | end of epoch 1239 (average epoch stats below)
2022-08-18 01:28:02 | INFO | train | epoch 1239 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 6634.6 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 100359 | lr 0.000199642 | gnorm 0.368 | train_wall 39 | gb_free 10.1 | wall 98178
2022-08-18 01:28:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:28:02 | INFO | fairseq.trainer | begin training epoch 1240
2022-08-18 01:28:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:28:25 | INFO | train_inner | epoch 1240:     41 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=7079.1, ups=1.28, wpb=5538.1, bsz=356.2, num_updates=100400, lr=0.000199601, gnorm=0.397, train_wall=49, gb_free=10.1, wall=98201
2022-08-18 01:28:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:28:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:28:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:28:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:28:54 | INFO | valid | epoch 1240 | valid on 'valid' subset | loss 5.25 | nll_loss 2.664 | ppl 6.34 | bleu 56.49 | wps 1884.6 | wpb 933.5 | bsz 59.6 | num_updates 100440 | best_bleu 57.52
2022-08-18 01:28:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1240 @ 100440 updates
2022-08-18 01:28:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1240.pt
2022-08-18 01:28:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1240.pt
2022-08-18 01:29:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1240.pt (epoch 1240 @ 100440 updates, score 56.49) (writing took 38.272384248673916 seconds)
2022-08-18 01:29:32 | INFO | fairseq_cli.train | end of epoch 1240 (average epoch stats below)
2022-08-18 01:29:32 | INFO | train | epoch 1240 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 4974.4 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 100440 | lr 0.000199561 | gnorm 0.437 | train_wall 40 | gb_free 10.1 | wall 98268
2022-08-18 01:29:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:29:32 | INFO | fairseq.trainer | begin training epoch 1241
2022-08-18 01:29:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:30:02 | INFO | train_inner | epoch 1241:     60 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=5688.2, ups=1.03, wpb=5536.2, bsz=361, num_updates=100500, lr=0.000199502, gnorm=0.369, train_wall=48, gb_free=10.1, wall=98298
2022-08-18 01:30:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:30:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:30:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:30:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:30:22 | INFO | valid | epoch 1241 | valid on 'valid' subset | loss 5.246 | nll_loss 2.66 | ppl 6.32 | bleu 56.33 | wps 1852.1 | wpb 933.5 | bsz 59.6 | num_updates 100521 | best_bleu 57.52
2022-08-18 01:30:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1241 @ 100521 updates
2022-08-18 01:30:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1241.pt
2022-08-18 01:30:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1241.pt
2022-08-18 01:30:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1241.pt (epoch 1241 @ 100521 updates, score 56.33) (writing took 21.08266704902053 seconds)
2022-08-18 01:30:43 | INFO | fairseq_cli.train | end of epoch 1241 (average epoch stats below)
2022-08-18 01:30:43 | INFO | train | epoch 1241 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 6321 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 100521 | lr 0.000199481 | gnorm 0.336 | train_wall 39 | gb_free 10.2 | wall 98339
2022-08-18 01:30:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:30:43 | INFO | fairseq.trainer | begin training epoch 1242
2022-08-18 01:30:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:31:25 | INFO | train_inner | epoch 1242:     79 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=6666.8, ups=1.21, wpb=5517, bsz=357.5, num_updates=100600, lr=0.000199403, gnorm=0.347, train_wall=50, gb_free=10.2, wall=98381
2022-08-18 01:31:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:31:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:31:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:31:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:31:35 | INFO | valid | epoch 1242 | valid on 'valid' subset | loss 5.249 | nll_loss 2.66 | ppl 6.32 | bleu 56.15 | wps 1841.6 | wpb 933.5 | bsz 59.6 | num_updates 100602 | best_bleu 57.52
2022-08-18 01:31:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1242 @ 100602 updates
2022-08-18 01:31:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1242.pt
2022-08-18 01:31:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1242.pt
2022-08-18 01:31:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1242.pt (epoch 1242 @ 100602 updates, score 56.15) (writing took 13.914096109569073 seconds)
2022-08-18 01:31:49 | INFO | fairseq_cli.train | end of epoch 1242 (average epoch stats below)
2022-08-18 01:31:49 | INFO | train | epoch 1242 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 6745.9 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 100602 | lr 0.000199401 | gnorm 0.351 | train_wall 41 | gb_free 10.2 | wall 98405
2022-08-18 01:31:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:31:49 | INFO | fairseq.trainer | begin training epoch 1243
2022-08-18 01:31:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:32:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:32:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:32:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:32:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:32:40 | INFO | valid | epoch 1243 | valid on 'valid' subset | loss 5.239 | nll_loss 2.652 | ppl 6.29 | bleu 56.34 | wps 1891.9 | wpb 933.5 | bsz 59.6 | num_updates 100683 | best_bleu 57.52
2022-08-18 01:32:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1243 @ 100683 updates
2022-08-18 01:32:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1243.pt
2022-08-18 01:32:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1243.pt
2022-08-18 01:33:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1243.pt (epoch 1243 @ 100683 updates, score 56.34) (writing took 33.444977417588234 seconds)
2022-08-18 01:33:14 | INFO | fairseq_cli.train | end of epoch 1243 (average epoch stats below)
2022-08-18 01:33:14 | INFO | train | epoch 1243 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5301.9 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 100683 | lr 0.00019932 | gnorm 0.358 | train_wall 40 | gb_free 10 | wall 98490
2022-08-18 01:33:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:33:14 | INFO | fairseq.trainer | begin training epoch 1244
2022-08-18 01:33:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:33:24 | INFO | train_inner | epoch 1244:     17 / 81 loss=3.371, nll_loss=0.339, ppl=1.26, wps=4620.9, ups=0.84, wpb=5489.7, bsz=355.8, num_updates=100700, lr=0.000199304, gnorm=0.356, train_wall=49, gb_free=10.1, wall=98500
2022-08-18 01:33:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:33:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:33:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:33:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:34:06 | INFO | valid | epoch 1244 | valid on 'valid' subset | loss 5.256 | nll_loss 2.673 | ppl 6.38 | bleu 55.91 | wps 1721.8 | wpb 933.5 | bsz 59.6 | num_updates 100764 | best_bleu 57.52
2022-08-18 01:34:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1244 @ 100764 updates
2022-08-18 01:34:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1244.pt
2022-08-18 01:34:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1244.pt
2022-08-18 01:34:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1244.pt (epoch 1244 @ 100764 updates, score 55.91) (writing took 16.51115331053734 seconds)
2022-08-18 01:34:22 | INFO | fairseq_cli.train | end of epoch 1244 (average epoch stats below)
2022-08-18 01:34:22 | INFO | train | epoch 1244 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6499 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 100764 | lr 0.00019924 | gnorm 0.386 | train_wall 40 | gb_free 10.1 | wall 98559
2022-08-18 01:34:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:34:23 | INFO | fairseq.trainer | begin training epoch 1245
2022-08-18 01:34:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:34:42 | INFO | train_inner | epoch 1245:     36 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=7050.6, ups=1.28, wpb=5526.5, bsz=362.2, num_updates=100800, lr=0.000199205, gnorm=0.379, train_wall=50, gb_free=10.1, wall=98578
2022-08-18 01:35:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:35:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:35:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:35:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:35:13 | INFO | valid | epoch 1245 | valid on 'valid' subset | loss 5.249 | nll_loss 2.665 | ppl 6.34 | bleu 56.05 | wps 1912.3 | wpb 933.5 | bsz 59.6 | num_updates 100845 | best_bleu 57.52
2022-08-18 01:35:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1245 @ 100845 updates
2022-08-18 01:35:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1245.pt
2022-08-18 01:35:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1245.pt
2022-08-18 01:35:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1245.pt (epoch 1245 @ 100845 updates, score 56.05) (writing took 19.975066367536783 seconds)
2022-08-18 01:35:33 | INFO | fairseq_cli.train | end of epoch 1245 (average epoch stats below)
2022-08-18 01:35:33 | INFO | train | epoch 1245 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 6300.3 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 100845 | lr 0.00019916 | gnorm 0.331 | train_wall 40 | gb_free 10.1 | wall 98630
2022-08-18 01:35:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:35:34 | INFO | fairseq.trainer | begin training epoch 1246
2022-08-18 01:35:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:36:03 | INFO | train_inner | epoch 1246:     55 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=6849.2, ups=1.23, wpb=5553.9, bsz=357.1, num_updates=100900, lr=0.000199106, gnorm=0.342, train_wall=49, gb_free=10.1, wall=98659
2022-08-18 01:36:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:36:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:36:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:36:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:36:26 | INFO | valid | epoch 1246 | valid on 'valid' subset | loss 5.264 | nll_loss 2.68 | ppl 6.41 | bleu 55.76 | wps 1854 | wpb 933.5 | bsz 59.6 | num_updates 100926 | best_bleu 57.52
2022-08-18 01:36:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1246 @ 100926 updates
2022-08-18 01:36:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1246.pt
2022-08-18 01:36:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1246.pt
2022-08-18 01:37:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1246.pt (epoch 1246 @ 100926 updates, score 55.76) (writing took 46.056261107325554 seconds)
2022-08-18 01:37:12 | INFO | fairseq_cli.train | end of epoch 1246 (average epoch stats below)
2022-08-18 01:37:12 | INFO | train | epoch 1246 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 4531.3 | ups 0.82 | wpb 5523.2 | bsz 358 | num_updates 100926 | lr 0.00019908 | gnorm 0.386 | train_wall 41 | gb_free 10 | wall 98728
2022-08-18 01:37:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:37:12 | INFO | fairseq.trainer | begin training epoch 1247
2022-08-18 01:37:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:37:53 | INFO | train_inner | epoch 1247:     74 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=5013.2, ups=0.91, wpb=5509.3, bsz=356.9, num_updates=101000, lr=0.000199007, gnorm=0.368, train_wall=50, gb_free=10.1, wall=98769
2022-08-18 01:37:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:37:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:37:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:37:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:38:06 | INFO | valid | epoch 1247 | valid on 'valid' subset | loss 5.247 | nll_loss 2.66 | ppl 6.32 | bleu 56.45 | wps 1852.3 | wpb 933.5 | bsz 59.6 | num_updates 101007 | best_bleu 57.52
2022-08-18 01:38:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1247 @ 101007 updates
2022-08-18 01:38:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1247.pt
2022-08-18 01:38:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1247.pt
2022-08-18 01:38:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1247.pt (epoch 1247 @ 101007 updates, score 56.45) (writing took 24.076236572116613 seconds)
2022-08-18 01:38:30 | INFO | fairseq_cli.train | end of epoch 1247 (average epoch stats below)
2022-08-18 01:38:30 | INFO | train | epoch 1247 | loss 3.371 | nll_loss 0.339 | ppl 1.26 | wps 5757.6 | ups 1.04 | wpb 5523.2 | bsz 358 | num_updates 101007 | lr 0.000199001 | gnorm 0.347 | train_wall 40 | gb_free 10.1 | wall 98806
2022-08-18 01:38:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:38:30 | INFO | fairseq.trainer | begin training epoch 1248
2022-08-18 01:38:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:39:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:39:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:39:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:39:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:39:21 | INFO | valid | epoch 1248 | valid on 'valid' subset | loss 5.266 | nll_loss 2.685 | ppl 6.43 | bleu 55.91 | wps 1834.3 | wpb 933.5 | bsz 59.6 | num_updates 101088 | best_bleu 57.52
2022-08-18 01:39:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1248 @ 101088 updates
2022-08-18 01:39:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1248.pt
2022-08-18 01:39:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1248.pt
2022-08-18 01:39:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1248.pt (epoch 1248 @ 101088 updates, score 55.91) (writing took 18.020593877881765 seconds)
2022-08-18 01:39:39 | INFO | fairseq_cli.train | end of epoch 1248 (average epoch stats below)
2022-08-18 01:39:39 | INFO | train | epoch 1248 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6435 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 101088 | lr 0.000198921 | gnorm 0.357 | train_wall 40 | gb_free 10.1 | wall 98876
2022-08-18 01:39:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:39:40 | INFO | fairseq.trainer | begin training epoch 1249
2022-08-18 01:39:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:39:47 | INFO | train_inner | epoch 1249:     12 / 81 loss=3.371, nll_loss=0.339, ppl=1.26, wps=4843.9, ups=0.88, wpb=5520, bsz=356.6, num_updates=101100, lr=0.000198909, gnorm=0.35, train_wall=49, gb_free=10, wall=98883
2022-08-18 01:40:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:40:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:40:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:40:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:40:33 | INFO | valid | epoch 1249 | valid on 'valid' subset | loss 5.254 | nll_loss 2.671 | ppl 6.37 | bleu 55.94 | wps 1604 | wpb 933.5 | bsz 59.6 | num_updates 101169 | best_bleu 57.52
2022-08-18 01:40:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1249 @ 101169 updates
2022-08-18 01:40:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1249.pt
2022-08-18 01:40:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1249.pt
2022-08-18 01:41:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1249.pt (epoch 1249 @ 101169 updates, score 55.94) (writing took 39.04156081005931 seconds)
2022-08-18 01:41:12 | INFO | fairseq_cli.train | end of epoch 1249 (average epoch stats below)
2022-08-18 01:41:12 | INFO | train | epoch 1249 | loss 3.371 | nll_loss 0.339 | ppl 1.26 | wps 4841.9 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 101169 | lr 0.000198841 | gnorm 0.315 | train_wall 40 | gb_free 10.1 | wall 98968
2022-08-18 01:41:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:41:12 | INFO | fairseq.trainer | begin training epoch 1250
2022-08-18 01:41:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:41:29 | INFO | train_inner | epoch 1250:     31 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=5389.1, ups=0.98, wpb=5521.3, bsz=358.6, num_updates=101200, lr=0.000198811, gnorm=0.311, train_wall=49, gb_free=10.1, wall=98986
2022-08-18 01:41:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:41:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:41:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:41:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:42:05 | INFO | valid | epoch 1250 | valid on 'valid' subset | loss 5.256 | nll_loss 2.674 | ppl 6.38 | bleu 55.53 | wps 1582.8 | wpb 933.5 | bsz 59.6 | num_updates 101250 | best_bleu 57.52
2022-08-18 01:42:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1250 @ 101250 updates
2022-08-18 01:42:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1250.pt
2022-08-18 01:42:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1250.pt
2022-08-18 01:42:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1250.pt (epoch 1250 @ 101250 updates, score 55.53) (writing took 26.660090513527393 seconds)
2022-08-18 01:42:32 | INFO | fairseq_cli.train | end of epoch 1250 (average epoch stats below)
2022-08-18 01:42:32 | INFO | train | epoch 1250 | loss 3.371 | nll_loss 0.339 | ppl 1.26 | wps 5551.1 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 101250 | lr 0.000198762 | gnorm 0.352 | train_wall 40 | gb_free 10.1 | wall 99049
2022-08-18 01:42:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:42:33 | INFO | fairseq.trainer | begin training epoch 1251
2022-08-18 01:42:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:43:00 | INFO | train_inner | epoch 1251:     50 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=6052.7, ups=1.1, wpb=5513.2, bsz=357.5, num_updates=101300, lr=0.000198713, gnorm=0.377, train_wall=50, gb_free=10.2, wall=99077
2022-08-18 01:43:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:43:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:43:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:43:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:43:26 | INFO | valid | epoch 1251 | valid on 'valid' subset | loss 5.243 | nll_loss 2.653 | ppl 6.29 | bleu 56.29 | wps 1775.3 | wpb 933.5 | bsz 59.6 | num_updates 101331 | best_bleu 57.52
2022-08-18 01:43:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1251 @ 101331 updates
2022-08-18 01:43:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1251.pt
2022-08-18 01:43:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1251.pt
2022-08-18 01:43:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1251.pt (epoch 1251 @ 101331 updates, score 56.29) (writing took 14.774653118103743 seconds)
2022-08-18 01:43:41 | INFO | fairseq_cli.train | end of epoch 1251 (average epoch stats below)
2022-08-18 01:43:41 | INFO | train | epoch 1251 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 6496.5 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 101331 | lr 0.000198682 | gnorm 0.369 | train_wall 41 | gb_free 10.1 | wall 99117
2022-08-18 01:43:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:43:41 | INFO | fairseq.trainer | begin training epoch 1252
2022-08-18 01:43:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:44:19 | INFO | train_inner | epoch 1252:     69 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=7065.5, ups=1.27, wpb=5552.5, bsz=359.4, num_updates=101400, lr=0.000198615, gnorm=0.382, train_wall=51, gb_free=10, wall=99155
2022-08-18 01:44:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:44:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:44:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:44:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:44:34 | INFO | valid | epoch 1252 | valid on 'valid' subset | loss 5.255 | nll_loss 2.669 | ppl 6.36 | bleu 56.68 | wps 1853.6 | wpb 933.5 | bsz 59.6 | num_updates 101412 | best_bleu 57.52
2022-08-18 01:44:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1252 @ 101412 updates
2022-08-18 01:44:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1252.pt
2022-08-18 01:44:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1252.pt
2022-08-18 01:45:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1252.pt (epoch 1252 @ 101412 updates, score 56.68) (writing took 34.24819527193904 seconds)
2022-08-18 01:45:09 | INFO | fairseq_cli.train | end of epoch 1252 (average epoch stats below)
2022-08-18 01:45:09 | INFO | train | epoch 1252 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 5120.6 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 101412 | lr 0.000198603 | gnorm 0.408 | train_wall 41 | gb_free 10.1 | wall 99205
2022-08-18 01:45:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:45:09 | INFO | fairseq.trainer | begin training epoch 1253
2022-08-18 01:45:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:45:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:45:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:45:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:45:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:46:01 | INFO | valid | epoch 1253 | valid on 'valid' subset | loss 5.265 | nll_loss 2.683 | ppl 6.42 | bleu 56.21 | wps 1787.2 | wpb 933.5 | bsz 59.6 | num_updates 101493 | best_bleu 57.52
2022-08-18 01:46:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1253 @ 101493 updates
2022-08-18 01:46:01 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1253.pt
2022-08-18 01:46:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1253.pt
2022-08-18 01:47:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1253.pt (epoch 1253 @ 101493 updates, score 56.21) (writing took 75.27803729847074 seconds)
2022-08-18 01:47:17 | INFO | fairseq_cli.train | end of epoch 1253 (average epoch stats below)
2022-08-18 01:47:17 | INFO | train | epoch 1253 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 3486 | ups 0.63 | wpb 5523.2 | bsz 358 | num_updates 101493 | lr 0.000198524 | gnorm 0.366 | train_wall 41 | gb_free 10.1 | wall 99333
2022-08-18 01:47:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:47:17 | INFO | fairseq.trainer | begin training epoch 1254
2022-08-18 01:47:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:47:22 | INFO | train_inner | epoch 1254:      7 / 81 loss=3.371, nll_loss=0.339, ppl=1.26, wps=3002.7, ups=0.55, wpb=5486, bsz=354.2, num_updates=101500, lr=0.000198517, gnorm=0.39, train_wall=51, gb_free=10.1, wall=99338
2022-08-18 01:48:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:48:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:48:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:48:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:48:08 | INFO | valid | epoch 1254 | valid on 'valid' subset | loss 5.255 | nll_loss 2.67 | ppl 6.36 | bleu 56.73 | wps 1981.4 | wpb 933.5 | bsz 59.6 | num_updates 101574 | best_bleu 57.52
2022-08-18 01:48:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1254 @ 101574 updates
2022-08-18 01:48:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1254.pt
2022-08-18 01:48:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1254.pt
2022-08-18 01:48:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1254.pt (epoch 1254 @ 101574 updates, score 56.73) (writing took 16.726082790642977 seconds)
2022-08-18 01:48:25 | INFO | fairseq_cli.train | end of epoch 1254 (average epoch stats below)
2022-08-18 01:48:25 | INFO | train | epoch 1254 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 6541.5 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 101574 | lr 0.000198444 | gnorm 0.359 | train_wall 40 | gb_free 10.1 | wall 99402
2022-08-18 01:48:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:48:26 | INFO | fairseq.trainer | begin training epoch 1255
2022-08-18 01:48:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:48:40 | INFO | train_inner | epoch 1255:     26 / 81 loss=3.371, nll_loss=0.339, ppl=1.26, wps=7126.1, ups=1.28, wpb=5572.6, bsz=362.6, num_updates=101600, lr=0.000198419, gnorm=0.41, train_wall=50, gb_free=10.1, wall=99416
2022-08-18 01:49:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:49:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:49:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:49:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:49:17 | INFO | valid | epoch 1255 | valid on 'valid' subset | loss 5.273 | nll_loss 2.693 | ppl 6.47 | bleu 56.67 | wps 1832.9 | wpb 933.5 | bsz 59.6 | num_updates 101655 | best_bleu 57.52
2022-08-18 01:49:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1255 @ 101655 updates
2022-08-18 01:49:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1255.pt
2022-08-18 01:49:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1255.pt
2022-08-18 01:49:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1255.pt (epoch 1255 @ 101655 updates, score 56.67) (writing took 26.466214139014482 seconds)
2022-08-18 01:49:44 | INFO | fairseq_cli.train | end of epoch 1255 (average epoch stats below)
2022-08-18 01:49:44 | INFO | train | epoch 1255 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 5713.6 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 101655 | lr 0.000198365 | gnorm 0.462 | train_wall 40 | gb_free 10.1 | wall 99480
2022-08-18 01:49:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:49:44 | INFO | fairseq.trainer | begin training epoch 1256
2022-08-18 01:49:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:50:09 | INFO | train_inner | epoch 1256:     45 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=6180.2, ups=1.12, wpb=5522.3, bsz=360.6, num_updates=101700, lr=0.000198321, gnorm=0.397, train_wall=49, gb_free=10.1, wall=99506
2022-08-18 01:50:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:50:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:50:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:50:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:50:38 | INFO | valid | epoch 1256 | valid on 'valid' subset | loss 5.258 | nll_loss 2.671 | ppl 6.37 | bleu 56.32 | wps 1756.3 | wpb 933.5 | bsz 59.6 | num_updates 101736 | best_bleu 57.52
2022-08-18 01:50:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1256 @ 101736 updates
2022-08-18 01:50:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1256.pt
2022-08-18 01:50:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1256.pt
2022-08-18 01:51:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1256.pt (epoch 1256 @ 101736 updates, score 56.32) (writing took 39.15899658203125 seconds)
2022-08-18 01:51:17 | INFO | fairseq_cli.train | end of epoch 1256 (average epoch stats below)
2022-08-18 01:51:17 | INFO | train | epoch 1256 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 4786.7 | ups 0.87 | wpb 5523.2 | bsz 358 | num_updates 101736 | lr 0.000198286 | gnorm 0.379 | train_wall 40 | gb_free 10.1 | wall 99573
2022-08-18 01:51:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:51:17 | INFO | fairseq.trainer | begin training epoch 1257
2022-08-18 01:51:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:51:51 | INFO | train_inner | epoch 1257:     64 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=5417.3, ups=0.98, wpb=5503.5, bsz=353.4, num_updates=101800, lr=0.000198224, gnorm=0.401, train_wall=49, gb_free=10.1, wall=99607
2022-08-18 01:51:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:52:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:52:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:52:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:52:09 | INFO | valid | epoch 1257 | valid on 'valid' subset | loss 5.261 | nll_loss 2.678 | ppl 6.4 | bleu 55.72 | wps 1894.4 | wpb 933.5 | bsz 59.6 | num_updates 101817 | best_bleu 57.52
2022-08-18 01:52:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1257 @ 101817 updates
2022-08-18 01:52:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1257.pt
2022-08-18 01:52:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1257.pt
2022-08-18 01:52:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1257.pt (epoch 1257 @ 101817 updates, score 55.72) (writing took 21.90950781479478 seconds)
2022-08-18 01:52:31 | INFO | fairseq_cli.train | end of epoch 1257 (average epoch stats below)
2022-08-18 01:52:31 | INFO | train | epoch 1257 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 6084.7 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 101817 | lr 0.000198207 | gnorm 0.387 | train_wall 40 | gb_free 10.2 | wall 99647
2022-08-18 01:52:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:52:31 | INFO | fairseq.trainer | begin training epoch 1258
2022-08-18 01:52:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:53:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:53:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:53:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:53:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:53:22 | INFO | valid | epoch 1258 | valid on 'valid' subset | loss 5.229 | nll_loss 2.637 | ppl 6.22 | bleu 55.87 | wps 1855.3 | wpb 933.5 | bsz 59.6 | num_updates 101898 | best_bleu 57.52
2022-08-18 01:53:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1258 @ 101898 updates
2022-08-18 01:53:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1258.pt
2022-08-18 01:53:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1258.pt
2022-08-18 01:53:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1258.pt (epoch 1258 @ 101898 updates, score 55.87) (writing took 27.68197602778673 seconds)
2022-08-18 01:53:50 | INFO | fairseq_cli.train | end of epoch 1258 (average epoch stats below)
2022-08-18 01:53:50 | INFO | train | epoch 1258 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 5629.1 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 101898 | lr 0.000198129 | gnorm 0.373 | train_wall 40 | gb_free 10.1 | wall 99726
2022-08-18 01:53:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:53:50 | INFO | fairseq.trainer | begin training epoch 1259
2022-08-18 01:53:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:53:52 | INFO | train_inner | epoch 1259:      2 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=4528, ups=0.82, wpb=5503.8, bsz=358.6, num_updates=101900, lr=0.000198127, gnorm=0.366, train_wall=50, gb_free=10.1, wall=99729
2022-08-18 01:54:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:54:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:54:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:54:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:54:42 | INFO | valid | epoch 1259 | valid on 'valid' subset | loss 5.242 | nll_loss 2.657 | ppl 6.31 | bleu 56.11 | wps 1912.5 | wpb 933.5 | bsz 59.6 | num_updates 101979 | best_bleu 57.52
2022-08-18 01:54:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1259 @ 101979 updates
2022-08-18 01:54:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1259.pt
2022-08-18 01:54:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1259.pt
2022-08-18 01:55:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1259.pt (epoch 1259 @ 101979 updates, score 56.11) (writing took 36.12305397540331 seconds)
2022-08-18 01:55:18 | INFO | fairseq_cli.train | end of epoch 1259 (average epoch stats below)
2022-08-18 01:55:18 | INFO | train | epoch 1259 | loss 3.371 | nll_loss 0.339 | ppl 1.26 | wps 5070.2 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 101979 | lr 0.00019805 | gnorm 0.38 | train_wall 40 | gb_free 10.1 | wall 99815
2022-08-18 01:55:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:55:19 | INFO | fairseq.trainer | begin training epoch 1260
2022-08-18 01:55:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:55:30 | INFO | train_inner | epoch 1260:     21 / 81 loss=3.371, nll_loss=0.339, ppl=1.26, wps=5674.4, ups=1.03, wpb=5523, bsz=353.5, num_updates=102000, lr=0.00019803, gnorm=0.368, train_wall=49, gb_free=10.1, wall=99826
2022-08-18 01:56:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:56:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:56:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:56:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:56:09 | INFO | valid | epoch 1260 | valid on 'valid' subset | loss 5.225 | nll_loss 2.633 | ppl 6.2 | bleu 56.52 | wps 1894.1 | wpb 933.5 | bsz 59.6 | num_updates 102060 | best_bleu 57.52
2022-08-18 01:56:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1260 @ 102060 updates
2022-08-18 01:56:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1260.pt
2022-08-18 01:56:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1260.pt
2022-08-18 01:56:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1260.pt (epoch 1260 @ 102060 updates, score 56.52) (writing took 39.42232212796807 seconds)
2022-08-18 01:56:49 | INFO | fairseq_cli.train | end of epoch 1260 (average epoch stats below)
2022-08-18 01:56:49 | INFO | train | epoch 1260 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 4949 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 102060 | lr 0.000197971 | gnorm 0.316 | train_wall 40 | gb_free 10.2 | wall 99905
2022-08-18 01:56:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:56:49 | INFO | fairseq.trainer | begin training epoch 1261
2022-08-18 01:56:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:57:11 | INFO | train_inner | epoch 1261:     40 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=5485, ups=0.99, wpb=5546.4, bsz=367.5, num_updates=102100, lr=0.000197933, gnorm=0.329, train_wall=50, gb_free=10.1, wall=99927
2022-08-18 01:57:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:57:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:57:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:57:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:57:41 | INFO | valid | epoch 1261 | valid on 'valid' subset | loss 5.25 | nll_loss 2.664 | ppl 6.34 | bleu 56 | wps 1902.7 | wpb 933.5 | bsz 59.6 | num_updates 102141 | best_bleu 57.52
2022-08-18 01:57:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1261 @ 102141 updates
2022-08-18 01:57:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1261.pt
2022-08-18 01:57:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1261.pt
2022-08-18 01:58:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1261.pt (epoch 1261 @ 102141 updates, score 56.0) (writing took 19.261740569025278 seconds)
2022-08-18 01:58:00 | INFO | fairseq_cli.train | end of epoch 1261 (average epoch stats below)
2022-08-18 01:58:00 | INFO | train | epoch 1261 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 6244.2 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 102141 | lr 0.000197893 | gnorm 0.39 | train_wall 41 | gb_free 10.1 | wall 99977
2022-08-18 01:58:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:58:01 | INFO | fairseq.trainer | begin training epoch 1262
2022-08-18 01:58:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:58:37 | INFO | train_inner | epoch 1262:     59 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=6406.1, ups=1.16, wpb=5504.9, bsz=352.4, num_updates=102200, lr=0.000197836, gnorm=0.377, train_wall=50, gb_free=10, wall=100013
2022-08-18 01:58:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:58:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:58:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:58:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 01:58:57 | INFO | valid | epoch 1262 | valid on 'valid' subset | loss 5.247 | nll_loss 2.659 | ppl 6.32 | bleu 56.35 | wps 1838.3 | wpb 933.5 | bsz 59.6 | num_updates 102222 | best_bleu 57.52
2022-08-18 01:58:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1262 @ 102222 updates
2022-08-18 01:58:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1262.pt
2022-08-18 01:58:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1262.pt
2022-08-18 01:59:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1262.pt (epoch 1262 @ 102222 updates, score 56.35) (writing took 15.490476530045271 seconds)
2022-08-18 01:59:13 | INFO | fairseq_cli.train | end of epoch 1262 (average epoch stats below)
2022-08-18 01:59:13 | INFO | train | epoch 1262 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 6155.3 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 102222 | lr 0.000197814 | gnorm 0.375 | train_wall 40 | gb_free 10.1 | wall 100049
2022-08-18 01:59:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 01:59:13 | INFO | fairseq.trainer | begin training epoch 1263
2022-08-18 01:59:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 01:59:55 | INFO | train_inner | epoch 1263:     78 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=7109, ups=1.28, wpb=5547.6, bsz=361.4, num_updates=102300, lr=0.000197739, gnorm=0.432, train_wall=50, gb_free=10.1, wall=100091
2022-08-18 01:59:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 01:59:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 01:59:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 01:59:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:00:07 | INFO | valid | epoch 1263 | valid on 'valid' subset | loss 5.233 | nll_loss 2.643 | ppl 6.25 | bleu 56.62 | wps 1660.7 | wpb 933.5 | bsz 59.6 | num_updates 102303 | best_bleu 57.52
2022-08-18 02:00:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1263 @ 102303 updates
2022-08-18 02:00:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1263.pt
2022-08-18 02:00:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1263.pt
2022-08-18 02:00:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1263.pt (epoch 1263 @ 102303 updates, score 56.62) (writing took 37.503314942121506 seconds)
2022-08-18 02:00:44 | INFO | fairseq_cli.train | end of epoch 1263 (average epoch stats below)
2022-08-18 02:00:44 | INFO | train | epoch 1263 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 4899.1 | ups 0.89 | wpb 5523.2 | bsz 358 | num_updates 102303 | lr 0.000197736 | gnorm 0.433 | train_wall 41 | gb_free 10.2 | wall 100141
2022-08-18 02:00:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:00:45 | INFO | fairseq.trainer | begin training epoch 1264
2022-08-18 02:00:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:01:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:01:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:01:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:01:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:01:36 | INFO | valid | epoch 1264 | valid on 'valid' subset | loss 5.251 | nll_loss 2.668 | ppl 6.35 | bleu 55.66 | wps 1864.7 | wpb 933.5 | bsz 59.6 | num_updates 102384 | best_bleu 57.52
2022-08-18 02:01:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1264 @ 102384 updates
2022-08-18 02:01:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1264.pt
2022-08-18 02:01:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1264.pt
2022-08-18 02:02:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1264.pt (epoch 1264 @ 102384 updates, score 55.66) (writing took 28.87650854513049 seconds)
2022-08-18 02:02:05 | INFO | fairseq_cli.train | end of epoch 1264 (average epoch stats below)
2022-08-18 02:02:05 | INFO | train | epoch 1264 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5550.8 | ups 1 | wpb 5523.2 | bsz 358 | num_updates 102384 | lr 0.000197658 | gnorm 0.357 | train_wall 40 | gb_free 10.2 | wall 100221
2022-08-18 02:02:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:02:05 | INFO | fairseq.trainer | begin training epoch 1265
2022-08-18 02:02:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:02:15 | INFO | train_inner | epoch 1265:     16 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=3943.5, ups=0.72, wpb=5509.9, bsz=355, num_updates=102400, lr=0.000197642, gnorm=0.356, train_wall=49, gb_free=10.1, wall=100231
2022-08-18 02:02:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:02:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:02:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:02:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:02:57 | INFO | valid | epoch 1265 | valid on 'valid' subset | loss 5.244 | nll_loss 2.655 | ppl 6.3 | bleu 56.63 | wps 1860.6 | wpb 933.5 | bsz 59.6 | num_updates 102465 | best_bleu 57.52
2022-08-18 02:02:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1265 @ 102465 updates
2022-08-18 02:02:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1265.pt
2022-08-18 02:02:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1265.pt
2022-08-18 02:03:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1265.pt (epoch 1265 @ 102465 updates, score 56.63) (writing took 28.651934172958136 seconds)
2022-08-18 02:03:26 | INFO | fairseq_cli.train | end of epoch 1265 (average epoch stats below)
2022-08-18 02:03:26 | INFO | train | epoch 1265 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 5509.6 | ups 1 | wpb 5523.2 | bsz 358 | num_updates 102465 | lr 0.00019758 | gnorm 0.54 | train_wall 40 | gb_free 10.1 | wall 100302
2022-08-18 02:03:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:03:26 | INFO | fairseq.trainer | begin training epoch 1266
2022-08-18 02:03:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:03:46 | INFO | train_inner | epoch 1266:     35 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=6035.3, ups=1.09, wpb=5519.6, bsz=358.9, num_updates=102500, lr=0.000197546, gnorm=0.511, train_wall=50, gb_free=10, wall=100322
2022-08-18 02:04:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:04:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:04:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:04:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:04:18 | INFO | valid | epoch 1266 | valid on 'valid' subset | loss 5.256 | nll_loss 2.671 | ppl 6.37 | bleu 56.48 | wps 1864.6 | wpb 933.5 | bsz 59.6 | num_updates 102546 | best_bleu 57.52
2022-08-18 02:04:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1266 @ 102546 updates
2022-08-18 02:04:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1266.pt
2022-08-18 02:04:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1266.pt
2022-08-18 02:04:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1266.pt (epoch 1266 @ 102546 updates, score 56.48) (writing took 16.772944174706936 seconds)
2022-08-18 02:04:35 | INFO | fairseq_cli.train | end of epoch 1266 (average epoch stats below)
2022-08-18 02:04:35 | INFO | train | epoch 1266 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6484.6 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 102546 | lr 0.000197502 | gnorm 0.347 | train_wall 41 | gb_free 10.1 | wall 100371
2022-08-18 02:04:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:04:35 | INFO | fairseq.trainer | begin training epoch 1267
2022-08-18 02:04:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:05:04 | INFO | train_inner | epoch 1267:     54 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=7098.1, ups=1.28, wpb=5533.3, bsz=364.8, num_updates=102600, lr=0.00019745, gnorm=0.383, train_wall=50, gb_free=10, wall=100400
2022-08-18 02:05:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:05:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:05:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:05:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:05:27 | INFO | valid | epoch 1267 | valid on 'valid' subset | loss 5.249 | nll_loss 2.659 | ppl 6.32 | bleu 55.87 | wps 1851.7 | wpb 933.5 | bsz 59.6 | num_updates 102627 | best_bleu 57.52
2022-08-18 02:05:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1267 @ 102627 updates
2022-08-18 02:05:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1267.pt
2022-08-18 02:05:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1267.pt
2022-08-18 02:06:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1267.pt (epoch 1267 @ 102627 updates, score 55.87) (writing took 62.580204881727695 seconds)
2022-08-18 02:06:30 | INFO | fairseq_cli.train | end of epoch 1267 (average epoch stats below)
2022-08-18 02:06:30 | INFO | train | epoch 1267 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 3907.9 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 102627 | lr 0.000197424 | gnorm 0.403 | train_wall 40 | gb_free 10.3 | wall 100486
2022-08-18 02:06:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:06:30 | INFO | fairseq.trainer | begin training epoch 1268
2022-08-18 02:06:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:07:13 | INFO | train_inner | epoch 1268:     73 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=4272, ups=0.77, wpb=5515.9, bsz=349.7, num_updates=102700, lr=0.000197353, gnorm=0.418, train_wall=50, gb_free=10, wall=100529
2022-08-18 02:07:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:07:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:07:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:07:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:07:27 | INFO | valid | epoch 1268 | valid on 'valid' subset | loss 5.23 | nll_loss 2.642 | ppl 6.24 | bleu 56.24 | wps 1792.9 | wpb 933.5 | bsz 59.6 | num_updates 102708 | best_bleu 57.52
2022-08-18 02:07:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1268 @ 102708 updates
2022-08-18 02:07:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1268.pt
2022-08-18 02:07:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1268.pt
2022-08-18 02:07:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1268.pt (epoch 1268 @ 102708 updates, score 56.24) (writing took 31.755457878112793 seconds)
2022-08-18 02:07:59 | INFO | fairseq_cli.train | end of epoch 1268 (average epoch stats below)
2022-08-18 02:07:59 | INFO | train | epoch 1268 | loss 3.371 | nll_loss 0.339 | ppl 1.26 | wps 4988.2 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 102708 | lr 0.000197346 | gnorm 0.422 | train_wall 40 | gb_free 10.1 | wall 100576
2022-08-18 02:08:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:08:00 | INFO | fairseq.trainer | begin training epoch 1269
2022-08-18 02:08:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:08:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:08:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:08:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:08:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:08:53 | INFO | valid | epoch 1269 | valid on 'valid' subset | loss 5.252 | nll_loss 2.667 | ppl 6.35 | bleu 56 | wps 1894.7 | wpb 933.5 | bsz 59.6 | num_updates 102789 | best_bleu 57.52
2022-08-18 02:08:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1269 @ 102789 updates
2022-08-18 02:08:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1269.pt
2022-08-18 02:08:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1269.pt
2022-08-18 02:09:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1269.pt (epoch 1269 @ 102789 updates, score 56.0) (writing took 36.81716791167855 seconds)
2022-08-18 02:09:30 | INFO | fairseq_cli.train | end of epoch 1269 (average epoch stats below)
2022-08-18 02:09:30 | INFO | train | epoch 1269 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 4961.6 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 102789 | lr 0.000197268 | gnorm 0.438 | train_wall 41 | gb_free 10 | wall 100666
2022-08-18 02:09:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:09:30 | INFO | fairseq.trainer | begin training epoch 1270
2022-08-18 02:09:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:09:37 | INFO | train_inner | epoch 1270:     11 / 81 loss=3.371, nll_loss=0.339, ppl=1.26, wps=3833.2, ups=0.7, wpb=5512.1, bsz=361.6, num_updates=102800, lr=0.000197257, gnorm=0.423, train_wall=50, gb_free=10.1, wall=100673
2022-08-18 02:10:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:10:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:10:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:10:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:10:22 | INFO | valid | epoch 1270 | valid on 'valid' subset | loss 5.249 | nll_loss 2.662 | ppl 6.33 | bleu 56.33 | wps 1863.1 | wpb 933.5 | bsz 59.6 | num_updates 102870 | best_bleu 57.52
2022-08-18 02:10:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1270 @ 102870 updates
2022-08-18 02:10:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1270.pt
2022-08-18 02:10:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1270.pt
2022-08-18 02:10:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1270.pt (epoch 1270 @ 102870 updates, score 56.33) (writing took 37.6324699819088 seconds)
2022-08-18 02:11:00 | INFO | fairseq_cli.train | end of epoch 1270 (average epoch stats below)
2022-08-18 02:11:00 | INFO | train | epoch 1270 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 4964.1 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 102870 | lr 0.00019719 | gnorm 0.453 | train_wall 40 | gb_free 10.1 | wall 100756
2022-08-18 02:11:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:11:00 | INFO | fairseq.trainer | begin training epoch 1271
2022-08-18 02:11:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:11:17 | INFO | train_inner | epoch 1271:     30 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=5534.7, ups=1, wpb=5555, bsz=358.2, num_updates=102900, lr=0.000197162, gnorm=0.416, train_wall=49, gb_free=10, wall=100774
2022-08-18 02:11:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:11:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:11:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:11:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:11:53 | INFO | valid | epoch 1271 | valid on 'valid' subset | loss 5.256 | nll_loss 2.675 | ppl 6.39 | bleu 56.43 | wps 1930.4 | wpb 933.5 | bsz 59.6 | num_updates 102951 | best_bleu 57.52
2022-08-18 02:11:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1271 @ 102951 updates
2022-08-18 02:11:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1271.pt
2022-08-18 02:11:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1271.pt
2022-08-18 02:12:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1271.pt (epoch 1271 @ 102951 updates, score 56.43) (writing took 17.494002893567085 seconds)
2022-08-18 02:12:11 | INFO | fairseq_cli.train | end of epoch 1271 (average epoch stats below)
2022-08-18 02:12:11 | INFO | train | epoch 1271 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6266.4 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 102951 | lr 0.000197113 | gnorm 0.305 | train_wall 41 | gb_free 10 | wall 100827
2022-08-18 02:12:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:12:11 | INFO | fairseq.trainer | begin training epoch 1272
2022-08-18 02:12:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:12:39 | INFO | train_inner | epoch 1272:     49 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=6748.1, ups=1.22, wpb=5512.2, bsz=355.4, num_updates=103000, lr=0.000197066, gnorm=0.456, train_wall=50, gb_free=10, wall=100855
2022-08-18 02:12:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:12:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:12:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:12:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:13:06 | INFO | valid | epoch 1272 | valid on 'valid' subset | loss 5.257 | nll_loss 2.675 | ppl 6.39 | bleu 55.7 | wps 1659 | wpb 933.5 | bsz 59.6 | num_updates 103032 | best_bleu 57.52
2022-08-18 02:13:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1272 @ 103032 updates
2022-08-18 02:13:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1272.pt
2022-08-18 02:13:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1272.pt
2022-08-18 02:13:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1272.pt (epoch 1272 @ 103032 updates, score 55.7) (writing took 17.32729047909379 seconds)
2022-08-18 02:13:23 | INFO | fairseq_cli.train | end of epoch 1272 (average epoch stats below)
2022-08-18 02:13:23 | INFO | train | epoch 1272 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 6201.4 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 103032 | lr 0.000197035 | gnorm 0.525 | train_wall 41 | gb_free 10.2 | wall 100899
2022-08-18 02:13:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:13:23 | INFO | fairseq.trainer | begin training epoch 1273
2022-08-18 02:13:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:13:59 | INFO | train_inner | epoch 1273:     68 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=6928, ups=1.25, wpb=5546.7, bsz=358.3, num_updates=103100, lr=0.00019697, gnorm=0.371, train_wall=50, gb_free=10.1, wall=100935
2022-08-18 02:14:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:14:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:14:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:14:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:14:15 | INFO | valid | epoch 1273 | valid on 'valid' subset | loss 5.241 | nll_loss 2.654 | ppl 6.29 | bleu 56.32 | wps 1782.9 | wpb 933.5 | bsz 59.6 | num_updates 103113 | best_bleu 57.52
2022-08-18 02:14:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1273 @ 103113 updates
2022-08-18 02:14:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1273.pt
2022-08-18 02:14:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1273.pt
2022-08-18 02:14:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1273.pt (epoch 1273 @ 103113 updates, score 56.32) (writing took 35.948012560606 seconds)
2022-08-18 02:14:51 | INFO | fairseq_cli.train | end of epoch 1273 (average epoch stats below)
2022-08-18 02:14:51 | INFO | train | epoch 1273 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 5090.7 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 103113 | lr 0.000196958 | gnorm 0.359 | train_wall 40 | gb_free 10.1 | wall 100987
2022-08-18 02:14:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:14:51 | INFO | fairseq.trainer | begin training epoch 1274
2022-08-18 02:14:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:15:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:15:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:15:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:15:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:15:42 | INFO | valid | epoch 1274 | valid on 'valid' subset | loss 5.236 | nll_loss 2.648 | ppl 6.27 | bleu 56.45 | wps 1966.6 | wpb 933.5 | bsz 59.6 | num_updates 103194 | best_bleu 57.52
2022-08-18 02:15:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1274 @ 103194 updates
2022-08-18 02:15:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1274.pt
2022-08-18 02:15:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1274.pt
2022-08-18 02:16:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1274.pt (epoch 1274 @ 103194 updates, score 56.45) (writing took 18.923090375959873 seconds)
2022-08-18 02:16:01 | INFO | fairseq_cli.train | end of epoch 1274 (average epoch stats below)
2022-08-18 02:16:01 | INFO | train | epoch 1274 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6402.4 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 103194 | lr 0.000196881 | gnorm 0.333 | train_wall 40 | gb_free 10.1 | wall 101057
2022-08-18 02:16:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:16:01 | INFO | fairseq.trainer | begin training epoch 1275
2022-08-18 02:16:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:16:05 | INFO | train_inner | epoch 1275:      6 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=4331.3, ups=0.79, wpb=5466.9, bsz=356.7, num_updates=103200, lr=0.000196875, gnorm=0.326, train_wall=49, gb_free=10.1, wall=101062
2022-08-18 02:16:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:16:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:16:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:16:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:16:53 | INFO | valid | epoch 1275 | valid on 'valid' subset | loss 5.234 | nll_loss 2.645 | ppl 6.26 | bleu 56.15 | wps 1897.6 | wpb 933.5 | bsz 59.6 | num_updates 103275 | best_bleu 57.52
2022-08-18 02:16:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1275 @ 103275 updates
2022-08-18 02:16:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1275.pt
2022-08-18 02:16:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1275.pt
2022-08-18 02:17:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1275.pt (epoch 1275 @ 103275 updates, score 56.15) (writing took 15.558345388621092 seconds)
2022-08-18 02:17:09 | INFO | fairseq_cli.train | end of epoch 1275 (average epoch stats below)
2022-08-18 02:17:09 | INFO | train | epoch 1275 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 6585.2 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 103275 | lr 0.000196803 | gnorm 0.415 | train_wall 40 | gb_free 10.1 | wall 101125
2022-08-18 02:17:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:17:09 | INFO | fairseq.trainer | begin training epoch 1276
2022-08-18 02:17:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:17:25 | INFO | train_inner | epoch 1276:     25 / 81 loss=3.372, nll_loss=0.34, ppl=1.27, wps=6974.1, ups=1.26, wpb=5519.1, bsz=355.8, num_updates=103300, lr=0.000196779, gnorm=0.439, train_wall=50, gb_free=10.1, wall=101141
2022-08-18 02:17:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:17:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:17:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:17:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:18:07 | INFO | valid | epoch 1276 | valid on 'valid' subset | loss 5.253 | nll_loss 2.667 | ppl 6.35 | bleu 56.51 | wps 1924 | wpb 933.5 | bsz 59.6 | num_updates 103356 | best_bleu 57.52
2022-08-18 02:18:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1276 @ 103356 updates
2022-08-18 02:18:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1276.pt
2022-08-18 02:18:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1276.pt
2022-08-18 02:18:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1276.pt (epoch 1276 @ 103356 updates, score 56.51) (writing took 30.937738426029682 seconds)
2022-08-18 02:18:38 | INFO | fairseq_cli.train | end of epoch 1276 (average epoch stats below)
2022-08-18 02:18:38 | INFO | train | epoch 1276 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 5042.8 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 103356 | lr 0.000196726 | gnorm 0.396 | train_wall 41 | gb_free 10.1 | wall 101214
2022-08-18 02:18:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:18:38 | INFO | fairseq.trainer | begin training epoch 1277
2022-08-18 02:18:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:19:02 | INFO | train_inner | epoch 1277:     44 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=5671.4, ups=1.02, wpb=5535.5, bsz=361.6, num_updates=103400, lr=0.000196684, gnorm=0.32, train_wall=50, gb_free=10, wall=101238
2022-08-18 02:19:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:19:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:19:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:19:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:19:30 | INFO | valid | epoch 1277 | valid on 'valid' subset | loss 5.251 | nll_loss 2.667 | ppl 6.35 | bleu 56.55 | wps 1767.8 | wpb 933.5 | bsz 59.6 | num_updates 103437 | best_bleu 57.52
2022-08-18 02:19:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1277 @ 103437 updates
2022-08-18 02:19:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1277.pt
2022-08-18 02:19:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1277.pt
2022-08-18 02:19:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1277.pt (epoch 1277 @ 103437 updates, score 56.55) (writing took 19.545949183404446 seconds)
2022-08-18 02:19:50 | INFO | fairseq_cli.train | end of epoch 1277 (average epoch stats below)
2022-08-18 02:19:50 | INFO | train | epoch 1277 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6182.1 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 103437 | lr 0.000196649 | gnorm 0.297 | train_wall 40 | gb_free 10.2 | wall 101286
2022-08-18 02:19:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:19:50 | INFO | fairseq.trainer | begin training epoch 1278
2022-08-18 02:19:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:20:24 | INFO | train_inner | epoch 1278:     63 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=6822.2, ups=1.23, wpb=5555.7, bsz=360.4, num_updates=103500, lr=0.000196589, gnorm=0.313, train_wall=50, gb_free=10.1, wall=101320
2022-08-18 02:20:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:20:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:20:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:20:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:20:41 | INFO | valid | epoch 1278 | valid on 'valid' subset | loss 5.247 | nll_loss 2.662 | ppl 6.33 | bleu 56.63 | wps 1902 | wpb 933.5 | bsz 59.6 | num_updates 103518 | best_bleu 57.52
2022-08-18 02:20:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1278 @ 103518 updates
2022-08-18 02:20:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1278.pt
2022-08-18 02:20:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1278.pt
2022-08-18 02:20:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1278.pt (epoch 1278 @ 103518 updates, score 56.63) (writing took 14.322118755429983 seconds)
2022-08-18 02:20:56 | INFO | fairseq_cli.train | end of epoch 1278 (average epoch stats below)
2022-08-18 02:20:56 | INFO | train | epoch 1278 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6794 | ups 1.23 | wpb 5523.2 | bsz 358 | num_updates 103518 | lr 0.000196572 | gnorm 0.346 | train_wall 40 | gb_free 10.3 | wall 101352
2022-08-18 02:20:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:20:56 | INFO | fairseq.trainer | begin training epoch 1279
2022-08-18 02:20:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:21:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:21:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:21:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:21:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:21:48 | INFO | valid | epoch 1279 | valid on 'valid' subset | loss 5.24 | nll_loss 2.65 | ppl 6.28 | bleu 56.89 | wps 1600.7 | wpb 933.5 | bsz 59.6 | num_updates 103599 | best_bleu 57.52
2022-08-18 02:21:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1279 @ 103599 updates
2022-08-18 02:21:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1279.pt
2022-08-18 02:21:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1279.pt
2022-08-18 02:22:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1279.pt (epoch 1279 @ 103599 updates, score 56.89) (writing took 39.57744712755084 seconds)
2022-08-18 02:22:28 | INFO | fairseq_cli.train | end of epoch 1279 (average epoch stats below)
2022-08-18 02:22:28 | INFO | train | epoch 1279 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 4878.1 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 103599 | lr 0.000196495 | gnorm 0.458 | train_wall 40 | gb_free 10.2 | wall 101444
2022-08-18 02:22:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:22:28 | INFO | fairseq.trainer | begin training epoch 1280
2022-08-18 02:22:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:22:29 | INFO | train_inner | epoch 1280:      1 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=4367.2, ups=0.79, wpb=5493.9, bsz=354.6, num_updates=103600, lr=0.000196494, gnorm=0.461, train_wall=49, gb_free=10.1, wall=101446
2022-08-18 02:23:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:23:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:23:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:23:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:23:27 | INFO | valid | epoch 1280 | valid on 'valid' subset | loss 5.239 | nll_loss 2.653 | ppl 6.29 | bleu 56.33 | wps 1458.9 | wpb 933.5 | bsz 59.6 | num_updates 103680 | best_bleu 57.52
2022-08-18 02:23:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1280 @ 103680 updates
2022-08-18 02:23:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1280.pt
2022-08-18 02:23:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1280.pt
2022-08-18 02:23:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1280.pt (epoch 1280 @ 103680 updates, score 56.33) (writing took 19.99212261661887 seconds)
2022-08-18 02:23:47 | INFO | fairseq_cli.train | end of epoch 1280 (average epoch stats below)
2022-08-18 02:23:47 | INFO | train | epoch 1280 | loss 3.371 | nll_loss 0.339 | ppl 1.26 | wps 5646.5 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 103680 | lr 0.000196419 | gnorm 0.361 | train_wall 43 | gb_free 10.4 | wall 101523
2022-08-18 02:23:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:23:47 | INFO | fairseq.trainer | begin training epoch 1281
2022-08-18 02:23:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:23:59 | INFO | train_inner | epoch 1281:     20 / 81 loss=3.371, nll_loss=0.339, ppl=1.26, wps=6197.2, ups=1.12, wpb=5535.3, bsz=356.6, num_updates=103700, lr=0.0001964, gnorm=0.374, train_wall=52, gb_free=10.1, wall=101535
2022-08-18 02:25:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:25:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:25:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:25:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:25:17 | INFO | valid | epoch 1281 | valid on 'valid' subset | loss 5.255 | nll_loss 2.665 | ppl 6.34 | bleu 56.12 | wps 1172.4 | wpb 933.5 | bsz 59.6 | num_updates 103761 | best_bleu 57.52
2022-08-18 02:25:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1281 @ 103761 updates
2022-08-18 02:25:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1281.pt
2022-08-18 02:25:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1281.pt
2022-08-18 02:25:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1281.pt (epoch 1281 @ 103761 updates, score 56.12) (writing took 14.731814097613096 seconds)
2022-08-18 02:25:32 | INFO | fairseq_cli.train | end of epoch 1281 (average epoch stats below)
2022-08-18 02:25:32 | INFO | train | epoch 1281 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 4264.3 | ups 0.77 | wpb 5523.2 | bsz 358 | num_updates 103761 | lr 0.000196342 | gnorm 0.382 | train_wall 71 | gb_free 10.1 | wall 101628
2022-08-18 02:25:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:25:32 | INFO | fairseq.trainer | begin training epoch 1282
2022-08-18 02:25:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:25:53 | INFO | train_inner | epoch 1282:     39 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=4814.7, ups=0.87, wpb=5517.4, bsz=361.3, num_updates=103800, lr=0.000196305, gnorm=0.396, train_wall=80, gb_free=10, wall=101649
2022-08-18 02:26:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:26:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:26:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:26:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:26:24 | INFO | valid | epoch 1282 | valid on 'valid' subset | loss 5.241 | nll_loss 2.653 | ppl 6.29 | bleu 56.06 | wps 1885.9 | wpb 933.5 | bsz 59.6 | num_updates 103842 | best_bleu 57.52
2022-08-18 02:26:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1282 @ 103842 updates
2022-08-18 02:26:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1282.pt
2022-08-18 02:26:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1282.pt
2022-08-18 02:27:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1282.pt (epoch 1282 @ 103842 updates, score 56.06) (writing took 43.78252239897847 seconds)
2022-08-18 02:27:07 | INFO | fairseq_cli.train | end of epoch 1282 (average epoch stats below)
2022-08-18 02:27:07 | INFO | train | epoch 1282 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 4675.4 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 103842 | lr 0.000196265 | gnorm 0.432 | train_wall 40 | gb_free 10.1 | wall 101724
2022-08-18 02:27:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:27:08 | INFO | fairseq.trainer | begin training epoch 1283
2022-08-18 02:27:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:27:37 | INFO | train_inner | epoch 1283:     58 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=5343.9, ups=0.96, wpb=5542.4, bsz=357.4, num_updates=103900, lr=0.00019621, gnorm=0.416, train_wall=48, gb_free=10.1, wall=101753
2022-08-18 02:27:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:27:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:27:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:27:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:27:58 | INFO | valid | epoch 1283 | valid on 'valid' subset | loss 5.238 | nll_loss 2.648 | ppl 6.27 | bleu 56.65 | wps 1748.4 | wpb 933.5 | bsz 59.6 | num_updates 103923 | best_bleu 57.52
2022-08-18 02:27:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1283 @ 103923 updates
2022-08-18 02:27:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1283.pt
2022-08-18 02:28:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1283.pt
2022-08-18 02:28:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1283.pt (epoch 1283 @ 103923 updates, score 56.65) (writing took 35.052650418132544 seconds)
2022-08-18 02:28:33 | INFO | fairseq_cli.train | end of epoch 1283 (average epoch stats below)
2022-08-18 02:28:33 | INFO | train | epoch 1283 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 5210.8 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 103923 | lr 0.000196189 | gnorm 0.426 | train_wall 38 | gb_free 10.1 | wall 101810
2022-08-18 02:28:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:28:34 | INFO | fairseq.trainer | begin training epoch 1284
2022-08-18 02:28:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:29:12 | INFO | train_inner | epoch 1284:     77 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=5787, ups=1.05, wpb=5515.3, bsz=359.3, num_updates=104000, lr=0.000196116, gnorm=0.345, train_wall=48, gb_free=10.1, wall=101849
2022-08-18 02:29:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:29:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:29:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:29:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:29:24 | INFO | valid | epoch 1284 | valid on 'valid' subset | loss 5.274 | nll_loss 2.691 | ppl 6.46 | bleu 56.37 | wps 1780.2 | wpb 933.5 | bsz 59.6 | num_updates 104004 | best_bleu 57.52
2022-08-18 02:29:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1284 @ 104004 updates
2022-08-18 02:29:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1284.pt
2022-08-18 02:29:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1284.pt
2022-08-18 02:29:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1284.pt (epoch 1284 @ 104004 updates, score 56.37) (writing took 17.993007257580757 seconds)
2022-08-18 02:29:42 | INFO | fairseq_cli.train | end of epoch 1284 (average epoch stats below)
2022-08-18 02:29:42 | INFO | train | epoch 1284 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6527.8 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 104004 | lr 0.000196112 | gnorm 0.305 | train_wall 39 | gb_free 10.2 | wall 101878
2022-08-18 02:29:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:29:42 | INFO | fairseq.trainer | begin training epoch 1285
2022-08-18 02:29:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:30:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:30:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:30:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:30:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:30:33 | INFO | valid | epoch 1285 | valid on 'valid' subset | loss 5.263 | nll_loss 2.683 | ppl 6.42 | bleu 56.26 | wps 1918.7 | wpb 933.5 | bsz 59.6 | num_updates 104085 | best_bleu 57.52
2022-08-18 02:30:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1285 @ 104085 updates
2022-08-18 02:30:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1285.pt
2022-08-18 02:30:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1285.pt
2022-08-18 02:30:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1285.pt (epoch 1285 @ 104085 updates, score 56.26) (writing took 18.527296237647533 seconds)
2022-08-18 02:30:51 | INFO | fairseq_cli.train | end of epoch 1285 (average epoch stats below)
2022-08-18 02:30:51 | INFO | train | epoch 1285 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6430.7 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 104085 | lr 0.000196036 | gnorm 0.389 | train_wall 40 | gb_free 10.2 | wall 101948
2022-08-18 02:30:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:30:52 | INFO | fairseq.trainer | begin training epoch 1286
2022-08-18 02:30:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:31:00 | INFO | train_inner | epoch 1286:     15 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=5107.2, ups=0.93, wpb=5500.4, bsz=354.5, num_updates=104100, lr=0.000196022, gnorm=0.374, train_wall=49, gb_free=10, wall=101956
2022-08-18 02:31:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:31:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:31:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:31:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:31:41 | INFO | valid | epoch 1286 | valid on 'valid' subset | loss 5.259 | nll_loss 2.675 | ppl 6.39 | bleu 56.78 | wps 1827.5 | wpb 933.5 | bsz 59.6 | num_updates 104166 | best_bleu 57.52
2022-08-18 02:31:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1286 @ 104166 updates
2022-08-18 02:31:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1286.pt
2022-08-18 02:31:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1286.pt
2022-08-18 02:31:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1286.pt (epoch 1286 @ 104166 updates, score 56.78) (writing took 2.404514480382204 seconds)
2022-08-18 02:31:44 | INFO | fairseq_cli.train | end of epoch 1286 (average epoch stats below)
2022-08-18 02:31:44 | INFO | train | epoch 1286 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 8516.5 | ups 1.54 | wpb 5523.2 | bsz 358 | num_updates 104166 | lr 0.00019596 | gnorm 0.353 | train_wall 39 | gb_free 10.1 | wall 102000
2022-08-18 02:31:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:31:44 | INFO | fairseq.trainer | begin training epoch 1287
2022-08-18 02:31:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:32:04 | INFO | train_inner | epoch 1287:     34 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=8705.5, ups=1.57, wpb=5539.5, bsz=364, num_updates=104200, lr=0.000195928, gnorm=0.393, train_wall=49, gb_free=10.1, wall=102020
2022-08-18 02:32:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:32:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:32:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:32:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:32:37 | INFO | valid | epoch 1287 | valid on 'valid' subset | loss 5.253 | nll_loss 2.669 | ppl 6.36 | bleu 56.66 | wps 1799.8 | wpb 933.5 | bsz 59.6 | num_updates 104247 | best_bleu 57.52
2022-08-18 02:32:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1287 @ 104247 updates
2022-08-18 02:32:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1287.pt
2022-08-18 02:32:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1287.pt
2022-08-18 02:33:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1287.pt (epoch 1287 @ 104247 updates, score 56.66) (writing took 35.72771182283759 seconds)
2022-08-18 02:33:13 | INFO | fairseq_cli.train | end of epoch 1287 (average epoch stats below)
2022-08-18 02:33:13 | INFO | train | epoch 1287 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5012.4 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 104247 | lr 0.000195884 | gnorm 0.424 | train_wall 40 | gb_free 10.1 | wall 102089
2022-08-18 02:33:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:33:13 | INFO | fairseq.trainer | begin training epoch 1288
2022-08-18 02:33:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:33:40 | INFO | train_inner | epoch 1288:     53 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=5727.3, ups=1.03, wpb=5538.9, bsz=354, num_updates=104300, lr=0.000195834, gnorm=0.393, train_wall=48, gb_free=10, wall=102117
2022-08-18 02:33:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:33:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:33:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:33:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:34:04 | INFO | valid | epoch 1288 | valid on 'valid' subset | loss 5.259 | nll_loss 2.678 | ppl 6.4 | bleu 56.18 | wps 1806.6 | wpb 933.5 | bsz 59.6 | num_updates 104328 | best_bleu 57.52
2022-08-18 02:34:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1288 @ 104328 updates
2022-08-18 02:34:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1288.pt
2022-08-18 02:34:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1288.pt
2022-08-18 02:34:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1288.pt (epoch 1288 @ 104328 updates, score 56.18) (writing took 39.7528278157115 seconds)
2022-08-18 02:34:43 | INFO | fairseq_cli.train | end of epoch 1288 (average epoch stats below)
2022-08-18 02:34:43 | INFO | train | epoch 1288 | loss 3.371 | nll_loss 0.339 | ppl 1.26 | wps 4960.2 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 104328 | lr 0.000195808 | gnorm 0.381 | train_wall 38 | gb_free 10.1 | wall 102180
2022-08-18 02:34:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:34:44 | INFO | fairseq.trainer | begin training epoch 1289
2022-08-18 02:34:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:35:25 | INFO | train_inner | epoch 1289:     72 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=5270.9, ups=0.96, wpb=5517.1, bsz=360.1, num_updates=104400, lr=0.00019574, gnorm=0.378, train_wall=49, gb_free=10.1, wall=102221
2022-08-18 02:35:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:35:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:35:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:35:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:35:39 | INFO | valid | epoch 1289 | valid on 'valid' subset | loss 5.273 | nll_loss 2.694 | ppl 6.47 | bleu 55.91 | wps 1869 | wpb 933.5 | bsz 59.6 | num_updates 104409 | best_bleu 57.52
2022-08-18 02:35:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1289 @ 104409 updates
2022-08-18 02:35:39 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1289.pt
2022-08-18 02:35:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1289.pt
2022-08-18 02:35:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1289.pt (epoch 1289 @ 104409 updates, score 55.91) (writing took 19.60816754773259 seconds)
2022-08-18 02:35:59 | INFO | fairseq_cli.train | end of epoch 1289 (average epoch stats below)
2022-08-18 02:35:59 | INFO | train | epoch 1289 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 5957.7 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 104409 | lr 0.000195732 | gnorm 0.375 | train_wall 40 | gb_free 10.1 | wall 102255
2022-08-18 02:35:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:35:59 | INFO | fairseq.trainer | begin training epoch 1290
2022-08-18 02:35:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:36:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:36:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:36:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:36:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:36:49 | INFO | valid | epoch 1290 | valid on 'valid' subset | loss 5.239 | nll_loss 2.652 | ppl 6.28 | bleu 57.16 | wps 1776.8 | wpb 933.5 | bsz 59.6 | num_updates 104490 | best_bleu 57.52
2022-08-18 02:36:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1290 @ 104490 updates
2022-08-18 02:36:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1290.pt
2022-08-18 02:36:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1290.pt
2022-08-18 02:37:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1290.pt (epoch 1290 @ 104490 updates, score 57.16) (writing took 24.525067448616028 seconds)
2022-08-18 02:37:14 | INFO | fairseq_cli.train | end of epoch 1290 (average epoch stats below)
2022-08-18 02:37:14 | INFO | train | epoch 1290 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5919 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 104490 | lr 0.000195656 | gnorm 0.31 | train_wall 39 | gb_free 10.2 | wall 102330
2022-08-18 02:37:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:37:14 | INFO | fairseq.trainer | begin training epoch 1291
2022-08-18 02:37:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:37:21 | INFO | train_inner | epoch 1291:     10 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=4755.3, ups=0.86, wpb=5503.8, bsz=357, num_updates=104500, lr=0.000195646, gnorm=0.309, train_wall=48, gb_free=10, wall=102337
2022-08-18 02:37:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:37:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:37:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:37:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:38:06 | INFO | valid | epoch 1291 | valid on 'valid' subset | loss 5.247 | nll_loss 2.663 | ppl 6.33 | bleu 56.67 | wps 1760.7 | wpb 933.5 | bsz 59.6 | num_updates 104571 | best_bleu 57.52
2022-08-18 02:38:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1291 @ 104571 updates
2022-08-18 02:38:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1291.pt
2022-08-18 02:38:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1291.pt
2022-08-18 02:38:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1291.pt (epoch 1291 @ 104571 updates, score 56.67) (writing took 33.026047844439745 seconds)
2022-08-18 02:38:39 | INFO | fairseq_cli.train | end of epoch 1291 (average epoch stats below)
2022-08-18 02:38:39 | INFO | train | epoch 1291 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5266.2 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 104571 | lr 0.00019558 | gnorm 0.348 | train_wall 39 | gb_free 10.1 | wall 102415
2022-08-18 02:38:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:38:39 | INFO | fairseq.trainer | begin training epoch 1292
2022-08-18 02:38:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:38:55 | INFO | train_inner | epoch 1292:     29 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=5845.2, ups=1.06, wpb=5517.3, bsz=353.8, num_updates=104600, lr=0.000195553, gnorm=0.352, train_wall=49, gb_free=10, wall=102431
2022-08-18 02:39:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:39:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:39:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:39:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:39:32 | INFO | valid | epoch 1292 | valid on 'valid' subset | loss 5.252 | nll_loss 2.663 | ppl 6.34 | bleu 56.11 | wps 1623.4 | wpb 933.5 | bsz 59.6 | num_updates 104652 | best_bleu 57.52
2022-08-18 02:39:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1292 @ 104652 updates
2022-08-18 02:39:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1292.pt
2022-08-18 02:39:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1292.pt
2022-08-18 02:39:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1292.pt (epoch 1292 @ 104652 updates, score 56.11) (writing took 21.254765275865793 seconds)
2022-08-18 02:39:53 | INFO | fairseq_cli.train | end of epoch 1292 (average epoch stats below)
2022-08-18 02:39:53 | INFO | train | epoch 1292 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 6039.6 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 104652 | lr 0.000195504 | gnorm 0.433 | train_wall 40 | gb_free 10.2 | wall 102489
2022-08-18 02:39:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:39:53 | INFO | fairseq.trainer | begin training epoch 1293
2022-08-18 02:39:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:40:19 | INFO | train_inner | epoch 1293:     48 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=6649.6, ups=1.2, wpb=5543, bsz=360.2, num_updates=104700, lr=0.000195459, gnorm=0.472, train_wall=49, gb_free=10.1, wall=102515
2022-08-18 02:40:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:40:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:40:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:40:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:40:46 | INFO | valid | epoch 1293 | valid on 'valid' subset | loss 5.256 | nll_loss 2.674 | ppl 6.38 | bleu 56.87 | wps 1653.9 | wpb 933.5 | bsz 59.6 | num_updates 104733 | best_bleu 57.52
2022-08-18 02:40:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1293 @ 104733 updates
2022-08-18 02:40:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1293.pt
2022-08-18 02:40:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1293.pt
2022-08-18 02:41:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1293.pt (epoch 1293 @ 104733 updates, score 56.87) (writing took 20.38741923868656 seconds)
2022-08-18 02:41:06 | INFO | fairseq_cli.train | end of epoch 1293 (average epoch stats below)
2022-08-18 02:41:06 | INFO | train | epoch 1293 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6135.4 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 104733 | lr 0.000195429 | gnorm 0.445 | train_wall 40 | gb_free 10.1 | wall 102562
2022-08-18 02:41:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:41:06 | INFO | fairseq.trainer | begin training epoch 1294
2022-08-18 02:41:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:41:41 | INFO | train_inner | epoch 1294:     67 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=6736, ups=1.21, wpb=5549.5, bsz=362.7, num_updates=104800, lr=0.000195366, gnorm=0.373, train_wall=49, gb_free=10.1, wall=102597
2022-08-18 02:41:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:41:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:41:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:41:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:41:57 | INFO | valid | epoch 1294 | valid on 'valid' subset | loss 5.25 | nll_loss 2.665 | ppl 6.34 | bleu 56.35 | wps 1759.8 | wpb 933.5 | bsz 59.6 | num_updates 104814 | best_bleu 57.52
2022-08-18 02:41:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1294 @ 104814 updates
2022-08-18 02:41:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1294.pt
2022-08-18 02:41:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1294.pt
2022-08-18 02:42:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1294.pt (epoch 1294 @ 104814 updates, score 56.35) (writing took 34.526567336171865 seconds)
2022-08-18 02:42:32 | INFO | fairseq_cli.train | end of epoch 1294 (average epoch stats below)
2022-08-18 02:42:32 | INFO | train | epoch 1294 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5198.8 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 104814 | lr 0.000195353 | gnorm 0.382 | train_wall 39 | gb_free 10.2 | wall 102648
2022-08-18 02:42:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:42:32 | INFO | fairseq.trainer | begin training epoch 1295
2022-08-18 02:42:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:43:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:43:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:43:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:43:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:43:23 | INFO | valid | epoch 1295 | valid on 'valid' subset | loss 5.246 | nll_loss 2.66 | ppl 6.32 | bleu 56.51 | wps 1857.4 | wpb 933.5 | bsz 59.6 | num_updates 104895 | best_bleu 57.52
2022-08-18 02:43:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1295 @ 104895 updates
2022-08-18 02:43:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1295.pt
2022-08-18 02:43:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1295.pt
2022-08-18 02:43:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1295.pt (epoch 1295 @ 104895 updates, score 56.51) (writing took 15.338754817843437 seconds)
2022-08-18 02:43:39 | INFO | fairseq_cli.train | end of epoch 1295 (average epoch stats below)
2022-08-18 02:43:39 | INFO | train | epoch 1295 | loss 3.37 | nll_loss 0.339 | ppl 1.27 | wps 6729 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 104895 | lr 0.000195278 | gnorm 0.397 | train_wall 39 | gb_free 10.1 | wall 102715
2022-08-18 02:43:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:43:39 | INFO | fairseq.trainer | begin training epoch 1296
2022-08-18 02:43:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:43:42 | INFO | train_inner | epoch 1296:      5 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=4525.8, ups=0.82, wpb=5493.1, bsz=353.7, num_updates=104900, lr=0.000195273, gnorm=0.41, train_wall=48, gb_free=10.1, wall=102719
2022-08-18 02:44:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:44:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:44:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:44:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:44:29 | INFO | valid | epoch 1296 | valid on 'valid' subset | loss 5.258 | nll_loss 2.671 | ppl 6.37 | bleu 55.88 | wps 1883.1 | wpb 933.5 | bsz 59.6 | num_updates 104976 | best_bleu 57.52
2022-08-18 02:44:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1296 @ 104976 updates
2022-08-18 02:44:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1296.pt
2022-08-18 02:44:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1296.pt
2022-08-18 02:44:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1296.pt (epoch 1296 @ 104976 updates, score 55.88) (writing took 14.724745478481054 seconds)
2022-08-18 02:44:44 | INFO | fairseq_cli.train | end of epoch 1296 (average epoch stats below)
2022-08-18 02:44:44 | INFO | train | epoch 1296 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 6870.9 | ups 1.24 | wpb 5523.2 | bsz 358 | num_updates 104976 | lr 0.000195202 | gnorm 0.454 | train_wall 39 | gb_free 10.2 | wall 102780
2022-08-18 02:44:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:44:44 | INFO | fairseq.trainer | begin training epoch 1297
2022-08-18 02:44:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:44:57 | INFO | train_inner | epoch 1297:     24 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=7404.8, ups=1.34, wpb=5528.7, bsz=358.7, num_updates=105000, lr=0.00019518, gnorm=0.47, train_wall=48, gb_free=10.1, wall=102793
2022-08-18 02:45:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:45:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:45:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:45:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:45:35 | INFO | valid | epoch 1297 | valid on 'valid' subset | loss 5.274 | nll_loss 2.693 | ppl 6.47 | bleu 55.46 | wps 1863.3 | wpb 933.5 | bsz 59.6 | num_updates 105057 | best_bleu 57.52
2022-08-18 02:45:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1297 @ 105057 updates
2022-08-18 02:45:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1297.pt
2022-08-18 02:45:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1297.pt
2022-08-18 02:46:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1297.pt (epoch 1297 @ 105057 updates, score 55.46) (writing took 37.658894404768944 seconds)
2022-08-18 02:46:13 | INFO | fairseq_cli.train | end of epoch 1297 (average epoch stats below)
2022-08-18 02:46:13 | INFO | train | epoch 1297 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 5006 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 105057 | lr 0.000195127 | gnorm 0.398 | train_wall 40 | gb_free 10.1 | wall 102869
2022-08-18 02:46:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:46:13 | INFO | fairseq.trainer | begin training epoch 1298
2022-08-18 02:46:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:46:36 | INFO | train_inner | epoch 1298:     43 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=5578.6, ups=1.01, wpb=5503.7, bsz=356.6, num_updates=105100, lr=0.000195087, gnorm=0.394, train_wall=49, gb_free=10.1, wall=102892
2022-08-18 02:46:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:46:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:46:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:46:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:47:04 | INFO | valid | epoch 1298 | valid on 'valid' subset | loss 5.259 | nll_loss 2.673 | ppl 6.38 | bleu 55.95 | wps 1850.1 | wpb 933.5 | bsz 59.6 | num_updates 105138 | best_bleu 57.52
2022-08-18 02:47:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1298 @ 105138 updates
2022-08-18 02:47:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1298.pt
2022-08-18 02:47:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1298.pt
2022-08-18 02:47:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1298.pt (epoch 1298 @ 105138 updates, score 55.95) (writing took 16.482808612287045 seconds)
2022-08-18 02:47:20 | INFO | fairseq_cli.train | end of epoch 1298 (average epoch stats below)
2022-08-18 02:47:20 | INFO | train | epoch 1298 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 6662.6 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 105138 | lr 0.000195052 | gnorm 0.465 | train_wall 39 | gb_free 10.1 | wall 102936
2022-08-18 02:47:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:47:20 | INFO | fairseq.trainer | begin training epoch 1299
2022-08-18 02:47:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:47:52 | INFO | train_inner | epoch 1299:     62 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=7270.7, ups=1.31, wpb=5529.4, bsz=358.3, num_updates=105200, lr=0.000194994, gnorm=0.432, train_wall=48, gb_free=10.1, wall=102968
2022-08-18 02:48:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:48:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:48:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:48:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:48:10 | INFO | valid | epoch 1299 | valid on 'valid' subset | loss 5.261 | nll_loss 2.68 | ppl 6.41 | bleu 55.96 | wps 1863.3 | wpb 933.5 | bsz 59.6 | num_updates 105219 | best_bleu 57.52
2022-08-18 02:48:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1299 @ 105219 updates
2022-08-18 02:48:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1299.pt
2022-08-18 02:48:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1299.pt
2022-08-18 02:48:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1299.pt (epoch 1299 @ 105219 updates, score 55.96) (writing took 31.340833958238363 seconds)
2022-08-18 02:48:42 | INFO | fairseq_cli.train | end of epoch 1299 (average epoch stats below)
2022-08-18 02:48:42 | INFO | train | epoch 1299 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5483.6 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 105219 | lr 0.000194977 | gnorm 0.363 | train_wall 39 | gb_free 10.1 | wall 103018
2022-08-18 02:48:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:48:42 | INFO | fairseq.trainer | begin training epoch 1300
2022-08-18 02:48:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:49:23 | INFO | train_inner | epoch 1300:     81 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=6050.2, ups=1.1, wpb=5518.2, bsz=358.3, num_updates=105300, lr=0.000194902, gnorm=0.337, train_wall=48, gb_free=10.1, wall=103059
2022-08-18 02:49:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:49:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:49:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:49:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:49:32 | INFO | valid | epoch 1300 | valid on 'valid' subset | loss 5.255 | nll_loss 2.673 | ppl 6.38 | bleu 56.25 | wps 1876 | wpb 933.5 | bsz 59.6 | num_updates 105300 | best_bleu 57.52
2022-08-18 02:49:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1300 @ 105300 updates
2022-08-18 02:49:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1300.pt
2022-08-18 02:49:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1300.pt
2022-08-18 02:50:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1300.pt (epoch 1300 @ 105300 updates, score 56.25) (writing took 36.26830884069204 seconds)
2022-08-18 02:50:09 | INFO | fairseq_cli.train | end of epoch 1300 (average epoch stats below)
2022-08-18 02:50:09 | INFO | train | epoch 1300 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5158.4 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 105300 | lr 0.000194902 | gnorm 0.352 | train_wall 39 | gb_free 10.1 | wall 103105
2022-08-18 02:50:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:50:09 | INFO | fairseq.trainer | begin training epoch 1301
2022-08-18 02:50:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:50:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:50:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:50:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:50:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:51:00 | INFO | valid | epoch 1301 | valid on 'valid' subset | loss 5.251 | nll_loss 2.665 | ppl 6.34 | bleu 56.41 | wps 1722.1 | wpb 933.5 | bsz 59.6 | num_updates 105381 | best_bleu 57.52
2022-08-18 02:51:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1301 @ 105381 updates
2022-08-18 02:51:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1301.pt
2022-08-18 02:51:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1301.pt
2022-08-18 02:51:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1301.pt (epoch 1301 @ 105381 updates, score 56.41) (writing took 15.607164796441793 seconds)
2022-08-18 02:51:16 | INFO | fairseq_cli.train | end of epoch 1301 (average epoch stats below)
2022-08-18 02:51:16 | INFO | train | epoch 1301 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 6660.3 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 105381 | lr 0.000194827 | gnorm 0.418 | train_wall 39 | gb_free 10.3 | wall 103172
2022-08-18 02:51:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:51:16 | INFO | fairseq.trainer | begin training epoch 1302
2022-08-18 02:51:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:51:26 | INFO | train_inner | epoch 1302:     19 / 81 loss=3.371, nll_loss=0.339, ppl=1.26, wps=4466, ups=0.81, wpb=5497, bsz=357.9, num_updates=105400, lr=0.000194809, gnorm=0.418, train_wall=48, gb_free=10.1, wall=103182
2022-08-18 02:51:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:51:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:51:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:51:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:52:05 | INFO | valid | epoch 1302 | valid on 'valid' subset | loss 5.248 | nll_loss 2.661 | ppl 6.33 | bleu 56.41 | wps 1910.4 | wpb 933.5 | bsz 59.6 | num_updates 105462 | best_bleu 57.52
2022-08-18 02:52:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1302 @ 105462 updates
2022-08-18 02:52:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1302.pt
2022-08-18 02:52:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1302.pt
2022-08-18 02:52:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1302.pt (epoch 1302 @ 105462 updates, score 56.41) (writing took 20.90339406207204 seconds)
2022-08-18 02:52:26 | INFO | fairseq_cli.train | end of epoch 1302 (average epoch stats below)
2022-08-18 02:52:27 | INFO | train | epoch 1302 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6325.4 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 105462 | lr 0.000194752 | gnorm 3.172 | train_wall 39 | gb_free 10.2 | wall 103243
2022-08-18 02:52:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:52:27 | INFO | fairseq.trainer | begin training epoch 1303
2022-08-18 02:52:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:52:47 | INFO | train_inner | epoch 1303:     38 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=6834.4, ups=1.23, wpb=5554.7, bsz=355.9, num_updates=105500, lr=0.000194717, gnorm=2.637, train_wall=49, gb_free=10.1, wall=103263
2022-08-18 02:53:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:53:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:53:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:53:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:53:19 | INFO | valid | epoch 1303 | valid on 'valid' subset | loss 5.251 | nll_loss 2.664 | ppl 6.34 | bleu 55.51 | wps 1773 | wpb 933.5 | bsz 59.6 | num_updates 105543 | best_bleu 57.52
2022-08-18 02:53:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1303 @ 105543 updates
2022-08-18 02:53:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1303.pt
2022-08-18 02:53:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1303.pt
2022-08-18 02:54:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1303.pt (epoch 1303 @ 105543 updates, score 55.51) (writing took 41.82334092631936 seconds)
2022-08-18 02:54:01 | INFO | fairseq_cli.train | end of epoch 1303 (average epoch stats below)
2022-08-18 02:54:01 | INFO | train | epoch 1303 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 4726 | ups 0.86 | wpb 5523.2 | bsz 358 | num_updates 105543 | lr 0.000194677 | gnorm 0.377 | train_wall 41 | gb_free 10.1 | wall 103337
2022-08-18 02:54:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:54:01 | INFO | fairseq.trainer | begin training epoch 1304
2022-08-18 02:54:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:54:32 | INFO | train_inner | epoch 1304:     57 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=5294, ups=0.96, wpb=5522.9, bsz=359.4, num_updates=105600, lr=0.000194625, gnorm=0.336, train_wall=50, gb_free=10.1, wall=103368
2022-08-18 02:54:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:54:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:54:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:54:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:54:53 | INFO | valid | epoch 1304 | valid on 'valid' subset | loss 5.263 | nll_loss 2.679 | ppl 6.4 | bleu 56.52 | wps 1857.4 | wpb 933.5 | bsz 59.6 | num_updates 105624 | best_bleu 57.52
2022-08-18 02:54:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1304 @ 105624 updates
2022-08-18 02:54:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1304.pt
2022-08-18 02:54:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1304.pt
2022-08-18 02:55:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1304.pt (epoch 1304 @ 105624 updates, score 56.52) (writing took 16.41321362927556 seconds)
2022-08-18 02:55:09 | INFO | fairseq_cli.train | end of epoch 1304 (average epoch stats below)
2022-08-18 02:55:09 | INFO | train | epoch 1304 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6581.7 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 105624 | lr 0.000194603 | gnorm 0.317 | train_wall 40 | gb_free 10 | wall 103405
2022-08-18 02:55:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:55:09 | INFO | fairseq.trainer | begin training epoch 1305
2022-08-18 02:55:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:55:48 | INFO | train_inner | epoch 1305:     76 / 81 loss=3.37, nll_loss=0.339, ppl=1.27, wps=7269.2, ups=1.31, wpb=5538.1, bsz=359.6, num_updates=105700, lr=0.000194533, gnorm=0.652, train_wall=49, gb_free=10.1, wall=103444
2022-08-18 02:55:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:55:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:55:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:55:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:55:59 | INFO | valid | epoch 1305 | valid on 'valid' subset | loss 5.257 | nll_loss 2.671 | ppl 6.37 | bleu 55.44 | wps 1904.5 | wpb 933.5 | bsz 59.6 | num_updates 105705 | best_bleu 57.52
2022-08-18 02:55:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1305 @ 105705 updates
2022-08-18 02:55:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1305.pt
2022-08-18 02:56:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1305.pt
2022-08-18 02:56:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1305.pt (epoch 1305 @ 105705 updates, score 55.44) (writing took 19.134220972657204 seconds)
2022-08-18 02:56:18 | INFO | fairseq_cli.train | end of epoch 1305 (average epoch stats below)
2022-08-18 02:56:19 | INFO | train | epoch 1305 | loss 3.37 | nll_loss 0.339 | ppl 1.27 | wps 6450.6 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 105705 | lr 0.000194528 | gnorm 0.727 | train_wall 39 | gb_free 10.1 | wall 103475
2022-08-18 02:56:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:56:19 | INFO | fairseq.trainer | begin training epoch 1306
2022-08-18 02:56:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:56:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:57:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:57:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:57:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:57:08 | INFO | valid | epoch 1306 | valid on 'valid' subset | loss 5.256 | nll_loss 2.672 | ppl 6.37 | bleu 55.64 | wps 1858.5 | wpb 933.5 | bsz 59.6 | num_updates 105786 | best_bleu 57.52
2022-08-18 02:57:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1306 @ 105786 updates
2022-08-18 02:57:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1306.pt
2022-08-18 02:57:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1306.pt
2022-08-18 02:57:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1306.pt (epoch 1306 @ 105786 updates, score 55.64) (writing took 20.124620340764523 seconds)
2022-08-18 02:57:28 | INFO | fairseq_cli.train | end of epoch 1306 (average epoch stats below)
2022-08-18 02:57:28 | INFO | train | epoch 1306 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 6402.3 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 105786 | lr 0.000194454 | gnorm 0.456 | train_wall 38 | gb_free 10.1 | wall 103545
2022-08-18 02:57:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:57:29 | INFO | fairseq.trainer | begin training epoch 1307
2022-08-18 02:57:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:57:37 | INFO | train_inner | epoch 1307:     14 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=5059.3, ups=0.92, wpb=5499.3, bsz=356.5, num_updates=105800, lr=0.000194441, gnorm=0.432, train_wall=47, gb_free=10, wall=103553
2022-08-18 02:58:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:58:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:58:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:58:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:58:20 | INFO | valid | epoch 1307 | valid on 'valid' subset | loss 5.255 | nll_loss 2.671 | ppl 6.37 | bleu 55.96 | wps 1801.2 | wpb 933.5 | bsz 59.6 | num_updates 105867 | best_bleu 57.52
2022-08-18 02:58:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1307 @ 105867 updates
2022-08-18 02:58:20 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1307.pt
2022-08-18 02:58:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1307.pt
2022-08-18 02:58:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1307.pt (epoch 1307 @ 105867 updates, score 55.96) (writing took 12.888825491070747 seconds)
2022-08-18 02:58:33 | INFO | fairseq_cli.train | end of epoch 1307 (average epoch stats below)
2022-08-18 02:58:33 | INFO | train | epoch 1307 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 6926.2 | ups 1.25 | wpb 5523.2 | bsz 358 | num_updates 105867 | lr 0.000194379 | gnorm 0.392 | train_wall 40 | gb_free 10.2 | wall 103609
2022-08-18 02:58:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 02:58:33 | INFO | fairseq.trainer | begin training epoch 1308
2022-08-18 02:58:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 02:58:51 | INFO | train_inner | epoch 1308:     33 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=7469.1, ups=1.35, wpb=5539.6, bsz=363.4, num_updates=105900, lr=0.000194349, gnorm=0.387, train_wall=50, gb_free=10.1, wall=103627
2022-08-18 02:59:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 02:59:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 02:59:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 02:59:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 02:59:23 | INFO | valid | epoch 1308 | valid on 'valid' subset | loss 5.263 | nll_loss 2.683 | ppl 6.42 | bleu 56.23 | wps 1900.4 | wpb 933.5 | bsz 59.6 | num_updates 105948 | best_bleu 57.52
2022-08-18 02:59:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1308 @ 105948 updates
2022-08-18 02:59:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1308.pt
2022-08-18 02:59:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1308.pt
2022-08-18 03:00:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1308.pt (epoch 1308 @ 105948 updates, score 56.23) (writing took 37.341533459722996 seconds)
2022-08-18 03:00:00 | INFO | fairseq_cli.train | end of epoch 1308 (average epoch stats below)
2022-08-18 03:00:00 | INFO | train | epoch 1308 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5126.6 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 105948 | lr 0.000194305 | gnorm 0.378 | train_wall 39 | gb_free 10.2 | wall 103696
2022-08-18 03:00:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:00:00 | INFO | fairseq.trainer | begin training epoch 1309
2022-08-18 03:00:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:00:27 | INFO | train_inner | epoch 1309:     52 / 81 loss=3.371, nll_loss=0.339, ppl=1.26, wps=5742.8, ups=1.04, wpb=5516, bsz=356.2, num_updates=106000, lr=0.000194257, gnorm=0.344, train_wall=48, gb_free=10, wall=103723
2022-08-18 03:00:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:00:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:00:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:00:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:00:50 | INFO | valid | epoch 1309 | valid on 'valid' subset | loss 5.268 | nll_loss 2.689 | ppl 6.45 | bleu 56.04 | wps 1843.6 | wpb 933.5 | bsz 59.6 | num_updates 106029 | best_bleu 57.52
2022-08-18 03:00:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1309 @ 106029 updates
2022-08-18 03:00:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1309.pt
2022-08-18 03:00:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1309.pt
2022-08-18 03:00:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1309.pt (epoch 1309 @ 106029 updates, score 56.04) (writing took 2.357483170926571 seconds)
2022-08-18 03:00:52 | INFO | fairseq_cli.train | end of epoch 1309 (average epoch stats below)
2022-08-18 03:00:52 | INFO | train | epoch 1309 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 8617.2 | ups 1.56 | wpb 5523.2 | bsz 358 | num_updates 106029 | lr 0.000194231 | gnorm 0.301 | train_wall 38 | gb_free 10.1 | wall 103748
2022-08-18 03:00:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:00:52 | INFO | fairseq.trainer | begin training epoch 1310
2022-08-18 03:00:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:01:31 | INFO | train_inner | epoch 1310:     71 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=8660, ups=1.56, wpb=5546.2, bsz=359.7, num_updates=106100, lr=0.000194166, gnorm=0.525, train_wall=49, gb_free=10.1, wall=103787
2022-08-18 03:01:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:01:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:01:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:01:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:01:45 | INFO | valid | epoch 1310 | valid on 'valid' subset | loss 5.254 | nll_loss 2.669 | ppl 6.36 | bleu 56.31 | wps 1926.2 | wpb 933.5 | bsz 59.6 | num_updates 106110 | best_bleu 57.52
2022-08-18 03:01:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1310 @ 106110 updates
2022-08-18 03:01:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1310.pt
2022-08-18 03:01:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1310.pt
2022-08-18 03:01:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1310.pt (epoch 1310 @ 106110 updates, score 56.31) (writing took 14.576040901243687 seconds)
2022-08-18 03:01:59 | INFO | fairseq_cli.train | end of epoch 1310 (average epoch stats below)
2022-08-18 03:01:59 | INFO | train | epoch 1310 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6667.4 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 106110 | lr 0.000194156 | gnorm 0.572 | train_wall 40 | gb_free 10.2 | wall 103815
2022-08-18 03:01:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:01:59 | INFO | fairseq.trainer | begin training epoch 1311
2022-08-18 03:01:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:02:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:02:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:02:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:02:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:02:49 | INFO | valid | epoch 1311 | valid on 'valid' subset | loss 5.243 | nll_loss 2.655 | ppl 6.3 | bleu 56.48 | wps 1726.3 | wpb 933.5 | bsz 59.6 | num_updates 106191 | best_bleu 57.52
2022-08-18 03:02:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1311 @ 106191 updates
2022-08-18 03:02:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1311.pt
2022-08-18 03:02:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1311.pt
2022-08-18 03:03:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1311.pt (epoch 1311 @ 106191 updates, score 56.48) (writing took 18.703678216785192 seconds)
2022-08-18 03:03:08 | INFO | fairseq_cli.train | end of epoch 1311 (average epoch stats below)
2022-08-18 03:03:08 | INFO | train | epoch 1311 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6505.2 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 106191 | lr 0.000194082 | gnorm 0.364 | train_wall 38 | gb_free 10.1 | wall 103884
2022-08-18 03:03:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:03:08 | INFO | fairseq.trainer | begin training epoch 1312
2022-08-18 03:03:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:03:14 | INFO | train_inner | epoch 1312:      9 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=5309.7, ups=0.97, wpb=5495.8, bsz=353.8, num_updates=106200, lr=0.000194074, gnorm=0.367, train_wall=47, gb_free=10, wall=103890
2022-08-18 03:03:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:03:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:03:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:03:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:04:02 | INFO | valid | epoch 1312 | valid on 'valid' subset | loss 5.251 | nll_loss 2.667 | ppl 6.35 | bleu 56.42 | wps 1908.6 | wpb 933.5 | bsz 59.6 | num_updates 106272 | best_bleu 57.52
2022-08-18 03:04:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1312 @ 106272 updates
2022-08-18 03:04:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1312.pt
2022-08-18 03:04:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1312.pt
2022-08-18 03:04:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1312.pt (epoch 1312 @ 106272 updates, score 56.42) (writing took 30.760645292699337 seconds)
2022-08-18 03:04:33 | INFO | fairseq_cli.train | end of epoch 1312 (average epoch stats below)
2022-08-18 03:04:33 | INFO | train | epoch 1312 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 5265.4 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 106272 | lr 0.000194008 | gnorm 0.363 | train_wall 40 | gb_free 10.2 | wall 103969
2022-08-18 03:04:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:04:33 | INFO | fairseq.trainer | begin training epoch 1313
2022-08-18 03:04:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:04:48 | INFO | train_inner | epoch 1313:     28 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=5893.9, ups=1.07, wpb=5529.8, bsz=361.8, num_updates=106300, lr=0.000193983, gnorm=0.355, train_wall=49, gb_free=10.1, wall=103984
2022-08-18 03:05:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:05:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:05:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:05:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:05:24 | INFO | valid | epoch 1313 | valid on 'valid' subset | loss 5.253 | nll_loss 2.669 | ppl 6.36 | bleu 56.07 | wps 1734.1 | wpb 933.5 | bsz 59.6 | num_updates 106353 | best_bleu 57.52
2022-08-18 03:05:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1313 @ 106353 updates
2022-08-18 03:05:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1313.pt
2022-08-18 03:05:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1313.pt
2022-08-18 03:06:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1313.pt (epoch 1313 @ 106353 updates, score 56.07) (writing took 38.75174218416214 seconds)
2022-08-18 03:06:03 | INFO | fairseq_cli.train | end of epoch 1313 (average epoch stats below)
2022-08-18 03:06:03 | INFO | train | epoch 1313 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 4986.8 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 106353 | lr 0.000193935 | gnorm 0.389 | train_wall 39 | gb_free 10.1 | wall 104059
2022-08-18 03:06:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:06:03 | INFO | fairseq.trainer | begin training epoch 1314
2022-08-18 03:06:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:06:28 | INFO | train_inner | epoch 1314:     47 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=5544.9, ups=1, wpb=5528.5, bsz=354.2, num_updates=106400, lr=0.000193892, gnorm=0.375, train_wall=49, gb_free=10.1, wall=104084
2022-08-18 03:06:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:06:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:06:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:06:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:06:54 | INFO | valid | epoch 1314 | valid on 'valid' subset | loss 5.251 | nll_loss 2.666 | ppl 6.35 | bleu 56.49 | wps 1871.3 | wpb 933.5 | bsz 59.6 | num_updates 106434 | best_bleu 57.52
2022-08-18 03:06:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1314 @ 106434 updates
2022-08-18 03:06:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1314.pt
2022-08-18 03:06:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1314.pt
2022-08-18 03:06:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1314.pt (epoch 1314 @ 106434 updates, score 56.49) (writing took 2.3760337717831135 seconds)
2022-08-18 03:06:56 | INFO | fairseq_cli.train | end of epoch 1314 (average epoch stats below)
2022-08-18 03:06:56 | INFO | train | epoch 1314 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 8349.8 | ups 1.51 | wpb 5523.2 | bsz 358 | num_updates 106434 | lr 0.000193861 | gnorm 0.343 | train_wall 40 | gb_free 10.2 | wall 104113
2022-08-18 03:06:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:06:56 | INFO | fairseq.trainer | begin training epoch 1315
2022-08-18 03:06:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:07:30 | INFO | train_inner | epoch 1315:     66 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=8861.4, ups=1.61, wpb=5518.2, bsz=358.3, num_updates=106500, lr=0.000193801, gnorm=0.327, train_wall=49, gb_free=10.1, wall=104146
2022-08-18 03:07:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:07:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:07:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:07:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:07:48 | INFO | valid | epoch 1315 | valid on 'valid' subset | loss 5.26 | nll_loss 2.674 | ppl 6.38 | bleu 56.52 | wps 1700.5 | wpb 933.5 | bsz 59.6 | num_updates 106515 | best_bleu 57.52
2022-08-18 03:07:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1315 @ 106515 updates
2022-08-18 03:07:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1315.pt
2022-08-18 03:07:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1315.pt
2022-08-18 03:08:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1315.pt (epoch 1315 @ 106515 updates, score 56.52) (writing took 18.63325635716319 seconds)
2022-08-18 03:08:06 | INFO | fairseq_cli.train | end of epoch 1315 (average epoch stats below)
2022-08-18 03:08:06 | INFO | train | epoch 1315 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6390.4 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 106515 | lr 0.000193787 | gnorm 0.344 | train_wall 39 | gb_free 10.1 | wall 104183
2022-08-18 03:08:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:08:07 | INFO | fairseq.trainer | begin training epoch 1316
2022-08-18 03:08:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:08:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:08:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:08:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:08:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:08:58 | INFO | valid | epoch 1316 | valid on 'valid' subset | loss 5.256 | nll_loss 2.673 | ppl 6.38 | bleu 56.57 | wps 1786.4 | wpb 933.5 | bsz 59.6 | num_updates 106596 | best_bleu 57.52
2022-08-18 03:08:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1316 @ 106596 updates
2022-08-18 03:08:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1316.pt
2022-08-18 03:08:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1316.pt
2022-08-18 03:09:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1316.pt (epoch 1316 @ 106596 updates, score 56.57) (writing took 2.5711103044450283 seconds)
2022-08-18 03:09:00 | INFO | fairseq_cli.train | end of epoch 1316 (average epoch stats below)
2022-08-18 03:09:00 | INFO | train | epoch 1316 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 8263.7 | ups 1.5 | wpb 5523.2 | bsz 358 | num_updates 106596 | lr 0.000193713 | gnorm 0.444 | train_wall 40 | gb_free 10 | wall 104237
2022-08-18 03:09:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:09:01 | INFO | fairseq.trainer | begin training epoch 1317
2022-08-18 03:09:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:09:04 | INFO | train_inner | epoch 1317:      4 / 81 loss=3.37, nll_loss=0.339, ppl=1.27, wps=5876, ups=1.07, wpb=5510.1, bsz=358.1, num_updates=106600, lr=0.00019371, gnorm=0.441, train_wall=49, gb_free=10.1, wall=104240
2022-08-18 03:09:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:09:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:09:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:09:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:09:51 | INFO | valid | epoch 1317 | valid on 'valid' subset | loss 5.265 | nll_loss 2.684 | ppl 6.43 | bleu 56.6 | wps 1804.1 | wpb 933.5 | bsz 59.6 | num_updates 106677 | best_bleu 57.52
2022-08-18 03:09:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1317 @ 106677 updates
2022-08-18 03:09:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1317.pt
2022-08-18 03:09:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1317.pt
2022-08-18 03:10:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1317.pt (epoch 1317 @ 106677 updates, score 56.6) (writing took 35.11609514057636 seconds)
2022-08-18 03:10:27 | INFO | fairseq_cli.train | end of epoch 1317 (average epoch stats below)
2022-08-18 03:10:27 | INFO | train | epoch 1317 | loss 3.37 | nll_loss 0.339 | ppl 1.27 | wps 5184.4 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 106677 | lr 0.00019364 | gnorm 0.484 | train_wall 39 | gb_free 10.2 | wall 104323
2022-08-18 03:10:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:10:27 | INFO | fairseq.trainer | begin training epoch 1318
2022-08-18 03:10:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:10:39 | INFO | train_inner | epoch 1318:     23 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=5818.5, ups=1.05, wpb=5553, bsz=355.6, num_updates=106700, lr=0.000193619, gnorm=0.459, train_wall=48, gb_free=10.1, wall=104336
2022-08-18 03:11:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:11:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:11:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:11:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:11:17 | INFO | valid | epoch 1318 | valid on 'valid' subset | loss 5.244 | nll_loss 2.658 | ppl 6.31 | bleu 56.62 | wps 1742.3 | wpb 933.5 | bsz 59.6 | num_updates 106758 | best_bleu 57.52
2022-08-18 03:11:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1318 @ 106758 updates
2022-08-18 03:11:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1318.pt
2022-08-18 03:11:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1318.pt
2022-08-18 03:11:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1318.pt (epoch 1318 @ 106758 updates, score 56.62) (writing took 2.568842589855194 seconds)
2022-08-18 03:11:20 | INFO | fairseq_cli.train | end of epoch 1318 (average epoch stats below)
2022-08-18 03:11:20 | INFO | train | epoch 1318 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 8402.9 | ups 1.52 | wpb 5523.2 | bsz 358 | num_updates 106758 | lr 0.000193566 | gnorm 0.373 | train_wall 39 | gb_free 10.1 | wall 104376
2022-08-18 03:11:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:11:20 | INFO | fairseq.trainer | begin training epoch 1319
2022-08-18 03:11:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:11:42 | INFO | train_inner | epoch 1319:     42 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=8807.8, ups=1.6, wpb=5508.5, bsz=363.1, num_updates=106800, lr=0.000193528, gnorm=0.358, train_wall=48, gb_free=10.1, wall=104398
2022-08-18 03:12:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:12:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:12:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:12:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:12:11 | INFO | valid | epoch 1319 | valid on 'valid' subset | loss 5.239 | nll_loss 2.649 | ppl 6.27 | bleu 56.85 | wps 1695.4 | wpb 933.5 | bsz 59.6 | num_updates 106839 | best_bleu 57.52
2022-08-18 03:12:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1319 @ 106839 updates
2022-08-18 03:12:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1319.pt
2022-08-18 03:12:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1319.pt
2022-08-18 03:12:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1319.pt (epoch 1319 @ 106839 updates, score 56.85) (writing took 28.387057848274708 seconds)
2022-08-18 03:12:40 | INFO | fairseq_cli.train | end of epoch 1319 (average epoch stats below)
2022-08-18 03:12:40 | INFO | train | epoch 1319 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5590.7 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 106839 | lr 0.000193493 | gnorm 0.375 | train_wall 40 | gb_free 10.2 | wall 104456
2022-08-18 03:12:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:12:40 | INFO | fairseq.trainer | begin training epoch 1320
2022-08-18 03:12:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:13:16 | INFO | train_inner | epoch 1320:     61 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=5919.7, ups=1.07, wpb=5552.8, bsz=358.4, num_updates=106900, lr=0.000193438, gnorm=0.404, train_wall=49, gb_free=10.1, wall=104492
2022-08-18 03:13:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:13:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:13:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:13:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:13:35 | INFO | valid | epoch 1320 | valid on 'valid' subset | loss 5.266 | nll_loss 2.686 | ppl 6.44 | bleu 56.31 | wps 1825.7 | wpb 933.5 | bsz 59.6 | num_updates 106920 | best_bleu 57.52
2022-08-18 03:13:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1320 @ 106920 updates
2022-08-18 03:13:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1320.pt
2022-08-18 03:13:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1320.pt
2022-08-18 03:14:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1320.pt (epoch 1320 @ 106920 updates, score 56.31) (writing took 33.374252025038004 seconds)
2022-08-18 03:14:09 | INFO | fairseq_cli.train | end of epoch 1320 (average epoch stats below)
2022-08-18 03:14:09 | INFO | train | epoch 1320 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 5041.1 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 106920 | lr 0.00019342 | gnorm 0.374 | train_wall 39 | gb_free 10.1 | wall 104545
2022-08-18 03:14:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:14:09 | INFO | fairseq.trainer | begin training epoch 1321
2022-08-18 03:14:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:14:49 | INFO | train_inner | epoch 1321:     80 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=5883.8, ups=1.07, wpb=5517.7, bsz=356.4, num_updates=107000, lr=0.000193347, gnorm=0.337, train_wall=49, gb_free=10, wall=104586
2022-08-18 03:14:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:14:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:14:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:14:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:14:59 | INFO | valid | epoch 1321 | valid on 'valid' subset | loss 5.269 | nll_loss 2.688 | ppl 6.44 | bleu 56.27 | wps 1846.5 | wpb 933.5 | bsz 59.6 | num_updates 107001 | best_bleu 57.52
2022-08-18 03:14:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1321 @ 107001 updates
2022-08-18 03:14:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1321.pt
2022-08-18 03:15:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1321.pt
2022-08-18 03:15:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1321.pt (epoch 1321 @ 107001 updates, score 56.27) (writing took 20.37595245987177 seconds)
2022-08-18 03:15:20 | INFO | fairseq_cli.train | end of epoch 1321 (average epoch stats below)
2022-08-18 03:15:20 | INFO | train | epoch 1321 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6326.8 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 107001 | lr 0.000193346 | gnorm 0.34 | train_wall 39 | gb_free 10.3 | wall 104616
2022-08-18 03:15:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:15:20 | INFO | fairseq.trainer | begin training epoch 1322
2022-08-18 03:15:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:16:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:16:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:16:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:16:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:16:10 | INFO | valid | epoch 1322 | valid on 'valid' subset | loss 5.264 | nll_loss 2.679 | ppl 6.4 | bleu 56.31 | wps 1881.8 | wpb 933.5 | bsz 59.6 | num_updates 107082 | best_bleu 57.52
2022-08-18 03:16:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1322 @ 107082 updates
2022-08-18 03:16:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1322.pt
2022-08-18 03:16:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1322.pt
2022-08-18 03:16:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1322.pt (epoch 1322 @ 107082 updates, score 56.31) (writing took 14.540922250598669 seconds)
2022-08-18 03:16:25 | INFO | fairseq_cli.train | end of epoch 1322 (average epoch stats below)
2022-08-18 03:16:25 | INFO | train | epoch 1322 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6856 | ups 1.24 | wpb 5523.2 | bsz 358 | num_updates 107082 | lr 0.000193273 | gnorm 0.36 | train_wall 39 | gb_free 10.2 | wall 104681
2022-08-18 03:16:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:16:25 | INFO | fairseq.trainer | begin training epoch 1323
2022-08-18 03:16:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:16:35 | INFO | train_inner | epoch 1323:     18 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=5204.9, ups=0.95, wpb=5491.5, bsz=355.8, num_updates=107100, lr=0.000193257, gnorm=0.383, train_wall=48, gb_free=10, wall=104691
2022-08-18 03:17:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:17:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:17:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:17:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:17:18 | INFO | valid | epoch 1323 | valid on 'valid' subset | loss 5.27 | nll_loss 2.685 | ppl 6.43 | bleu 56.13 | wps 1884.4 | wpb 933.5 | bsz 59.6 | num_updates 107163 | best_bleu 57.52
2022-08-18 03:17:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1323 @ 107163 updates
2022-08-18 03:17:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1323.pt
2022-08-18 03:17:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1323.pt
2022-08-18 03:18:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1323.pt (epoch 1323 @ 107163 updates, score 56.13) (writing took 43.52508341521025 seconds)
2022-08-18 03:18:01 | INFO | fairseq_cli.train | end of epoch 1323 (average epoch stats below)
2022-08-18 03:18:01 | INFO | train | epoch 1323 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 4629.3 | ups 0.84 | wpb 5523.2 | bsz 358 | num_updates 107163 | lr 0.0001932 | gnorm 0.411 | train_wall 39 | gb_free 10.1 | wall 104778
2022-08-18 03:18:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:18:02 | INFO | fairseq.trainer | begin training epoch 1324
2022-08-18 03:18:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:18:21 | INFO | train_inner | epoch 1324:     37 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=5236.7, ups=0.95, wpb=5529.2, bsz=360.9, num_updates=107200, lr=0.000193167, gnorm=0.381, train_wall=48, gb_free=10.1, wall=104797
2022-08-18 03:18:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:18:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:18:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:18:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:18:52 | INFO | valid | epoch 1324 | valid on 'valid' subset | loss 5.256 | nll_loss 2.671 | ppl 6.37 | bleu 56.28 | wps 1752.5 | wpb 933.5 | bsz 59.6 | num_updates 107244 | best_bleu 57.52
2022-08-18 03:18:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1324 @ 107244 updates
2022-08-18 03:18:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1324.pt
2022-08-18 03:18:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1324.pt
2022-08-18 03:19:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1324.pt (epoch 1324 @ 107244 updates, score 56.28) (writing took 44.48014526069164 seconds)
2022-08-18 03:19:36 | INFO | fairseq_cli.train | end of epoch 1324 (average epoch stats below)
2022-08-18 03:19:37 | INFO | train | epoch 1324 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 4705.2 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 107244 | lr 0.000193127 | gnorm 0.357 | train_wall 39 | gb_free 10.1 | wall 104873
2022-08-18 03:19:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:19:37 | INFO | fairseq.trainer | begin training epoch 1325
2022-08-18 03:19:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:20:05 | INFO | train_inner | epoch 1325:     56 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=5335, ups=0.96, wpb=5552.7, bsz=353.6, num_updates=107300, lr=0.000193077, gnorm=0.362, train_wall=48, gb_free=10, wall=104901
2022-08-18 03:20:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:20:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:20:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:20:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:20:27 | INFO | valid | epoch 1325 | valid on 'valid' subset | loss 5.251 | nll_loss 2.666 | ppl 6.34 | bleu 56.87 | wps 1802.1 | wpb 933.5 | bsz 59.6 | num_updates 107325 | best_bleu 57.52
2022-08-18 03:20:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1325 @ 107325 updates
2022-08-18 03:20:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1325.pt
2022-08-18 03:20:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1325.pt
2022-08-18 03:20:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1325.pt (epoch 1325 @ 107325 updates, score 56.87) (writing took 14.744336880743504 seconds)
2022-08-18 03:20:42 | INFO | fairseq_cli.train | end of epoch 1325 (average epoch stats below)
2022-08-18 03:20:42 | INFO | train | epoch 1325 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6877.4 | ups 1.25 | wpb 5523.2 | bsz 358 | num_updates 107325 | lr 0.000193054 | gnorm 0.395 | train_wall 39 | gb_free 10.1 | wall 104938
2022-08-18 03:20:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:20:42 | INFO | fairseq.trainer | begin training epoch 1326
2022-08-18 03:20:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:21:22 | INFO | train_inner | epoch 1326:     75 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=7143.7, ups=1.29, wpb=5517.6, bsz=361.4, num_updates=107400, lr=0.000192987, gnorm=0.362, train_wall=49, gb_free=10.1, wall=104978
2022-08-18 03:21:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:21:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:21:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:21:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:21:35 | INFO | valid | epoch 1326 | valid on 'valid' subset | loss 5.255 | nll_loss 2.671 | ppl 6.37 | bleu 56.62 | wps 1679 | wpb 933.5 | bsz 59.6 | num_updates 107406 | best_bleu 57.52
2022-08-18 03:21:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1326 @ 107406 updates
2022-08-18 03:21:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1326.pt
2022-08-18 03:21:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1326.pt
2022-08-18 03:22:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1326.pt (epoch 1326 @ 107406 updates, score 56.62) (writing took 38.37905767932534 seconds)
2022-08-18 03:22:14 | INFO | fairseq_cli.train | end of epoch 1326 (average epoch stats below)
2022-08-18 03:22:14 | INFO | train | epoch 1326 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 4863.4 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 107406 | lr 0.000192982 | gnorm 0.335 | train_wall 39 | gb_free 10.1 | wall 105030
2022-08-18 03:22:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:22:14 | INFO | fairseq.trainer | begin training epoch 1327
2022-08-18 03:22:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:22:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:22:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:22:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:22:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:23:05 | INFO | valid | epoch 1327 | valid on 'valid' subset | loss 5.258 | nll_loss 2.671 | ppl 6.37 | bleu 56.41 | wps 1807.2 | wpb 933.5 | bsz 59.6 | num_updates 107487 | best_bleu 57.52
2022-08-18 03:23:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1327 @ 107487 updates
2022-08-18 03:23:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1327.pt
2022-08-18 03:23:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1327.pt
2022-08-18 03:23:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1327.pt (epoch 1327 @ 107487 updates, score 56.41) (writing took 21.294981956481934 seconds)
2022-08-18 03:23:27 | INFO | fairseq_cli.train | end of epoch 1327 (average epoch stats below)
2022-08-18 03:23:27 | INFO | train | epoch 1327 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6126.7 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 107487 | lr 0.000192909 | gnorm 0.348 | train_wall 40 | gb_free 10.2 | wall 105103
2022-08-18 03:23:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:23:27 | INFO | fairseq.trainer | begin training epoch 1328
2022-08-18 03:23:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:23:34 | INFO | train_inner | epoch 1328:     13 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=4162.3, ups=0.76, wpb=5511, bsz=359.4, num_updates=107500, lr=0.000192897, gnorm=0.347, train_wall=49, gb_free=10, wall=105110
2022-08-18 03:24:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:24:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:24:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:24:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:24:17 | INFO | valid | epoch 1328 | valid on 'valid' subset | loss 5.264 | nll_loss 2.681 | ppl 6.41 | bleu 56.19 | wps 1790.2 | wpb 933.5 | bsz 59.6 | num_updates 107568 | best_bleu 57.52
2022-08-18 03:24:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1328 @ 107568 updates
2022-08-18 03:24:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1328.pt
2022-08-18 03:24:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1328.pt
2022-08-18 03:25:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1328.pt (epoch 1328 @ 107568 updates, score 56.19) (writing took 44.19989103451371 seconds)
2022-08-18 03:25:01 | INFO | fairseq_cli.train | end of epoch 1328 (average epoch stats below)
2022-08-18 03:25:01 | INFO | train | epoch 1328 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 4728.6 | ups 0.86 | wpb 5523.2 | bsz 358 | num_updates 107568 | lr 0.000192836 | gnorm 0.392 | train_wall 39 | gb_free 10 | wall 105197
2022-08-18 03:25:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:25:01 | INFO | fairseq.trainer | begin training epoch 1329
2022-08-18 03:25:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:25:20 | INFO | train_inner | epoch 1329:     32 / 81 loss=3.371, nll_loss=0.339, ppl=1.26, wps=5229.4, ups=0.95, wpb=5528.2, bsz=354.1, num_updates=107600, lr=0.000192807, gnorm=0.367, train_wall=48, gb_free=10.1, wall=105216
2022-08-18 03:25:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:25:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:25:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:25:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:25:56 | INFO | valid | epoch 1329 | valid on 'valid' subset | loss 5.258 | nll_loss 2.671 | ppl 6.37 | bleu 56.37 | wps 1878.9 | wpb 933.5 | bsz 59.6 | num_updates 107649 | best_bleu 57.52
2022-08-18 03:25:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1329 @ 107649 updates
2022-08-18 03:25:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1329.pt
2022-08-18 03:25:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1329.pt
2022-08-18 03:26:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1329.pt (epoch 1329 @ 107649 updates, score 56.37) (writing took 31.888241469860077 seconds)
2022-08-18 03:26:28 | INFO | fairseq_cli.train | end of epoch 1329 (average epoch stats below)
2022-08-18 03:26:28 | INFO | train | epoch 1329 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 5170.6 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 107649 | lr 0.000192764 | gnorm 0.302 | train_wall 39 | gb_free 10.1 | wall 105284
2022-08-18 03:26:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:26:28 | INFO | fairseq.trainer | begin training epoch 1330
2022-08-18 03:26:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:26:54 | INFO | train_inner | epoch 1330:     51 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5851.7, ups=1.06, wpb=5509.8, bsz=359, num_updates=107700, lr=0.000192718, gnorm=0.332, train_wall=48, gb_free=10.2, wall=105310
2022-08-18 03:27:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:27:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:27:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:27:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:27:18 | INFO | valid | epoch 1330 | valid on 'valid' subset | loss 5.259 | nll_loss 2.669 | ppl 6.36 | bleu 56.25 | wps 1801.1 | wpb 933.5 | bsz 59.6 | num_updates 107730 | best_bleu 57.52
2022-08-18 03:27:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1330 @ 107730 updates
2022-08-18 03:27:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1330.pt
2022-08-18 03:27:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1330.pt
2022-08-18 03:27:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1330.pt (epoch 1330 @ 107730 updates, score 56.25) (writing took 36.28043960034847 seconds)
2022-08-18 03:27:54 | INFO | fairseq_cli.train | end of epoch 1330 (average epoch stats below)
2022-08-18 03:27:54 | INFO | train | epoch 1330 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 5159.2 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 107730 | lr 0.000192691 | gnorm 0.366 | train_wall 39 | gb_free 10.1 | wall 105371
2022-08-18 03:27:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:27:55 | INFO | fairseq.trainer | begin training epoch 1331
2022-08-18 03:27:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:28:30 | INFO | train_inner | epoch 1331:     70 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=5737.4, ups=1.04, wpb=5521.9, bsz=359.6, num_updates=107800, lr=0.000192629, gnorm=0.391, train_wall=48, gb_free=10.1, wall=105407
2022-08-18 03:28:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:28:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:28:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:28:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:28:45 | INFO | valid | epoch 1331 | valid on 'valid' subset | loss 5.269 | nll_loss 2.686 | ppl 6.43 | bleu 56.13 | wps 1831.3 | wpb 933.5 | bsz 59.6 | num_updates 107811 | best_bleu 57.52
2022-08-18 03:28:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1331 @ 107811 updates
2022-08-18 03:28:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1331.pt
2022-08-18 03:28:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1331.pt
2022-08-18 03:29:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1331.pt (epoch 1331 @ 107811 updates, score 56.13) (writing took 14.556410539895296 seconds)
2022-08-18 03:29:00 | INFO | fairseq_cli.train | end of epoch 1331 (average epoch stats below)
2022-08-18 03:29:00 | INFO | train | epoch 1331 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6842.7 | ups 1.24 | wpb 5523.2 | bsz 358 | num_updates 107811 | lr 0.000192619 | gnorm 0.4 | train_wall 39 | gb_free 10.1 | wall 105436
2022-08-18 03:29:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:29:00 | INFO | fairseq.trainer | begin training epoch 1332
2022-08-18 03:29:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:29:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:29:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:29:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:29:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:29:50 | INFO | valid | epoch 1332 | valid on 'valid' subset | loss 5.259 | nll_loss 2.672 | ppl 6.37 | bleu 56.37 | wps 1910.3 | wpb 933.5 | bsz 59.6 | num_updates 107892 | best_bleu 57.52
2022-08-18 03:29:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1332 @ 107892 updates
2022-08-18 03:29:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1332.pt
2022-08-18 03:29:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1332.pt
2022-08-18 03:30:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1332.pt (epoch 1332 @ 107892 updates, score 56.37) (writing took 31.487807396799326 seconds)
2022-08-18 03:30:22 | INFO | fairseq_cli.train | end of epoch 1332 (average epoch stats below)
2022-08-18 03:30:22 | INFO | train | epoch 1332 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5444.7 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 107892 | lr 0.000192546 | gnorm 0.373 | train_wall 39 | gb_free 10.1 | wall 105518
2022-08-18 03:30:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:30:22 | INFO | fairseq.trainer | begin training epoch 1333
2022-08-18 03:30:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:30:27 | INFO | train_inner | epoch 1333:      8 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=4738.8, ups=0.86, wpb=5532.1, bsz=357.8, num_updates=107900, lr=0.000192539, gnorm=0.375, train_wall=48, gb_free=10.1, wall=105523
2022-08-18 03:31:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:31:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:31:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:31:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:31:11 | INFO | valid | epoch 1333 | valid on 'valid' subset | loss 5.27 | nll_loss 2.685 | ppl 6.43 | bleu 56.53 | wps 1839.6 | wpb 933.5 | bsz 59.6 | num_updates 107973 | best_bleu 57.52
2022-08-18 03:31:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1333 @ 107973 updates
2022-08-18 03:31:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1333.pt
2022-08-18 03:31:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1333.pt
2022-08-18 03:31:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1333.pt (epoch 1333 @ 107973 updates, score 56.53) (writing took 23.858147520571947 seconds)
2022-08-18 03:31:35 | INFO | fairseq_cli.train | end of epoch 1333 (average epoch stats below)
2022-08-18 03:31:35 | INFO | train | epoch 1333 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6110.7 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 107973 | lr 0.000192474 | gnorm 0.341 | train_wall 38 | gb_free 10.1 | wall 105591
2022-08-18 03:31:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:31:35 | INFO | fairseq.trainer | begin training epoch 1334
2022-08-18 03:31:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:31:50 | INFO | train_inner | epoch 1334:     27 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=6605.7, ups=1.2, wpb=5496, bsz=351.7, num_updates=108000, lr=0.00019245, gnorm=0.375, train_wall=48, gb_free=10.1, wall=105607
2022-08-18 03:32:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:32:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:32:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:32:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:32:26 | INFO | valid | epoch 1334 | valid on 'valid' subset | loss 5.269 | nll_loss 2.685 | ppl 6.43 | bleu 56.33 | wps 1778 | wpb 933.5 | bsz 59.6 | num_updates 108054 | best_bleu 57.52
2022-08-18 03:32:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1334 @ 108054 updates
2022-08-18 03:32:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1334.pt
2022-08-18 03:32:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1334.pt
2022-08-18 03:32:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1334.pt (epoch 1334 @ 108054 updates, score 56.33) (writing took 31.91387015581131 seconds)
2022-08-18 03:32:58 | INFO | fairseq_cli.train | end of epoch 1334 (average epoch stats below)
2022-08-18 03:32:58 | INFO | train | epoch 1334 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 5388.6 | ups 0.98 | wpb 5523.2 | bsz 358 | num_updates 108054 | lr 0.000192402 | gnorm 0.397 | train_wall 40 | gb_free 10.2 | wall 105674
2022-08-18 03:32:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:32:58 | INFO | fairseq.trainer | begin training epoch 1335
2022-08-18 03:32:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:33:23 | INFO | train_inner | epoch 1335:     46 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=5973.1, ups=1.08, wpb=5550.2, bsz=362.9, num_updates=108100, lr=0.000192361, gnorm=0.393, train_wall=48, gb_free=10, wall=105699
2022-08-18 03:33:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:33:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:33:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:33:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:33:51 | INFO | valid | epoch 1335 | valid on 'valid' subset | loss 5.282 | nll_loss 2.702 | ppl 6.51 | bleu 56.14 | wps 1681.7 | wpb 933.5 | bsz 59.6 | num_updates 108135 | best_bleu 57.52
2022-08-18 03:33:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1335 @ 108135 updates
2022-08-18 03:33:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1335.pt
2022-08-18 03:33:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1335.pt
2022-08-18 03:34:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1335.pt (epoch 1335 @ 108135 updates, score 56.14) (writing took 26.902690317481756 seconds)
2022-08-18 03:34:18 | INFO | fairseq_cli.train | end of epoch 1335 (average epoch stats below)
2022-08-18 03:34:18 | INFO | train | epoch 1335 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5617.4 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 108135 | lr 0.00019233 | gnorm 0.387 | train_wall 40 | gb_free 10.2 | wall 105754
2022-08-18 03:34:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:34:18 | INFO | fairseq.trainer | begin training epoch 1336
2022-08-18 03:34:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:34:51 | INFO | train_inner | epoch 1336:     65 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=6300.9, ups=1.14, wpb=5526.3, bsz=362.4, num_updates=108200, lr=0.000192272, gnorm=0.387, train_wall=49, gb_free=10.1, wall=105787
2022-08-18 03:34:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:35:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:35:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:35:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:35:09 | INFO | valid | epoch 1336 | valid on 'valid' subset | loss 5.265 | nll_loss 2.684 | ppl 6.43 | bleu 56.17 | wps 1762.7 | wpb 933.5 | bsz 59.6 | num_updates 108216 | best_bleu 57.52
2022-08-18 03:35:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1336 @ 108216 updates
2022-08-18 03:35:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1336.pt
2022-08-18 03:35:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1336.pt
2022-08-18 03:35:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1336.pt (epoch 1336 @ 108216 updates, score 56.17) (writing took 30.912334445863962 seconds)
2022-08-18 03:35:40 | INFO | fairseq_cli.train | end of epoch 1336 (average epoch stats below)
2022-08-18 03:35:40 | INFO | train | epoch 1336 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5469.3 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 108216 | lr 0.000192258 | gnorm 0.406 | train_wall 39 | gb_free 10.2 | wall 105836
2022-08-18 03:35:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:35:40 | INFO | fairseq.trainer | begin training epoch 1337
2022-08-18 03:35:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:36:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:36:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:36:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:36:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:36:31 | INFO | valid | epoch 1337 | valid on 'valid' subset | loss 5.249 | nll_loss 2.662 | ppl 6.33 | bleu 56.24 | wps 1849.8 | wpb 933.5 | bsz 59.6 | num_updates 108297 | best_bleu 57.52
2022-08-18 03:36:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1337 @ 108297 updates
2022-08-18 03:36:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1337.pt
2022-08-18 03:36:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1337.pt
2022-08-18 03:36:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1337.pt (epoch 1337 @ 108297 updates, score 56.24) (writing took 15.98794163018465 seconds)
2022-08-18 03:36:47 | INFO | fairseq_cli.train | end of epoch 1337 (average epoch stats below)
2022-08-18 03:36:47 | INFO | train | epoch 1337 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6645.7 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 108297 | lr 0.000192186 | gnorm 0.387 | train_wall 40 | gb_free 10.1 | wall 105903
2022-08-18 03:36:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:36:47 | INFO | fairseq.trainer | begin training epoch 1338
2022-08-18 03:36:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:36:50 | INFO | train_inner | epoch 1338:      3 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=4635.8, ups=0.84, wpb=5506.8, bsz=354.5, num_updates=108300, lr=0.000192183, gnorm=0.388, train_wall=49, gb_free=10.1, wall=105906
2022-08-18 03:37:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:37:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:37:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:37:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:37:39 | INFO | valid | epoch 1338 | valid on 'valid' subset | loss 5.249 | nll_loss 2.66 | ppl 6.32 | bleu 56.28 | wps 1664.3 | wpb 933.5 | bsz 59.6 | num_updates 108378 | best_bleu 57.52
2022-08-18 03:37:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1338 @ 108378 updates
2022-08-18 03:37:39 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1338.pt
2022-08-18 03:37:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1338.pt
2022-08-18 03:37:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1338.pt (epoch 1338 @ 108378 updates, score 56.28) (writing took 17.110250610858202 seconds)
2022-08-18 03:37:56 | INFO | fairseq_cli.train | end of epoch 1338 (average epoch stats below)
2022-08-18 03:37:56 | INFO | train | epoch 1338 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6480.9 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 108378 | lr 0.000192114 | gnorm 0.39 | train_wall 40 | gb_free 10.2 | wall 105972
2022-08-18 03:37:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:37:56 | INFO | fairseq.trainer | begin training epoch 1339
2022-08-18 03:37:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:38:08 | INFO | train_inner | epoch 1339:     22 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=7019.1, ups=1.28, wpb=5502.3, bsz=356.1, num_updates=108400, lr=0.000192095, gnorm=0.378, train_wall=49, gb_free=10.1, wall=105984
2022-08-18 03:38:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:38:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:38:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:38:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:38:47 | INFO | valid | epoch 1339 | valid on 'valid' subset | loss 5.248 | nll_loss 2.662 | ppl 6.33 | bleu 56.68 | wps 1818.8 | wpb 933.5 | bsz 59.6 | num_updates 108459 | best_bleu 57.52
2022-08-18 03:38:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1339 @ 108459 updates
2022-08-18 03:38:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1339.pt
2022-08-18 03:38:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1339.pt
2022-08-18 03:39:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1339.pt (epoch 1339 @ 108459 updates, score 56.68) (writing took 19.67451674491167 seconds)
2022-08-18 03:39:07 | INFO | fairseq_cli.train | end of epoch 1339 (average epoch stats below)
2022-08-18 03:39:07 | INFO | train | epoch 1339 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6345.1 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 108459 | lr 0.000192042 | gnorm 0.405 | train_wall 39 | gb_free 10.1 | wall 106043
2022-08-18 03:39:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:39:07 | INFO | fairseq.trainer | begin training epoch 1340
2022-08-18 03:39:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:39:30 | INFO | train_inner | epoch 1340:     41 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=6860.3, ups=1.23, wpb=5586.4, bsz=361.7, num_updates=108500, lr=0.000192006, gnorm=0.395, train_wall=49, gb_free=10.1, wall=106066
2022-08-18 03:39:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:39:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:39:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:39:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:39:59 | INFO | valid | epoch 1340 | valid on 'valid' subset | loss 5.264 | nll_loss 2.682 | ppl 6.42 | bleu 56.09 | wps 1750.7 | wpb 933.5 | bsz 59.6 | num_updates 108540 | best_bleu 57.52
2022-08-18 03:39:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1340 @ 108540 updates
2022-08-18 03:39:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1340.pt
2022-08-18 03:40:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1340.pt
2022-08-18 03:40:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1340.pt (epoch 1340 @ 108540 updates, score 56.09) (writing took 14.514072429388762 seconds)
2022-08-18 03:40:13 | INFO | fairseq_cli.train | end of epoch 1340 (average epoch stats below)
2022-08-18 03:40:13 | INFO | train | epoch 1340 | loss 3.371 | nll_loss 0.339 | ppl 1.27 | wps 6690.4 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 108540 | lr 0.000191971 | gnorm 0.372 | train_wall 40 | gb_free 10.1 | wall 106110
2022-08-18 03:40:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:40:14 | INFO | fairseq.trainer | begin training epoch 1341
2022-08-18 03:40:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:40:44 | INFO | train_inner | epoch 1341:     60 / 81 loss=3.37, nll_loss=0.339, ppl=1.27, wps=7334.1, ups=1.34, wpb=5483.8, bsz=352.9, num_updates=108600, lr=0.000191918, gnorm=0.359, train_wall=48, gb_free=10, wall=106141
2022-08-18 03:40:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:40:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:40:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:40:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:41:04 | INFO | valid | epoch 1341 | valid on 'valid' subset | loss 5.276 | nll_loss 2.695 | ppl 6.48 | bleu 55.89 | wps 1856.9 | wpb 933.5 | bsz 59.6 | num_updates 108621 | best_bleu 57.52
2022-08-18 03:41:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1341 @ 108621 updates
2022-08-18 03:41:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1341.pt
2022-08-18 03:41:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1341.pt
2022-08-18 03:41:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1341.pt (epoch 1341 @ 108621 updates, score 55.89) (writing took 41.86679123342037 seconds)
2022-08-18 03:41:46 | INFO | fairseq_cli.train | end of epoch 1341 (average epoch stats below)
2022-08-18 03:41:46 | INFO | train | epoch 1341 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 4826.8 | ups 0.87 | wpb 5523.2 | bsz 358 | num_updates 108621 | lr 0.000191899 | gnorm 0.34 | train_wall 40 | gb_free 10.2 | wall 106202
2022-08-18 03:41:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:41:46 | INFO | fairseq.trainer | begin training epoch 1342
2022-08-18 03:41:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:42:30 | INFO | train_inner | epoch 1342:     79 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=5222.5, ups=0.94, wpb=5541, bsz=362.6, num_updates=108700, lr=0.000191829, gnorm=0.359, train_wall=49, gb_free=10.1, wall=106247
2022-08-18 03:42:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:42:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:42:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:42:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:42:41 | INFO | valid | epoch 1342 | valid on 'valid' subset | loss 5.266 | nll_loss 2.682 | ppl 6.42 | bleu 56.78 | wps 1826.8 | wpb 933.5 | bsz 59.6 | num_updates 108702 | best_bleu 57.52
2022-08-18 03:42:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1342 @ 108702 updates
2022-08-18 03:42:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1342.pt
2022-08-18 03:42:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1342.pt
2022-08-18 03:43:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1342.pt (epoch 1342 @ 108702 updates, score 56.78) (writing took 18.61181353405118 seconds)
2022-08-18 03:43:00 | INFO | fairseq_cli.train | end of epoch 1342 (average epoch stats below)
2022-08-18 03:43:00 | INFO | train | epoch 1342 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6078.3 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 108702 | lr 0.000191828 | gnorm 0.37 | train_wall 39 | gb_free 10.1 | wall 106276
2022-08-18 03:43:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:43:00 | INFO | fairseq.trainer | begin training epoch 1343
2022-08-18 03:43:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:43:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:43:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:43:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:43:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:43:52 | INFO | valid | epoch 1343 | valid on 'valid' subset | loss 5.261 | nll_loss 2.678 | ppl 6.4 | bleu 56.12 | wps 1712.7 | wpb 933.5 | bsz 59.6 | num_updates 108783 | best_bleu 57.52
2022-08-18 03:43:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1343 @ 108783 updates
2022-08-18 03:43:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1343.pt
2022-08-18 03:43:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1343.pt
2022-08-18 03:44:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1343.pt (epoch 1343 @ 108783 updates, score 56.12) (writing took 22.155154533684254 seconds)
2022-08-18 03:44:14 | INFO | fairseq_cli.train | end of epoch 1343 (average epoch stats below)
2022-08-18 03:44:14 | INFO | train | epoch 1343 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 5990.7 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 108783 | lr 0.000191756 | gnorm 0.356 | train_wall 40 | gb_free 10.1 | wall 106351
2022-08-18 03:44:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:44:15 | INFO | fairseq.trainer | begin training epoch 1344
2022-08-18 03:44:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:44:24 | INFO | train_inner | epoch 1344:     17 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=4861.1, ups=0.88, wpb=5518.3, bsz=355, num_updates=108800, lr=0.000191741, gnorm=0.397, train_wall=49, gb_free=10, wall=106360
2022-08-18 03:44:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:44:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:44:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:44:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:45:06 | INFO | valid | epoch 1344 | valid on 'valid' subset | loss 5.262 | nll_loss 2.678 | ppl 6.4 | bleu 56.46 | wps 1727.5 | wpb 933.5 | bsz 59.6 | num_updates 108864 | best_bleu 57.52
2022-08-18 03:45:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1344 @ 108864 updates
2022-08-18 03:45:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1344.pt
2022-08-18 03:45:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1344.pt
2022-08-18 03:45:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1344.pt (epoch 1344 @ 108864 updates, score 56.46) (writing took 33.018049996346235 seconds)
2022-08-18 03:45:39 | INFO | fairseq_cli.train | end of epoch 1344 (average epoch stats below)
2022-08-18 03:45:39 | INFO | train | epoch 1344 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 5292.3 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 108864 | lr 0.000191685 | gnorm 0.448 | train_wall 40 | gb_free 10.1 | wall 106435
2022-08-18 03:45:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:45:39 | INFO | fairseq.trainer | begin training epoch 1345
2022-08-18 03:45:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:45:58 | INFO | train_inner | epoch 1345:     36 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=5854, ups=1.06, wpb=5508.6, bsz=359.4, num_updates=108900, lr=0.000191653, gnorm=0.373, train_wall=49, gb_free=10, wall=106454
2022-08-18 03:46:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:46:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:46:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:46:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:46:30 | INFO | valid | epoch 1345 | valid on 'valid' subset | loss 5.256 | nll_loss 2.674 | ppl 6.38 | bleu 56.28 | wps 1779.2 | wpb 933.5 | bsz 59.6 | num_updates 108945 | best_bleu 57.52
2022-08-18 03:46:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1345 @ 108945 updates
2022-08-18 03:46:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1345.pt
2022-08-18 03:46:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1345.pt
2022-08-18 03:46:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1345.pt (epoch 1345 @ 108945 updates, score 56.28) (writing took 18.674706391990185 seconds)
2022-08-18 03:46:49 | INFO | fairseq_cli.train | end of epoch 1345 (average epoch stats below)
2022-08-18 03:46:49 | INFO | train | epoch 1345 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6385.6 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 108945 | lr 0.000191614 | gnorm 0.311 | train_wall 39 | gb_free 10.1 | wall 106505
2022-08-18 03:46:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:46:49 | INFO | fairseq.trainer | begin training epoch 1346
2022-08-18 03:46:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:47:18 | INFO | train_inner | epoch 1346:     55 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=6987.3, ups=1.26, wpb=5563.7, bsz=362.8, num_updates=109000, lr=0.000191565, gnorm=0.32, train_wall=49, gb_free=10.1, wall=106534
2022-08-18 03:47:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:47:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:47:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:47:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:47:40 | INFO | valid | epoch 1346 | valid on 'valid' subset | loss 5.259 | nll_loss 2.675 | ppl 6.39 | bleu 56.25 | wps 1766.1 | wpb 933.5 | bsz 59.6 | num_updates 109026 | best_bleu 57.52
2022-08-18 03:47:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1346 @ 109026 updates
2022-08-18 03:47:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1346.pt
2022-08-18 03:47:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1346.pt
2022-08-18 03:48:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1346.pt (epoch 1346 @ 109026 updates, score 56.25) (writing took 21.209825363010168 seconds)
2022-08-18 03:48:02 | INFO | fairseq_cli.train | end of epoch 1346 (average epoch stats below)
2022-08-18 03:48:02 | INFO | train | epoch 1346 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6148.9 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 109026 | lr 0.000191542 | gnorm 0.322 | train_wall 40 | gb_free 10.1 | wall 106578
2022-08-18 03:48:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:48:02 | INFO | fairseq.trainer | begin training epoch 1347
2022-08-18 03:48:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:48:39 | INFO | train_inner | epoch 1347:     74 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=6771.1, ups=1.23, wpb=5493.8, bsz=355.4, num_updates=109100, lr=0.000191477, gnorm=0.372, train_wall=48, gb_free=10.1, wall=106615
2022-08-18 03:48:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:48:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:48:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:48:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:48:52 | INFO | valid | epoch 1347 | valid on 'valid' subset | loss 5.246 | nll_loss 2.659 | ppl 6.32 | bleu 56.36 | wps 1856.9 | wpb 933.5 | bsz 59.6 | num_updates 109107 | best_bleu 57.52
2022-08-18 03:48:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1347 @ 109107 updates
2022-08-18 03:48:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1347.pt
2022-08-18 03:48:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1347.pt
2022-08-18 03:49:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1347.pt (epoch 1347 @ 109107 updates, score 56.36) (writing took 33.64188754186034 seconds)
2022-08-18 03:49:25 | INFO | fairseq_cli.train | end of epoch 1347 (average epoch stats below)
2022-08-18 03:49:25 | INFO | train | epoch 1347 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5350.8 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 109107 | lr 0.000191471 | gnorm 0.395 | train_wall 39 | gb_free 10.1 | wall 106662
2022-08-18 03:49:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:49:26 | INFO | fairseq.trainer | begin training epoch 1348
2022-08-18 03:49:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:50:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:50:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:50:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:50:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:50:18 | INFO | valid | epoch 1348 | valid on 'valid' subset | loss 5.249 | nll_loss 2.659 | ppl 6.32 | bleu 56.85 | wps 1812 | wpb 933.5 | bsz 59.6 | num_updates 109188 | best_bleu 57.52
2022-08-18 03:50:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1348 @ 109188 updates
2022-08-18 03:50:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1348.pt
2022-08-18 03:50:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1348.pt
2022-08-18 03:50:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1348.pt (epoch 1348 @ 109188 updates, score 56.85) (writing took 29.700562838464975 seconds)
2022-08-18 03:50:47 | INFO | fairseq_cli.train | end of epoch 1348 (average epoch stats below)
2022-08-18 03:50:47 | INFO | train | epoch 1348 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 5456.3 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 109188 | lr 0.0001914 | gnorm 0.495 | train_wall 40 | gb_free 10.1 | wall 106744
2022-08-18 03:50:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:50:48 | INFO | fairseq.trainer | begin training epoch 1349
2022-08-18 03:50:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:50:55 | INFO | train_inner | epoch 1349:     12 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=4047.6, ups=0.74, wpb=5504.8, bsz=355.6, num_updates=109200, lr=0.00019139, gnorm=0.47, train_wall=48, gb_free=10.1, wall=106751
2022-08-18 03:51:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:51:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:51:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:51:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:51:38 | INFO | valid | epoch 1349 | valid on 'valid' subset | loss 5.262 | nll_loss 2.679 | ppl 6.4 | bleu 55.34 | wps 1833 | wpb 933.5 | bsz 59.6 | num_updates 109269 | best_bleu 57.52
2022-08-18 03:51:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1349 @ 109269 updates
2022-08-18 03:51:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1349.pt
2022-08-18 03:51:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1349.pt
2022-08-18 03:51:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1349.pt (epoch 1349 @ 109269 updates, score 55.34) (writing took 19.298758901655674 seconds)
2022-08-18 03:51:58 | INFO | fairseq_cli.train | end of epoch 1349 (average epoch stats below)
2022-08-18 03:51:58 | INFO | train | epoch 1349 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6363.1 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 109269 | lr 0.000191329 | gnorm 0.469 | train_wall 39 | gb_free 10.2 | wall 106814
2022-08-18 03:51:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:51:58 | INFO | fairseq.trainer | begin training epoch 1350
2022-08-18 03:51:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:52:14 | INFO | train_inner | epoch 1350:     31 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=7007, ups=1.26, wpb=5542.4, bsz=360, num_updates=109300, lr=0.000191302, gnorm=0.467, train_wall=48, gb_free=10.1, wall=106830
2022-08-18 03:52:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:52:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:52:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:52:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:52:49 | INFO | valid | epoch 1350 | valid on 'valid' subset | loss 5.27 | nll_loss 2.684 | ppl 6.43 | bleu 56.28 | wps 1619.8 | wpb 933.5 | bsz 59.6 | num_updates 109350 | best_bleu 57.52
2022-08-18 03:52:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1350 @ 109350 updates
2022-08-18 03:52:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1350.pt
2022-08-18 03:52:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1350.pt
2022-08-18 03:53:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1350.pt (epoch 1350 @ 109350 updates, score 56.28) (writing took 24.577622912824154 seconds)
2022-08-18 03:53:14 | INFO | fairseq_cli.train | end of epoch 1350 (average epoch stats below)
2022-08-18 03:53:14 | INFO | train | epoch 1350 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 5889.8 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 109350 | lr 0.000191258 | gnorm 0.355 | train_wall 39 | gb_free 10.1 | wall 106890
2022-08-18 03:53:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:53:14 | INFO | fairseq.trainer | begin training epoch 1351
2022-08-18 03:53:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:53:40 | INFO | train_inner | epoch 1351:     50 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=6481.3, ups=1.17, wpb=5549.4, bsz=358.2, num_updates=109400, lr=0.000191215, gnorm=0.326, train_wall=48, gb_free=10.1, wall=106916
2022-08-18 03:53:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:53:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:53:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:53:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:54:04 | INFO | valid | epoch 1351 | valid on 'valid' subset | loss 5.256 | nll_loss 2.67 | ppl 6.37 | bleu 56.31 | wps 1693.7 | wpb 933.5 | bsz 59.6 | num_updates 109431 | best_bleu 57.52
2022-08-18 03:54:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1351 @ 109431 updates
2022-08-18 03:54:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1351.pt
2022-08-18 03:54:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1351.pt
2022-08-18 03:54:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1351.pt (epoch 1351 @ 109431 updates, score 56.31) (writing took 31.766103897243738 seconds)
2022-08-18 03:54:36 | INFO | fairseq_cli.train | end of epoch 1351 (average epoch stats below)
2022-08-18 03:54:36 | INFO | train | epoch 1351 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5413.6 | ups 0.98 | wpb 5523.2 | bsz 358 | num_updates 109431 | lr 0.000191188 | gnorm 0.361 | train_wall 39 | gb_free 10.1 | wall 106973
2022-08-18 03:54:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:54:37 | INFO | fairseq.trainer | begin training epoch 1352
2022-08-18 03:54:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:55:12 | INFO | train_inner | epoch 1352:     69 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=5978, ups=1.09, wpb=5509.4, bsz=357.5, num_updates=109500, lr=0.000191127, gnorm=0.403, train_wall=48, gb_free=10.1, wall=107008
2022-08-18 03:55:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:55:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:55:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:55:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:55:27 | INFO | valid | epoch 1352 | valid on 'valid' subset | loss 5.28 | nll_loss 2.7 | ppl 6.5 | bleu 55.99 | wps 1845.6 | wpb 933.5 | bsz 59.6 | num_updates 109512 | best_bleu 57.52
2022-08-18 03:55:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1352 @ 109512 updates
2022-08-18 03:55:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1352.pt
2022-08-18 03:55:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1352.pt
2022-08-18 03:55:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1352.pt (epoch 1352 @ 109512 updates, score 55.99) (writing took 16.63827284798026 seconds)
2022-08-18 03:55:44 | INFO | fairseq_cli.train | end of epoch 1352 (average epoch stats below)
2022-08-18 03:55:44 | INFO | train | epoch 1352 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6613.7 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 109512 | lr 0.000191117 | gnorm 0.404 | train_wall 39 | gb_free 10.2 | wall 107040
2022-08-18 03:55:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:55:44 | INFO | fairseq.trainer | begin training epoch 1353
2022-08-18 03:55:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:56:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:56:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:56:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:56:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:56:35 | INFO | valid | epoch 1353 | valid on 'valid' subset | loss 5.265 | nll_loss 2.682 | ppl 6.42 | bleu 56.59 | wps 1828.5 | wpb 933.5 | bsz 59.6 | num_updates 109593 | best_bleu 57.52
2022-08-18 03:56:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1353 @ 109593 updates
2022-08-18 03:56:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1353.pt
2022-08-18 03:56:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1353.pt
2022-08-18 03:56:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1353.pt (epoch 1353 @ 109593 updates, score 56.59) (writing took 18.80916427075863 seconds)
2022-08-18 03:56:54 | INFO | fairseq_cli.train | end of epoch 1353 (average epoch stats below)
2022-08-18 03:56:54 | INFO | train | epoch 1353 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6415.1 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 109593 | lr 0.000191046 | gnorm 0.39 | train_wall 40 | gb_free 10.1 | wall 107110
2022-08-18 03:56:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:56:54 | INFO | fairseq.trainer | begin training epoch 1354
2022-08-18 03:56:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:56:59 | INFO | train_inner | epoch 1354:      7 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=5113.4, ups=0.93, wpb=5498.8, bsz=359.7, num_updates=109600, lr=0.00019104, gnorm=0.395, train_wall=49, gb_free=10.1, wall=107116
2022-08-18 03:57:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:57:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:57:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:57:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:57:46 | INFO | valid | epoch 1354 | valid on 'valid' subset | loss 5.261 | nll_loss 2.676 | ppl 6.39 | bleu 56.25 | wps 1759.1 | wpb 933.5 | bsz 59.6 | num_updates 109674 | best_bleu 57.52
2022-08-18 03:57:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1354 @ 109674 updates
2022-08-18 03:57:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1354.pt
2022-08-18 03:57:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1354.pt
2022-08-18 03:58:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1354.pt (epoch 1354 @ 109674 updates, score 56.25) (writing took 39.05678867548704 seconds)
2022-08-18 03:58:25 | INFO | fairseq_cli.train | end of epoch 1354 (average epoch stats below)
2022-08-18 03:58:25 | INFO | train | epoch 1354 | loss 3.37 | nll_loss 0.339 | ppl 1.27 | wps 4892.2 | ups 0.89 | wpb 5523.2 | bsz 358 | num_updates 109674 | lr 0.000190976 | gnorm 0.388 | train_wall 40 | gb_free 10.1 | wall 107201
2022-08-18 03:58:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:58:25 | INFO | fairseq.trainer | begin training epoch 1355
2022-08-18 03:58:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 03:58:39 | INFO | train_inner | epoch 1355:     26 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=5539.5, ups=1, wpb=5541, bsz=356.4, num_updates=109700, lr=0.000190953, gnorm=0.4, train_wall=49, gb_free=10, wall=107216
2022-08-18 03:59:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 03:59:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 03:59:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 03:59:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 03:59:16 | INFO | valid | epoch 1355 | valid on 'valid' subset | loss 5.245 | nll_loss 2.659 | ppl 6.32 | bleu 56.44 | wps 1627.4 | wpb 933.5 | bsz 59.6 | num_updates 109755 | best_bleu 57.52
2022-08-18 03:59:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1355 @ 109755 updates
2022-08-18 03:59:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1355.pt
2022-08-18 03:59:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1355.pt
2022-08-18 03:59:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1355.pt (epoch 1355 @ 109755 updates, score 56.44) (writing took 22.356807336211205 seconds)
2022-08-18 03:59:39 | INFO | fairseq_cli.train | end of epoch 1355 (average epoch stats below)
2022-08-18 03:59:39 | INFO | train | epoch 1355 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6060.3 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 109755 | lr 0.000190905 | gnorm 0.407 | train_wall 39 | gb_free 10.2 | wall 107275
2022-08-18 03:59:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 03:59:39 | INFO | fairseq.trainer | begin training epoch 1356
2022-08-18 03:59:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:00:02 | INFO | train_inner | epoch 1356:     45 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=6643.5, ups=1.21, wpb=5509.6, bsz=358.8, num_updates=109800, lr=0.000190866, gnorm=0.399, train_wall=48, gb_free=10, wall=107299
2022-08-18 04:00:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:00:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:00:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:00:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:00:30 | INFO | valid | epoch 1356 | valid on 'valid' subset | loss 5.243 | nll_loss 2.656 | ppl 6.3 | bleu 56.89 | wps 1796.5 | wpb 933.5 | bsz 59.6 | num_updates 109836 | best_bleu 57.52
2022-08-18 04:00:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1356 @ 109836 updates
2022-08-18 04:00:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1356.pt
2022-08-18 04:00:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1356.pt
2022-08-18 04:00:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1356.pt (epoch 1356 @ 109836 updates, score 56.89) (writing took 15.216324526816607 seconds)
2022-08-18 04:00:45 | INFO | fairseq_cli.train | end of epoch 1356 (average epoch stats below)
2022-08-18 04:00:45 | INFO | train | epoch 1356 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6764.7 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 109836 | lr 0.000190835 | gnorm 0.373 | train_wall 39 | gb_free 10.2 | wall 107341
2022-08-18 04:00:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:00:45 | INFO | fairseq.trainer | begin training epoch 1357
2022-08-18 04:00:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:01:18 | INFO | train_inner | epoch 1357:     64 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=7246, ups=1.31, wpb=5516.3, bsz=355.8, num_updates=109900, lr=0.000190779, gnorm=0.4, train_wall=49, gb_free=10.1, wall=107375
2022-08-18 04:01:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:01:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:01:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:01:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:01:37 | INFO | valid | epoch 1357 | valid on 'valid' subset | loss 5.241 | nll_loss 2.651 | ppl 6.28 | bleu 56.63 | wps 1803 | wpb 933.5 | bsz 59.6 | num_updates 109917 | best_bleu 57.52
2022-08-18 04:01:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1357 @ 109917 updates
2022-08-18 04:01:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1357.pt
2022-08-18 04:01:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1357.pt
2022-08-18 04:02:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1357.pt (epoch 1357 @ 109917 updates, score 56.63) (writing took 29.456734284758568 seconds)
2022-08-18 04:02:06 | INFO | fairseq_cli.train | end of epoch 1357 (average epoch stats below)
2022-08-18 04:02:06 | INFO | train | epoch 1357 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 5520.9 | ups 1 | wpb 5523.2 | bsz 358 | num_updates 109917 | lr 0.000190765 | gnorm 0.419 | train_wall 39 | gb_free 10 | wall 107422
2022-08-18 04:02:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:02:06 | INFO | fairseq.trainer | begin training epoch 1358
2022-08-18 04:02:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:02:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:02:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:02:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:02:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:02:57 | INFO | valid | epoch 1358 | valid on 'valid' subset | loss 5.25 | nll_loss 2.666 | ppl 6.35 | bleu 56.4 | wps 1919.4 | wpb 933.5 | bsz 59.6 | num_updates 109998 | best_bleu 57.52
2022-08-18 04:02:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1358 @ 109998 updates
2022-08-18 04:02:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1358.pt
2022-08-18 04:02:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1358.pt
2022-08-18 04:03:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1358.pt (epoch 1358 @ 109998 updates, score 56.4) (writing took 15.018101591616869 seconds)
2022-08-18 04:03:13 | INFO | fairseq_cli.train | end of epoch 1358 (average epoch stats below)
2022-08-18 04:03:13 | INFO | train | epoch 1358 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6736.9 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 109998 | lr 0.000190694 | gnorm 0.39 | train_wall 40 | gb_free 10.2 | wall 107489
2022-08-18 04:03:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:03:13 | INFO | fairseq.trainer | begin training epoch 1359
2022-08-18 04:03:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:03:15 | INFO | train_inner | epoch 1359:      2 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=4738.7, ups=0.86, wpb=5517.4, bsz=358.3, num_updates=110000, lr=0.000190693, gnorm=0.382, train_wall=49, gb_free=10.1, wall=107491
2022-08-18 04:03:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:03:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:03:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:03:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:04:04 | INFO | valid | epoch 1359 | valid on 'valid' subset | loss 5.26 | nll_loss 2.674 | ppl 6.38 | bleu 56.62 | wps 1800.2 | wpb 933.5 | bsz 59.6 | num_updates 110079 | best_bleu 57.52
2022-08-18 04:04:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1359 @ 110079 updates
2022-08-18 04:04:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1359.pt
2022-08-18 04:04:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1359.pt
2022-08-18 04:04:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1359.pt (epoch 1359 @ 110079 updates, score 56.62) (writing took 13.910130113363266 seconds)
2022-08-18 04:04:18 | INFO | fairseq_cli.train | end of epoch 1359 (average epoch stats below)
2022-08-18 04:04:18 | INFO | train | epoch 1359 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6809.7 | ups 1.23 | wpb 5523.2 | bsz 358 | num_updates 110079 | lr 0.000190624 | gnorm 0.379 | train_wall 40 | gb_free 10.2 | wall 107554
2022-08-18 04:04:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:04:18 | INFO | fairseq.trainer | begin training epoch 1360
2022-08-18 04:04:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:04:30 | INFO | train_inner | epoch 1360:     21 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=7323.7, ups=1.33, wpb=5501.6, bsz=355.9, num_updates=110100, lr=0.000190606, gnorm=0.373, train_wall=49, gb_free=10.1, wall=107566
2022-08-18 04:05:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:05:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:05:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:05:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:05:09 | INFO | valid | epoch 1360 | valid on 'valid' subset | loss 5.254 | nll_loss 2.667 | ppl 6.35 | bleu 56.78 | wps 1861.4 | wpb 933.5 | bsz 59.6 | num_updates 110160 | best_bleu 57.52
2022-08-18 04:05:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1360 @ 110160 updates
2022-08-18 04:05:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1360.pt
2022-08-18 04:05:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1360.pt
2022-08-18 04:05:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1360.pt (epoch 1360 @ 110160 updates, score 56.78) (writing took 35.89735168218613 seconds)
2022-08-18 04:05:45 | INFO | fairseq_cli.train | end of epoch 1360 (average epoch stats below)
2022-08-18 04:05:45 | INFO | train | epoch 1360 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 5162.7 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 110160 | lr 0.000190554 | gnorm 0.334 | train_wall 39 | gb_free 10.1 | wall 107641
2022-08-18 04:05:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:05:45 | INFO | fairseq.trainer | begin training epoch 1361
2022-08-18 04:05:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:06:06 | INFO | train_inner | epoch 1361:     40 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5762, ups=1.04, wpb=5531.5, bsz=359.8, num_updates=110200, lr=0.000190519, gnorm=0.38, train_wall=49, gb_free=10.1, wall=107662
2022-08-18 04:06:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:06:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:06:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:06:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:06:36 | INFO | valid | epoch 1361 | valid on 'valid' subset | loss 5.248 | nll_loss 2.66 | ppl 6.32 | bleu 56.66 | wps 1764.5 | wpb 933.5 | bsz 59.6 | num_updates 110241 | best_bleu 57.52
2022-08-18 04:06:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1361 @ 110241 updates
2022-08-18 04:06:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1361.pt
2022-08-18 04:06:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1361.pt
2022-08-18 04:06:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1361.pt (epoch 1361 @ 110241 updates, score 56.66) (writing took 19.948219042271376 seconds)
2022-08-18 04:06:56 | INFO | fairseq_cli.train | end of epoch 1361 (average epoch stats below)
2022-08-18 04:06:56 | INFO | train | epoch 1361 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6292.6 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 110241 | lr 0.000190484 | gnorm 0.453 | train_wall 39 | gb_free 10.2 | wall 107712
2022-08-18 04:06:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:06:56 | INFO | fairseq.trainer | begin training epoch 1362
2022-08-18 04:06:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:07:26 | INFO | train_inner | epoch 1362:     59 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=6933.1, ups=1.25, wpb=5567.8, bsz=361.6, num_updates=110300, lr=0.000190433, gnorm=0.442, train_wall=49, gb_free=10.1, wall=107743
2022-08-18 04:07:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:07:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:07:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:07:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:07:47 | INFO | valid | epoch 1362 | valid on 'valid' subset | loss 5.247 | nll_loss 2.657 | ppl 6.31 | bleu 56.49 | wps 1826.3 | wpb 933.5 | bsz 59.6 | num_updates 110322 | best_bleu 57.52
2022-08-18 04:07:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1362 @ 110322 updates
2022-08-18 04:07:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1362.pt
2022-08-18 04:07:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1362.pt
2022-08-18 04:08:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1362.pt (epoch 1362 @ 110322 updates, score 56.49) (writing took 14.769824247807264 seconds)
2022-08-18 04:08:02 | INFO | fairseq_cli.train | end of epoch 1362 (average epoch stats below)
2022-08-18 04:08:02 | INFO | train | epoch 1362 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6822.1 | ups 1.24 | wpb 5523.2 | bsz 358 | num_updates 110322 | lr 0.000190414 | gnorm 0.456 | train_wall 39 | gb_free 10.1 | wall 107778
2022-08-18 04:08:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:08:02 | INFO | fairseq.trainer | begin training epoch 1363
2022-08-18 04:08:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:08:42 | INFO | train_inner | epoch 1363:     78 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=7322.6, ups=1.33, wpb=5521.9, bsz=356.4, num_updates=110400, lr=0.000190347, gnorm=0.461, train_wall=49, gb_free=10.1, wall=107818
2022-08-18 04:08:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:08:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:08:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:08:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:08:52 | INFO | valid | epoch 1363 | valid on 'valid' subset | loss 5.25 | nll_loss 2.66 | ppl 6.32 | bleu 55.85 | wps 1798.4 | wpb 933.5 | bsz 59.6 | num_updates 110403 | best_bleu 57.52
2022-08-18 04:08:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1363 @ 110403 updates
2022-08-18 04:08:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1363.pt
2022-08-18 04:08:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1363.pt
2022-08-18 04:09:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1363.pt (epoch 1363 @ 110403 updates, score 55.85) (writing took 36.03862493485212 seconds)
2022-08-18 04:09:29 | INFO | fairseq_cli.train | end of epoch 1363 (average epoch stats below)
2022-08-18 04:09:29 | INFO | train | epoch 1363 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 5137.6 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 110403 | lr 0.000190344 | gnorm 0.464 | train_wall 40 | gb_free 10.2 | wall 107865
2022-08-18 04:09:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:09:29 | INFO | fairseq.trainer | begin training epoch 1364
2022-08-18 04:09:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:10:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:10:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:10:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:10:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:10:19 | INFO | valid | epoch 1364 | valid on 'valid' subset | loss 5.249 | nll_loss 2.662 | ppl 6.33 | bleu 56.5 | wps 1779.8 | wpb 933.5 | bsz 59.6 | num_updates 110484 | best_bleu 57.52
2022-08-18 04:10:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1364 @ 110484 updates
2022-08-18 04:10:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1364.pt
2022-08-18 04:10:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1364.pt
2022-08-18 04:10:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1364.pt (epoch 1364 @ 110484 updates, score 56.5) (writing took 18.49816893786192 seconds)
2022-08-18 04:10:38 | INFO | fairseq_cli.train | end of epoch 1364 (average epoch stats below)
2022-08-18 04:10:38 | INFO | train | epoch 1364 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 6502.9 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 110484 | lr 0.000190274 | gnorm 0.642 | train_wall 39 | gb_free 10.1 | wall 107934
2022-08-18 04:10:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:10:38 | INFO | fairseq.trainer | begin training epoch 1365
2022-08-18 04:10:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:10:46 | INFO | train_inner | epoch 1365:     16 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=4418.5, ups=0.8, wpb=5501.3, bsz=357.2, num_updates=110500, lr=0.000190261, gnorm=0.604, train_wall=47, gb_free=10.1, wall=107942
2022-08-18 04:11:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:11:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:11:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:11:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:11:28 | INFO | valid | epoch 1365 | valid on 'valid' subset | loss 5.256 | nll_loss 2.67 | ppl 6.36 | bleu 56.43 | wps 1790.9 | wpb 933.5 | bsz 59.6 | num_updates 110565 | best_bleu 57.52
2022-08-18 04:11:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1365 @ 110565 updates
2022-08-18 04:11:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1365.pt
2022-08-18 04:11:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1365.pt
2022-08-18 04:11:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1365.pt (epoch 1365 @ 110565 updates, score 56.43) (writing took 19.266731966286898 seconds)
2022-08-18 04:11:47 | INFO | fairseq_cli.train | end of epoch 1365 (average epoch stats below)
2022-08-18 04:11:47 | INFO | train | epoch 1365 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6430.6 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 110565 | lr 0.000190205 | gnorm 0.417 | train_wall 39 | gb_free 10.2 | wall 108003
2022-08-18 04:11:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:11:47 | INFO | fairseq.trainer | begin training epoch 1366
2022-08-18 04:11:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:12:06 | INFO | train_inner | epoch 1366:     35 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=6885.5, ups=1.25, wpb=5521.1, bsz=361.4, num_updates=110600, lr=0.000190175, gnorm=0.422, train_wall=49, gb_free=10.1, wall=108023
2022-08-18 04:12:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:12:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:12:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:12:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:12:38 | INFO | valid | epoch 1366 | valid on 'valid' subset | loss 5.247 | nll_loss 2.659 | ppl 6.31 | bleu 56.8 | wps 1891.7 | wpb 933.5 | bsz 59.6 | num_updates 110646 | best_bleu 57.52
2022-08-18 04:12:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1366 @ 110646 updates
2022-08-18 04:12:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1366.pt
2022-08-18 04:12:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1366.pt
2022-08-18 04:13:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1366.pt (epoch 1366 @ 110646 updates, score 56.8) (writing took 22.762674771249294 seconds)
2022-08-18 04:13:01 | INFO | fairseq_cli.train | end of epoch 1366 (average epoch stats below)
2022-08-18 04:13:01 | INFO | train | epoch 1366 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6066.2 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 110646 | lr 0.000190135 | gnorm 0.86 | train_wall 39 | gb_free 10.2 | wall 108077
2022-08-18 04:13:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:13:01 | INFO | fairseq.trainer | begin training epoch 1367
2022-08-18 04:13:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:13:29 | INFO | train_inner | epoch 1367:     54 / 81 loss=3.371, nll_loss=0.339, ppl=1.27, wps=6651.5, ups=1.2, wpb=5521.1, bsz=353.7, num_updates=110700, lr=0.000190089, gnorm=0.757, train_wall=49, gb_free=10.1, wall=108106
2022-08-18 04:13:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:13:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:13:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:13:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:13:52 | INFO | valid | epoch 1367 | valid on 'valid' subset | loss 5.247 | nll_loss 2.66 | ppl 6.32 | bleu 56.77 | wps 1908.2 | wpb 933.5 | bsz 59.6 | num_updates 110727 | best_bleu 57.52
2022-08-18 04:13:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1367 @ 110727 updates
2022-08-18 04:13:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1367.pt
2022-08-18 04:13:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1367.pt
2022-08-18 04:14:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1367.pt (epoch 1367 @ 110727 updates, score 56.77) (writing took 16.952514313161373 seconds)
2022-08-18 04:14:09 | INFO | fairseq_cli.train | end of epoch 1367 (average epoch stats below)
2022-08-18 04:14:09 | INFO | train | epoch 1367 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6598.4 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 110727 | lr 0.000190065 | gnorm 0.355 | train_wall 40 | gb_free 10.1 | wall 108145
2022-08-18 04:14:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:14:09 | INFO | fairseq.trainer | begin training epoch 1368
2022-08-18 04:14:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:14:46 | INFO | train_inner | epoch 1368:     73 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=7232.1, ups=1.3, wpb=5549, bsz=361.9, num_updates=110800, lr=0.000190003, gnorm=0.371, train_wall=48, gb_free=10.1, wall=108182
2022-08-18 04:14:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:14:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:14:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:14:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:15:00 | INFO | valid | epoch 1368 | valid on 'valid' subset | loss 5.24 | nll_loss 2.65 | ppl 6.27 | bleu 57.15 | wps 1642.3 | wpb 933.5 | bsz 59.6 | num_updates 110808 | best_bleu 57.52
2022-08-18 04:15:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1368 @ 110808 updates
2022-08-18 04:15:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1368.pt
2022-08-18 04:15:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1368.pt
2022-08-18 04:15:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1368.pt (epoch 1368 @ 110808 updates, score 57.15) (writing took 46.35266523435712 seconds)
2022-08-18 04:15:47 | INFO | fairseq_cli.train | end of epoch 1368 (average epoch stats below)
2022-08-18 04:15:47 | INFO | train | epoch 1368 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 4555.8 | ups 0.82 | wpb 5523.2 | bsz 358 | num_updates 110808 | lr 0.000189996 | gnorm 0.383 | train_wall 39 | gb_free 10.1 | wall 108243
2022-08-18 04:15:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:15:47 | INFO | fairseq.trainer | begin training epoch 1369
2022-08-18 04:15:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:16:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:16:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:16:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:16:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:16:37 | INFO | valid | epoch 1369 | valid on 'valid' subset | loss 5.244 | nll_loss 2.656 | ppl 6.3 | bleu 56.45 | wps 1802.5 | wpb 933.5 | bsz 59.6 | num_updates 110889 | best_bleu 57.52
2022-08-18 04:16:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1369 @ 110889 updates
2022-08-18 04:16:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1369.pt
2022-08-18 04:16:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1369.pt
2022-08-18 04:16:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1369.pt (epoch 1369 @ 110889 updates, score 56.45) (writing took 20.275029979646206 seconds)
2022-08-18 04:16:57 | INFO | fairseq_cli.train | end of epoch 1369 (average epoch stats below)
2022-08-18 04:16:57 | INFO | train | epoch 1369 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6354.6 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 110889 | lr 0.000189927 | gnorm 0.349 | train_wall 39 | gb_free 10.1 | wall 108313
2022-08-18 04:16:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:16:57 | INFO | fairseq.trainer | begin training epoch 1370
2022-08-18 04:16:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:17:04 | INFO | train_inner | epoch 1370:     11 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=3999.3, ups=0.73, wpb=5500, bsz=351.2, num_updates=110900, lr=0.000189917, gnorm=0.355, train_wall=48, gb_free=10.1, wall=108320
2022-08-18 04:17:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:17:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:17:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:17:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:17:49 | INFO | valid | epoch 1370 | valid on 'valid' subset | loss 5.241 | nll_loss 2.655 | ppl 6.3 | bleu 56.87 | wps 1582.5 | wpb 933.5 | bsz 59.6 | num_updates 110970 | best_bleu 57.52
2022-08-18 04:17:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1370 @ 110970 updates
2022-08-18 04:17:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1370.pt
2022-08-18 04:17:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1370.pt
2022-08-18 04:18:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1370.pt (epoch 1370 @ 110970 updates, score 56.87) (writing took 16.16001684591174 seconds)
2022-08-18 04:18:05 | INFO | fairseq_cli.train | end of epoch 1370 (average epoch stats below)
2022-08-18 04:18:05 | INFO | train | epoch 1370 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6616.5 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 110970 | lr 0.000189857 | gnorm 0.321 | train_wall 39 | gb_free 10.1 | wall 108381
2022-08-18 04:18:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:18:05 | INFO | fairseq.trainer | begin training epoch 1371
2022-08-18 04:18:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:18:22 | INFO | train_inner | epoch 1371:     30 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=7145.6, ups=1.29, wpb=5559, bsz=364.3, num_updates=111000, lr=0.000189832, gnorm=0.316, train_wall=49, gb_free=10.1, wall=108398
2022-08-18 04:18:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:18:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:18:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:18:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:18:56 | INFO | valid | epoch 1371 | valid on 'valid' subset | loss 5.259 | nll_loss 2.679 | ppl 6.4 | bleu 56.17 | wps 1925.8 | wpb 933.5 | bsz 59.6 | num_updates 111051 | best_bleu 57.52
2022-08-18 04:18:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1371 @ 111051 updates
2022-08-18 04:18:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1371.pt
2022-08-18 04:18:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1371.pt
2022-08-18 04:19:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1371.pt (epoch 1371 @ 111051 updates, score 56.17) (writing took 36.610880360007286 seconds)
2022-08-18 04:19:33 | INFO | fairseq_cli.train | end of epoch 1371 (average epoch stats below)
2022-08-18 04:19:33 | INFO | train | epoch 1371 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5083.8 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 111051 | lr 0.000189788 | gnorm 0.387 | train_wall 40 | gb_free 10.1 | wall 108469
2022-08-18 04:19:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:19:33 | INFO | fairseq.trainer | begin training epoch 1372
2022-08-18 04:19:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:20:01 | INFO | train_inner | epoch 1372:     49 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5541.5, ups=1, wpb=5518.8, bsz=357.7, num_updates=111100, lr=0.000189746, gnorm=0.358, train_wall=49, gb_free=10.1, wall=108497
2022-08-18 04:20:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:20:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:20:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:20:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:20:28 | INFO | valid | epoch 1372 | valid on 'valid' subset | loss 5.267 | nll_loss 2.683 | ppl 6.42 | bleu 56.35 | wps 1809 | wpb 933.5 | bsz 59.6 | num_updates 111132 | best_bleu 57.52
2022-08-18 04:20:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1372 @ 111132 updates
2022-08-18 04:20:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1372.pt
2022-08-18 04:20:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1372.pt
2022-08-18 04:20:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1372.pt (epoch 1372 @ 111132 updates, score 56.35) (writing took 20.128647550940514 seconds)
2022-08-18 04:20:48 | INFO | fairseq_cli.train | end of epoch 1372 (average epoch stats below)
2022-08-18 04:20:48 | INFO | train | epoch 1372 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5956.9 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 111132 | lr 0.000189719 | gnorm 0.481 | train_wall 39 | gb_free 10.1 | wall 108544
2022-08-18 04:20:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:20:48 | INFO | fairseq.trainer | begin training epoch 1373
2022-08-18 04:20:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:21:23 | INFO | train_inner | epoch 1373:     68 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=6733.8, ups=1.22, wpb=5513, bsz=354.5, num_updates=111200, lr=0.000189661, gnorm=0.521, train_wall=49, gb_free=10, wall=108579
2022-08-18 04:21:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:21:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:21:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:21:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:21:38 | INFO | valid | epoch 1373 | valid on 'valid' subset | loss 5.255 | nll_loss 2.67 | ppl 6.37 | bleu 56.24 | wps 1885.1 | wpb 933.5 | bsz 59.6 | num_updates 111213 | best_bleu 57.52
2022-08-18 04:21:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1373 @ 111213 updates
2022-08-18 04:21:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1373.pt
2022-08-18 04:21:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1373.pt
2022-08-18 04:21:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1373.pt (epoch 1373 @ 111213 updates, score 56.24) (writing took 15.201202690601349 seconds)
2022-08-18 04:21:54 | INFO | fairseq_cli.train | end of epoch 1373 (average epoch stats below)
2022-08-18 04:21:54 | INFO | train | epoch 1373 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 6814.4 | ups 1.23 | wpb 5523.2 | bsz 358 | num_updates 111213 | lr 0.00018965 | gnorm 0.398 | train_wall 39 | gb_free 10.2 | wall 108610
2022-08-18 04:21:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:21:54 | INFO | fairseq.trainer | begin training epoch 1374
2022-08-18 04:21:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:22:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:22:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:22:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:22:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:22:45 | INFO | valid | epoch 1374 | valid on 'valid' subset | loss 5.255 | nll_loss 2.669 | ppl 6.36 | bleu 56.61 | wps 1697 | wpb 933.5 | bsz 59.6 | num_updates 111294 | best_bleu 57.52
2022-08-18 04:22:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1374 @ 111294 updates
2022-08-18 04:22:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1374.pt
2022-08-18 04:22:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1374.pt
2022-08-18 04:23:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1374.pt (epoch 1374 @ 111294 updates, score 56.61) (writing took 37.03466507419944 seconds)
2022-08-18 04:23:22 | INFO | fairseq_cli.train | end of epoch 1374 (average epoch stats below)
2022-08-18 04:23:22 | INFO | train | epoch 1374 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5049.5 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 111294 | lr 0.000189581 | gnorm 0.328 | train_wall 39 | gb_free 10.1 | wall 108698
2022-08-18 04:23:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:23:22 | INFO | fairseq.trainer | begin training epoch 1375
2022-08-18 04:23:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:23:27 | INFO | train_inner | epoch 1375:      6 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=4456.2, ups=0.81, wpb=5507.2, bsz=358.9, num_updates=111300, lr=0.000189576, gnorm=0.335, train_wall=48, gb_free=10.1, wall=108703
2022-08-18 04:24:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:24:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:24:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:24:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:24:13 | INFO | valid | epoch 1375 | valid on 'valid' subset | loss 5.252 | nll_loss 2.668 | ppl 6.36 | bleu 57.21 | wps 1781.1 | wpb 933.5 | bsz 59.6 | num_updates 111375 | best_bleu 57.52
2022-08-18 04:24:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1375 @ 111375 updates
2022-08-18 04:24:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1375.pt
2022-08-18 04:24:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1375.pt
2022-08-18 04:24:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1375.pt (epoch 1375 @ 111375 updates, score 57.21) (writing took 45.240948390215635 seconds)
2022-08-18 04:24:58 | INFO | fairseq_cli.train | end of epoch 1375 (average epoch stats below)
2022-08-18 04:24:58 | INFO | train | epoch 1375 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 4648.8 | ups 0.84 | wpb 5523.2 | bsz 358 | num_updates 111375 | lr 0.000189512 | gnorm 0.422 | train_wall 39 | gb_free 10.1 | wall 108795
2022-08-18 04:24:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:24:59 | INFO | fairseq.trainer | begin training epoch 1376
2022-08-18 04:24:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:25:12 | INFO | train_inner | epoch 1376:     25 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=5231.3, ups=0.95, wpb=5522, bsz=360.5, num_updates=111400, lr=0.00018949, gnorm=0.405, train_wall=49, gb_free=10.1, wall=108808
2022-08-18 04:25:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:25:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:25:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:25:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:25:49 | INFO | valid | epoch 1376 | valid on 'valid' subset | loss 5.251 | nll_loss 2.663 | ppl 6.33 | bleu 56.5 | wps 1778.2 | wpb 933.5 | bsz 59.6 | num_updates 111456 | best_bleu 57.52
2022-08-18 04:25:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1376 @ 111456 updates
2022-08-18 04:25:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1376.pt
2022-08-18 04:25:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1376.pt
2022-08-18 04:26:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1376.pt (epoch 1376 @ 111456 updates, score 56.5) (writing took 14.602450106292963 seconds)
2022-08-18 04:26:04 | INFO | fairseq_cli.train | end of epoch 1376 (average epoch stats below)
2022-08-18 04:26:04 | INFO | train | epoch 1376 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6864.7 | ups 1.24 | wpb 5523.2 | bsz 358 | num_updates 111456 | lr 0.000189443 | gnorm 0.308 | train_wall 39 | gb_free 10.2 | wall 108860
2022-08-18 04:26:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:26:04 | INFO | fairseq.trainer | begin training epoch 1377
2022-08-18 04:26:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:26:26 | INFO | train_inner | epoch 1377:     44 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=7500.5, ups=1.35, wpb=5539.3, bsz=356.2, num_updates=111500, lr=0.000189405, gnorm=0.315, train_wall=48, gb_free=10.1, wall=108882
2022-08-18 04:26:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:26:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:26:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:26:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:26:54 | INFO | valid | epoch 1377 | valid on 'valid' subset | loss 5.248 | nll_loss 2.663 | ppl 6.33 | bleu 56.98 | wps 1645.9 | wpb 933.5 | bsz 59.6 | num_updates 111537 | best_bleu 57.52
2022-08-18 04:26:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1377 @ 111537 updates
2022-08-18 04:26:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1377.pt
2022-08-18 04:26:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1377.pt
2022-08-18 04:26:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1377.pt (epoch 1377 @ 111537 updates, score 56.98) (writing took 2.4723310880362988 seconds)
2022-08-18 04:26:56 | INFO | fairseq_cli.train | end of epoch 1377 (average epoch stats below)
2022-08-18 04:26:56 | INFO | train | epoch 1377 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 8470 | ups 1.53 | wpb 5523.2 | bsz 358 | num_updates 111537 | lr 0.000189374 | gnorm 0.351 | train_wall 38 | gb_free 10.1 | wall 108913
2022-08-18 04:26:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:26:57 | INFO | fairseq.trainer | begin training epoch 1378
2022-08-18 04:26:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:27:30 | INFO | train_inner | epoch 1378:     63 / 81 loss=3.37, nll_loss=0.339, ppl=1.27, wps=8626.4, ups=1.56, wpb=5523.6, bsz=359, num_updates=111600, lr=0.000189321, gnorm=0.383, train_wall=48, gb_free=10.1, wall=108946
2022-08-18 04:27:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:27:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:27:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:27:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:27:48 | INFO | valid | epoch 1378 | valid on 'valid' subset | loss 5.272 | nll_loss 2.69 | ppl 6.45 | bleu 55.99 | wps 1755.7 | wpb 933.5 | bsz 59.6 | num_updates 111618 | best_bleu 57.52
2022-08-18 04:27:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1378 @ 111618 updates
2022-08-18 04:27:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1378.pt
2022-08-18 04:27:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1378.pt
2022-08-18 04:28:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1378.pt (epoch 1378 @ 111618 updates, score 55.99) (writing took 19.65390170738101 seconds)
2022-08-18 04:28:09 | INFO | fairseq_cli.train | end of epoch 1378 (average epoch stats below)
2022-08-18 04:28:09 | INFO | train | epoch 1378 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 6201.8 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 111618 | lr 0.000189305 | gnorm 0.43 | train_wall 39 | gb_free 10.1 | wall 108985
2022-08-18 04:28:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:28:09 | INFO | fairseq.trainer | begin training epoch 1379
2022-08-18 04:28:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:28:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:28:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:28:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:28:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:29:00 | INFO | valid | epoch 1379 | valid on 'valid' subset | loss 5.263 | nll_loss 2.682 | ppl 6.42 | bleu 56.36 | wps 1742.6 | wpb 933.5 | bsz 59.6 | num_updates 111699 | best_bleu 57.52
2022-08-18 04:29:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1379 @ 111699 updates
2022-08-18 04:29:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1379.pt
2022-08-18 04:29:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1379.pt
2022-08-18 04:29:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1379.pt (epoch 1379 @ 111699 updates, score 56.36) (writing took 36.706821244210005 seconds)
2022-08-18 04:29:36 | INFO | fairseq_cli.train | end of epoch 1379 (average epoch stats below)
2022-08-18 04:29:36 | INFO | train | epoch 1379 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 5095.5 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 111699 | lr 0.000189237 | gnorm 0.37 | train_wall 39 | gb_free 10.2 | wall 109073
2022-08-18 04:29:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:29:37 | INFO | fairseq.trainer | begin training epoch 1380
2022-08-18 04:29:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:29:38 | INFO | train_inner | epoch 1380:      1 / 81 loss=3.37, nll_loss=0.339, ppl=1.27, wps=4296.6, ups=0.78, wpb=5505, bsz=356.2, num_updates=111700, lr=0.000189236, gnorm=0.405, train_wall=48, gb_free=10.1, wall=109074
2022-08-18 04:30:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:30:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:30:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:30:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:30:29 | INFO | valid | epoch 1380 | valid on 'valid' subset | loss 5.275 | nll_loss 2.696 | ppl 6.48 | bleu 56 | wps 1814.4 | wpb 933.5 | bsz 59.6 | num_updates 111780 | best_bleu 57.52
2022-08-18 04:30:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1380 @ 111780 updates
2022-08-18 04:30:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1380.pt
2022-08-18 04:30:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1380.pt
2022-08-18 04:30:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1380.pt (epoch 1380 @ 111780 updates, score 56.0) (writing took 2.734503034502268 seconds)
2022-08-18 04:30:32 | INFO | fairseq_cli.train | end of epoch 1380 (average epoch stats below)
2022-08-18 04:30:32 | INFO | train | epoch 1380 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 8110.6 | ups 1.47 | wpb 5523.2 | bsz 358 | num_updates 111780 | lr 0.000189168 | gnorm 0.385 | train_wall 41 | gb_free 10.1 | wall 109128
2022-08-18 04:30:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:30:32 | INFO | fairseq.trainer | begin training epoch 1381
2022-08-18 04:30:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:30:43 | INFO | train_inner | epoch 1381:     20 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=8525.8, ups=1.54, wpb=5529.6, bsz=360.9, num_updates=111800, lr=0.000189151, gnorm=0.373, train_wall=50, gb_free=10.1, wall=109139
2022-08-18 04:31:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:31:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:31:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:31:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:31:23 | INFO | valid | epoch 1381 | valid on 'valid' subset | loss 5.252 | nll_loss 2.666 | ppl 6.35 | bleu 56.64 | wps 1596 | wpb 933.5 | bsz 59.6 | num_updates 111861 | best_bleu 57.52
2022-08-18 04:31:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1381 @ 111861 updates
2022-08-18 04:31:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1381.pt
2022-08-18 04:31:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1381.pt
2022-08-18 04:31:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1381.pt (epoch 1381 @ 111861 updates, score 56.64) (writing took 20.67745701596141 seconds)
2022-08-18 04:31:44 | INFO | fairseq_cli.train | end of epoch 1381 (average epoch stats below)
2022-08-18 04:31:44 | INFO | train | epoch 1381 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6159.8 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 111861 | lr 0.0001891 | gnorm 0.379 | train_wall 39 | gb_free 10.1 | wall 109200
2022-08-18 04:31:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:31:44 | INFO | fairseq.trainer | begin training epoch 1382
2022-08-18 04:31:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:32:05 | INFO | train_inner | epoch 1382:     39 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=6762.4, ups=1.22, wpb=5532.2, bsz=358.8, num_updates=111900, lr=0.000189067, gnorm=0.375, train_wall=48, gb_free=10.1, wall=109221
2022-08-18 04:32:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:32:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:32:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:32:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:32:36 | INFO | valid | epoch 1382 | valid on 'valid' subset | loss 5.263 | nll_loss 2.681 | ppl 6.41 | bleu 56.67 | wps 1800.9 | wpb 933.5 | bsz 59.6 | num_updates 111942 | best_bleu 57.52
2022-08-18 04:32:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1382 @ 111942 updates
2022-08-18 04:32:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1382.pt
2022-08-18 04:32:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1382.pt
2022-08-18 04:33:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1382.pt (epoch 1382 @ 111942 updates, score 56.67) (writing took 24.623079381883144 seconds)
2022-08-18 04:33:01 | INFO | fairseq_cli.train | end of epoch 1382 (average epoch stats below)
2022-08-18 04:33:01 | INFO | train | epoch 1382 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 5863.8 | ups 1.06 | wpb 5523.2 | bsz 358 | num_updates 111942 | lr 0.000189031 | gnorm 0.331 | train_wall 39 | gb_free 10.1 | wall 109277
2022-08-18 04:33:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:33:01 | INFO | fairseq.trainer | begin training epoch 1383
2022-08-18 04:33:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:33:31 | INFO | train_inner | epoch 1383:     58 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=6416.6, ups=1.16, wpb=5539.9, bsz=353.9, num_updates=112000, lr=0.000188982, gnorm=0.399, train_wall=50, gb_free=10.1, wall=109307
2022-08-18 04:33:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:33:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:33:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:33:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:33:52 | INFO | valid | epoch 1383 | valid on 'valid' subset | loss 5.249 | nll_loss 2.665 | ppl 6.34 | bleu 56.99 | wps 1852.6 | wpb 933.5 | bsz 59.6 | num_updates 112023 | best_bleu 57.52
2022-08-18 04:33:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1383 @ 112023 updates
2022-08-18 04:33:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1383.pt
2022-08-18 04:33:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1383.pt
2022-08-18 04:34:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1383.pt (epoch 1383 @ 112023 updates, score 56.99) (writing took 16.940269980579615 seconds)
2022-08-18 04:34:09 | INFO | fairseq_cli.train | end of epoch 1383 (average epoch stats below)
2022-08-18 04:34:09 | INFO | train | epoch 1383 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6528.4 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 112023 | lr 0.000188963 | gnorm 0.441 | train_wall 41 | gb_free 10.2 | wall 109345
2022-08-18 04:34:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:34:09 | INFO | fairseq.trainer | begin training epoch 1384
2022-08-18 04:34:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:34:48 | INFO | train_inner | epoch 1384:     77 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=7132.4, ups=1.29, wpb=5511.4, bsz=359.4, num_updates=112100, lr=0.000188898, gnorm=0.36, train_wall=49, gb_free=10.1, wall=109385
2022-08-18 04:34:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:34:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:34:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:34:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:35:00 | INFO | valid | epoch 1384 | valid on 'valid' subset | loss 5.267 | nll_loss 2.684 | ppl 6.43 | bleu 56.01 | wps 1795.8 | wpb 933.5 | bsz 59.6 | num_updates 112104 | best_bleu 57.52
2022-08-18 04:35:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1384 @ 112104 updates
2022-08-18 04:35:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1384.pt
2022-08-18 04:35:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1384.pt
2022-08-18 04:35:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1384.pt (epoch 1384 @ 112104 updates, score 56.01) (writing took 34.005880784243345 seconds)
2022-08-18 04:35:34 | INFO | fairseq_cli.train | end of epoch 1384 (average epoch stats below)
2022-08-18 04:35:34 | INFO | train | epoch 1384 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5266.6 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 112104 | lr 0.000188895 | gnorm 0.347 | train_wall 40 | gb_free 10.1 | wall 109430
2022-08-18 04:35:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:35:34 | INFO | fairseq.trainer | begin training epoch 1385
2022-08-18 04:35:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:36:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:36:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:36:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:36:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:36:24 | INFO | valid | epoch 1385 | valid on 'valid' subset | loss 5.262 | nll_loss 2.674 | ppl 6.38 | bleu 56.08 | wps 1876 | wpb 933.5 | bsz 59.6 | num_updates 112185 | best_bleu 57.52
2022-08-18 04:36:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1385 @ 112185 updates
2022-08-18 04:36:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1385.pt
2022-08-18 04:36:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1385.pt
2022-08-18 04:37:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1385.pt (epoch 1385 @ 112185 updates, score 56.08) (writing took 48.31133884191513 seconds)
2022-08-18 04:37:13 | INFO | fairseq_cli.train | end of epoch 1385 (average epoch stats below)
2022-08-18 04:37:13 | INFO | train | epoch 1385 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 4523.3 | ups 0.82 | wpb 5523.2 | bsz 358 | num_updates 112185 | lr 0.000188826 | gnorm 0.381 | train_wall 38 | gb_free 10.2 | wall 109529
2022-08-18 04:37:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:37:13 | INFO | fairseq.trainer | begin training epoch 1386
2022-08-18 04:37:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:37:22 | INFO | train_inner | epoch 1386:     15 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=3583.7, ups=0.65, wpb=5489.7, bsz=353.9, num_updates=112200, lr=0.000188814, gnorm=0.384, train_wall=47, gb_free=10.1, wall=109538
2022-08-18 04:37:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:37:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:37:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:37:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:38:03 | INFO | valid | epoch 1386 | valid on 'valid' subset | loss 5.266 | nll_loss 2.683 | ppl 6.42 | bleu 56.78 | wps 1795.6 | wpb 933.5 | bsz 59.6 | num_updates 112266 | best_bleu 57.52
2022-08-18 04:38:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1386 @ 112266 updates
2022-08-18 04:38:03 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1386.pt
2022-08-18 04:38:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1386.pt
2022-08-18 04:38:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1386.pt (epoch 1386 @ 112266 updates, score 56.78) (writing took 16.018213909119368 seconds)
2022-08-18 04:38:19 | INFO | fairseq_cli.train | end of epoch 1386 (average epoch stats below)
2022-08-18 04:38:19 | INFO | train | epoch 1386 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6776.3 | ups 1.23 | wpb 5523.2 | bsz 358 | num_updates 112266 | lr 0.000188758 | gnorm 0.343 | train_wall 38 | gb_free 10.1 | wall 109595
2022-08-18 04:38:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:38:19 | INFO | fairseq.trainer | begin training epoch 1387
2022-08-18 04:38:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:38:37 | INFO | train_inner | epoch 1387:     34 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=7339.6, ups=1.32, wpb=5549.1, bsz=363.1, num_updates=112300, lr=0.00018873, gnorm=0.343, train_wall=48, gb_free=10.1, wall=109613
2022-08-18 04:39:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:39:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:39:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:39:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:39:09 | INFO | valid | epoch 1387 | valid on 'valid' subset | loss 5.26 | nll_loss 2.678 | ppl 6.4 | bleu 56.86 | wps 1876.9 | wpb 933.5 | bsz 59.6 | num_updates 112347 | best_bleu 57.52
2022-08-18 04:39:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1387 @ 112347 updates
2022-08-18 04:39:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1387.pt
2022-08-18 04:39:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1387.pt
2022-08-18 04:39:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1387.pt (epoch 1387 @ 112347 updates, score 56.86) (writing took 14.401828199625015 seconds)
2022-08-18 04:39:24 | INFO | fairseq_cli.train | end of epoch 1387 (average epoch stats below)
2022-08-18 04:39:24 | INFO | train | epoch 1387 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6888.8 | ups 1.25 | wpb 5523.2 | bsz 358 | num_updates 112347 | lr 0.00018869 | gnorm 0.438 | train_wall 40 | gb_free 10.2 | wall 109660
2022-08-18 04:39:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:39:24 | INFO | fairseq.trainer | begin training epoch 1388
2022-08-18 04:39:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:39:52 | INFO | train_inner | epoch 1388:     53 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=7432.5, ups=1.35, wpb=5524, bsz=356.9, num_updates=112400, lr=0.000188646, gnorm=0.44, train_wall=49, gb_free=10.1, wall=109688
2022-08-18 04:40:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:40:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:40:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:40:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:40:15 | INFO | valid | epoch 1388 | valid on 'valid' subset | loss 5.254 | nll_loss 2.667 | ppl 6.35 | bleu 56.69 | wps 1759 | wpb 933.5 | bsz 59.6 | num_updates 112428 | best_bleu 57.52
2022-08-18 04:40:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1388 @ 112428 updates
2022-08-18 04:40:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1388.pt
2022-08-18 04:40:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1388.pt
2022-08-18 04:41:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1388.pt (epoch 1388 @ 112428 updates, score 56.69) (writing took 47.509772937744856 seconds)
2022-08-18 04:41:02 | INFO | fairseq_cli.train | end of epoch 1388 (average epoch stats below)
2022-08-18 04:41:02 | INFO | train | epoch 1388 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 4544 | ups 0.82 | wpb 5523.2 | bsz 358 | num_updates 112428 | lr 0.000188622 | gnorm 0.404 | train_wall 39 | gb_free 10.1 | wall 109759
2022-08-18 04:41:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:41:03 | INFO | fairseq.trainer | begin training epoch 1389
2022-08-18 04:41:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:41:39 | INFO | train_inner | epoch 1389:     72 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=5161.2, ups=0.93, wpb=5533.4, bsz=356.9, num_updates=112500, lr=0.000188562, gnorm=0.363, train_wall=47, gb_free=10, wall=109795
2022-08-18 04:41:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:41:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:41:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:41:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:41:52 | INFO | valid | epoch 1389 | valid on 'valid' subset | loss 5.248 | nll_loss 2.664 | ppl 6.34 | bleu 56.25 | wps 1871.1 | wpb 933.5 | bsz 59.6 | num_updates 112509 | best_bleu 57.52
2022-08-18 04:41:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1389 @ 112509 updates
2022-08-18 04:41:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1389.pt
2022-08-18 04:41:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1389.pt
2022-08-18 04:42:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1389.pt (epoch 1389 @ 112509 updates, score 56.25) (writing took 16.385201081633568 seconds)
2022-08-18 04:42:09 | INFO | fairseq_cli.train | end of epoch 1389 (average epoch stats below)
2022-08-18 04:42:09 | INFO | train | epoch 1389 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6737.9 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 112509 | lr 0.000188554 | gnorm 0.368 | train_wall 38 | gb_free 10 | wall 109825
2022-08-18 04:42:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:42:09 | INFO | fairseq.trainer | begin training epoch 1390
2022-08-18 04:42:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:42:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:42:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:42:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:42:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:42:59 | INFO | valid | epoch 1390 | valid on 'valid' subset | loss 5.259 | nll_loss 2.678 | ppl 6.4 | bleu 56.69 | wps 1767 | wpb 933.5 | bsz 59.6 | num_updates 112590 | best_bleu 57.52
2022-08-18 04:42:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1390 @ 112590 updates
2022-08-18 04:42:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1390.pt
2022-08-18 04:43:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1390.pt
2022-08-18 04:43:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1390.pt (epoch 1390 @ 112590 updates, score 56.69) (writing took 25.601308189332485 seconds)
2022-08-18 04:43:25 | INFO | fairseq_cli.train | end of epoch 1390 (average epoch stats below)
2022-08-18 04:43:25 | INFO | train | epoch 1390 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5873.5 | ups 1.06 | wpb 5523.2 | bsz 358 | num_updates 112590 | lr 0.000188486 | gnorm 0.379 | train_wall 39 | gb_free 10.1 | wall 109901
2022-08-18 04:43:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:43:25 | INFO | fairseq.trainer | begin training epoch 1391
2022-08-18 04:43:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:43:31 | INFO | train_inner | epoch 1391:     10 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=4891.8, ups=0.89, wpb=5499, bsz=359.1, num_updates=112600, lr=0.000188478, gnorm=0.396, train_wall=48, gb_free=10.1, wall=109907
2022-08-18 04:44:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:44:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:44:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:44:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:44:16 | INFO | valid | epoch 1391 | valid on 'valid' subset | loss 5.242 | nll_loss 2.655 | ppl 6.3 | bleu 56.61 | wps 1651.3 | wpb 933.5 | bsz 59.6 | num_updates 112671 | best_bleu 57.52
2022-08-18 04:44:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1391 @ 112671 updates
2022-08-18 04:44:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1391.pt
2022-08-18 04:44:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1391.pt
2022-08-18 04:44:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1391.pt (epoch 1391 @ 112671 updates, score 56.61) (writing took 27.263707675039768 seconds)
2022-08-18 04:44:43 | INFO | fairseq_cli.train | end of epoch 1391 (average epoch stats below)
2022-08-18 04:44:43 | INFO | train | epoch 1391 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 5705.2 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 112671 | lr 0.000188419 | gnorm 0.427 | train_wall 39 | gb_free 10.1 | wall 109980
2022-08-18 04:44:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:44:44 | INFO | fairseq.trainer | begin training epoch 1392
2022-08-18 04:44:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:44:59 | INFO | train_inner | epoch 1392:     29 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=6313.5, ups=1.14, wpb=5519.1, bsz=357.7, num_updates=112700, lr=0.000188394, gnorm=0.423, train_wall=48, gb_free=10.1, wall=109995
2022-08-18 04:45:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:45:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:45:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:45:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:45:34 | INFO | valid | epoch 1392 | valid on 'valid' subset | loss 5.25 | nll_loss 2.667 | ppl 6.35 | bleu 56.12 | wps 1871.3 | wpb 933.5 | bsz 59.6 | num_updates 112752 | best_bleu 57.52
2022-08-18 04:45:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1392 @ 112752 updates
2022-08-18 04:45:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1392.pt
2022-08-18 04:45:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1392.pt
2022-08-18 04:46:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1392.pt (epoch 1392 @ 112752 updates, score 56.12) (writing took 27.92489456012845 seconds)
2022-08-18 04:46:02 | INFO | fairseq_cli.train | end of epoch 1392 (average epoch stats below)
2022-08-18 04:46:02 | INFO | train | epoch 1392 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 5699.4 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 112752 | lr 0.000188351 | gnorm 0.383 | train_wall 39 | gb_free 10.2 | wall 110058
2022-08-18 04:46:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:46:02 | INFO | fairseq.trainer | begin training epoch 1393
2022-08-18 04:46:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:46:26 | INFO | train_inner | epoch 1393:     48 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6300.1, ups=1.14, wpb=5530.6, bsz=358.6, num_updates=112800, lr=0.000188311, gnorm=0.381, train_wall=48, gb_free=10.1, wall=110083
2022-08-18 04:46:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:46:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:46:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:46:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:46:53 | INFO | valid | epoch 1393 | valid on 'valid' subset | loss 5.246 | nll_loss 2.661 | ppl 6.32 | bleu 56.33 | wps 1684 | wpb 933.5 | bsz 59.6 | num_updates 112833 | best_bleu 57.52
2022-08-18 04:46:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1393 @ 112833 updates
2022-08-18 04:46:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1393.pt
2022-08-18 04:46:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1393.pt
2022-08-18 04:47:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1393.pt (epoch 1393 @ 112833 updates, score 56.33) (writing took 15.993738930672407 seconds)
2022-08-18 04:47:09 | INFO | fairseq_cli.train | end of epoch 1393 (average epoch stats below)
2022-08-18 04:47:09 | INFO | train | epoch 1393 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6643.9 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 112833 | lr 0.000188283 | gnorm 0.381 | train_wall 39 | gb_free 10.1 | wall 110125
2022-08-18 04:47:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:47:09 | INFO | fairseq.trainer | begin training epoch 1394
2022-08-18 04:47:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:47:44 | INFO | train_inner | epoch 1394:     67 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=7129.2, ups=1.29, wpb=5527.4, bsz=356.5, num_updates=112900, lr=0.000188227, gnorm=0.372, train_wall=49, gb_free=10.1, wall=110160
2022-08-18 04:47:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:47:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:47:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:47:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:48:00 | INFO | valid | epoch 1394 | valid on 'valid' subset | loss 5.259 | nll_loss 2.674 | ppl 6.38 | bleu 56.6 | wps 1757.8 | wpb 933.5 | bsz 59.6 | num_updates 112914 | best_bleu 57.52
2022-08-18 04:48:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1394 @ 112914 updates
2022-08-18 04:48:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1394.pt
2022-08-18 04:48:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1394.pt
2022-08-18 04:48:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1394.pt (epoch 1394 @ 112914 updates, score 56.6) (writing took 31.532225836068392 seconds)
2022-08-18 04:48:32 | INFO | fairseq_cli.train | end of epoch 1394 (average epoch stats below)
2022-08-18 04:48:32 | INFO | train | epoch 1394 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5404.1 | ups 0.98 | wpb 5523.2 | bsz 358 | num_updates 112914 | lr 0.000188216 | gnorm 0.357 | train_wall 39 | gb_free 10.3 | wall 110208
2022-08-18 04:48:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:48:32 | INFO | fairseq.trainer | begin training epoch 1395
2022-08-18 04:48:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:49:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:49:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:49:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:49:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:49:23 | INFO | valid | epoch 1395 | valid on 'valid' subset | loss 5.268 | nll_loss 2.685 | ppl 6.43 | bleu 56.14 | wps 1780.3 | wpb 933.5 | bsz 59.6 | num_updates 112995 | best_bleu 57.52
2022-08-18 04:49:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1395 @ 112995 updates
2022-08-18 04:49:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1395.pt
2022-08-18 04:49:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1395.pt
2022-08-18 04:49:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1395.pt (epoch 1395 @ 112995 updates, score 56.14) (writing took 35.229721054434776 seconds)
2022-08-18 04:49:59 | INFO | fairseq_cli.train | end of epoch 1395 (average epoch stats below)
2022-08-18 04:49:59 | INFO | train | epoch 1395 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5168.5 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 112995 | lr 0.000188148 | gnorm 0.393 | train_wall 40 | gb_free 10.1 | wall 110295
2022-08-18 04:49:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:49:59 | INFO | fairseq.trainer | begin training epoch 1396
2022-08-18 04:49:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:50:02 | INFO | train_inner | epoch 1396:      5 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=3981.3, ups=0.72, wpb=5510.9, bsz=357.6, num_updates=113000, lr=0.000188144, gnorm=0.37, train_wall=48, gb_free=10.1, wall=110299
2022-08-18 04:50:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:50:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:50:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:50:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:50:50 | INFO | valid | epoch 1396 | valid on 'valid' subset | loss 5.268 | nll_loss 2.685 | ppl 6.43 | bleu 56.56 | wps 1800.3 | wpb 933.5 | bsz 59.6 | num_updates 113076 | best_bleu 57.52
2022-08-18 04:50:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1396 @ 113076 updates
2022-08-18 04:50:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1396.pt
2022-08-18 04:50:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1396.pt
2022-08-18 04:51:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1396.pt (epoch 1396 @ 113076 updates, score 56.56) (writing took 13.834079656749964 seconds)
2022-08-18 04:51:04 | INFO | fairseq_cli.train | end of epoch 1396 (average epoch stats below)
2022-08-18 04:51:04 | INFO | train | epoch 1396 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6823.5 | ups 1.24 | wpb 5523.2 | bsz 358 | num_updates 113076 | lr 0.000188081 | gnorm 0.443 | train_wall 40 | gb_free 10.1 | wall 110360
2022-08-18 04:51:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:51:04 | INFO | fairseq.trainer | begin training epoch 1397
2022-08-18 04:51:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:51:18 | INFO | train_inner | epoch 1397:     24 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=7370.7, ups=1.33, wpb=5549.1, bsz=361.1, num_updates=113100, lr=0.000188061, gnorm=0.463, train_wall=49, gb_free=10.1, wall=110374
2022-08-18 04:51:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:51:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:51:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:51:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:51:56 | INFO | valid | epoch 1397 | valid on 'valid' subset | loss 5.261 | nll_loss 2.676 | ppl 6.39 | bleu 56.16 | wps 1808.2 | wpb 933.5 | bsz 59.6 | num_updates 113157 | best_bleu 57.52
2022-08-18 04:51:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1397 @ 113157 updates
2022-08-18 04:51:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1397.pt
2022-08-18 04:51:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1397.pt
2022-08-18 04:52:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1397.pt (epoch 1397 @ 113157 updates, score 56.16) (writing took 12.967970911413431 seconds)
2022-08-18 04:52:09 | INFO | fairseq_cli.train | end of epoch 1397 (average epoch stats below)
2022-08-18 04:52:09 | INFO | train | epoch 1397 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6932.3 | ups 1.26 | wpb 5523.2 | bsz 358 | num_updates 113157 | lr 0.000188014 | gnorm 0.407 | train_wall 39 | gb_free 10.1 | wall 110425
2022-08-18 04:52:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:52:09 | INFO | fairseq.trainer | begin training epoch 1398
2022-08-18 04:52:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:52:31 | INFO | train_inner | epoch 1398:     43 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=7531.8, ups=1.37, wpb=5514.9, bsz=354.5, num_updates=113200, lr=0.000187978, gnorm=0.332, train_wall=49, gb_free=10.1, wall=110447
2022-08-18 04:52:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:52:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:52:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:52:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:52:59 | INFO | valid | epoch 1398 | valid on 'valid' subset | loss 5.246 | nll_loss 2.66 | ppl 6.32 | bleu 56.4 | wps 1880.1 | wpb 933.5 | bsz 59.6 | num_updates 113238 | best_bleu 57.52
2022-08-18 04:52:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1398 @ 113238 updates
2022-08-18 04:52:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1398.pt
2022-08-18 04:53:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1398.pt
2022-08-18 04:53:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1398.pt (epoch 1398 @ 113238 updates, score 56.4) (writing took 14.3619519546628 seconds)
2022-08-18 04:53:14 | INFO | fairseq_cli.train | end of epoch 1398 (average epoch stats below)
2022-08-18 04:53:14 | INFO | train | epoch 1398 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6894.4 | ups 1.25 | wpb 5523.2 | bsz 358 | num_updates 113238 | lr 0.000187946 | gnorm 0.328 | train_wall 40 | gb_free 10.1 | wall 110490
2022-08-18 04:53:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:53:14 | INFO | fairseq.trainer | begin training epoch 1399
2022-08-18 04:53:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:53:46 | INFO | train_inner | epoch 1399:     62 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=7388, ups=1.34, wpb=5521.9, bsz=356.5, num_updates=113300, lr=0.000187895, gnorm=0.382, train_wall=49, gb_free=10.1, wall=110522
2022-08-18 04:53:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:53:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:53:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:53:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:54:04 | INFO | valid | epoch 1399 | valid on 'valid' subset | loss 5.239 | nll_loss 2.651 | ppl 6.28 | bleu 56.77 | wps 1831.6 | wpb 933.5 | bsz 59.6 | num_updates 113319 | best_bleu 57.52
2022-08-18 04:54:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1399 @ 113319 updates
2022-08-18 04:54:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1399.pt
2022-08-18 04:54:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1399.pt
2022-08-18 04:54:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1399.pt (epoch 1399 @ 113319 updates, score 56.77) (writing took 14.118730146437883 seconds)
2022-08-18 04:54:18 | INFO | fairseq_cli.train | end of epoch 1399 (average epoch stats below)
2022-08-18 04:54:18 | INFO | train | epoch 1399 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6904.6 | ups 1.25 | wpb 5523.2 | bsz 358 | num_updates 113319 | lr 0.000187879 | gnorm 0.371 | train_wall 39 | gb_free 10.1 | wall 110555
2022-08-18 04:54:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:54:19 | INFO | fairseq.trainer | begin training epoch 1400
2022-08-18 04:54:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:54:59 | INFO | train_inner | epoch 1400:     81 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=7537.1, ups=1.37, wpb=5509.4, bsz=360.7, num_updates=113400, lr=0.000187812, gnorm=0.34, train_wall=47, gb_free=10.1, wall=110595
2022-08-18 04:54:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:55:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:55:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:55:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:55:09 | INFO | valid | epoch 1400 | valid on 'valid' subset | loss 5.25 | nll_loss 2.664 | ppl 6.34 | bleu 56.4 | wps 1782.8 | wpb 933.5 | bsz 59.6 | num_updates 113400 | best_bleu 57.52
2022-08-18 04:55:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1400 @ 113400 updates
2022-08-18 04:55:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1400.pt
2022-08-18 04:55:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1400.pt
2022-08-18 04:55:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1400.pt (epoch 1400 @ 113400 updates, score 56.4) (writing took 38.62753540277481 seconds)
2022-08-18 04:55:47 | INFO | fairseq_cli.train | end of epoch 1400 (average epoch stats below)
2022-08-18 04:55:47 | INFO | train | epoch 1400 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5029.7 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 113400 | lr 0.000187812 | gnorm 0.34 | train_wall 38 | gb_free 10.1 | wall 110644
2022-08-18 04:55:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:55:47 | INFO | fairseq.trainer | begin training epoch 1401
2022-08-18 04:55:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:56:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:56:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:56:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:56:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:56:38 | INFO | valid | epoch 1401 | valid on 'valid' subset | loss 5.263 | nll_loss 2.68 | ppl 6.41 | bleu 55.95 | wps 1784 | wpb 933.5 | bsz 59.6 | num_updates 113481 | best_bleu 57.52
2022-08-18 04:56:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1401 @ 113481 updates
2022-08-18 04:56:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1401.pt
2022-08-18 04:56:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1401.pt
2022-08-18 04:56:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1401.pt (epoch 1401 @ 113481 updates, score 55.95) (writing took 18.79332558810711 seconds)
2022-08-18 04:56:57 | INFO | fairseq_cli.train | end of epoch 1401 (average epoch stats below)
2022-08-18 04:56:57 | INFO | train | epoch 1401 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6377.6 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 113481 | lr 0.000187745 | gnorm 0.32 | train_wall 39 | gb_free 10.1 | wall 110714
2022-08-18 04:56:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:56:58 | INFO | fairseq.trainer | begin training epoch 1402
2022-08-18 04:56:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:57:09 | INFO | train_inner | epoch 1402:     19 / 81 loss=3.369, nll_loss=0.337, ppl=1.26, wps=4264.1, ups=0.77, wpb=5537.7, bsz=360.3, num_updates=113500, lr=0.000187729, gnorm=0.335, train_wall=49, gb_free=10.1, wall=110725
2022-08-18 04:57:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:57:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:57:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:57:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:57:49 | INFO | valid | epoch 1402 | valid on 'valid' subset | loss 5.244 | nll_loss 2.656 | ppl 6.3 | bleu 57.01 | wps 1815.6 | wpb 933.5 | bsz 59.6 | num_updates 113562 | best_bleu 57.52
2022-08-18 04:57:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1402 @ 113562 updates
2022-08-18 04:57:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1402.pt
2022-08-18 04:57:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1402.pt
2022-08-18 04:58:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1402.pt (epoch 1402 @ 113562 updates, score 57.01) (writing took 28.524838522076607 seconds)
2022-08-18 04:58:17 | INFO | fairseq_cli.train | end of epoch 1402 (average epoch stats below)
2022-08-18 04:58:17 | INFO | train | epoch 1402 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 5598.8 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 113562 | lr 0.000187678 | gnorm 0.382 | train_wall 39 | gb_free 10.3 | wall 110794
2022-08-18 04:58:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:58:18 | INFO | fairseq.trainer | begin training epoch 1403
2022-08-18 04:58:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 04:58:38 | INFO | train_inner | epoch 1403:     38 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=6203.8, ups=1.12, wpb=5527.4, bsz=357.2, num_updates=113600, lr=0.000187647, gnorm=0.363, train_wall=49, gb_free=10.1, wall=110814
2022-08-18 04:58:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 04:59:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 04:59:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 04:59:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 04:59:08 | INFO | valid | epoch 1403 | valid on 'valid' subset | loss 5.258 | nll_loss 2.672 | ppl 6.37 | bleu 56.26 | wps 1865.9 | wpb 933.5 | bsz 59.6 | num_updates 113643 | best_bleu 57.52
2022-08-18 04:59:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1403 @ 113643 updates
2022-08-18 04:59:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1403.pt
2022-08-18 04:59:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1403.pt
2022-08-18 04:59:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1403.pt (epoch 1403 @ 113643 updates, score 56.26) (writing took 36.37059514969587 seconds)
2022-08-18 04:59:45 | INFO | fairseq_cli.train | end of epoch 1403 (average epoch stats below)
2022-08-18 04:59:45 | INFO | train | epoch 1403 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5120.6 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 113643 | lr 0.000187611 | gnorm 0.387 | train_wall 40 | gb_free 10.1 | wall 110881
2022-08-18 04:59:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 04:59:45 | INFO | fairseq.trainer | begin training epoch 1404
2022-08-18 04:59:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:00:14 | INFO | train_inner | epoch 1404:     57 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=5726.1, ups=1.04, wpb=5518.2, bsz=355.4, num_updates=113700, lr=0.000187564, gnorm=0.42, train_wall=49, gb_free=10, wall=110910
2022-08-18 05:00:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:00:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:00:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:00:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:00:35 | INFO | valid | epoch 1404 | valid on 'valid' subset | loss 5.252 | nll_loss 2.663 | ppl 6.33 | bleu 56.63 | wps 1901.5 | wpb 933.5 | bsz 59.6 | num_updates 113724 | best_bleu 57.52
2022-08-18 05:00:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1404 @ 113724 updates
2022-08-18 05:00:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1404.pt
2022-08-18 05:00:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1404.pt
2022-08-18 05:01:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1404.pt (epoch 1404 @ 113724 updates, score 56.63) (writing took 36.25941976159811 seconds)
2022-08-18 05:01:12 | INFO | fairseq_cli.train | end of epoch 1404 (average epoch stats below)
2022-08-18 05:01:12 | INFO | train | epoch 1404 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5132.2 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 113724 | lr 0.000187544 | gnorm 0.392 | train_wall 40 | gb_free 10.2 | wall 110968
2022-08-18 05:01:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:01:12 | INFO | fairseq.trainer | begin training epoch 1405
2022-08-18 05:01:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:01:53 | INFO | train_inner | epoch 1405:     76 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5589.4, ups=1.01, wpb=5546.8, bsz=361.7, num_updates=113800, lr=0.000187482, gnorm=0.339, train_wall=49, gb_free=10.2, wall=111010
2022-08-18 05:01:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:01:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:01:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:01:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:02:05 | INFO | valid | epoch 1405 | valid on 'valid' subset | loss 5.252 | nll_loss 2.664 | ppl 6.34 | bleu 56.59 | wps 1795 | wpb 933.5 | bsz 59.6 | num_updates 113805 | best_bleu 57.52
2022-08-18 05:02:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1405 @ 113805 updates
2022-08-18 05:02:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1405.pt
2022-08-18 05:02:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1405.pt
2022-08-18 05:02:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1405.pt (epoch 1405 @ 113805 updates, score 56.59) (writing took 20.073671102523804 seconds)
2022-08-18 05:02:26 | INFO | fairseq_cli.train | end of epoch 1405 (average epoch stats below)
2022-08-18 05:02:26 | INFO | train | epoch 1405 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6078.8 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 113805 | lr 0.000187478 | gnorm 0.34 | train_wall 39 | gb_free 10.1 | wall 111042
2022-08-18 05:02:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:02:26 | INFO | fairseq.trainer | begin training epoch 1406
2022-08-18 05:02:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:03:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:03:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:03:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:03:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:03:17 | INFO | valid | epoch 1406 | valid on 'valid' subset | loss 5.257 | nll_loss 2.67 | ppl 6.37 | bleu 56.52 | wps 1937.9 | wpb 933.5 | bsz 59.6 | num_updates 113886 | best_bleu 57.52
2022-08-18 05:03:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1406 @ 113886 updates
2022-08-18 05:03:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1406.pt
2022-08-18 05:03:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1406.pt
2022-08-18 05:03:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1406.pt (epoch 1406 @ 113886 updates, score 56.52) (writing took 22.016039710491896 seconds)
2022-08-18 05:03:39 | INFO | fairseq_cli.train | end of epoch 1406 (average epoch stats below)
2022-08-18 05:03:39 | INFO | train | epoch 1406 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6086.3 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 113886 | lr 0.000187411 | gnorm 0.403 | train_wall 39 | gb_free 10.1 | wall 111115
2022-08-18 05:03:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:03:39 | INFO | fairseq.trainer | begin training epoch 1407
2022-08-18 05:03:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:03:47 | INFO | train_inner | epoch 1407:     14 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=4835.4, ups=0.88, wpb=5491.8, bsz=355.2, num_updates=113900, lr=0.000187399, gnorm=0.382, train_wall=48, gb_free=10.1, wall=111123
2022-08-18 05:04:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:04:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:04:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:04:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:04:30 | INFO | valid | epoch 1407 | valid on 'valid' subset | loss 5.247 | nll_loss 2.659 | ppl 6.32 | bleu 55.9 | wps 1752.5 | wpb 933.5 | bsz 59.6 | num_updates 113967 | best_bleu 57.52
2022-08-18 05:04:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1407 @ 113967 updates
2022-08-18 05:04:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1407.pt
2022-08-18 05:04:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1407.pt
2022-08-18 05:04:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1407.pt (epoch 1407 @ 113967 updates, score 55.9) (writing took 28.242962561547756 seconds)
2022-08-18 05:04:58 | INFO | fairseq_cli.train | end of epoch 1407 (average epoch stats below)
2022-08-18 05:04:58 | INFO | train | epoch 1407 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5653.2 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 113967 | lr 0.000187344 | gnorm 0.366 | train_wall 39 | gb_free 10.2 | wall 111194
2022-08-18 05:04:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:04:58 | INFO | fairseq.trainer | begin training epoch 1408
2022-08-18 05:04:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:05:16 | INFO | train_inner | epoch 1408:     33 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=6222.1, ups=1.13, wpb=5520.9, bsz=355.4, num_updates=114000, lr=0.000187317, gnorm=0.372, train_wall=49, gb_free=10, wall=111212
2022-08-18 05:05:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:05:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:05:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:05:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:05:49 | INFO | valid | epoch 1408 | valid on 'valid' subset | loss 5.279 | nll_loss 2.699 | ppl 6.49 | bleu 56.51 | wps 1759.2 | wpb 933.5 | bsz 59.6 | num_updates 114048 | best_bleu 57.52
2022-08-18 05:05:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1408 @ 114048 updates
2022-08-18 05:05:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1408.pt
2022-08-18 05:05:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1408.pt
2022-08-18 05:06:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1408.pt (epoch 1408 @ 114048 updates, score 56.51) (writing took 19.289316937327385 seconds)
2022-08-18 05:06:09 | INFO | fairseq_cli.train | end of epoch 1408 (average epoch stats below)
2022-08-18 05:06:09 | INFO | train | epoch 1408 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6324.1 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 114048 | lr 0.000187278 | gnorm 0.353 | train_wall 40 | gb_free 10.2 | wall 111265
2022-08-18 05:06:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:06:09 | INFO | fairseq.trainer | begin training epoch 1409
2022-08-18 05:06:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:06:36 | INFO | train_inner | epoch 1409:     52 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=6915.5, ups=1.25, wpb=5538.1, bsz=361.7, num_updates=114100, lr=0.000187235, gnorm=0.399, train_wall=49, gb_free=10, wall=111292
2022-08-18 05:06:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:06:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:06:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:06:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:07:00 | INFO | valid | epoch 1409 | valid on 'valid' subset | loss 5.252 | nll_loss 2.666 | ppl 6.35 | bleu 56.8 | wps 1752.1 | wpb 933.5 | bsz 59.6 | num_updates 114129 | best_bleu 57.52
2022-08-18 05:07:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1409 @ 114129 updates
2022-08-18 05:07:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1409.pt
2022-08-18 05:07:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1409.pt
2022-08-18 05:07:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1409.pt (epoch 1409 @ 114129 updates, score 56.8) (writing took 22.964236736297607 seconds)
2022-08-18 05:07:23 | INFO | fairseq_cli.train | end of epoch 1409 (average epoch stats below)
2022-08-18 05:07:23 | INFO | train | epoch 1409 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6014.8 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 114129 | lr 0.000187211 | gnorm 0.421 | train_wall 40 | gb_free 10.1 | wall 111340
2022-08-18 05:07:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:07:24 | INFO | fairseq.trainer | begin training epoch 1410
2022-08-18 05:07:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:08:00 | INFO | train_inner | epoch 1410:     71 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=6559.9, ups=1.19, wpb=5520.6, bsz=356.7, num_updates=114200, lr=0.000187153, gnorm=0.393, train_wall=49, gb_free=10.1, wall=111376
2022-08-18 05:08:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:08:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:08:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:08:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:08:14 | INFO | valid | epoch 1410 | valid on 'valid' subset | loss 5.272 | nll_loss 2.692 | ppl 6.46 | bleu 56.08 | wps 1854.4 | wpb 933.5 | bsz 59.6 | num_updates 114210 | best_bleu 57.52
2022-08-18 05:08:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1410 @ 114210 updates
2022-08-18 05:08:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1410.pt
2022-08-18 05:08:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1410.pt
2022-08-18 05:08:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1410.pt (epoch 1410 @ 114210 updates, score 56.08) (writing took 36.8675490655005 seconds)
2022-08-18 05:08:51 | INFO | fairseq_cli.train | end of epoch 1410 (average epoch stats below)
2022-08-18 05:08:51 | INFO | train | epoch 1410 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5109 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 114210 | lr 0.000187145 | gnorm 0.384 | train_wall 39 | gb_free 10.1 | wall 111427
2022-08-18 05:08:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:08:51 | INFO | fairseq.trainer | begin training epoch 1411
2022-08-18 05:08:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:09:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:09:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:09:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:09:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:09:42 | INFO | valid | epoch 1411 | valid on 'valid' subset | loss 5.262 | nll_loss 2.679 | ppl 6.41 | bleu 56.67 | wps 1707.4 | wpb 933.5 | bsz 59.6 | num_updates 114291 | best_bleu 57.52
2022-08-18 05:09:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1411 @ 114291 updates
2022-08-18 05:09:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1411.pt
2022-08-18 05:09:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1411.pt
2022-08-18 05:09:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1411.pt (epoch 1411 @ 114291 updates, score 56.67) (writing took 15.805650774389505 seconds)
2022-08-18 05:09:58 | INFO | fairseq_cli.train | end of epoch 1411 (average epoch stats below)
2022-08-18 05:09:58 | INFO | train | epoch 1411 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6698.8 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 114291 | lr 0.000187079 | gnorm 0.366 | train_wall 39 | gb_free 10.1 | wall 111494
2022-08-18 05:09:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:09:58 | INFO | fairseq.trainer | begin training epoch 1412
2022-08-18 05:09:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:10:04 | INFO | train_inner | epoch 1412:      9 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=4456.3, ups=0.81, wpb=5520.3, bsz=358.6, num_updates=114300, lr=0.000187071, gnorm=0.362, train_wall=48, gb_free=10, wall=111500
2022-08-18 05:10:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:10:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:10:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:10:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:10:49 | INFO | valid | epoch 1412 | valid on 'valid' subset | loss 5.264 | nll_loss 2.68 | ppl 6.41 | bleu 56.81 | wps 1781 | wpb 933.5 | bsz 59.6 | num_updates 114372 | best_bleu 57.52
2022-08-18 05:10:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1412 @ 114372 updates
2022-08-18 05:10:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1412.pt
2022-08-18 05:10:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1412.pt
2022-08-18 05:11:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1412.pt (epoch 1412 @ 114372 updates, score 56.81) (writing took 15.709317158907652 seconds)
2022-08-18 05:11:05 | INFO | fairseq_cli.train | end of epoch 1412 (average epoch stats below)
2022-08-18 05:11:05 | INFO | train | epoch 1412 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6630.3 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 114372 | lr 0.000187012 | gnorm 0.336 | train_wall 39 | gb_free 10.1 | wall 111561
2022-08-18 05:11:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:11:05 | INFO | fairseq.trainer | begin training epoch 1413
2022-08-18 05:11:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:11:21 | INFO | train_inner | epoch 1413:     28 / 81 loss=3.369, nll_loss=0.337, ppl=1.26, wps=7110, ups=1.29, wpb=5515.1, bsz=358.7, num_updates=114400, lr=0.000186989, gnorm=0.326, train_wall=50, gb_free=10, wall=111578
2022-08-18 05:11:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:11:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:11:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:11:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:11:57 | INFO | valid | epoch 1413 | valid on 'valid' subset | loss 5.259 | nll_loss 2.672 | ppl 6.37 | bleu 56.27 | wps 1781.6 | wpb 933.5 | bsz 59.6 | num_updates 114453 | best_bleu 57.52
2022-08-18 05:11:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1413 @ 114453 updates
2022-08-18 05:11:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1413.pt
2022-08-18 05:11:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1413.pt
2022-08-18 05:12:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1413.pt (epoch 1413 @ 114453 updates, score 56.27) (writing took 32.36263510584831 seconds)
2022-08-18 05:12:30 | INFO | fairseq_cli.train | end of epoch 1413 (average epoch stats below)
2022-08-18 05:12:30 | INFO | train | epoch 1413 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5295.8 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 114453 | lr 0.000186946 | gnorm 0.354 | train_wall 41 | gb_free 10.1 | wall 111646
2022-08-18 05:12:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:12:30 | INFO | fairseq.trainer | begin training epoch 1414
2022-08-18 05:12:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:12:56 | INFO | train_inner | epoch 1414:     47 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=5817.3, ups=1.05, wpb=5515.6, bsz=357.4, num_updates=114500, lr=0.000186908, gnorm=0.424, train_wall=48, gb_free=10.2, wall=111672
2022-08-18 05:13:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:13:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:13:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:13:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:13:23 | INFO | valid | epoch 1414 | valid on 'valid' subset | loss 5.26 | nll_loss 2.676 | ppl 6.39 | bleu 56.56 | wps 1794.7 | wpb 933.5 | bsz 59.6 | num_updates 114534 | best_bleu 57.52
2022-08-18 05:13:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1414 @ 114534 updates
2022-08-18 05:13:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1414.pt
2022-08-18 05:13:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1414.pt
2022-08-18 05:13:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1414.pt (epoch 1414 @ 114534 updates, score 56.56) (writing took 16.506774630397558 seconds)
2022-08-18 05:13:40 | INFO | fairseq_cli.train | end of epoch 1414 (average epoch stats below)
2022-08-18 05:13:40 | INFO | train | epoch 1414 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6343.5 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 114534 | lr 0.00018688 | gnorm 0.472 | train_wall 39 | gb_free 10.1 | wall 111716
2022-08-18 05:13:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:13:40 | INFO | fairseq.trainer | begin training epoch 1415
2022-08-18 05:13:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:14:15 | INFO | train_inner | epoch 1415:     66 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=7069.5, ups=1.27, wpb=5554.2, bsz=362.5, num_updates=114600, lr=0.000186826, gnorm=0.432, train_wall=49, gb_free=10.1, wall=111751
2022-08-18 05:14:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:14:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:14:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:14:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:14:31 | INFO | valid | epoch 1415 | valid on 'valid' subset | loss 5.252 | nll_loss 2.666 | ppl 6.35 | bleu 56.9 | wps 1865.7 | wpb 933.5 | bsz 59.6 | num_updates 114615 | best_bleu 57.52
2022-08-18 05:14:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1415 @ 114615 updates
2022-08-18 05:14:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1415.pt
2022-08-18 05:14:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1415.pt
2022-08-18 05:15:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1415.pt (epoch 1415 @ 114615 updates, score 56.9) (writing took 42.830752085894346 seconds)
2022-08-18 05:15:14 | INFO | fairseq_cli.train | end of epoch 1415 (average epoch stats below)
2022-08-18 05:15:14 | INFO | train | epoch 1415 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 4751.9 | ups 0.86 | wpb 5523.2 | bsz 358 | num_updates 114615 | lr 0.000186814 | gnorm 0.401 | train_wall 40 | gb_free 10.1 | wall 111811
2022-08-18 05:15:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:15:15 | INFO | fairseq.trainer | begin training epoch 1416
2022-08-18 05:15:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:15:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:15:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:15:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:15:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:16:05 | INFO | valid | epoch 1416 | valid on 'valid' subset | loss 5.255 | nll_loss 2.671 | ppl 6.37 | bleu 55.81 | wps 1808.6 | wpb 933.5 | bsz 59.6 | num_updates 114696 | best_bleu 57.52
2022-08-18 05:16:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1416 @ 114696 updates
2022-08-18 05:16:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1416.pt
2022-08-18 05:16:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1416.pt
2022-08-18 05:16:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1416.pt (epoch 1416 @ 114696 updates, score 55.81) (writing took 41.36043145135045 seconds)
2022-08-18 05:16:47 | INFO | fairseq_cli.train | end of epoch 1416 (average epoch stats below)
2022-08-18 05:16:47 | INFO | train | epoch 1416 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 4842.6 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 114696 | lr 0.000186748 | gnorm 0.382 | train_wall 39 | gb_free 10.2 | wall 111903
2022-08-18 05:16:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:16:47 | INFO | fairseq.trainer | begin training epoch 1417
2022-08-18 05:16:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:16:50 | INFO | train_inner | epoch 1417:      4 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=3540.5, ups=0.64, wpb=5497.7, bsz=354.3, num_updates=114700, lr=0.000186745, gnorm=0.369, train_wall=48, gb_free=10, wall=111906
2022-08-18 05:17:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:17:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:17:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:17:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:17:38 | INFO | valid | epoch 1417 | valid on 'valid' subset | loss 5.274 | nll_loss 2.693 | ppl 6.47 | bleu 56.58 | wps 1737.1 | wpb 933.5 | bsz 59.6 | num_updates 114777 | best_bleu 57.52
2022-08-18 05:17:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1417 @ 114777 updates
2022-08-18 05:17:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1417.pt
2022-08-18 05:17:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1417.pt
2022-08-18 05:18:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1417.pt (epoch 1417 @ 114777 updates, score 56.58) (writing took 23.25315746292472 seconds)
2022-08-18 05:18:01 | INFO | fairseq_cli.train | end of epoch 1417 (average epoch stats below)
2022-08-18 05:18:01 | INFO | train | epoch 1417 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5994.6 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 114777 | lr 0.000186682 | gnorm 0.361 | train_wall 39 | gb_free 10.1 | wall 111978
2022-08-18 05:18:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:18:01 | INFO | fairseq.trainer | begin training epoch 1418
2022-08-18 05:18:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:18:15 | INFO | train_inner | epoch 1418:     23 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6520.3, ups=1.18, wpb=5527.8, bsz=354.8, num_updates=114800, lr=0.000186663, gnorm=0.37, train_wall=49, gb_free=10.1, wall=111991
2022-08-18 05:18:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:18:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:18:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:18:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:18:53 | INFO | valid | epoch 1418 | valid on 'valid' subset | loss 5.275 | nll_loss 2.691 | ppl 6.46 | bleu 55.64 | wps 1831.1 | wpb 933.5 | bsz 59.6 | num_updates 114858 | best_bleu 57.52
2022-08-18 05:18:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1418 @ 114858 updates
2022-08-18 05:18:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1418.pt
2022-08-18 05:18:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1418.pt
2022-08-18 05:19:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1418.pt (epoch 1418 @ 114858 updates, score 55.64) (writing took 27.114837519824505 seconds)
2022-08-18 05:19:21 | INFO | fairseq_cli.train | end of epoch 1418 (average epoch stats below)
2022-08-18 05:19:21 | INFO | train | epoch 1418 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5640.8 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 114858 | lr 0.000186616 | gnorm 0.392 | train_wall 39 | gb_free 10.2 | wall 112057
2022-08-18 05:19:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:19:21 | INFO | fairseq.trainer | begin training epoch 1419
2022-08-18 05:19:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:19:46 | INFO | train_inner | epoch 1419:     42 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=6088.8, ups=1.1, wpb=5537.9, bsz=362.8, num_updates=114900, lr=0.000186582, gnorm=0.434, train_wall=48, gb_free=10.1, wall=112082
2022-08-18 05:20:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:20:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:20:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:20:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:20:15 | INFO | valid | epoch 1419 | valid on 'valid' subset | loss 5.263 | nll_loss 2.681 | ppl 6.41 | bleu 56.23 | wps 1790.5 | wpb 933.5 | bsz 59.6 | num_updates 114939 | best_bleu 57.52
2022-08-18 05:20:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1419 @ 114939 updates
2022-08-18 05:20:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1419.pt
2022-08-18 05:20:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1419.pt
2022-08-18 05:20:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1419.pt (epoch 1419 @ 114939 updates, score 56.23) (writing took 32.50413317605853 seconds)
2022-08-18 05:20:48 | INFO | fairseq_cli.train | end of epoch 1419 (average epoch stats below)
2022-08-18 05:20:48 | INFO | train | epoch 1419 | loss 3.37 | nll_loss 0.339 | ppl 1.27 | wps 5146.9 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 114939 | lr 0.00018655 | gnorm 0.419 | train_wall 39 | gb_free 10.2 | wall 112144
2022-08-18 05:20:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:20:48 | INFO | fairseq.trainer | begin training epoch 1420
2022-08-18 05:20:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:21:20 | INFO | train_inner | epoch 1420:     61 / 81 loss=3.37, nll_loss=0.339, ppl=1.27, wps=5902.1, ups=1.07, wpb=5533.8, bsz=358.2, num_updates=115000, lr=0.000186501, gnorm=0.377, train_wall=50, gb_free=10.1, wall=112176
2022-08-18 05:21:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:21:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:21:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:21:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:21:39 | INFO | valid | epoch 1420 | valid on 'valid' subset | loss 5.252 | nll_loss 2.666 | ppl 6.35 | bleu 56.82 | wps 1882.6 | wpb 933.5 | bsz 59.6 | num_updates 115020 | best_bleu 57.52
2022-08-18 05:21:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1420 @ 115020 updates
2022-08-18 05:21:39 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1420.pt
2022-08-18 05:21:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1420.pt
2022-08-18 05:22:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1420.pt (epoch 1420 @ 115020 updates, score 56.82) (writing took 35.345294661819935 seconds)
2022-08-18 05:22:14 | INFO | fairseq_cli.train | end of epoch 1420 (average epoch stats below)
2022-08-18 05:22:14 | INFO | train | epoch 1420 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5167.1 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 115020 | lr 0.000186485 | gnorm 0.412 | train_wall 40 | gb_free 10.2 | wall 112230
2022-08-18 05:22:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:22:14 | INFO | fairseq.trainer | begin training epoch 1421
2022-08-18 05:22:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:22:55 | INFO | train_inner | epoch 1421:     80 / 81 loss=3.37, nll_loss=0.339, ppl=1.27, wps=5807.1, ups=1.05, wpb=5518.4, bsz=357.4, num_updates=115100, lr=0.00018642, gnorm=0.403, train_wall=48, gb_free=10.1, wall=112271
2022-08-18 05:22:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:22:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:22:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:22:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:23:04 | INFO | valid | epoch 1421 | valid on 'valid' subset | loss 5.254 | nll_loss 2.665 | ppl 6.34 | bleu 56.79 | wps 1826.1 | wpb 933.5 | bsz 59.6 | num_updates 115101 | best_bleu 57.52
2022-08-18 05:23:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1421 @ 115101 updates
2022-08-18 05:23:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1421.pt
2022-08-18 05:23:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1421.pt
2022-08-18 05:23:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1421.pt (epoch 1421 @ 115101 updates, score 56.79) (writing took 16.813875392079353 seconds)
2022-08-18 05:23:21 | INFO | fairseq_cli.train | end of epoch 1421 (average epoch stats below)
2022-08-18 05:23:21 | INFO | train | epoch 1421 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6665.5 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 115101 | lr 0.000186419 | gnorm 0.39 | train_wall 39 | gb_free 10.1 | wall 112297
2022-08-18 05:23:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:23:22 | INFO | fairseq.trainer | begin training epoch 1422
2022-08-18 05:23:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:24:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:24:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:24:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:24:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:24:12 | INFO | valid | epoch 1422 | valid on 'valid' subset | loss 5.268 | nll_loss 2.685 | ppl 6.43 | bleu 56.04 | wps 1931.7 | wpb 933.5 | bsz 59.6 | num_updates 115182 | best_bleu 57.52
2022-08-18 05:24:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1422 @ 115182 updates
2022-08-18 05:24:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1422.pt
2022-08-18 05:24:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1422.pt
2022-08-18 05:24:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1422.pt (epoch 1422 @ 115182 updates, score 56.04) (writing took 32.16469545662403 seconds)
2022-08-18 05:24:44 | INFO | fairseq_cli.train | end of epoch 1422 (average epoch stats below)
2022-08-18 05:24:44 | INFO | train | epoch 1422 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5387.8 | ups 0.98 | wpb 5523.2 | bsz 358 | num_updates 115182 | lr 0.000186354 | gnorm 0.376 | train_wall 40 | gb_free 10.1 | wall 112381
2022-08-18 05:24:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:24:45 | INFO | fairseq.trainer | begin training epoch 1423
2022-08-18 05:24:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:24:54 | INFO | train_inner | epoch 1423:     18 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=4584.2, ups=0.83, wpb=5491.2, bsz=355.8, num_updates=115200, lr=0.000186339, gnorm=0.382, train_wall=49, gb_free=10, wall=112391
2022-08-18 05:25:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:25:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:25:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:25:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:25:36 | INFO | valid | epoch 1423 | valid on 'valid' subset | loss 5.248 | nll_loss 2.661 | ppl 6.32 | bleu 56.15 | wps 1683 | wpb 933.5 | bsz 59.6 | num_updates 115263 | best_bleu 57.52
2022-08-18 05:25:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1423 @ 115263 updates
2022-08-18 05:25:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1423.pt
2022-08-18 05:25:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1423.pt
2022-08-18 05:26:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1423.pt (epoch 1423 @ 115263 updates, score 56.15) (writing took 35.551824286580086 seconds)
2022-08-18 05:26:12 | INFO | fairseq_cli.train | end of epoch 1423 (average epoch stats below)
2022-08-18 05:26:12 | INFO | train | epoch 1423 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5127.5 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 115263 | lr 0.000186288 | gnorm 0.387 | train_wall 40 | gb_free 10.1 | wall 112468
2022-08-18 05:26:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:26:12 | INFO | fairseq.trainer | begin training epoch 1424
2022-08-18 05:26:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:26:31 | INFO | train_inner | epoch 1424:     37 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5730.1, ups=1.04, wpb=5535.5, bsz=359.7, num_updates=115300, lr=0.000186258, gnorm=0.385, train_wall=49, gb_free=10.1, wall=112487
2022-08-18 05:26:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:26:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:26:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:26:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:27:02 | INFO | valid | epoch 1424 | valid on 'valid' subset | loss 5.28 | nll_loss 2.701 | ppl 6.5 | bleu 56.36 | wps 1867.6 | wpb 933.5 | bsz 59.6 | num_updates 115344 | best_bleu 57.52
2022-08-18 05:27:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1424 @ 115344 updates
2022-08-18 05:27:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1424.pt
2022-08-18 05:27:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1424.pt
2022-08-18 05:27:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1424.pt (epoch 1424 @ 115344 updates, score 56.36) (writing took 20.93956473097205 seconds)
2022-08-18 05:27:23 | INFO | fairseq_cli.train | end of epoch 1424 (average epoch stats below)
2022-08-18 05:27:23 | INFO | train | epoch 1424 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6294.4 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 115344 | lr 0.000186223 | gnorm 0.386 | train_wall 39 | gb_free 10.2 | wall 112539
2022-08-18 05:27:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:27:23 | INFO | fairseq.trainer | begin training epoch 1425
2022-08-18 05:27:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:27:52 | INFO | train_inner | epoch 1425:     56 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6799, ups=1.23, wpb=5540, bsz=356.2, num_updates=115400, lr=0.000186177, gnorm=0.357, train_wall=48, gb_free=10.1, wall=112569
2022-08-18 05:28:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:28:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:28:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:28:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:28:14 | INFO | valid | epoch 1425 | valid on 'valid' subset | loss 5.27 | nll_loss 2.688 | ppl 6.44 | bleu 56.54 | wps 1875.2 | wpb 933.5 | bsz 59.6 | num_updates 115425 | best_bleu 57.52
2022-08-18 05:28:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1425 @ 115425 updates
2022-08-18 05:28:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1425.pt
2022-08-18 05:28:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1425.pt
2022-08-18 05:28:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1425.pt (epoch 1425 @ 115425 updates, score 56.54) (writing took 17.793989919126034 seconds)
2022-08-18 05:28:32 | INFO | fairseq_cli.train | end of epoch 1425 (average epoch stats below)
2022-08-18 05:28:32 | INFO | train | epoch 1425 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6486.2 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 115425 | lr 0.000186157 | gnorm 0.346 | train_wall 39 | gb_free 10.2 | wall 112608
2022-08-18 05:28:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:28:32 | INFO | fairseq.trainer | begin training epoch 1426
2022-08-18 05:28:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:29:11 | INFO | train_inner | epoch 1426:     75 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=7070.3, ups=1.28, wpb=5518.9, bsz=358.2, num_updates=115500, lr=0.000186097, gnorm=0.422, train_wall=48, gb_free=10.1, wall=112647
2022-08-18 05:29:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:29:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:29:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:29:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:29:22 | INFO | valid | epoch 1426 | valid on 'valid' subset | loss 5.258 | nll_loss 2.674 | ppl 6.38 | bleu 56.54 | wps 1848.3 | wpb 933.5 | bsz 59.6 | num_updates 115506 | best_bleu 57.52
2022-08-18 05:29:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1426 @ 115506 updates
2022-08-18 05:29:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1426.pt
2022-08-18 05:29:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1426.pt
2022-08-18 05:30:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1426.pt (epoch 1426 @ 115506 updates, score 56.54) (writing took 39.7520301528275 seconds)
2022-08-18 05:30:02 | INFO | fairseq_cli.train | end of epoch 1426 (average epoch stats below)
2022-08-18 05:30:02 | INFO | train | epoch 1426 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 4929.5 | ups 0.89 | wpb 5523.2 | bsz 358 | num_updates 115506 | lr 0.000186092 | gnorm 0.43 | train_wall 39 | gb_free 10 | wall 112699
2022-08-18 05:30:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:30:03 | INFO | fairseq.trainer | begin training epoch 1427
2022-08-18 05:30:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:30:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:30:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:30:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:30:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:30:54 | INFO | valid | epoch 1427 | valid on 'valid' subset | loss 5.276 | nll_loss 2.698 | ppl 6.49 | bleu 55.49 | wps 1806.8 | wpb 933.5 | bsz 59.6 | num_updates 115587 | best_bleu 57.52
2022-08-18 05:30:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1427 @ 115587 updates
2022-08-18 05:30:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1427.pt
2022-08-18 05:30:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1427.pt
2022-08-18 05:31:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1427.pt (epoch 1427 @ 115587 updates, score 55.49) (writing took 22.481732174754143 seconds)
2022-08-18 05:31:17 | INFO | fairseq_cli.train | end of epoch 1427 (average epoch stats below)
2022-08-18 05:31:17 | INFO | train | epoch 1427 | loss 3.37 | nll_loss 0.34 | ppl 1.27 | wps 6006.6 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 115587 | lr 0.000186027 | gnorm 0.501 | train_wall 40 | gb_free 10.1 | wall 112773
2022-08-18 05:31:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:31:17 | INFO | fairseq.trainer | begin training epoch 1428
2022-08-18 05:31:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:31:24 | INFO | train_inner | epoch 1428:     13 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=4123.8, ups=0.75, wpb=5522.1, bsz=359.2, num_updates=115600, lr=0.000186016, gnorm=0.462, train_wall=49, gb_free=10.1, wall=112781
2022-08-18 05:31:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:31:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:31:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:31:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:32:06 | INFO | valid | epoch 1428 | valid on 'valid' subset | loss 5.271 | nll_loss 2.69 | ppl 6.45 | bleu 55.91 | wps 1807.6 | wpb 933.5 | bsz 59.6 | num_updates 115668 | best_bleu 57.52
2022-08-18 05:32:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1428 @ 115668 updates
2022-08-18 05:32:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1428.pt
2022-08-18 05:32:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1428.pt
2022-08-18 05:32:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1428.pt (epoch 1428 @ 115668 updates, score 55.91) (writing took 22.356058906763792 seconds)
2022-08-18 05:32:29 | INFO | fairseq_cli.train | end of epoch 1428 (average epoch stats below)
2022-08-18 05:32:29 | INFO | train | epoch 1428 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6206.7 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 115668 | lr 0.000185962 | gnorm 0.353 | train_wall 38 | gb_free 10.2 | wall 112845
2022-08-18 05:32:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:32:29 | INFO | fairseq.trainer | begin training epoch 1429
2022-08-18 05:32:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:32:46 | INFO | train_inner | epoch 1429:     32 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6773.1, ups=1.23, wpb=5514, bsz=357.8, num_updates=115700, lr=0.000185936, gnorm=0.378, train_wall=48, gb_free=10.1, wall=112862
2022-08-18 05:33:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:33:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:33:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:33:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:33:19 | INFO | valid | epoch 1429 | valid on 'valid' subset | loss 5.269 | nll_loss 2.684 | ppl 6.42 | bleu 56.47 | wps 1853.1 | wpb 933.5 | bsz 59.6 | num_updates 115749 | best_bleu 57.52
2022-08-18 05:33:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1429 @ 115749 updates
2022-08-18 05:33:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1429.pt
2022-08-18 05:33:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1429.pt
2022-08-18 05:33:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1429.pt (epoch 1429 @ 115749 updates, score 56.47) (writing took 37.35902078449726 seconds)
2022-08-18 05:33:56 | INFO | fairseq_cli.train | end of epoch 1429 (average epoch stats below)
2022-08-18 05:33:56 | INFO | train | epoch 1429 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5123.4 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 115749 | lr 0.000185897 | gnorm 0.351 | train_wall 39 | gb_free 10 | wall 112932
2022-08-18 05:33:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:33:57 | INFO | fairseq.trainer | begin training epoch 1430
2022-08-18 05:33:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:34:22 | INFO | train_inner | epoch 1430:     51 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5757.2, ups=1.04, wpb=5541.5, bsz=358.4, num_updates=115800, lr=0.000185856, gnorm=0.347, train_wall=48, gb_free=10.2, wall=112958
2022-08-18 05:34:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:34:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:34:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:34:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:34:46 | INFO | valid | epoch 1430 | valid on 'valid' subset | loss 5.253 | nll_loss 2.664 | ppl 6.34 | bleu 57.19 | wps 1848.9 | wpb 933.5 | bsz 59.6 | num_updates 115830 | best_bleu 57.52
2022-08-18 05:34:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1430 @ 115830 updates
2022-08-18 05:34:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1430.pt
2022-08-18 05:34:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1430.pt
2022-08-18 05:35:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1430.pt (epoch 1430 @ 115830 updates, score 57.19) (writing took 23.654910143464804 seconds)
2022-08-18 05:35:10 | INFO | fairseq_cli.train | end of epoch 1430 (average epoch stats below)
2022-08-18 05:35:10 | INFO | train | epoch 1430 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6101.5 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 115830 | lr 0.000185832 | gnorm 0.384 | train_wall 38 | gb_free 10.2 | wall 113006
2022-08-18 05:35:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:35:10 | INFO | fairseq.trainer | begin training epoch 1431
2022-08-18 05:35:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:35:46 | INFO | train_inner | epoch 1431:     70 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=6564.6, ups=1.19, wpb=5529.6, bsz=358.3, num_updates=115900, lr=0.000185775, gnorm=0.39, train_wall=49, gb_free=10, wall=113043
2022-08-18 05:35:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:35:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:35:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:35:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:36:01 | INFO | valid | epoch 1431 | valid on 'valid' subset | loss 5.275 | nll_loss 2.691 | ppl 6.46 | bleu 56.41 | wps 1828.7 | wpb 933.5 | bsz 59.6 | num_updates 115911 | best_bleu 57.52
2022-08-18 05:36:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1431 @ 115911 updates
2022-08-18 05:36:01 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1431.pt
2022-08-18 05:36:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1431.pt
2022-08-18 05:36:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1431.pt (epoch 1431 @ 115911 updates, score 56.41) (writing took 15.097109142690897 seconds)
2022-08-18 05:36:16 | INFO | fairseq_cli.train | end of epoch 1431 (average epoch stats below)
2022-08-18 05:36:16 | INFO | train | epoch 1431 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6694.9 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 115911 | lr 0.000185767 | gnorm 0.389 | train_wall 40 | gb_free 10.1 | wall 113073
2022-08-18 05:36:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:36:17 | INFO | fairseq.trainer | begin training epoch 1432
2022-08-18 05:36:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:36:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:37:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:37:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:37:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:37:08 | INFO | valid | epoch 1432 | valid on 'valid' subset | loss 5.268 | nll_loss 2.685 | ppl 6.43 | bleu 56.6 | wps 1696.6 | wpb 933.5 | bsz 59.6 | num_updates 115992 | best_bleu 57.52
2022-08-18 05:37:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1432 @ 115992 updates
2022-08-18 05:37:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1432.pt
2022-08-18 05:37:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1432.pt
2022-08-18 05:37:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1432.pt (epoch 1432 @ 115992 updates, score 56.6) (writing took 36.886040426790714 seconds)
2022-08-18 05:37:45 | INFO | fairseq_cli.train | end of epoch 1432 (average epoch stats below)
2022-08-18 05:37:45 | INFO | train | epoch 1432 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5034.4 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 115992 | lr 0.000185702 | gnorm 0.407 | train_wall 39 | gb_free 10.1 | wall 113162
2022-08-18 05:37:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:37:45 | INFO | fairseq.trainer | begin training epoch 1433
2022-08-18 05:37:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:37:50 | INFO | train_inner | epoch 1433:      8 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=4427.5, ups=0.81, wpb=5490.8, bsz=354.6, num_updates=116000, lr=0.000185695, gnorm=0.397, train_wall=49, gb_free=10.1, wall=113167
2022-08-18 05:38:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:38:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:38:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:38:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:38:35 | INFO | valid | epoch 1433 | valid on 'valid' subset | loss 5.269 | nll_loss 2.689 | ppl 6.45 | bleu 56.56 | wps 1838 | wpb 933.5 | bsz 59.6 | num_updates 116073 | best_bleu 57.52
2022-08-18 05:38:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1433 @ 116073 updates
2022-08-18 05:38:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1433.pt
2022-08-18 05:38:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1433.pt
2022-08-18 05:38:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1433.pt (epoch 1433 @ 116073 updates, score 56.56) (writing took 21.982164345681667 seconds)
2022-08-18 05:38:58 | INFO | fairseq_cli.train | end of epoch 1433 (average epoch stats below)
2022-08-18 05:38:58 | INFO | train | epoch 1433 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6191.6 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 116073 | lr 0.000185637 | gnorm 0.383 | train_wall 39 | gb_free 10.1 | wall 113234
2022-08-18 05:38:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:38:58 | INFO | fairseq.trainer | begin training epoch 1434
2022-08-18 05:38:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:39:12 | INFO | train_inner | epoch 1434:     27 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6738, ups=1.22, wpb=5530.9, bsz=362.1, num_updates=116100, lr=0.000185615, gnorm=0.39, train_wall=48, gb_free=10, wall=113249
2022-08-18 05:39:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:39:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:39:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:39:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:39:48 | INFO | valid | epoch 1434 | valid on 'valid' subset | loss 5.263 | nll_loss 2.68 | ppl 6.41 | bleu 56.4 | wps 1777.6 | wpb 933.5 | bsz 59.6 | num_updates 116154 | best_bleu 57.52
2022-08-18 05:39:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1434 @ 116154 updates
2022-08-18 05:39:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1434.pt
2022-08-18 05:39:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1434.pt
2022-08-18 05:40:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1434.pt (epoch 1434 @ 116154 updates, score 56.4) (writing took 22.269382283091545 seconds)
2022-08-18 05:40:11 | INFO | fairseq_cli.train | end of epoch 1434 (average epoch stats below)
2022-08-18 05:40:11 | INFO | train | epoch 1434 | loss 3.37 | nll_loss 0.339 | ppl 1.27 | wps 6124.1 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 116154 | lr 0.000185572 | gnorm 0.387 | train_wall 39 | gb_free 10.1 | wall 113307
2022-08-18 05:40:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:40:11 | INFO | fairseq.trainer | begin training epoch 1435
2022-08-18 05:40:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:40:37 | INFO | train_inner | epoch 1435:     46 / 81 loss=3.37, nll_loss=0.339, ppl=1.27, wps=6588.2, ups=1.19, wpb=5538.9, bsz=356.4, num_updates=116200, lr=0.000185535, gnorm=0.381, train_wall=48, gb_free=10.1, wall=113333
2022-08-18 05:40:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:40:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:40:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:40:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:41:04 | INFO | valid | epoch 1435 | valid on 'valid' subset | loss 5.26 | nll_loss 2.676 | ppl 6.39 | bleu 56.86 | wps 1831.1 | wpb 933.5 | bsz 59.6 | num_updates 116235 | best_bleu 57.52
2022-08-18 05:41:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1435 @ 116235 updates
2022-08-18 05:41:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1435.pt
2022-08-18 05:41:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1435.pt
2022-08-18 05:41:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1435.pt (epoch 1435 @ 116235 updates, score 56.86) (writing took 34.82811389863491 seconds)
2022-08-18 05:41:39 | INFO | fairseq_cli.train | end of epoch 1435 (average epoch stats below)
2022-08-18 05:41:39 | INFO | train | epoch 1435 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5075.1 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 116235 | lr 0.000185508 | gnorm 0.4 | train_wall 40 | gb_free 10.1 | wall 113395
2022-08-18 05:41:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:41:39 | INFO | fairseq.trainer | begin training epoch 1436
2022-08-18 05:41:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:42:13 | INFO | train_inner | epoch 1436:     65 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5739.1, ups=1.04, wpb=5521.8, bsz=354.3, num_updates=116300, lr=0.000185456, gnorm=0.517, train_wall=49, gb_free=10.1, wall=113429
2022-08-18 05:42:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:42:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:42:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:42:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:42:30 | INFO | valid | epoch 1436 | valid on 'valid' subset | loss 5.269 | nll_loss 2.686 | ppl 6.44 | bleu 56.56 | wps 1835.4 | wpb 933.5 | bsz 59.6 | num_updates 116316 | best_bleu 57.52
2022-08-18 05:42:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1436 @ 116316 updates
2022-08-18 05:42:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1436.pt
2022-08-18 05:42:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1436.pt
2022-08-18 05:43:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1436.pt (epoch 1436 @ 116316 updates, score 56.56) (writing took 36.109978165477514 seconds)
2022-08-18 05:43:07 | INFO | fairseq_cli.train | end of epoch 1436 (average epoch stats below)
2022-08-18 05:43:07 | INFO | train | epoch 1436 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5095.7 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 116316 | lr 0.000185443 | gnorm 0.515 | train_wall 40 | gb_free 10.1 | wall 113483
2022-08-18 05:43:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:43:07 | INFO | fairseq.trainer | begin training epoch 1437
2022-08-18 05:43:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:43:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:43:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:43:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:43:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:43:58 | INFO | valid | epoch 1437 | valid on 'valid' subset | loss 5.264 | nll_loss 2.683 | ppl 6.42 | bleu 56.53 | wps 1747.5 | wpb 933.5 | bsz 59.6 | num_updates 116397 | best_bleu 57.52
2022-08-18 05:43:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1437 @ 116397 updates
2022-08-18 05:43:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1437.pt
2022-08-18 05:44:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1437.pt
2022-08-18 05:44:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1437.pt (epoch 1437 @ 116397 updates, score 56.53) (writing took 20.974134977906942 seconds)
2022-08-18 05:44:20 | INFO | fairseq_cli.train | end of epoch 1437 (average epoch stats below)
2022-08-18 05:44:20 | INFO | train | epoch 1437 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6125.9 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 116397 | lr 0.000185378 | gnorm 0.362 | train_wall 40 | gb_free 10.1 | wall 113556
2022-08-18 05:44:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:44:20 | INFO | fairseq.trainer | begin training epoch 1438
2022-08-18 05:44:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:44:22 | INFO | train_inner | epoch 1438:      3 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=4247.4, ups=0.77, wpb=5500.4, bsz=360.9, num_updates=116400, lr=0.000185376, gnorm=0.356, train_wall=49, gb_free=10, wall=113558
2022-08-18 05:45:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:45:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:45:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:45:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:45:14 | INFO | valid | epoch 1438 | valid on 'valid' subset | loss 5.28 | nll_loss 2.701 | ppl 6.5 | bleu 56.49 | wps 1558.1 | wpb 933.5 | bsz 59.6 | num_updates 116478 | best_bleu 57.52
2022-08-18 05:45:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1438 @ 116478 updates
2022-08-18 05:45:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1438.pt
2022-08-18 05:45:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1438.pt
2022-08-18 05:45:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1438.pt (epoch 1438 @ 116478 updates, score 56.49) (writing took 25.091646160930395 seconds)
2022-08-18 05:45:39 | INFO | fairseq_cli.train | end of epoch 1438 (average epoch stats below)
2022-08-18 05:45:39 | INFO | train | epoch 1438 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5606.5 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 116478 | lr 0.000185314 | gnorm 0.358 | train_wall 39 | gb_free 10.2 | wall 113636
2022-08-18 05:45:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:45:40 | INFO | fairseq.trainer | begin training epoch 1439
2022-08-18 05:45:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:45:52 | INFO | train_inner | epoch 1439:     22 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=6158, ups=1.11, wpb=5538.9, bsz=355.6, num_updates=116500, lr=0.000185296, gnorm=0.346, train_wall=49, gb_free=10.1, wall=113648
2022-08-18 05:46:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:46:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:46:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:46:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:46:31 | INFO | valid | epoch 1439 | valid on 'valid' subset | loss 5.269 | nll_loss 2.688 | ppl 6.45 | bleu 56.43 | wps 1767.7 | wpb 933.5 | bsz 59.6 | num_updates 116559 | best_bleu 57.52
2022-08-18 05:46:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1439 @ 116559 updates
2022-08-18 05:46:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1439.pt
2022-08-18 05:46:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1439.pt
2022-08-18 05:47:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1439.pt (epoch 1439 @ 116559 updates, score 56.43) (writing took 36.94294897839427 seconds)
2022-08-18 05:47:08 | INFO | fairseq_cli.train | end of epoch 1439 (average epoch stats below)
2022-08-18 05:47:08 | INFO | train | epoch 1439 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5054.5 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 116559 | lr 0.00018525 | gnorm 0.393 | train_wall 39 | gb_free 10.1 | wall 113724
2022-08-18 05:47:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:47:08 | INFO | fairseq.trainer | begin training epoch 1440
2022-08-18 05:47:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:47:30 | INFO | train_inner | epoch 1440:     41 / 81 loss=3.37, nll_loss=0.339, ppl=1.27, wps=5668, ups=1.02, wpb=5546.8, bsz=369.6, num_updates=116600, lr=0.000185217, gnorm=0.419, train_wall=49, gb_free=10.1, wall=113746
2022-08-18 05:47:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:47:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:47:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:47:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:47:59 | INFO | valid | epoch 1440 | valid on 'valid' subset | loss 5.264 | nll_loss 2.677 | ppl 6.39 | bleu 56.04 | wps 1831.4 | wpb 933.5 | bsz 59.6 | num_updates 116640 | best_bleu 57.52
2022-08-18 05:47:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1440 @ 116640 updates
2022-08-18 05:47:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1440.pt
2022-08-18 05:48:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1440.pt
2022-08-18 05:48:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1440.pt (epoch 1440 @ 116640 updates, score 56.04) (writing took 22.050183437764645 seconds)
2022-08-18 05:48:21 | INFO | fairseq_cli.train | end of epoch 1440 (average epoch stats below)
2022-08-18 05:48:21 | INFO | train | epoch 1440 | loss 3.37 | nll_loss 0.339 | ppl 1.27 | wps 6113.7 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 116640 | lr 0.000185185 | gnorm 0.387 | train_wall 39 | gb_free 10.2 | wall 113797
2022-08-18 05:48:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:48:21 | INFO | fairseq.trainer | begin training epoch 1441
2022-08-18 05:48:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:48:53 | INFO | train_inner | epoch 1441:     60 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=6699.8, ups=1.21, wpb=5527, bsz=353, num_updates=116700, lr=0.000185138, gnorm=0.364, train_wall=48, gb_free=10.1, wall=113829
2022-08-18 05:49:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:49:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:49:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:49:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:49:11 | INFO | valid | epoch 1441 | valid on 'valid' subset | loss 5.277 | nll_loss 2.695 | ppl 6.47 | bleu 56.26 | wps 1929.8 | wpb 933.5 | bsz 59.6 | num_updates 116721 | best_bleu 57.52
2022-08-18 05:49:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1441 @ 116721 updates
2022-08-18 05:49:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1441.pt
2022-08-18 05:49:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1441.pt
2022-08-18 05:49:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1441.pt (epoch 1441 @ 116721 updates, score 56.26) (writing took 15.650937415659428 seconds)
2022-08-18 05:49:27 | INFO | fairseq_cli.train | end of epoch 1441 (average epoch stats below)
2022-08-18 05:49:27 | INFO | train | epoch 1441 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6787.4 | ups 1.23 | wpb 5523.2 | bsz 358 | num_updates 116721 | lr 0.000185121 | gnorm 0.368 | train_wall 39 | gb_free 10.2 | wall 113863
2022-08-18 05:49:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:49:27 | INFO | fairseq.trainer | begin training epoch 1442
2022-08-18 05:49:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:50:07 | INFO | train_inner | epoch 1442:     79 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=7428.3, ups=1.35, wpb=5503.5, bsz=355.9, num_updates=116800, lr=0.000185058, gnorm=0.377, train_wall=48, gb_free=10.1, wall=113903
2022-08-18 05:50:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:50:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:50:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:50:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:50:17 | INFO | valid | epoch 1442 | valid on 'valid' subset | loss 5.286 | nll_loss 2.709 | ppl 6.54 | bleu 56.23 | wps 1805.4 | wpb 933.5 | bsz 59.6 | num_updates 116802 | best_bleu 57.52
2022-08-18 05:50:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1442 @ 116802 updates
2022-08-18 05:50:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1442.pt
2022-08-18 05:50:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1442.pt
2022-08-18 05:50:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1442.pt (epoch 1442 @ 116802 updates, score 56.23) (writing took 42.048745311796665 seconds)
2022-08-18 05:50:59 | INFO | fairseq_cli.train | end of epoch 1442 (average epoch stats below)
2022-08-18 05:50:59 | INFO | train | epoch 1442 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 4843.4 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 116802 | lr 0.000185057 | gnorm 0.378 | train_wall 39 | gb_free 10.1 | wall 113956
2022-08-18 05:51:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:51:00 | INFO | fairseq.trainer | begin training epoch 1443
2022-08-18 05:51:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:51:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:51:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:51:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:51:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:51:49 | INFO | valid | epoch 1443 | valid on 'valid' subset | loss 5.286 | nll_loss 2.709 | ppl 6.54 | bleu 56.46 | wps 1799.3 | wpb 933.5 | bsz 59.6 | num_updates 116883 | best_bleu 57.52
2022-08-18 05:51:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1443 @ 116883 updates
2022-08-18 05:51:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1443.pt
2022-08-18 05:51:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1443.pt
2022-08-18 05:52:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1443.pt (epoch 1443 @ 116883 updates, score 56.46) (writing took 22.292434975504875 seconds)
2022-08-18 05:52:12 | INFO | fairseq_cli.train | end of epoch 1443 (average epoch stats below)
2022-08-18 05:52:12 | INFO | train | epoch 1443 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6193.5 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 116883 | lr 0.000184993 | gnorm 0.459 | train_wall 38 | gb_free 10.1 | wall 114028
2022-08-18 05:52:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:52:12 | INFO | fairseq.trainer | begin training epoch 1444
2022-08-18 05:52:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:52:21 | INFO | train_inner | epoch 1444:     17 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=4105.1, ups=0.74, wpb=5518.2, bsz=354.4, num_updates=116900, lr=0.000184979, gnorm=0.447, train_wall=47, gb_free=10, wall=114037
2022-08-18 05:52:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:52:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:52:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:52:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:53:02 | INFO | valid | epoch 1444 | valid on 'valid' subset | loss 5.261 | nll_loss 2.675 | ppl 6.39 | bleu 56.62 | wps 1837 | wpb 933.5 | bsz 59.6 | num_updates 116964 | best_bleu 57.52
2022-08-18 05:53:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1444 @ 116964 updates
2022-08-18 05:53:02 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1444.pt
2022-08-18 05:53:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1444.pt
2022-08-18 05:53:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1444.pt (epoch 1444 @ 116964 updates, score 56.62) (writing took 26.228597972542048 seconds)
2022-08-18 05:53:28 | INFO | fairseq_cli.train | end of epoch 1444 (average epoch stats below)
2022-08-18 05:53:28 | INFO | train | epoch 1444 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5849.7 | ups 1.06 | wpb 5523.2 | bsz 358 | num_updates 116964 | lr 0.000184929 | gnorm 0.431 | train_wall 39 | gb_free 10.2 | wall 114104
2022-08-18 05:53:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:53:28 | INFO | fairseq.trainer | begin training epoch 1445
2022-08-18 05:53:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:53:48 | INFO | train_inner | epoch 1445:     36 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=6316.4, ups=1.15, wpb=5507.9, bsz=367.1, num_updates=117000, lr=0.0001849, gnorm=0.42, train_wall=48, gb_free=10, wall=114125
2022-08-18 05:54:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:54:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:54:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:54:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:54:21 | INFO | valid | epoch 1445 | valid on 'valid' subset | loss 5.263 | nll_loss 2.681 | ppl 6.41 | bleu 56.61 | wps 1757 | wpb 933.5 | bsz 59.6 | num_updates 117045 | best_bleu 57.52
2022-08-18 05:54:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1445 @ 117045 updates
2022-08-18 05:54:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1445.pt
2022-08-18 05:54:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1445.pt
2022-08-18 05:55:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1445.pt (epoch 1445 @ 117045 updates, score 56.61) (writing took 41.24018481746316 seconds)
2022-08-18 05:55:02 | INFO | fairseq_cli.train | end of epoch 1445 (average epoch stats below)
2022-08-18 05:55:02 | INFO | train | epoch 1445 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 4764.4 | ups 0.86 | wpb 5523.2 | bsz 358 | num_updates 117045 | lr 0.000184865 | gnorm 0.356 | train_wall 39 | gb_free 10.3 | wall 114198
2022-08-18 05:55:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:55:02 | INFO | fairseq.trainer | begin training epoch 1446
2022-08-18 05:55:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:55:31 | INFO | train_inner | epoch 1446:     55 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5391.4, ups=0.98, wpb=5522.3, bsz=349, num_updates=117100, lr=0.000184821, gnorm=0.326, train_wall=49, gb_free=10.1, wall=114227
2022-08-18 05:55:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:55:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:55:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:55:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:55:53 | INFO | valid | epoch 1446 | valid on 'valid' subset | loss 5.284 | nll_loss 2.701 | ppl 6.5 | bleu 56.51 | wps 1845.5 | wpb 933.5 | bsz 59.6 | num_updates 117126 | best_bleu 57.52
2022-08-18 05:55:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1446 @ 117126 updates
2022-08-18 05:55:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1446.pt
2022-08-18 05:55:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1446.pt
2022-08-18 05:56:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1446.pt (epoch 1446 @ 117126 updates, score 56.51) (writing took 23.073922108858824 seconds)
2022-08-18 05:56:16 | INFO | fairseq_cli.train | end of epoch 1446 (average epoch stats below)
2022-08-18 05:56:16 | INFO | train | epoch 1446 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6050.3 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 117126 | lr 0.000184801 | gnorm 0.32 | train_wall 39 | gb_free 10 | wall 114272
2022-08-18 05:56:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:56:16 | INFO | fairseq.trainer | begin training epoch 1447
2022-08-18 05:56:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:56:54 | INFO | train_inner | epoch 1447:     74 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=6696.4, ups=1.21, wpb=5546.3, bsz=361.8, num_updates=117200, lr=0.000184742, gnorm=0.414, train_wall=49, gb_free=10.1, wall=114310
2022-08-18 05:56:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:56:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:56:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:56:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:57:06 | INFO | valid | epoch 1447 | valid on 'valid' subset | loss 5.259 | nll_loss 2.673 | ppl 6.38 | bleu 56.29 | wps 1796 | wpb 933.5 | bsz 59.6 | num_updates 117207 | best_bleu 57.52
2022-08-18 05:57:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1447 @ 117207 updates
2022-08-18 05:57:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1447.pt
2022-08-18 05:57:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1447.pt
2022-08-18 05:57:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1447.pt (epoch 1447 @ 117207 updates, score 56.29) (writing took 23.158161137253046 seconds)
2022-08-18 05:57:30 | INFO | fairseq_cli.train | end of epoch 1447 (average epoch stats below)
2022-08-18 05:57:30 | INFO | train | epoch 1447 | loss 3.371 | nll_loss 0.34 | ppl 1.27 | wps 6080 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 117207 | lr 0.000184737 | gnorm 0.433 | train_wall 39 | gb_free 10.3 | wall 114346
2022-08-18 05:57:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:57:30 | INFO | fairseq.trainer | begin training epoch 1448
2022-08-18 05:57:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:58:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:58:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:58:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:58:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:58:21 | INFO | valid | epoch 1448 | valid on 'valid' subset | loss 5.266 | nll_loss 2.68 | ppl 6.41 | bleu 56.36 | wps 1686.9 | wpb 933.5 | bsz 59.6 | num_updates 117288 | best_bleu 57.52
2022-08-18 05:58:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1448 @ 117288 updates
2022-08-18 05:58:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1448.pt
2022-08-18 05:58:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1448.pt
2022-08-18 05:59:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1448.pt (epoch 1448 @ 117288 updates, score 56.36) (writing took 38.95193909853697 seconds)
2022-08-18 05:59:00 | INFO | fairseq_cli.train | end of epoch 1448 (average epoch stats below)
2022-08-18 05:59:00 | INFO | train | epoch 1448 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 4947.7 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 117288 | lr 0.000184673 | gnorm 0.371 | train_wall 39 | gb_free 10.1 | wall 114436
2022-08-18 05:59:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 05:59:00 | INFO | fairseq.trainer | begin training epoch 1449
2022-08-18 05:59:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 05:59:08 | INFO | train_inner | epoch 1449:     12 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=4098, ups=0.75, wpb=5495.1, bsz=359, num_updates=117300, lr=0.000184663, gnorm=0.353, train_wall=48, gb_free=10.1, wall=114444
2022-08-18 05:59:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 05:59:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 05:59:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 05:59:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 05:59:51 | INFO | valid | epoch 1449 | valid on 'valid' subset | loss 5.251 | nll_loss 2.665 | ppl 6.34 | bleu 56.57 | wps 1933.8 | wpb 933.5 | bsz 59.6 | num_updates 117369 | best_bleu 57.52
2022-08-18 05:59:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1449 @ 117369 updates
2022-08-18 05:59:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1449.pt
2022-08-18 05:59:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1449.pt
2022-08-18 06:00:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1449.pt (epoch 1449 @ 117369 updates, score 56.57) (writing took 19.03438751026988 seconds)
2022-08-18 06:00:10 | INFO | fairseq_cli.train | end of epoch 1449 (average epoch stats below)
2022-08-18 06:00:10 | INFO | train | epoch 1449 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6381.2 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 117369 | lr 0.000184609 | gnorm 0.364 | train_wall 40 | gb_free 10.1 | wall 114506
2022-08-18 06:00:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:00:10 | INFO | fairseq.trainer | begin training epoch 1450
2022-08-18 06:00:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:00:26 | INFO | train_inner | epoch 1450:     31 / 81 loss=3.369, nll_loss=0.337, ppl=1.26, wps=7045.3, ups=1.27, wpb=5542.4, bsz=357.4, num_updates=117400, lr=0.000184585, gnorm=0.387, train_wall=49, gb_free=10.1, wall=114523
2022-08-18 06:00:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:00:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:00:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:00:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:01:01 | INFO | valid | epoch 1450 | valid on 'valid' subset | loss 5.26 | nll_loss 2.674 | ppl 6.38 | bleu 56.5 | wps 1916 | wpb 933.5 | bsz 59.6 | num_updates 117450 | best_bleu 57.52
2022-08-18 06:01:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1450 @ 117450 updates
2022-08-18 06:01:01 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1450.pt
2022-08-18 06:01:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1450.pt
2022-08-18 06:01:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1450.pt (epoch 1450 @ 117450 updates, score 56.5) (writing took 35.09795343875885 seconds)
2022-08-18 06:01:36 | INFO | fairseq_cli.train | end of epoch 1450 (average epoch stats below)
2022-08-18 06:01:36 | INFO | train | epoch 1450 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 5211.5 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 117450 | lr 0.000184546 | gnorm 0.398 | train_wall 40 | gb_free 10.2 | wall 114592
2022-08-18 06:01:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:01:36 | INFO | fairseq.trainer | begin training epoch 1451
2022-08-18 06:01:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:02:04 | INFO | train_inner | epoch 1451:     50 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=5639.5, ups=1.02, wpb=5530.8, bsz=355.1, num_updates=117500, lr=0.000184506, gnorm=0.334, train_wall=49, gb_free=10.1, wall=114621
2022-08-18 06:02:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:02:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:02:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:02:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:02:32 | INFO | valid | epoch 1451 | valid on 'valid' subset | loss 5.251 | nll_loss 2.663 | ppl 6.34 | bleu 57.11 | wps 1611.4 | wpb 933.5 | bsz 59.6 | num_updates 117531 | best_bleu 57.52
2022-08-18 06:02:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1451 @ 117531 updates
2022-08-18 06:02:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1451.pt
2022-08-18 06:02:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1451.pt
2022-08-18 06:03:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1451.pt (epoch 1451 @ 117531 updates, score 57.11) (writing took 37.49390992149711 seconds)
2022-08-18 06:03:10 | INFO | fairseq_cli.train | end of epoch 1451 (average epoch stats below)
2022-08-18 06:03:10 | INFO | train | epoch 1451 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 4773.1 | ups 0.86 | wpb 5523.2 | bsz 358 | num_updates 117531 | lr 0.000184482 | gnorm 0.304 | train_wall 39 | gb_free 10.1 | wall 114686
2022-08-18 06:03:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:03:10 | INFO | fairseq.trainer | begin training epoch 1452
2022-08-18 06:03:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:03:45 | INFO | train_inner | epoch 1452:     69 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=5512.1, ups=1, wpb=5518.1, bsz=357, num_updates=117600, lr=0.000184428, gnorm=0.395, train_wall=47, gb_free=10.1, wall=114721
2022-08-18 06:03:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:03:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:03:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:03:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:04:00 | INFO | valid | epoch 1452 | valid on 'valid' subset | loss 5.261 | nll_loss 2.673 | ppl 6.38 | bleu 56.87 | wps 1848.1 | wpb 933.5 | bsz 59.6 | num_updates 117612 | best_bleu 57.52
2022-08-18 06:04:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1452 @ 117612 updates
2022-08-18 06:04:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1452.pt
2022-08-18 06:04:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1452.pt
2022-08-18 06:04:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1452.pt (epoch 1452 @ 117612 updates, score 56.87) (writing took 2.3688285760581493 seconds)
2022-08-18 06:04:02 | INFO | fairseq_cli.train | end of epoch 1452 (average epoch stats below)
2022-08-18 06:04:02 | INFO | train | epoch 1452 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 8499.1 | ups 1.54 | wpb 5523.2 | bsz 358 | num_updates 117612 | lr 0.000184418 | gnorm 0.396 | train_wall 38 | gb_free 10.1 | wall 114739
2022-08-18 06:04:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:04:03 | INFO | fairseq.trainer | begin training epoch 1453
2022-08-18 06:04:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:04:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:04:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:04:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:04:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:04:54 | INFO | valid | epoch 1453 | valid on 'valid' subset | loss 5.247 | nll_loss 2.662 | ppl 6.33 | bleu 57.32 | wps 1826.5 | wpb 933.5 | bsz 59.6 | num_updates 117693 | best_bleu 57.52
2022-08-18 06:04:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1453 @ 117693 updates
2022-08-18 06:04:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1453.pt
2022-08-18 06:04:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1453.pt
2022-08-18 06:05:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1453.pt (epoch 1453 @ 117693 updates, score 57.32) (writing took 29.048823311924934 seconds)
2022-08-18 06:05:23 | INFO | fairseq_cli.train | end of epoch 1453 (average epoch stats below)
2022-08-18 06:05:23 | INFO | train | epoch 1453 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5561.9 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 117693 | lr 0.000184355 | gnorm 0.386 | train_wall 40 | gb_free 10.1 | wall 114819
2022-08-18 06:05:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:05:23 | INFO | fairseq.trainer | begin training epoch 1454
2022-08-18 06:05:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:05:27 | INFO | train_inner | epoch 1454:      7 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5360.4, ups=0.97, wpb=5500.3, bsz=359.7, num_updates=117700, lr=0.000184349, gnorm=0.382, train_wall=49, gb_free=10.1, wall=114823
2022-08-18 06:06:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:06:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:06:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:06:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:06:12 | INFO | valid | epoch 1454 | valid on 'valid' subset | loss 5.262 | nll_loss 2.678 | ppl 6.4 | bleu 56.89 | wps 1779.2 | wpb 933.5 | bsz 59.6 | num_updates 117774 | best_bleu 57.52
2022-08-18 06:06:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1454 @ 117774 updates
2022-08-18 06:06:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1454.pt
2022-08-18 06:06:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1454.pt
2022-08-18 06:06:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1454.pt (epoch 1454 @ 117774 updates, score 56.89) (writing took 34.61796738952398 seconds)
2022-08-18 06:06:47 | INFO | fairseq_cli.train | end of epoch 1454 (average epoch stats below)
2022-08-18 06:06:47 | INFO | train | epoch 1454 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5301.1 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 117774 | lr 0.000184291 | gnorm 0.463 | train_wall 38 | gb_free 10.1 | wall 114903
2022-08-18 06:06:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:06:47 | INFO | fairseq.trainer | begin training epoch 1455
2022-08-18 06:06:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:07:00 | INFO | train_inner | epoch 1455:     26 / 81 loss=3.37, nll_loss=0.339, ppl=1.27, wps=5922.5, ups=1.07, wpb=5520.8, bsz=352.7, num_updates=117800, lr=0.000184271, gnorm=0.443, train_wall=47, gb_free=10.1, wall=114917
2022-08-18 06:07:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:07:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:07:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:07:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:07:38 | INFO | valid | epoch 1455 | valid on 'valid' subset | loss 5.261 | nll_loss 2.677 | ppl 6.39 | bleu 56.87 | wps 1820.4 | wpb 933.5 | bsz 59.6 | num_updates 117855 | best_bleu 57.52
2022-08-18 06:07:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1455 @ 117855 updates
2022-08-18 06:07:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1455.pt
2022-08-18 06:07:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1455.pt
2022-08-18 06:07:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1455.pt (epoch 1455 @ 117855 updates, score 56.87) (writing took 13.576258428394794 seconds)
2022-08-18 06:07:51 | INFO | fairseq_cli.train | end of epoch 1455 (average epoch stats below)
2022-08-18 06:07:51 | INFO | train | epoch 1455 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6958.6 | ups 1.26 | wpb 5523.2 | bsz 358 | num_updates 117855 | lr 0.000184228 | gnorm 0.363 | train_wall 39 | gb_free 10.1 | wall 114968
2022-08-18 06:07:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:07:52 | INFO | fairseq.trainer | begin training epoch 1456
2022-08-18 06:07:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:08:15 | INFO | train_inner | epoch 1456:     45 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=7412.4, ups=1.34, wpb=5545.2, bsz=365.2, num_updates=117900, lr=0.000184193, gnorm=0.333, train_wall=48, gb_free=10.1, wall=114991
2022-08-18 06:08:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:08:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:08:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:08:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:08:43 | INFO | valid | epoch 1456 | valid on 'valid' subset | loss 5.262 | nll_loss 2.675 | ppl 6.39 | bleu 56.65 | wps 1716.5 | wpb 933.5 | bsz 59.6 | num_updates 117936 | best_bleu 57.52
2022-08-18 06:08:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1456 @ 117936 updates
2022-08-18 06:08:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1456.pt
2022-08-18 06:08:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1456.pt
2022-08-18 06:09:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1456.pt (epoch 1456 @ 117936 updates, score 56.65) (writing took 42.305002856999636 seconds)
2022-08-18 06:09:26 | INFO | fairseq_cli.train | end of epoch 1456 (average epoch stats below)
2022-08-18 06:09:26 | INFO | train | epoch 1456 | loss 3.369 | nll_loss 0.337 | ppl 1.26 | wps 4752.9 | ups 0.86 | wpb 5523.2 | bsz 358 | num_updates 117936 | lr 0.000184165 | gnorm 0.353 | train_wall 39 | gb_free 10 | wall 115062
2022-08-18 06:09:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:09:26 | INFO | fairseq.trainer | begin training epoch 1457
2022-08-18 06:09:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:10:00 | INFO | train_inner | epoch 1457:     64 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5281.2, ups=0.95, wpb=5535, bsz=359.9, num_updates=118000, lr=0.000184115, gnorm=0.385, train_wall=49, gb_free=10.2, wall=115096
2022-08-18 06:10:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:10:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:10:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:10:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:10:18 | INFO | valid | epoch 1457 | valid on 'valid' subset | loss 5.281 | nll_loss 2.699 | ppl 6.5 | bleu 56.34 | wps 1756.7 | wpb 933.5 | bsz 59.6 | num_updates 118017 | best_bleu 57.52
2022-08-18 06:10:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1457 @ 118017 updates
2022-08-18 06:10:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1457.pt
2022-08-18 06:10:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1457.pt
2022-08-18 06:10:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1457.pt (epoch 1457 @ 118017 updates, score 56.34) (writing took 21.471191741526127 seconds)
2022-08-18 06:10:39 | INFO | fairseq_cli.train | end of epoch 1457 (average epoch stats below)
2022-08-18 06:10:39 | INFO | train | epoch 1457 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6060.7 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 118017 | lr 0.000184102 | gnorm 0.421 | train_wall 39 | gb_free 10.1 | wall 115136
2022-08-18 06:10:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:10:40 | INFO | fairseq.trainer | begin training epoch 1458
2022-08-18 06:10:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:11:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:11:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:11:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:11:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:11:29 | INFO | valid | epoch 1458 | valid on 'valid' subset | loss 5.261 | nll_loss 2.677 | ppl 6.4 | bleu 56.63 | wps 1818.1 | wpb 933.5 | bsz 59.6 | num_updates 118098 | best_bleu 57.52
2022-08-18 06:11:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1458 @ 118098 updates
2022-08-18 06:11:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1458.pt
2022-08-18 06:11:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1458.pt
2022-08-18 06:11:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1458.pt (epoch 1458 @ 118098 updates, score 56.63) (writing took 25.119565542787313 seconds)
2022-08-18 06:11:54 | INFO | fairseq_cli.train | end of epoch 1458 (average epoch stats below)
2022-08-18 06:11:54 | INFO | train | epoch 1458 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5969.6 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 118098 | lr 0.000184039 | gnorm 0.407 | train_wall 38 | gb_free 10.1 | wall 115211
2022-08-18 06:11:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:11:55 | INFO | fairseq.trainer | begin training epoch 1459
2022-08-18 06:11:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:11:57 | INFO | train_inner | epoch 1459:      2 / 81 loss=3.37, nll_loss=0.339, ppl=1.27, wps=4711.1, ups=0.86, wpb=5496.3, bsz=355, num_updates=118100, lr=0.000184037, gnorm=0.437, train_wall=47, gb_free=10.1, wall=115213
2022-08-18 06:12:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:12:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:12:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:12:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:12:45 | INFO | valid | epoch 1459 | valid on 'valid' subset | loss 5.245 | nll_loss 2.655 | ppl 6.3 | bleu 57.14 | wps 1866.8 | wpb 933.5 | bsz 59.6 | num_updates 118179 | best_bleu 57.52
2022-08-18 06:12:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1459 @ 118179 updates
2022-08-18 06:12:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1459.pt
2022-08-18 06:12:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1459.pt
2022-08-18 06:13:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1459.pt (epoch 1459 @ 118179 updates, score 57.14) (writing took 35.797518849372864 seconds)
2022-08-18 06:13:21 | INFO | fairseq_cli.train | end of epoch 1459 (average epoch stats below)
2022-08-18 06:13:21 | INFO | train | epoch 1459 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5183.3 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 118179 | lr 0.000183975 | gnorm 0.407 | train_wall 39 | gb_free 10.2 | wall 115297
2022-08-18 06:13:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:13:21 | INFO | fairseq.trainer | begin training epoch 1460
2022-08-18 06:13:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:13:33 | INFO | train_inner | epoch 1460:     21 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=5737.4, ups=1.04, wpb=5519.1, bsz=358.6, num_updates=118200, lr=0.000183959, gnorm=0.42, train_wall=48, gb_free=10.2, wall=115309
2022-08-18 06:14:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:14:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:14:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:14:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:14:12 | INFO | valid | epoch 1460 | valid on 'valid' subset | loss 5.262 | nll_loss 2.678 | ppl 6.4 | bleu 56.25 | wps 1832.2 | wpb 933.5 | bsz 59.6 | num_updates 118260 | best_bleu 57.52
2022-08-18 06:14:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1460 @ 118260 updates
2022-08-18 06:14:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1460.pt
2022-08-18 06:14:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1460.pt
2022-08-18 06:14:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1460.pt (epoch 1460 @ 118260 updates, score 56.25) (writing took 40.954280003905296 seconds)
2022-08-18 06:14:53 | INFO | fairseq_cli.train | end of epoch 1460 (average epoch stats below)
2022-08-18 06:14:53 | INFO | train | epoch 1460 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 4831.1 | ups 0.87 | wpb 5523.2 | bsz 358 | num_updates 118260 | lr 0.000183912 | gnorm 0.374 | train_wall 40 | gb_free 10.1 | wall 115389
2022-08-18 06:14:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:14:53 | INFO | fairseq.trainer | begin training epoch 1461
2022-08-18 06:14:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:15:14 | INFO | train_inner | epoch 1461:     40 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5474.8, ups=0.99, wpb=5547.6, bsz=357.4, num_updates=118300, lr=0.000183881, gnorm=0.582, train_wall=49, gb_free=10.1, wall=115410
2022-08-18 06:15:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:15:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:15:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:15:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:15:43 | INFO | valid | epoch 1461 | valid on 'valid' subset | loss 5.277 | nll_loss 2.695 | ppl 6.47 | bleu 56.54 | wps 1886 | wpb 933.5 | bsz 59.6 | num_updates 118341 | best_bleu 57.52
2022-08-18 06:15:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1461 @ 118341 updates
2022-08-18 06:15:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1461.pt
2022-08-18 06:15:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1461.pt
2022-08-18 06:16:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1461.pt (epoch 1461 @ 118341 updates, score 56.54) (writing took 25.597159538418055 seconds)
2022-08-18 06:16:09 | INFO | fairseq_cli.train | end of epoch 1461 (average epoch stats below)
2022-08-18 06:16:09 | INFO | train | epoch 1461 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5924.5 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 118341 | lr 0.000183849 | gnorm 0.635 | train_wall 38 | gb_free 10.1 | wall 115465
2022-08-18 06:16:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:16:09 | INFO | fairseq.trainer | begin training epoch 1462
2022-08-18 06:16:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:16:39 | INFO | train_inner | epoch 1462:     59 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6483.8, ups=1.17, wpb=5525.4, bsz=359.6, num_updates=118400, lr=0.000183804, gnorm=0.383, train_wall=48, gb_free=10.1, wall=115496
2022-08-18 06:16:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:16:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:16:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:16:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:17:00 | INFO | valid | epoch 1462 | valid on 'valid' subset | loss 5.271 | nll_loss 2.687 | ppl 6.44 | bleu 56.17 | wps 1882.1 | wpb 933.5 | bsz 59.6 | num_updates 118422 | best_bleu 57.52
2022-08-18 06:17:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1462 @ 118422 updates
2022-08-18 06:17:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1462.pt
2022-08-18 06:17:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1462.pt
2022-08-18 06:17:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1462.pt (epoch 1462 @ 118422 updates, score 56.17) (writing took 25.22091531008482 seconds)
2022-08-18 06:17:25 | INFO | fairseq_cli.train | end of epoch 1462 (average epoch stats below)
2022-08-18 06:17:25 | INFO | train | epoch 1462 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5877.1 | ups 1.06 | wpb 5523.2 | bsz 358 | num_updates 118422 | lr 0.000183787 | gnorm 0.383 | train_wall 40 | gb_free 10.2 | wall 115541
2022-08-18 06:17:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:17:25 | INFO | fairseq.trainer | begin training epoch 1463
2022-08-18 06:17:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:18:05 | INFO | train_inner | epoch 1463:     78 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=6460.2, ups=1.17, wpb=5533.9, bsz=357.4, num_updates=118500, lr=0.000183726, gnorm=0.483, train_wall=49, gb_free=10, wall=115581
2022-08-18 06:18:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:18:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:18:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:18:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:18:16 | INFO | valid | epoch 1463 | valid on 'valid' subset | loss 5.268 | nll_loss 2.687 | ppl 6.44 | bleu 56.19 | wps 1745.6 | wpb 933.5 | bsz 59.6 | num_updates 118503 | best_bleu 57.52
2022-08-18 06:18:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1463 @ 118503 updates
2022-08-18 06:18:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1463.pt
2022-08-18 06:18:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1463.pt
2022-08-18 06:18:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1463.pt (epoch 1463 @ 118503 updates, score 56.19) (writing took 42.425246708095074 seconds)
2022-08-18 06:18:59 | INFO | fairseq_cli.train | end of epoch 1463 (average epoch stats below)
2022-08-18 06:18:59 | INFO | train | epoch 1463 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 4769.4 | ups 0.86 | wpb 5523.2 | bsz 358 | num_updates 118503 | lr 0.000183724 | gnorm 0.833 | train_wall 40 | gb_free 10.3 | wall 115635
2022-08-18 06:18:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:18:59 | INFO | fairseq.trainer | begin training epoch 1464
2022-08-18 06:18:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:19:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:19:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:19:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:19:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:19:49 | INFO | valid | epoch 1464 | valid on 'valid' subset | loss 5.264 | nll_loss 2.679 | ppl 6.4 | bleu 56.41 | wps 1873.7 | wpb 933.5 | bsz 59.6 | num_updates 118584 | best_bleu 57.52
2022-08-18 06:19:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1464 @ 118584 updates
2022-08-18 06:19:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1464.pt
2022-08-18 06:19:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1464.pt
2022-08-18 06:20:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1464.pt (epoch 1464 @ 118584 updates, score 56.41) (writing took 20.92316137254238 seconds)
2022-08-18 06:20:10 | INFO | fairseq_cli.train | end of epoch 1464 (average epoch stats below)
2022-08-18 06:20:10 | INFO | train | epoch 1464 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6279.1 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 118584 | lr 0.000183661 | gnorm 0.498 | train_wall 39 | gb_free 10.1 | wall 115706
2022-08-18 06:20:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:20:10 | INFO | fairseq.trainer | begin training epoch 1465
2022-08-18 06:20:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:20:19 | INFO | train_inner | epoch 1465:     16 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=4103.5, ups=0.75, wpb=5501.4, bsz=355.1, num_updates=118600, lr=0.000183649, gnorm=0.717, train_wall=48, gb_free=10.1, wall=115715
2022-08-18 06:20:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:20:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:20:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:20:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:21:00 | INFO | valid | epoch 1465 | valid on 'valid' subset | loss 5.287 | nll_loss 2.709 | ppl 6.54 | bleu 56.17 | wps 1853.2 | wpb 933.5 | bsz 59.6 | num_updates 118665 | best_bleu 57.52
2022-08-18 06:21:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1465 @ 118665 updates
2022-08-18 06:21:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1465.pt
2022-08-18 06:21:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1465.pt
2022-08-18 06:21:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1465.pt (epoch 1465 @ 118665 updates, score 56.17) (writing took 22.020860742777586 seconds)
2022-08-18 06:21:23 | INFO | fairseq_cli.train | end of epoch 1465 (average epoch stats below)
2022-08-18 06:21:23 | INFO | train | epoch 1465 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6164.1 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 118665 | lr 0.000183598 | gnorm 0.348 | train_wall 39 | gb_free 10.1 | wall 115779
2022-08-18 06:21:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:21:23 | INFO | fairseq.trainer | begin training epoch 1466
2022-08-18 06:21:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:21:43 | INFO | train_inner | epoch 1466:     35 / 81 loss=3.369, nll_loss=0.337, ppl=1.26, wps=6559.7, ups=1.19, wpb=5527.1, bsz=361.9, num_updates=118700, lr=0.000183571, gnorm=0.345, train_wall=48, gb_free=10.1, wall=115800
2022-08-18 06:22:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:22:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:22:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:22:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:22:18 | INFO | valid | epoch 1466 | valid on 'valid' subset | loss 5.267 | nll_loss 2.684 | ppl 6.43 | bleu 56.4 | wps 1766.6 | wpb 933.5 | bsz 59.6 | num_updates 118746 | best_bleu 57.52
2022-08-18 06:22:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1466 @ 118746 updates
2022-08-18 06:22:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1466.pt
2022-08-18 06:22:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1466.pt
2022-08-18 06:22:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1466.pt (epoch 1466 @ 118746 updates, score 56.4) (writing took 35.92578695714474 seconds)
2022-08-18 06:22:54 | INFO | fairseq_cli.train | end of epoch 1466 (average epoch stats below)
2022-08-18 06:22:54 | INFO | train | epoch 1466 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 4878.7 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 118746 | lr 0.000183536 | gnorm 0.361 | train_wall 39 | gb_free 10.2 | wall 115870
2022-08-18 06:22:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:22:54 | INFO | fairseq.trainer | begin training epoch 1467
2022-08-18 06:22:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:23:22 | INFO | train_inner | epoch 1467:     54 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5566.3, ups=1.02, wpb=5482.9, bsz=352.5, num_updates=118800, lr=0.000183494, gnorm=0.402, train_wall=48, gb_free=10.1, wall=115898
2022-08-18 06:23:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:23:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:23:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:23:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:23:46 | INFO | valid | epoch 1467 | valid on 'valid' subset | loss 5.274 | nll_loss 2.691 | ppl 6.46 | bleu 56.61 | wps 1626.4 | wpb 933.5 | bsz 59.6 | num_updates 118827 | best_bleu 57.52
2022-08-18 06:23:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1467 @ 118827 updates
2022-08-18 06:23:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1467.pt
2022-08-18 06:23:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1467.pt
2022-08-18 06:24:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1467.pt (epoch 1467 @ 118827 updates, score 56.61) (writing took 25.678897339850664 seconds)
2022-08-18 06:24:12 | INFO | fairseq_cli.train | end of epoch 1467 (average epoch stats below)
2022-08-18 06:24:12 | INFO | train | epoch 1467 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5754.8 | ups 1.04 | wpb 5523.2 | bsz 358 | num_updates 118827 | lr 0.000183473 | gnorm 0.389 | train_wall 40 | gb_free 10.2 | wall 115948
2022-08-18 06:24:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:24:12 | INFO | fairseq.trainer | begin training epoch 1468
2022-08-18 06:24:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:24:50 | INFO | train_inner | epoch 1468:     73 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6348.5, ups=1.14, wpb=5588.7, bsz=362.7, num_updates=118900, lr=0.000183417, gnorm=0.362, train_wall=50, gb_free=10, wall=115986
2022-08-18 06:24:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:24:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:24:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:24:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:25:03 | INFO | valid | epoch 1468 | valid on 'valid' subset | loss 5.271 | nll_loss 2.687 | ppl 6.44 | bleu 56.45 | wps 1843.2 | wpb 933.5 | bsz 59.6 | num_updates 118908 | best_bleu 57.52
2022-08-18 06:25:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1468 @ 118908 updates
2022-08-18 06:25:03 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1468.pt
2022-08-18 06:25:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1468.pt
2022-08-18 06:25:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1468.pt (epoch 1468 @ 118908 updates, score 56.45) (writing took 15.93633970618248 seconds)
2022-08-18 06:25:19 | INFO | fairseq_cli.train | end of epoch 1468 (average epoch stats below)
2022-08-18 06:25:19 | INFO | train | epoch 1468 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6657.8 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 118908 | lr 0.000183411 | gnorm 0.382 | train_wall 40 | gb_free 10.2 | wall 116015
2022-08-18 06:25:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:25:19 | INFO | fairseq.trainer | begin training epoch 1469
2022-08-18 06:25:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:26:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:26:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:26:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:26:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:26:09 | INFO | valid | epoch 1469 | valid on 'valid' subset | loss 5.286 | nll_loss 2.706 | ppl 6.53 | bleu 55.88 | wps 1845.9 | wpb 933.5 | bsz 59.6 | num_updates 118989 | best_bleu 57.52
2022-08-18 06:26:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1469 @ 118989 updates
2022-08-18 06:26:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1469.pt
2022-08-18 06:26:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1469.pt
2022-08-18 06:26:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1469.pt (epoch 1469 @ 118989 updates, score 55.88) (writing took 35.14114595577121 seconds)
2022-08-18 06:26:44 | INFO | fairseq_cli.train | end of epoch 1469 (average epoch stats below)
2022-08-18 06:26:44 | INFO | train | epoch 1469 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5249.6 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 118989 | lr 0.000183348 | gnorm 0.379 | train_wall 39 | gb_free 10.2 | wall 116101
2022-08-18 06:26:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:26:45 | INFO | fairseq.trainer | begin training epoch 1470
2022-08-18 06:26:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:26:51 | INFO | train_inner | epoch 1470:     11 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=4528.5, ups=0.83, wpb=5482.2, bsz=354.5, num_updates=119000, lr=0.00018334, gnorm=0.413, train_wall=47, gb_free=10.1, wall=116107
2022-08-18 06:27:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:27:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:27:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:27:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:27:35 | INFO | valid | epoch 1470 | valid on 'valid' subset | loss 5.285 | nll_loss 2.705 | ppl 6.52 | bleu 56.08 | wps 1756.1 | wpb 933.5 | bsz 59.6 | num_updates 119070 | best_bleu 57.52
2022-08-18 06:27:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1470 @ 119070 updates
2022-08-18 06:27:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1470.pt
2022-08-18 06:27:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1470.pt
2022-08-18 06:27:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1470.pt (epoch 1470 @ 119070 updates, score 56.08) (writing took 2.565338172018528 seconds)
2022-08-18 06:27:38 | INFO | fairseq_cli.train | end of epoch 1470 (average epoch stats below)
2022-08-18 06:27:38 | INFO | train | epoch 1470 | loss 3.37 | nll_loss 0.339 | ppl 1.27 | wps 8385.3 | ups 1.52 | wpb 5523.2 | bsz 358 | num_updates 119070 | lr 0.000183286 | gnorm 0.474 | train_wall 39 | gb_free 10.2 | wall 116154
2022-08-18 06:27:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:27:38 | INFO | fairseq.trainer | begin training epoch 1471
2022-08-18 06:27:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:27:54 | INFO | train_inner | epoch 1471:     30 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=8784, ups=1.59, wpb=5525.8, bsz=359.5, num_updates=119100, lr=0.000183263, gnorm=0.468, train_wall=48, gb_free=10.1, wall=116170
2022-08-18 06:28:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:28:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:28:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:28:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:28:29 | INFO | valid | epoch 1471 | valid on 'valid' subset | loss 5.27 | nll_loss 2.686 | ppl 6.43 | bleu 55.97 | wps 1759.2 | wpb 933.5 | bsz 59.6 | num_updates 119151 | best_bleu 57.52
2022-08-18 06:28:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1471 @ 119151 updates
2022-08-18 06:28:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1471.pt
2022-08-18 06:28:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1471.pt
2022-08-18 06:28:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1471.pt (epoch 1471 @ 119151 updates, score 55.97) (writing took 13.178118359297514 seconds)
2022-08-18 06:28:42 | INFO | fairseq_cli.train | end of epoch 1471 (average epoch stats below)
2022-08-18 06:28:42 | INFO | train | epoch 1471 | loss 3.371 | nll_loss 0.341 | ppl 1.27 | wps 6930 | ups 1.25 | wpb 5523.2 | bsz 358 | num_updates 119151 | lr 0.000183223 | gnorm 3.372 | train_wall 39 | gb_free 10 | wall 116219
2022-08-18 06:28:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:28:43 | INFO | fairseq.trainer | begin training epoch 1472
2022-08-18 06:28:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:29:10 | INFO | train_inner | epoch 1472:     49 / 81 loss=3.372, nll_loss=0.341, ppl=1.27, wps=7267.8, ups=1.31, wpb=5531.6, bsz=358.4, num_updates=119200, lr=0.000183186, gnorm=2.747, train_wall=49, gb_free=10.1, wall=116246
2022-08-18 06:29:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:29:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:29:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:29:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:29:38 | INFO | valid | epoch 1472 | valid on 'valid' subset | loss 5.289 | nll_loss 2.709 | ppl 6.54 | bleu 55.63 | wps 1857.1 | wpb 933.5 | bsz 59.6 | num_updates 119232 | best_bleu 57.52
2022-08-18 06:29:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1472 @ 119232 updates
2022-08-18 06:29:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1472.pt
2022-08-18 06:29:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1472.pt
2022-08-18 06:29:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1472.pt (epoch 1472 @ 119232 updates, score 55.63) (writing took 21.023536514490843 seconds)
2022-08-18 06:29:59 | INFO | fairseq_cli.train | end of epoch 1472 (average epoch stats below)
2022-08-18 06:29:59 | INFO | train | epoch 1472 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5842.4 | ups 1.06 | wpb 5523.2 | bsz 358 | num_updates 119232 | lr 0.000183161 | gnorm 0.375 | train_wall 39 | gb_free 10.1 | wall 116295
2022-08-18 06:29:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:29:59 | INFO | fairseq.trainer | begin training epoch 1473
2022-08-18 06:29:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:30:36 | INFO | train_inner | epoch 1473:     68 / 81 loss=3.37, nll_loss=0.339, ppl=1.27, wps=6474.5, ups=1.17, wpb=5530.3, bsz=360.2, num_updates=119300, lr=0.000183109, gnorm=0.465, train_wall=49, gb_free=10.1, wall=116332
2022-08-18 06:30:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:30:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:30:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:30:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:30:51 | INFO | valid | epoch 1473 | valid on 'valid' subset | loss 5.267 | nll_loss 2.682 | ppl 6.42 | bleu 56.48 | wps 1855.1 | wpb 933.5 | bsz 59.6 | num_updates 119313 | best_bleu 57.52
2022-08-18 06:30:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1473 @ 119313 updates
2022-08-18 06:30:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1473.pt
2022-08-18 06:30:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1473.pt
2022-08-18 06:31:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1473.pt (epoch 1473 @ 119313 updates, score 56.48) (writing took 13.53354961797595 seconds)
2022-08-18 06:31:05 | INFO | fairseq_cli.train | end of epoch 1473 (average epoch stats below)
2022-08-18 06:31:05 | INFO | train | epoch 1473 | loss 3.37 | nll_loss 0.339 | ppl 1.27 | wps 6794.3 | ups 1.23 | wpb 5523.2 | bsz 358 | num_updates 119313 | lr 0.000183099 | gnorm 0.485 | train_wall 40 | gb_free 10.2 | wall 116361
2022-08-18 06:31:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:31:05 | INFO | fairseq.trainer | begin training epoch 1474
2022-08-18 06:31:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:31:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:31:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:31:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:31:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:31:57 | INFO | valid | epoch 1474 | valid on 'valid' subset | loss 5.269 | nll_loss 2.685 | ppl 6.43 | bleu 56.45 | wps 1715.4 | wpb 933.5 | bsz 59.6 | num_updates 119394 | best_bleu 57.52
2022-08-18 06:31:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1474 @ 119394 updates
2022-08-18 06:31:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1474.pt
2022-08-18 06:31:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1474.pt
2022-08-18 06:32:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1474.pt (epoch 1474 @ 119394 updates, score 56.45) (writing took 38.67134079709649 seconds)
2022-08-18 06:32:35 | INFO | fairseq_cli.train | end of epoch 1474 (average epoch stats below)
2022-08-18 06:32:35 | INFO | train | epoch 1474 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 4935.1 | ups 0.89 | wpb 5523.2 | bsz 358 | num_updates 119394 | lr 0.000183037 | gnorm 0.335 | train_wall 40 | gb_free 10.2 | wall 116452
2022-08-18 06:32:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:32:36 | INFO | fairseq.trainer | begin training epoch 1475
2022-08-18 06:32:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:32:40 | INFO | train_inner | epoch 1475:      6 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=4429.4, ups=0.81, wpb=5498.8, bsz=354.5, num_updates=119400, lr=0.000183032, gnorm=0.358, train_wall=49, gb_free=10.1, wall=116456
2022-08-18 06:33:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:33:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:33:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:33:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:33:30 | INFO | valid | epoch 1475 | valid on 'valid' subset | loss 5.278 | nll_loss 2.696 | ppl 6.48 | bleu 56.45 | wps 1901 | wpb 933.5 | bsz 59.6 | num_updates 119475 | best_bleu 57.52
2022-08-18 06:33:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1475 @ 119475 updates
2022-08-18 06:33:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1475.pt
2022-08-18 06:33:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1475.pt
2022-08-18 06:33:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1475.pt (epoch 1475 @ 119475 updates, score 56.45) (writing took 19.247739888727665 seconds)
2022-08-18 06:33:50 | INFO | fairseq_cli.train | end of epoch 1475 (average epoch stats below)
2022-08-18 06:33:50 | INFO | train | epoch 1475 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6036.2 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 119475 | lr 0.000182975 | gnorm 0.372 | train_wall 39 | gb_free 10.1 | wall 116526
2022-08-18 06:33:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:33:50 | INFO | fairseq.trainer | begin training epoch 1476
2022-08-18 06:33:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:34:04 | INFO | train_inner | epoch 1476:     25 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6532.1, ups=1.18, wpb=5534, bsz=360.1, num_updates=119500, lr=0.000182956, gnorm=0.37, train_wall=49, gb_free=10, wall=116541
2022-08-18 06:34:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:34:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:34:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:34:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:34:42 | INFO | valid | epoch 1476 | valid on 'valid' subset | loss 5.274 | nll_loss 2.694 | ppl 6.47 | bleu 56.56 | wps 1854.6 | wpb 933.5 | bsz 59.6 | num_updates 119556 | best_bleu 57.52
2022-08-18 06:34:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1476 @ 119556 updates
2022-08-18 06:34:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1476.pt
2022-08-18 06:34:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1476.pt
2022-08-18 06:35:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1476.pt (epoch 1476 @ 119556 updates, score 56.56) (writing took 20.30262168124318 seconds)
2022-08-18 06:35:02 | INFO | fairseq_cli.train | end of epoch 1476 (average epoch stats below)
2022-08-18 06:35:02 | INFO | train | epoch 1476 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6137.5 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 119556 | lr 0.000182913 | gnorm 0.351 | train_wall 39 | gb_free 10.1 | wall 116599
2022-08-18 06:35:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:35:03 | INFO | fairseq.trainer | begin training epoch 1477
2022-08-18 06:35:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:35:26 | INFO | train_inner | epoch 1477:     44 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=6771.9, ups=1.23, wpb=5519.8, bsz=356.4, num_updates=119600, lr=0.000182879, gnorm=0.364, train_wall=49, gb_free=10, wall=116622
2022-08-18 06:35:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:35:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:35:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:35:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:35:54 | INFO | valid | epoch 1477 | valid on 'valid' subset | loss 5.275 | nll_loss 2.694 | ppl 6.47 | bleu 56.09 | wps 1760.5 | wpb 933.5 | bsz 59.6 | num_updates 119637 | best_bleu 57.52
2022-08-18 06:35:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1477 @ 119637 updates
2022-08-18 06:35:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1477.pt
2022-08-18 06:35:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1477.pt
2022-08-18 06:36:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1477.pt (epoch 1477 @ 119637 updates, score 56.09) (writing took 38.45271648094058 seconds)
2022-08-18 06:36:32 | INFO | fairseq_cli.train | end of epoch 1477 (average epoch stats below)
2022-08-18 06:36:32 | INFO | train | epoch 1477 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 4977.8 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 119637 | lr 0.000182851 | gnorm 0.476 | train_wall 40 | gb_free 10.2 | wall 116689
2022-08-18 06:36:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:36:32 | INFO | fairseq.trainer | begin training epoch 1478
2022-08-18 06:36:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:37:06 | INFO | train_inner | epoch 1478:     63 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5562.4, ups=1, wpb=5541.1, bsz=362.6, num_updates=119700, lr=0.000182803, gnorm=0.45, train_wall=49, gb_free=10.1, wall=116722
2022-08-18 06:37:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:37:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:37:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:37:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:37:24 | INFO | valid | epoch 1478 | valid on 'valid' subset | loss 5.275 | nll_loss 2.694 | ppl 6.47 | bleu 56.37 | wps 1795.4 | wpb 933.5 | bsz 59.6 | num_updates 119718 | best_bleu 57.52
2022-08-18 06:37:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1478 @ 119718 updates
2022-08-18 06:37:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1478.pt
2022-08-18 06:37:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1478.pt
2022-08-18 06:37:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1478.pt (epoch 1478 @ 119718 updates, score 56.37) (writing took 19.517372280359268 seconds)
2022-08-18 06:37:44 | INFO | fairseq_cli.train | end of epoch 1478 (average epoch stats below)
2022-08-18 06:37:44 | INFO | train | epoch 1478 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6283.6 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 119718 | lr 0.000182789 | gnorm 0.385 | train_wall 40 | gb_free 10.1 | wall 116760
2022-08-18 06:37:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:37:44 | INFO | fairseq.trainer | begin training epoch 1479
2022-08-18 06:37:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:38:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:38:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:38:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:38:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:38:35 | INFO | valid | epoch 1479 | valid on 'valid' subset | loss 5.304 | nll_loss 2.727 | ppl 6.62 | bleu 55.99 | wps 1827.9 | wpb 933.5 | bsz 59.6 | num_updates 119799 | best_bleu 57.52
2022-08-18 06:38:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1479 @ 119799 updates
2022-08-18 06:38:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1479.pt
2022-08-18 06:38:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1479.pt
2022-08-18 06:39:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1479.pt (epoch 1479 @ 119799 updates, score 55.99) (writing took 37.45637030526996 seconds)
2022-08-18 06:39:12 | INFO | fairseq_cli.train | end of epoch 1479 (average epoch stats below)
2022-08-18 06:39:12 | INFO | train | epoch 1479 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5042.8 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 119799 | lr 0.000182727 | gnorm 0.439 | train_wall 40 | gb_free 10.2 | wall 116848
2022-08-18 06:39:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:39:12 | INFO | fairseq.trainer | begin training epoch 1480
2022-08-18 06:39:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:39:14 | INFO | train_inner | epoch 1480:      1 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=4286.5, ups=0.78, wpb=5509.8, bsz=354.9, num_updates=119800, lr=0.000182727, gnorm=0.433, train_wall=49, gb_free=10, wall=116850
2022-08-18 06:39:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:39:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:39:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:39:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:40:05 | INFO | valid | epoch 1480 | valid on 'valid' subset | loss 5.306 | nll_loss 2.73 | ppl 6.63 | bleu 56.09 | wps 1718.1 | wpb 933.5 | bsz 59.6 | num_updates 119880 | best_bleu 57.52
2022-08-18 06:40:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1480 @ 119880 updates
2022-08-18 06:40:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1480.pt
2022-08-18 06:40:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1480.pt
2022-08-18 06:40:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1480.pt (epoch 1480 @ 119880 updates, score 56.09) (writing took 22.032714053988457 seconds)
2022-08-18 06:40:27 | INFO | fairseq_cli.train | end of epoch 1480 (average epoch stats below)
2022-08-18 06:40:27 | INFO | train | epoch 1480 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 5997.3 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 119880 | lr 0.000182666 | gnorm 0.317 | train_wall 41 | gb_free 10.1 | wall 116923
2022-08-18 06:40:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:40:27 | INFO | fairseq.trainer | begin training epoch 1481
2022-08-18 06:40:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:40:39 | INFO | train_inner | epoch 1481:     20 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=6541.9, ups=1.18, wpb=5544, bsz=359.8, num_updates=119900, lr=0.00018265, gnorm=0.329, train_wall=50, gb_free=10, wall=116935
2022-08-18 06:41:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:41:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:41:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:41:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:41:18 | INFO | valid | epoch 1481 | valid on 'valid' subset | loss 5.288 | nll_loss 2.709 | ppl 6.54 | bleu 55.56 | wps 1807.4 | wpb 933.5 | bsz 59.6 | num_updates 119961 | best_bleu 57.52
2022-08-18 06:41:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1481 @ 119961 updates
2022-08-18 06:41:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1481.pt
2022-08-18 06:41:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1481.pt
2022-08-18 06:42:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1481.pt (epoch 1481 @ 119961 updates, score 55.56) (writing took 42.776678778231144 seconds)
2022-08-18 06:42:01 | INFO | fairseq_cli.train | end of epoch 1481 (average epoch stats below)
2022-08-18 06:42:01 | INFO | train | epoch 1481 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 4760.1 | ups 0.86 | wpb 5523.2 | bsz 358 | num_updates 119961 | lr 0.000182604 | gnorm 0.361 | train_wall 39 | gb_free 10.1 | wall 117017
2022-08-18 06:42:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:42:01 | INFO | fairseq.trainer | begin training epoch 1482
2022-08-18 06:42:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:42:21 | INFO | train_inner | epoch 1482:     39 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=5369.1, ups=0.98, wpb=5503.2, bsz=351.3, num_updates=120000, lr=0.000182574, gnorm=0.366, train_wall=48, gb_free=10.1, wall=117038
2022-08-18 06:42:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:42:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:42:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:42:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:42:51 | INFO | valid | epoch 1482 | valid on 'valid' subset | loss 5.257 | nll_loss 2.671 | ppl 6.37 | bleu 56.28 | wps 1947.6 | wpb 933.5 | bsz 59.6 | num_updates 120042 | best_bleu 57.52
2022-08-18 06:42:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1482 @ 120042 updates
2022-08-18 06:42:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1482.pt
2022-08-18 06:42:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1482.pt
2022-08-18 06:43:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1482.pt (epoch 1482 @ 120042 updates, score 56.28) (writing took 18.526312980800867 seconds)
2022-08-18 06:43:10 | INFO | fairseq_cli.train | end of epoch 1482 (average epoch stats below)
2022-08-18 06:43:10 | INFO | train | epoch 1482 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6458.9 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 120042 | lr 0.000182542 | gnorm 0.342 | train_wall 40 | gb_free 10.1 | wall 117086
2022-08-18 06:43:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:43:10 | INFO | fairseq.trainer | begin training epoch 1483
2022-08-18 06:43:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:43:40 | INFO | train_inner | epoch 1483:     58 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=7013.2, ups=1.26, wpb=5550.6, bsz=364.4, num_updates=120100, lr=0.000182498, gnorm=0.515, train_wall=49, gb_free=10, wall=117117
2022-08-18 06:43:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:43:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:43:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:43:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:44:01 | INFO | valid | epoch 1483 | valid on 'valid' subset | loss 5.273 | nll_loss 2.689 | ppl 6.45 | bleu 56.63 | wps 1873.3 | wpb 933.5 | bsz 59.6 | num_updates 120123 | best_bleu 57.52
2022-08-18 06:44:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1483 @ 120123 updates
2022-08-18 06:44:01 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1483.pt
2022-08-18 06:44:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1483.pt
2022-08-18 06:44:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1483.pt (epoch 1483 @ 120123 updates, score 56.63) (writing took 28.445662282407284 seconds)
2022-08-18 06:44:30 | INFO | fairseq_cli.train | end of epoch 1483 (average epoch stats below)
2022-08-18 06:44:30 | INFO | train | epoch 1483 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5621.4 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 120123 | lr 0.000182481 | gnorm 0.578 | train_wall 39 | gb_free 10.1 | wall 117166
2022-08-18 06:44:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:44:30 | INFO | fairseq.trainer | begin training epoch 1484
2022-08-18 06:44:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:45:11 | INFO | train_inner | epoch 1484:     77 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6117.3, ups=1.11, wpb=5509.7, bsz=357.2, num_updates=120200, lr=0.000182422, gnorm=0.344, train_wall=49, gb_free=10.1, wall=117207
2022-08-18 06:45:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:45:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:45:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:45:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:45:22 | INFO | valid | epoch 1484 | valid on 'valid' subset | loss 5.275 | nll_loss 2.694 | ppl 6.47 | bleu 56.27 | wps 1835.2 | wpb 933.5 | bsz 59.6 | num_updates 120204 | best_bleu 57.52
2022-08-18 06:45:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1484 @ 120204 updates
2022-08-18 06:45:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1484.pt
2022-08-18 06:45:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1484.pt
2022-08-18 06:45:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1484.pt (epoch 1484 @ 120204 updates, score 56.27) (writing took 35.13261862099171 seconds)
2022-08-18 06:45:57 | INFO | fairseq_cli.train | end of epoch 1484 (average epoch stats below)
2022-08-18 06:45:57 | INFO | train | epoch 1484 | loss 3.369 | nll_loss 0.337 | ppl 1.26 | wps 5118.1 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 120204 | lr 0.000182419 | gnorm 0.339 | train_wall 40 | gb_free 10.1 | wall 117253
2022-08-18 06:45:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:45:57 | INFO | fairseq.trainer | begin training epoch 1485
2022-08-18 06:45:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:46:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:46:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:46:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:46:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:46:51 | INFO | valid | epoch 1485 | valid on 'valid' subset | loss 5.272 | nll_loss 2.69 | ppl 6.45 | bleu 56.36 | wps 1841.4 | wpb 933.5 | bsz 59.6 | num_updates 120285 | best_bleu 57.52
2022-08-18 06:46:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1485 @ 120285 updates
2022-08-18 06:46:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1485.pt
2022-08-18 06:46:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1485.pt
2022-08-18 06:47:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1485.pt (epoch 1485 @ 120285 updates, score 56.36) (writing took 31.916990216821432 seconds)
2022-08-18 06:47:24 | INFO | fairseq_cli.train | end of epoch 1485 (average epoch stats below)
2022-08-18 06:47:24 | INFO | train | epoch 1485 | loss 3.369 | nll_loss 0.337 | ppl 1.26 | wps 5177.5 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 120285 | lr 0.000182358 | gnorm 0.359 | train_wall 40 | gb_free 10.2 | wall 117340
2022-08-18 06:47:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:47:24 | INFO | fairseq.trainer | begin training epoch 1486
2022-08-18 06:47:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:47:33 | INFO | train_inner | epoch 1486:     15 / 81 loss=3.369, nll_loss=0.337, ppl=1.26, wps=3883.3, ups=0.7, wpb=5516.5, bsz=358.2, num_updates=120300, lr=0.000182346, gnorm=0.37, train_wall=49, gb_free=10, wall=117349
2022-08-18 06:48:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:48:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:48:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:48:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:48:16 | INFO | valid | epoch 1486 | valid on 'valid' subset | loss 5.266 | nll_loss 2.685 | ppl 6.43 | bleu 56.53 | wps 1672.2 | wpb 933.5 | bsz 59.6 | num_updates 120366 | best_bleu 57.52
2022-08-18 06:48:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1486 @ 120366 updates
2022-08-18 06:48:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1486.pt
2022-08-18 06:48:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1486.pt
2022-08-18 06:48:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1486.pt (epoch 1486 @ 120366 updates, score 56.53) (writing took 42.600424099713564 seconds)
2022-08-18 06:48:59 | INFO | fairseq_cli.train | end of epoch 1486 (average epoch stats below)
2022-08-18 06:48:59 | INFO | train | epoch 1486 | loss 3.369 | nll_loss 0.339 | ppl 1.26 | wps 4705.5 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 120366 | lr 0.000182296 | gnorm 0.375 | train_wall 40 | gb_free 10.2 | wall 117435
2022-08-18 06:48:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:48:59 | INFO | fairseq.trainer | begin training epoch 1487
2022-08-18 06:48:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:49:17 | INFO | train_inner | epoch 1487:     34 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5275.1, ups=0.96, wpb=5516.2, bsz=359.9, num_updates=120400, lr=0.000182271, gnorm=0.379, train_wall=49, gb_free=10, wall=117453
2022-08-18 06:49:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:49:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:49:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:49:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:49:50 | INFO | valid | epoch 1487 | valid on 'valid' subset | loss 5.275 | nll_loss 2.693 | ppl 6.47 | bleu 56.3 | wps 1793 | wpb 933.5 | bsz 59.6 | num_updates 120447 | best_bleu 57.52
2022-08-18 06:49:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1487 @ 120447 updates
2022-08-18 06:49:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1487.pt
2022-08-18 06:49:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1487.pt
2022-08-18 06:50:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1487.pt (epoch 1487 @ 120447 updates, score 56.3) (writing took 19.392594549804926 seconds)
2022-08-18 06:50:10 | INFO | fairseq_cli.train | end of epoch 1487 (average epoch stats below)
2022-08-18 06:50:10 | INFO | train | epoch 1487 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6292.5 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 120447 | lr 0.000182235 | gnorm 0.376 | train_wall 39 | gb_free 10.1 | wall 117506
2022-08-18 06:50:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:50:10 | INFO | fairseq.trainer | begin training epoch 1488
2022-08-18 06:50:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:50:37 | INFO | train_inner | epoch 1488:     53 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=6958.6, ups=1.26, wpb=5521.5, bsz=353.6, num_updates=120500, lr=0.000182195, gnorm=0.397, train_wall=48, gb_free=10.1, wall=117533
2022-08-18 06:50:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:50:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:50:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:50:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:50:59 | INFO | valid | epoch 1488 | valid on 'valid' subset | loss 5.261 | nll_loss 2.675 | ppl 6.39 | bleu 56.34 | wps 1852.6 | wpb 933.5 | bsz 59.6 | num_updates 120528 | best_bleu 57.52
2022-08-18 06:50:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1488 @ 120528 updates
2022-08-18 06:50:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1488.pt
2022-08-18 06:51:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1488.pt
2022-08-18 06:51:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1488.pt (epoch 1488 @ 120528 updates, score 56.34) (writing took 35.91367940232158 seconds)
2022-08-18 06:51:36 | INFO | fairseq_cli.train | end of epoch 1488 (average epoch stats below)
2022-08-18 06:51:36 | INFO | train | epoch 1488 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5208.9 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 120528 | lr 0.000182174 | gnorm 0.441 | train_wall 39 | gb_free 10.4 | wall 117592
2022-08-18 06:51:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:51:36 | INFO | fairseq.trainer | begin training epoch 1489
2022-08-18 06:51:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:52:13 | INFO | train_inner | epoch 1489:     72 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5777.4, ups=1.04, wpb=5545, bsz=359.2, num_updates=120600, lr=0.000182119, gnorm=0.383, train_wall=49, gb_free=10, wall=117629
2022-08-18 06:52:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:52:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:52:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:52:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:52:26 | INFO | valid | epoch 1489 | valid on 'valid' subset | loss 5.276 | nll_loss 2.696 | ppl 6.48 | bleu 56.54 | wps 1728.8 | wpb 933.5 | bsz 59.6 | num_updates 120609 | best_bleu 57.52
2022-08-18 06:52:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1489 @ 120609 updates
2022-08-18 06:52:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1489.pt
2022-08-18 06:52:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1489.pt
2022-08-18 06:52:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1489.pt (epoch 1489 @ 120609 updates, score 56.54) (writing took 32.396744813770056 seconds)
2022-08-18 06:52:59 | INFO | fairseq_cli.train | end of epoch 1489 (average epoch stats below)
2022-08-18 06:52:59 | INFO | train | epoch 1489 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5363.8 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 120609 | lr 0.000182113 | gnorm 0.411 | train_wall 39 | gb_free 10.2 | wall 117675
2022-08-18 06:52:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:52:59 | INFO | fairseq.trainer | begin training epoch 1490
2022-08-18 06:52:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:53:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:53:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:53:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:53:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:53:54 | INFO | valid | epoch 1490 | valid on 'valid' subset | loss 5.269 | nll_loss 2.686 | ppl 6.43 | bleu 56.57 | wps 1815.9 | wpb 933.5 | bsz 59.6 | num_updates 120690 | best_bleu 57.52
2022-08-18 06:53:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1490 @ 120690 updates
2022-08-18 06:53:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1490.pt
2022-08-18 06:53:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1490.pt
2022-08-18 06:54:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1490.pt (epoch 1490 @ 120690 updates, score 56.57) (writing took 26.198175188153982 seconds)
2022-08-18 06:54:20 | INFO | fairseq_cli.train | end of epoch 1490 (average epoch stats below)
2022-08-18 06:54:20 | INFO | train | epoch 1490 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5514.6 | ups 1 | wpb 5523.2 | bsz 358 | num_updates 120690 | lr 0.000182052 | gnorm 0.307 | train_wall 40 | gb_free 10.1 | wall 117756
2022-08-18 06:54:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:54:20 | INFO | fairseq.trainer | begin training epoch 1491
2022-08-18 06:54:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:54:27 | INFO | train_inner | epoch 1491:     10 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=4093.4, ups=0.75, wpb=5487.3, bsz=358, num_updates=120700, lr=0.000182044, gnorm=0.359, train_wall=48, gb_free=10, wall=117763
2022-08-18 06:55:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:55:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:55:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:55:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:55:11 | INFO | valid | epoch 1491 | valid on 'valid' subset | loss 5.273 | nll_loss 2.691 | ppl 6.46 | bleu 56.91 | wps 1852.4 | wpb 933.5 | bsz 59.6 | num_updates 120771 | best_bleu 57.52
2022-08-18 06:55:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1491 @ 120771 updates
2022-08-18 06:55:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1491.pt
2022-08-18 06:55:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1491.pt
2022-08-18 06:55:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1491.pt (epoch 1491 @ 120771 updates, score 56.91) (writing took 18.676005858927965 seconds)
2022-08-18 06:55:30 | INFO | fairseq_cli.train | end of epoch 1491 (average epoch stats below)
2022-08-18 06:55:30 | INFO | train | epoch 1491 | loss 3.369 | nll_loss 0.337 | ppl 1.26 | wps 6395.9 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 120771 | lr 0.00018199 | gnorm 0.356 | train_wall 40 | gb_free 10 | wall 117826
2022-08-18 06:55:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:55:30 | INFO | fairseq.trainer | begin training epoch 1492
2022-08-18 06:55:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:55:48 | INFO | train_inner | epoch 1492:     29 / 81 loss=3.369, nll_loss=0.337, ppl=1.26, wps=6835.1, ups=1.23, wpb=5552.4, bsz=358.4, num_updates=120800, lr=0.000181969, gnorm=0.364, train_wall=49, gb_free=10.1, wall=117844
2022-08-18 06:56:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:56:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:56:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:56:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:56:27 | INFO | valid | epoch 1492 | valid on 'valid' subset | loss 5.259 | nll_loss 2.673 | ppl 6.38 | bleu 56.73 | wps 1844.5 | wpb 933.5 | bsz 59.6 | num_updates 120852 | best_bleu 57.52
2022-08-18 06:56:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1492 @ 120852 updates
2022-08-18 06:56:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1492.pt
2022-08-18 06:56:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1492.pt
2022-08-18 06:56:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1492.pt (epoch 1492 @ 120852 updates, score 56.73) (writing took 32.542187459766865 seconds)
2022-08-18 06:56:59 | INFO | fairseq_cli.train | end of epoch 1492 (average epoch stats below)
2022-08-18 06:56:59 | INFO | train | epoch 1492 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5010.9 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 120852 | lr 0.000181929 | gnorm 0.414 | train_wall 40 | gb_free 10.1 | wall 117916
2022-08-18 06:57:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:57:00 | INFO | fairseq.trainer | begin training epoch 1493
2022-08-18 06:57:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:57:25 | INFO | train_inner | epoch 1493:     48 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5690.5, ups=1.03, wpb=5523.1, bsz=357.9, num_updates=120900, lr=0.000181893, gnorm=0.417, train_wall=49, gb_free=10, wall=117941
2022-08-18 06:57:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:57:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:57:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:57:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:57:50 | INFO | valid | epoch 1493 | valid on 'valid' subset | loss 5.286 | nll_loss 2.705 | ppl 6.52 | bleu 56.69 | wps 1915.7 | wpb 933.5 | bsz 59.6 | num_updates 120933 | best_bleu 57.52
2022-08-18 06:57:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1493 @ 120933 updates
2022-08-18 06:57:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1493.pt
2022-08-18 06:57:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1493.pt
2022-08-18 06:58:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1493.pt (epoch 1493 @ 120933 updates, score 56.69) (writing took 28.493572425097227 seconds)
2022-08-18 06:58:19 | INFO | fairseq_cli.train | end of epoch 1493 (average epoch stats below)
2022-08-18 06:58:19 | INFO | train | epoch 1493 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5646.3 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 120933 | lr 0.000181869 | gnorm 0.386 | train_wall 39 | gb_free 10.1 | wall 117995
2022-08-18 06:58:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:58:19 | INFO | fairseq.trainer | begin training epoch 1494
2022-08-18 06:58:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 06:58:54 | INFO | train_inner | epoch 1494:     67 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6211.4, ups=1.13, wpb=5519.6, bsz=360.8, num_updates=121000, lr=0.000181818, gnorm=0.432, train_wall=49, gb_free=10.1, wall=118030
2022-08-18 06:59:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 06:59:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 06:59:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 06:59:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 06:59:11 | INFO | valid | epoch 1494 | valid on 'valid' subset | loss 5.282 | nll_loss 2.703 | ppl 6.51 | bleu 56.77 | wps 1664.4 | wpb 933.5 | bsz 59.6 | num_updates 121014 | best_bleu 57.52
2022-08-18 06:59:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1494 @ 121014 updates
2022-08-18 06:59:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1494.pt
2022-08-18 06:59:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1494.pt
2022-08-18 06:59:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1494.pt (epoch 1494 @ 121014 updates, score 56.77) (writing took 17.610027384012938 seconds)
2022-08-18 06:59:29 | INFO | fairseq_cli.train | end of epoch 1494 (average epoch stats below)
2022-08-18 06:59:29 | INFO | train | epoch 1494 | loss 3.369 | nll_loss 0.337 | ppl 1.26 | wps 6365.8 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 121014 | lr 0.000181808 | gnorm 0.44 | train_wall 40 | gb_free 10.2 | wall 118065
2022-08-18 06:59:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 06:59:29 | INFO | fairseq.trainer | begin training epoch 1495
2022-08-18 06:59:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:00:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:00:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:00:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:00:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:00:20 | INFO | valid | epoch 1495 | valid on 'valid' subset | loss 5.27 | nll_loss 2.686 | ppl 6.44 | bleu 56.51 | wps 1871.8 | wpb 933.5 | bsz 59.6 | num_updates 121095 | best_bleu 57.52
2022-08-18 07:00:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1495 @ 121095 updates
2022-08-18 07:00:20 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1495.pt
2022-08-18 07:00:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1495.pt
2022-08-18 07:00:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1495.pt (epoch 1495 @ 121095 updates, score 56.51) (writing took 34.579968463629484 seconds)
2022-08-18 07:00:55 | INFO | fairseq_cli.train | end of epoch 1495 (average epoch stats below)
2022-08-18 07:00:55 | INFO | train | epoch 1495 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5188 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 121095 | lr 0.000181747 | gnorm 0.498 | train_wall 40 | gb_free 10.1 | wall 118151
2022-08-18 07:00:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:00:55 | INFO | fairseq.trainer | begin training epoch 1496
2022-08-18 07:00:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:00:59 | INFO | train_inner | epoch 1496:      5 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=4415.4, ups=0.8, wpb=5515.4, bsz=353.2, num_updates=121100, lr=0.000181743, gnorm=0.464, train_wall=50, gb_free=10.1, wall=118155
2022-08-18 07:01:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:01:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:01:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:01:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:01:46 | INFO | valid | epoch 1496 | valid on 'valid' subset | loss 5.273 | nll_loss 2.688 | ppl 6.44 | bleu 56.47 | wps 1894.3 | wpb 933.5 | bsz 59.6 | num_updates 121176 | best_bleu 57.52
2022-08-18 07:01:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1496 @ 121176 updates
2022-08-18 07:01:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1496.pt
2022-08-18 07:01:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1496.pt
2022-08-18 07:02:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1496.pt (epoch 1496 @ 121176 updates, score 56.47) (writing took 20.814336128532887 seconds)
2022-08-18 07:02:07 | INFO | fairseq_cli.train | end of epoch 1496 (average epoch stats below)
2022-08-18 07:02:07 | INFO | train | epoch 1496 | loss 3.37 | nll_loss 0.34 | ppl 1.27 | wps 6265.3 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 121176 | lr 0.000181686 | gnorm 0.513 | train_wall 39 | gb_free 10.1 | wall 118223
2022-08-18 07:02:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:02:07 | INFO | fairseq.trainer | begin training epoch 1497
2022-08-18 07:02:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:02:21 | INFO | train_inner | epoch 1497:     24 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=6766.9, ups=1.22, wpb=5538.8, bsz=361.6, num_updates=121200, lr=0.000181668, gnorm=0.458, train_wall=48, gb_free=10.1, wall=118237
2022-08-18 07:02:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:02:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:02:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:02:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:02:58 | INFO | valid | epoch 1497 | valid on 'valid' subset | loss 5.277 | nll_loss 2.696 | ppl 6.48 | bleu 56.27 | wps 1860.4 | wpb 933.5 | bsz 59.6 | num_updates 121257 | best_bleu 57.52
2022-08-18 07:02:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1497 @ 121257 updates
2022-08-18 07:02:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1497.pt
2022-08-18 07:02:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1497.pt
2022-08-18 07:03:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1497.pt (epoch 1497 @ 121257 updates, score 56.27) (writing took 15.250638466328382 seconds)
2022-08-18 07:03:13 | INFO | fairseq_cli.train | end of epoch 1497 (average epoch stats below)
2022-08-18 07:03:13 | INFO | train | epoch 1497 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6713.3 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 121257 | lr 0.000181625 | gnorm 0.331 | train_wall 39 | gb_free 10.2 | wall 118289
2022-08-18 07:03:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:03:13 | INFO | fairseq.trainer | begin training epoch 1498
2022-08-18 07:03:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:03:36 | INFO | train_inner | epoch 1498:     43 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=7275.6, ups=1.32, wpb=5526.7, bsz=354.6, num_updates=121300, lr=0.000181593, gnorm=0.369, train_wall=49, gb_free=10.1, wall=118313
2022-08-18 07:03:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:03:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:03:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:03:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:04:05 | INFO | valid | epoch 1498 | valid on 'valid' subset | loss 5.252 | nll_loss 2.664 | ppl 6.34 | bleu 57.13 | wps 1835.5 | wpb 933.5 | bsz 59.6 | num_updates 121338 | best_bleu 57.52
2022-08-18 07:04:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1498 @ 121338 updates
2022-08-18 07:04:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1498.pt
2022-08-18 07:04:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1498.pt
2022-08-18 07:04:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1498.pt (epoch 1498 @ 121338 updates, score 57.13) (writing took 39.70175224170089 seconds)
2022-08-18 07:04:45 | INFO | fairseq_cli.train | end of epoch 1498 (average epoch stats below)
2022-08-18 07:04:45 | INFO | train | epoch 1498 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 4883.9 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 121338 | lr 0.000181565 | gnorm 0.369 | train_wall 40 | gb_free 10.3 | wall 118381
2022-08-18 07:04:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:04:45 | INFO | fairseq.trainer | begin training epoch 1499
2022-08-18 07:04:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:05:17 | INFO | train_inner | epoch 1499:     62 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=5493.9, ups=0.99, wpb=5539.5, bsz=360.8, num_updates=121400, lr=0.000181518, gnorm=0.364, train_wall=50, gb_free=10, wall=118414
2022-08-18 07:05:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:05:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:05:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:05:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:05:36 | INFO | valid | epoch 1499 | valid on 'valid' subset | loss 5.27 | nll_loss 2.681 | ppl 6.42 | bleu 56.9 | wps 1881.3 | wpb 933.5 | bsz 59.6 | num_updates 121419 | best_bleu 57.52
2022-08-18 07:05:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1499 @ 121419 updates
2022-08-18 07:05:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1499.pt
2022-08-18 07:05:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1499.pt
2022-08-18 07:05:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1499.pt (epoch 1499 @ 121419 updates, score 56.9) (writing took 2.454556044191122 seconds)
2022-08-18 07:05:38 | INFO | fairseq_cli.train | end of epoch 1499 (average epoch stats below)
2022-08-18 07:05:38 | INFO | train | epoch 1499 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 8365.1 | ups 1.51 | wpb 5523.2 | bsz 358 | num_updates 121419 | lr 0.000181504 | gnorm 0.372 | train_wall 40 | gb_free 10.1 | wall 118434
2022-08-18 07:05:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:05:38 | INFO | fairseq.trainer | begin training epoch 1500
2022-08-18 07:05:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:06:20 | INFO | train_inner | epoch 1500:     81 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=8777.5, ups=1.6, wpb=5481.2, bsz=356.6, num_updates=121500, lr=0.000181444, gnorm=0.402, train_wall=49, gb_free=10.1, wall=118476
2022-08-18 07:06:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:06:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:06:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:06:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:06:30 | INFO | valid | epoch 1500 | valid on 'valid' subset | loss 5.276 | nll_loss 2.693 | ppl 6.47 | bleu 56.57 | wps 1725.6 | wpb 933.5 | bsz 59.6 | num_updates 121500 | best_bleu 57.52
2022-08-18 07:06:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1500 @ 121500 updates
2022-08-18 07:06:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1500.pt
2022-08-18 07:06:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1500.pt
2022-08-18 07:06:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1500.pt (epoch 1500 @ 121500 updates, score 56.57) (writing took 25.361106604337692 seconds)
2022-08-18 07:06:55 | INFO | fairseq_cli.train | end of epoch 1500 (average epoch stats below)
2022-08-18 07:06:55 | INFO | train | epoch 1500 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5812.5 | ups 1.05 | wpb 5523.2 | bsz 358 | num_updates 121500 | lr 0.000181444 | gnorm 0.406 | train_wall 40 | gb_free 10.1 | wall 118511
2022-08-18 07:06:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:06:55 | INFO | fairseq.trainer | begin training epoch 1501
2022-08-18 07:06:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:07:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:07:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:07:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:07:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:07:47 | INFO | valid | epoch 1501 | valid on 'valid' subset | loss 5.267 | nll_loss 2.683 | ppl 6.42 | bleu 56.76 | wps 1754.9 | wpb 933.5 | bsz 59.6 | num_updates 121581 | best_bleu 57.52
2022-08-18 07:07:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1501 @ 121581 updates
2022-08-18 07:07:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1501.pt
2022-08-18 07:07:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1501.pt
2022-08-18 07:07:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1501.pt (epoch 1501 @ 121581 updates, score 56.76) (writing took 2.5013433769345284 seconds)
2022-08-18 07:07:50 | INFO | fairseq_cli.train | end of epoch 1501 (average epoch stats below)
2022-08-18 07:07:50 | INFO | train | epoch 1501 | loss 3.369 | nll_loss 0.337 | ppl 1.26 | wps 8197.3 | ups 1.48 | wpb 5523.2 | bsz 358 | num_updates 121581 | lr 0.000181383 | gnorm 0.378 | train_wall 41 | gb_free 10.2 | wall 118566
2022-08-18 07:07:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:07:50 | INFO | fairseq.trainer | begin training epoch 1502
2022-08-18 07:07:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:08:00 | INFO | train_inner | epoch 1502:     19 / 81 loss=3.369, nll_loss=0.337, ppl=1.26, wps=5499.3, ups=1, wpb=5525, bsz=359.8, num_updates=121600, lr=0.000181369, gnorm=0.367, train_wall=50, gb_free=10.1, wall=118576
2022-08-18 07:08:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:08:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:08:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:08:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:08:41 | INFO | valid | epoch 1502 | valid on 'valid' subset | loss 5.277 | nll_loss 2.694 | ppl 6.47 | bleu 56.65 | wps 1856.1 | wpb 933.5 | bsz 59.6 | num_updates 121662 | best_bleu 57.52
2022-08-18 07:08:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1502 @ 121662 updates
2022-08-18 07:08:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1502.pt
2022-08-18 07:08:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1502.pt
2022-08-18 07:09:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1502.pt (epoch 1502 @ 121662 updates, score 56.65) (writing took 34.86732782423496 seconds)
2022-08-18 07:09:16 | INFO | fairseq_cli.train | end of epoch 1502 (average epoch stats below)
2022-08-18 07:09:16 | INFO | train | epoch 1502 | loss 3.369 | nll_loss 0.337 | ppl 1.26 | wps 5221 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 121662 | lr 0.000181323 | gnorm 0.373 | train_wall 40 | gb_free 10.1 | wall 118652
2022-08-18 07:09:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:09:16 | INFO | fairseq.trainer | begin training epoch 1503
2022-08-18 07:09:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:09:35 | INFO | train_inner | epoch 1503:     38 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5793.7, ups=1.05, wpb=5518.6, bsz=355, num_updates=121700, lr=0.000181295, gnorm=0.378, train_wall=49, gb_free=10.1, wall=118672
2022-08-18 07:09:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:09:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:09:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:09:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:10:06 | INFO | valid | epoch 1503 | valid on 'valid' subset | loss 5.271 | nll_loss 2.685 | ppl 6.43 | bleu 56.79 | wps 1772.4 | wpb 933.5 | bsz 59.6 | num_updates 121743 | best_bleu 57.52
2022-08-18 07:10:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1503 @ 121743 updates
2022-08-18 07:10:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1503.pt
2022-08-18 07:10:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1503.pt
2022-08-18 07:10:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1503.pt (epoch 1503 @ 121743 updates, score 56.79) (writing took 37.61624189838767 seconds)
2022-08-18 07:10:44 | INFO | fairseq_cli.train | end of epoch 1503 (average epoch stats below)
2022-08-18 07:10:44 | INFO | train | epoch 1503 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5045 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 121743 | lr 0.000181263 | gnorm 0.342 | train_wall 39 | gb_free 10.2 | wall 118740
2022-08-18 07:10:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:10:44 | INFO | fairseq.trainer | begin training epoch 1504
2022-08-18 07:10:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:11:14 | INFO | train_inner | epoch 1504:     57 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5609.6, ups=1.01, wpb=5552.9, bsz=364.2, num_updates=121800, lr=0.00018122, gnorm=0.386, train_wall=49, gb_free=10.1, wall=118771
2022-08-18 07:11:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:11:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:11:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:11:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:11:36 | INFO | valid | epoch 1504 | valid on 'valid' subset | loss 5.265 | nll_loss 2.68 | ppl 6.41 | bleu 56.76 | wps 1796.3 | wpb 933.5 | bsz 59.6 | num_updates 121824 | best_bleu 57.52
2022-08-18 07:11:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1504 @ 121824 updates
2022-08-18 07:11:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1504.pt
2022-08-18 07:11:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1504.pt
2022-08-18 07:11:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1504.pt (epoch 1504 @ 121824 updates, score 56.76) (writing took 16.702472895383835 seconds)
2022-08-18 07:11:53 | INFO | fairseq_cli.train | end of epoch 1504 (average epoch stats below)
2022-08-18 07:11:53 | INFO | train | epoch 1504 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6549.8 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 121824 | lr 0.000181202 | gnorm 0.442 | train_wall 40 | gb_free 10 | wall 118809
2022-08-18 07:11:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:11:53 | INFO | fairseq.trainer | begin training epoch 1505
2022-08-18 07:11:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:12:34 | INFO | train_inner | epoch 1505:     76 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=6973.3, ups=1.26, wpb=5516.3, bsz=353.7, num_updates=121900, lr=0.000181146, gnorm=0.463, train_wall=49, gb_free=10, wall=118850
2022-08-18 07:12:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:12:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:12:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:12:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:12:46 | INFO | valid | epoch 1505 | valid on 'valid' subset | loss 5.254 | nll_loss 2.666 | ppl 6.35 | bleu 56.52 | wps 1784.8 | wpb 933.5 | bsz 59.6 | num_updates 121905 | best_bleu 57.52
2022-08-18 07:12:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1505 @ 121905 updates
2022-08-18 07:12:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1505.pt
2022-08-18 07:12:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1505.pt
2022-08-18 07:13:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1505.pt (epoch 1505 @ 121905 updates, score 56.52) (writing took 25.398253690451384 seconds)
2022-08-18 07:13:11 | INFO | fairseq_cli.train | end of epoch 1505 (average epoch stats below)
2022-08-18 07:13:11 | INFO | train | epoch 1505 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 5674.6 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 121905 | lr 0.000181142 | gnorm 0.452 | train_wall 40 | gb_free 10.2 | wall 118888
2022-08-18 07:13:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:13:12 | INFO | fairseq.trainer | begin training epoch 1506
2022-08-18 07:13:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:13:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:13:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:13:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:13:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:14:01 | INFO | valid | epoch 1506 | valid on 'valid' subset | loss 5.271 | nll_loss 2.689 | ppl 6.45 | bleu 56.67 | wps 1722.6 | wpb 933.5 | bsz 59.6 | num_updates 121986 | best_bleu 57.52
2022-08-18 07:14:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1506 @ 121986 updates
2022-08-18 07:14:01 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1506.pt
2022-08-18 07:14:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1506.pt
2022-08-18 07:14:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1506.pt (epoch 1506 @ 121986 updates, score 56.67) (writing took 31.937260203063488 seconds)
2022-08-18 07:14:34 | INFO | fairseq_cli.train | end of epoch 1506 (average epoch stats below)
2022-08-18 07:14:34 | INFO | train | epoch 1506 | loss 3.37 | nll_loss 0.339 | ppl 1.27 | wps 5428.8 | ups 0.98 | wpb 5523.2 | bsz 358 | num_updates 121986 | lr 0.000181082 | gnorm 0.627 | train_wall 38 | gb_free 10.1 | wall 118970
2022-08-18 07:14:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:14:34 | INFO | fairseq.trainer | begin training epoch 1507
2022-08-18 07:14:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:14:42 | INFO | train_inner | epoch 1507:     14 / 81 loss=3.37, nll_loss=0.339, ppl=1.27, wps=4285, ups=0.78, wpb=5511.8, bsz=359.4, num_updates=122000, lr=0.000181071, gnorm=0.576, train_wall=48, gb_free=10.1, wall=118978
2022-08-18 07:15:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:15:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:15:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:15:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:15:25 | INFO | valid | epoch 1507 | valid on 'valid' subset | loss 5.272 | nll_loss 2.689 | ppl 6.45 | bleu 55.92 | wps 1785.3 | wpb 933.5 | bsz 59.6 | num_updates 122067 | best_bleu 57.52
2022-08-18 07:15:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1507 @ 122067 updates
2022-08-18 07:15:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1507.pt
2022-08-18 07:15:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1507.pt
2022-08-18 07:15:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1507.pt (epoch 1507 @ 122067 updates, score 55.92) (writing took 16.783260218799114 seconds)
2022-08-18 07:15:42 | INFO | fairseq_cli.train | end of epoch 1507 (average epoch stats below)
2022-08-18 07:15:42 | INFO | train | epoch 1507 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6566.2 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 122067 | lr 0.000181022 | gnorm 0.396 | train_wall 39 | gb_free 10.1 | wall 119038
2022-08-18 07:15:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:15:42 | INFO | fairseq.trainer | begin training epoch 1508
2022-08-18 07:15:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:16:00 | INFO | train_inner | epoch 1508:     33 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=7109.6, ups=1.29, wpb=5529.3, bsz=356.5, num_updates=122100, lr=0.000180997, gnorm=0.478, train_wall=49, gb_free=10, wall=119056
2022-08-18 07:16:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:16:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:16:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:16:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:16:35 | INFO | valid | epoch 1508 | valid on 'valid' subset | loss 5.258 | nll_loss 2.67 | ppl 6.37 | bleu 56.49 | wps 1875 | wpb 933.5 | bsz 59.6 | num_updates 122148 | best_bleu 57.52
2022-08-18 07:16:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1508 @ 122148 updates
2022-08-18 07:16:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1508.pt
2022-08-18 07:16:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1508.pt
2022-08-18 07:16:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1508.pt (epoch 1508 @ 122148 updates, score 56.49) (writing took 12.850246131420135 seconds)
2022-08-18 07:16:48 | INFO | fairseq_cli.train | end of epoch 1508 (average epoch stats below)
2022-08-18 07:16:48 | INFO | train | epoch 1508 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6814.4 | ups 1.23 | wpb 5523.2 | bsz 358 | num_updates 122148 | lr 0.000180962 | gnorm 0.465 | train_wall 40 | gb_free 10.1 | wall 119104
2022-08-18 07:16:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:16:48 | INFO | fairseq.trainer | begin training epoch 1509
2022-08-18 07:16:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:17:14 | INFO | train_inner | epoch 1509:     52 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=7419, ups=1.34, wpb=5523.7, bsz=357.4, num_updates=122200, lr=0.000180923, gnorm=0.361, train_wall=49, gb_free=10.1, wall=119131
2022-08-18 07:17:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:17:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:17:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:17:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:17:39 | INFO | valid | epoch 1509 | valid on 'valid' subset | loss 5.256 | nll_loss 2.668 | ppl 6.36 | bleu 56.62 | wps 1756.1 | wpb 933.5 | bsz 59.6 | num_updates 122229 | best_bleu 57.52
2022-08-18 07:17:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1509 @ 122229 updates
2022-08-18 07:17:39 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1509.pt
2022-08-18 07:17:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1509.pt
2022-08-18 07:18:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1509.pt (epoch 1509 @ 122229 updates, score 56.62) (writing took 38.88813128694892 seconds)
2022-08-18 07:18:18 | INFO | fairseq_cli.train | end of epoch 1509 (average epoch stats below)
2022-08-18 07:18:18 | INFO | train | epoch 1509 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 4966.2 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 122229 | lr 0.000180902 | gnorm 0.356 | train_wall 39 | gb_free 10 | wall 119194
2022-08-18 07:18:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:18:18 | INFO | fairseq.trainer | begin training epoch 1510
2022-08-18 07:18:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:18:55 | INFO | train_inner | epoch 1510:     71 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5521.8, ups=1, wpb=5525.4, bsz=359, num_updates=122300, lr=0.000180849, gnorm=0.351, train_wall=49, gb_free=10.1, wall=119231
2022-08-18 07:18:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:19:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:19:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:19:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:19:09 | INFO | valid | epoch 1510 | valid on 'valid' subset | loss 5.274 | nll_loss 2.69 | ppl 6.45 | bleu 56.19 | wps 1830.9 | wpb 933.5 | bsz 59.6 | num_updates 122310 | best_bleu 57.52
2022-08-18 07:19:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1510 @ 122310 updates
2022-08-18 07:19:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1510.pt
2022-08-18 07:19:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1510.pt
2022-08-18 07:19:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1510.pt (epoch 1510 @ 122310 updates, score 56.19) (writing took 17.630741719156504 seconds)
2022-08-18 07:19:27 | INFO | fairseq_cli.train | end of epoch 1510 (average epoch stats below)
2022-08-18 07:19:27 | INFO | train | epoch 1510 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6493.7 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 122310 | lr 0.000180842 | gnorm 0.375 | train_wall 40 | gb_free 10.1 | wall 119263
2022-08-18 07:19:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:19:27 | INFO | fairseq.trainer | begin training epoch 1511
2022-08-18 07:19:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:20:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:20:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:20:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:20:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:20:18 | INFO | valid | epoch 1511 | valid on 'valid' subset | loss 5.29 | nll_loss 2.712 | ppl 6.55 | bleu 56.29 | wps 1815 | wpb 933.5 | bsz 59.6 | num_updates 122391 | best_bleu 57.52
2022-08-18 07:20:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1511 @ 122391 updates
2022-08-18 07:20:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1511.pt
2022-08-18 07:20:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1511.pt
2022-08-18 07:20:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1511.pt (epoch 1511 @ 122391 updates, score 56.29) (writing took 28.476836513727903 seconds)
2022-08-18 07:20:47 | INFO | fairseq_cli.train | end of epoch 1511 (average epoch stats below)
2022-08-18 07:20:47 | INFO | train | epoch 1511 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5582.1 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 122391 | lr 0.000180782 | gnorm 0.449 | train_wall 40 | gb_free 10.1 | wall 119343
2022-08-18 07:20:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:20:47 | INFO | fairseq.trainer | begin training epoch 1512
2022-08-18 07:20:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:20:53 | INFO | train_inner | epoch 1512:      9 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=4662.6, ups=0.85, wpb=5515.5, bsz=358, num_updates=122400, lr=0.000180775, gnorm=0.46, train_wall=49, gb_free=10, wall=119349
2022-08-18 07:21:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:21:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:21:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:21:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:21:37 | INFO | valid | epoch 1512 | valid on 'valid' subset | loss 5.282 | nll_loss 2.7 | ppl 6.5 | bleu 56.51 | wps 1820.6 | wpb 933.5 | bsz 59.6 | num_updates 122472 | best_bleu 57.52
2022-08-18 07:21:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1512 @ 122472 updates
2022-08-18 07:21:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1512.pt
2022-08-18 07:21:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1512.pt
2022-08-18 07:22:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1512.pt (epoch 1512 @ 122472 updates, score 56.51) (writing took 39.52919481322169 seconds)
2022-08-18 07:22:17 | INFO | fairseq_cli.train | end of epoch 1512 (average epoch stats below)
2022-08-18 07:22:17 | INFO | train | epoch 1512 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 4952.3 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 122472 | lr 0.000180722 | gnorm 0.393 | train_wall 39 | gb_free 10.1 | wall 119433
2022-08-18 07:22:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:22:17 | INFO | fairseq.trainer | begin training epoch 1513
2022-08-18 07:22:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:22:32 | INFO | train_inner | epoch 1513:     28 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5541.5, ups=1.01, wpb=5503.4, bsz=353.8, num_updates=122500, lr=0.000180702, gnorm=0.387, train_wall=48, gb_free=10.1, wall=119448
2022-08-18 07:22:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:23:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:23:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:23:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:23:08 | INFO | valid | epoch 1513 | valid on 'valid' subset | loss 5.264 | nll_loss 2.678 | ppl 6.4 | bleu 56.73 | wps 1847.9 | wpb 933.5 | bsz 59.6 | num_updates 122553 | best_bleu 57.52
2022-08-18 07:23:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1513 @ 122553 updates
2022-08-18 07:23:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1513.pt
2022-08-18 07:23:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1513.pt
2022-08-18 07:23:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1513.pt (epoch 1513 @ 122553 updates, score 56.73) (writing took 38.69806208088994 seconds)
2022-08-18 07:23:46 | INFO | fairseq_cli.train | end of epoch 1513 (average epoch stats below)
2022-08-18 07:23:46 | INFO | train | epoch 1513 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5004.1 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 122553 | lr 0.000180663 | gnorm 0.416 | train_wall 39 | gb_free 10.1 | wall 119523
2022-08-18 07:23:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:23:47 | INFO | fairseq.trainer | begin training epoch 1514
2022-08-18 07:23:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:24:11 | INFO | train_inner | epoch 1514:     47 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=5628.6, ups=1.02, wpb=5544.7, bsz=358.5, num_updates=122600, lr=0.000180628, gnorm=0.403, train_wall=49, gb_free=10.1, wall=119547
2022-08-18 07:24:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:24:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:24:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:24:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:24:39 | INFO | valid | epoch 1514 | valid on 'valid' subset | loss 5.293 | nll_loss 2.718 | ppl 6.58 | bleu 56.35 | wps 1586.5 | wpb 933.5 | bsz 59.6 | num_updates 122634 | best_bleu 57.52
2022-08-18 07:24:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1514 @ 122634 updates
2022-08-18 07:24:39 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1514.pt
2022-08-18 07:24:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1514.pt
2022-08-18 07:25:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1514.pt (epoch 1514 @ 122634 updates, score 56.35) (writing took 29.26202917098999 seconds)
2022-08-18 07:25:08 | INFO | fairseq_cli.train | end of epoch 1514 (average epoch stats below)
2022-08-18 07:25:08 | INFO | train | epoch 1514 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5465.6 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 122634 | lr 0.000180603 | gnorm 0.371 | train_wall 40 | gb_free 10.1 | wall 119605
2022-08-18 07:25:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:25:09 | INFO | fairseq.trainer | begin training epoch 1515
2022-08-18 07:25:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:25:42 | INFO | train_inner | epoch 1515:     66 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6059.1, ups=1.1, wpb=5522, bsz=362.9, num_updates=122700, lr=0.000180554, gnorm=0.355, train_wall=49, gb_free=10.1, wall=119638
2022-08-18 07:25:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:25:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:25:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:25:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:25:59 | INFO | valid | epoch 1515 | valid on 'valid' subset | loss 5.263 | nll_loss 2.68 | ppl 6.41 | bleu 56.49 | wps 1844.5 | wpb 933.5 | bsz 59.6 | num_updates 122715 | best_bleu 57.52
2022-08-18 07:25:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1515 @ 122715 updates
2022-08-18 07:25:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1515.pt
2022-08-18 07:26:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1515.pt
2022-08-18 07:26:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1515.pt (epoch 1515 @ 122715 updates, score 56.49) (writing took 38.162819396704435 seconds)
2022-08-18 07:26:37 | INFO | fairseq_cli.train | end of epoch 1515 (average epoch stats below)
2022-08-18 07:26:37 | INFO | train | epoch 1515 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5033.3 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 122715 | lr 0.000180543 | gnorm 0.374 | train_wall 39 | gb_free 10.1 | wall 119693
2022-08-18 07:26:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:26:37 | INFO | fairseq.trainer | begin training epoch 1516
2022-08-18 07:26:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:27:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:27:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:27:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:27:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:27:27 | INFO | valid | epoch 1516 | valid on 'valid' subset | loss 5.268 | nll_loss 2.684 | ppl 6.42 | bleu 56.72 | wps 1930.2 | wpb 933.5 | bsz 59.6 | num_updates 122796 | best_bleu 57.52
2022-08-18 07:27:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1516 @ 122796 updates
2022-08-18 07:27:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1516.pt
2022-08-18 07:27:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1516.pt
2022-08-18 07:27:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1516.pt (epoch 1516 @ 122796 updates, score 56.72) (writing took 21.659194596111774 seconds)
2022-08-18 07:27:49 | INFO | fairseq_cli.train | end of epoch 1516 (average epoch stats below)
2022-08-18 07:27:49 | INFO | train | epoch 1516 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6224.1 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 122796 | lr 0.000180484 | gnorm 0.443 | train_wall 40 | gb_free 10.1 | wall 119765
2022-08-18 07:27:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:27:49 | INFO | fairseq.trainer | begin training epoch 1517
2022-08-18 07:27:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:27:52 | INFO | train_inner | epoch 1517:      4 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=4225.7, ups=0.77, wpb=5519, bsz=356.5, num_updates=122800, lr=0.000180481, gnorm=0.444, train_wall=49, gb_free=10.1, wall=119769
2022-08-18 07:28:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:28:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:28:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:28:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:28:40 | INFO | valid | epoch 1517 | valid on 'valid' subset | loss 5.25 | nll_loss 2.662 | ppl 6.33 | bleu 56.95 | wps 1829.2 | wpb 933.5 | bsz 59.6 | num_updates 122877 | best_bleu 57.52
2022-08-18 07:28:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1517 @ 122877 updates
2022-08-18 07:28:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1517.pt
2022-08-18 07:28:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1517.pt
2022-08-18 07:29:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1517.pt (epoch 1517 @ 122877 updates, score 56.95) (writing took 20.509019289165735 seconds)
2022-08-18 07:29:01 | INFO | fairseq_cli.train | end of epoch 1517 (average epoch stats below)
2022-08-18 07:29:01 | INFO | train | epoch 1517 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 6235 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 122877 | lr 0.000180424 | gnorm 0.424 | train_wall 39 | gb_free 10.2 | wall 119837
2022-08-18 07:29:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:29:01 | INFO | fairseq.trainer | begin training epoch 1518
2022-08-18 07:29:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:29:14 | INFO | train_inner | epoch 1518:     23 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=6784.6, ups=1.23, wpb=5520.1, bsz=358.6, num_updates=122900, lr=0.000180407, gnorm=0.403, train_wall=49, gb_free=10, wall=119850
2022-08-18 07:29:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:29:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:29:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:29:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:29:52 | INFO | valid | epoch 1518 | valid on 'valid' subset | loss 5.257 | nll_loss 2.669 | ppl 6.36 | bleu 56.78 | wps 1883.7 | wpb 933.5 | bsz 59.6 | num_updates 122958 | best_bleu 57.52
2022-08-18 07:29:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1518 @ 122958 updates
2022-08-18 07:29:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1518.pt
2022-08-18 07:29:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1518.pt
2022-08-18 07:30:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1518.pt (epoch 1518 @ 122958 updates, score 56.78) (writing took 14.038564998656511 seconds)
2022-08-18 07:30:06 | INFO | fairseq_cli.train | end of epoch 1518 (average epoch stats below)
2022-08-18 07:30:06 | INFO | train | epoch 1518 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6842.1 | ups 1.24 | wpb 5523.2 | bsz 358 | num_updates 122958 | lr 0.000180365 | gnorm 0.339 | train_wall 40 | gb_free 10.1 | wall 119902
2022-08-18 07:30:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:30:06 | INFO | fairseq.trainer | begin training epoch 1519
2022-08-18 07:30:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:30:28 | INFO | train_inner | epoch 1519:     42 / 81 loss=3.369, nll_loss=0.337, ppl=1.26, wps=7403.7, ups=1.34, wpb=5529.6, bsz=356.6, num_updates=123000, lr=0.000180334, gnorm=0.357, train_wall=49, gb_free=10.1, wall=119925
2022-08-18 07:30:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:30:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:30:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:30:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:30:57 | INFO | valid | epoch 1519 | valid on 'valid' subset | loss 5.266 | nll_loss 2.683 | ppl 6.42 | bleu 56.18 | wps 1850.6 | wpb 933.5 | bsz 59.6 | num_updates 123039 | best_bleu 57.52
2022-08-18 07:30:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1519 @ 123039 updates
2022-08-18 07:30:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1519.pt
2022-08-18 07:30:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1519.pt
2022-08-18 07:31:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1519.pt (epoch 1519 @ 123039 updates, score 56.18) (writing took 30.645697623491287 seconds)
2022-08-18 07:31:28 | INFO | fairseq_cli.train | end of epoch 1519 (average epoch stats below)
2022-08-18 07:31:28 | INFO | train | epoch 1519 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5480.7 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 123039 | lr 0.000180305 | gnorm 0.384 | train_wall 40 | gb_free 10.3 | wall 119984
2022-08-18 07:31:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:31:28 | INFO | fairseq.trainer | begin training epoch 1520
2022-08-18 07:31:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:32:04 | INFO | train_inner | epoch 1520:     61 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5798.5, ups=1.04, wpb=5563.8, bsz=366.4, num_updates=123100, lr=0.000180261, gnorm=0.397, train_wall=49, gb_free=10, wall=120021
2022-08-18 07:32:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:32:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:32:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:32:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:32:23 | INFO | valid | epoch 1520 | valid on 'valid' subset | loss 5.268 | nll_loss 2.685 | ppl 6.43 | bleu 56.57 | wps 1919.3 | wpb 933.5 | bsz 59.6 | num_updates 123120 | best_bleu 57.52
2022-08-18 07:32:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1520 @ 123120 updates
2022-08-18 07:32:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1520.pt
2022-08-18 07:32:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1520.pt
2022-08-18 07:32:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1520.pt (epoch 1520 @ 123120 updates, score 56.57) (writing took 26.892888385802507 seconds)
2022-08-18 07:32:50 | INFO | fairseq_cli.train | end of epoch 1520 (average epoch stats below)
2022-08-18 07:32:50 | INFO | train | epoch 1520 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5417.6 | ups 0.98 | wpb 5523.2 | bsz 358 | num_updates 123120 | lr 0.000180246 | gnorm 0.399 | train_wall 40 | gb_free 10.2 | wall 120067
2022-08-18 07:32:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:32:51 | INFO | fairseq.trainer | begin training epoch 1521
2022-08-18 07:32:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:33:32 | INFO | train_inner | epoch 1521:     80 / 81 loss=3.37, nll_loss=0.339, ppl=1.27, wps=6299.1, ups=1.14, wpb=5502.2, bsz=352.9, num_updates=123200, lr=0.000180187, gnorm=0.407, train_wall=49, gb_free=10, wall=120108
2022-08-18 07:33:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:33:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:33:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:33:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:33:42 | INFO | valid | epoch 1521 | valid on 'valid' subset | loss 5.264 | nll_loss 2.678 | ppl 6.4 | bleu 55.85 | wps 1820.3 | wpb 933.5 | bsz 59.6 | num_updates 123201 | best_bleu 57.52
2022-08-18 07:33:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1521 @ 123201 updates
2022-08-18 07:33:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1521.pt
2022-08-18 07:33:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1521.pt
2022-08-18 07:33:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1521.pt (epoch 1521 @ 123201 updates, score 55.85) (writing took 2.8379779122769833 seconds)
2022-08-18 07:33:45 | INFO | fairseq_cli.train | end of epoch 1521 (average epoch stats below)
2022-08-18 07:33:45 | INFO | train | epoch 1521 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 8253.7 | ups 1.49 | wpb 5523.2 | bsz 358 | num_updates 123201 | lr 0.000180187 | gnorm 0.422 | train_wall 40 | gb_free 10.1 | wall 120121
2022-08-18 07:33:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:33:45 | INFO | fairseq.trainer | begin training epoch 1522
2022-08-18 07:33:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:34:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:34:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:34:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:34:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:34:36 | INFO | valid | epoch 1522 | valid on 'valid' subset | loss 5.256 | nll_loss 2.665 | ppl 6.34 | bleu 56.69 | wps 1879.7 | wpb 933.5 | bsz 59.6 | num_updates 123282 | best_bleu 57.52
2022-08-18 07:34:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1522 @ 123282 updates
2022-08-18 07:34:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1522.pt
2022-08-18 07:34:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1522.pt
2022-08-18 07:35:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1522.pt (epoch 1522 @ 123282 updates, score 56.69) (writing took 26.788991879671812 seconds)
2022-08-18 07:35:03 | INFO | fairseq_cli.train | end of epoch 1522 (average epoch stats below)
2022-08-18 07:35:03 | INFO | train | epoch 1522 | loss 3.37 | nll_loss 0.338 | ppl 1.26 | wps 5696.7 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 123282 | lr 0.000180128 | gnorm 0.375 | train_wall 40 | gb_free 10.1 | wall 120199
2022-08-18 07:35:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:35:03 | INFO | fairseq.trainer | begin training epoch 1523
2022-08-18 07:35:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:35:16 | INFO | train_inner | epoch 1523:     18 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5301.7, ups=0.96, wpb=5512.4, bsz=360, num_updates=123300, lr=0.000180114, gnorm=0.364, train_wall=49, gb_free=10.1, wall=120212
2022-08-18 07:35:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:35:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:35:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:35:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:35:59 | INFO | valid | epoch 1523 | valid on 'valid' subset | loss 5.259 | nll_loss 2.672 | ppl 6.37 | bleu 56.29 | wps 1892.2 | wpb 933.5 | bsz 59.6 | num_updates 123363 | best_bleu 57.52
2022-08-18 07:35:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1523 @ 123363 updates
2022-08-18 07:35:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1523.pt
2022-08-18 07:36:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1523.pt
2022-08-18 07:36:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1523.pt (epoch 1523 @ 123363 updates, score 56.29) (writing took 36.33071483299136 seconds)
2022-08-18 07:36:35 | INFO | fairseq_cli.train | end of epoch 1523 (average epoch stats below)
2022-08-18 07:36:35 | INFO | train | epoch 1523 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 4857.5 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 123363 | lr 0.000180068 | gnorm 0.39 | train_wall 39 | gb_free 10.3 | wall 120292
2022-08-18 07:36:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:36:36 | INFO | fairseq.trainer | begin training epoch 1524
2022-08-18 07:36:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:36:56 | INFO | train_inner | epoch 1524:     37 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5492.6, ups=1, wpb=5513.3, bsz=352.2, num_updates=123400, lr=0.000180041, gnorm=0.404, train_wall=48, gb_free=10, wall=120312
2022-08-18 07:37:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:37:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:37:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:37:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:37:27 | INFO | valid | epoch 1524 | valid on 'valid' subset | loss 5.276 | nll_loss 2.692 | ppl 6.46 | bleu 56.22 | wps 1889.7 | wpb 933.5 | bsz 59.6 | num_updates 123444 | best_bleu 57.52
2022-08-18 07:37:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1524 @ 123444 updates
2022-08-18 07:37:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1524.pt
2022-08-18 07:37:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1524.pt
2022-08-18 07:37:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1524.pt (epoch 1524 @ 123444 updates, score 56.22) (writing took 19.89393613487482 seconds)
2022-08-18 07:37:47 | INFO | fairseq_cli.train | end of epoch 1524 (average epoch stats below)
2022-08-18 07:37:47 | INFO | train | epoch 1524 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6267.8 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 123444 | lr 0.000180009 | gnorm 0.405 | train_wall 39 | gb_free 10 | wall 120363
2022-08-18 07:37:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:37:47 | INFO | fairseq.trainer | begin training epoch 1525
2022-08-18 07:37:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:38:16 | INFO | train_inner | epoch 1525:     56 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=6885.3, ups=1.25, wpb=5529.4, bsz=360.3, num_updates=123500, lr=0.000179969, gnorm=0.478, train_wall=48, gb_free=10.1, wall=120393
2022-08-18 07:38:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:38:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:38:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:38:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:38:39 | INFO | valid | epoch 1525 | valid on 'valid' subset | loss 5.271 | nll_loss 2.687 | ppl 6.44 | bleu 56.06 | wps 1866.8 | wpb 933.5 | bsz 59.6 | num_updates 123525 | best_bleu 57.52
2022-08-18 07:38:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1525 @ 123525 updates
2022-08-18 07:38:39 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1525.pt
2022-08-18 07:38:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1525.pt
2022-08-18 07:39:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1525.pt (epoch 1525 @ 123525 updates, score 56.06) (writing took 21.85485351085663 seconds)
2022-08-18 07:39:01 | INFO | fairseq_cli.train | end of epoch 1525 (average epoch stats below)
2022-08-18 07:39:01 | INFO | train | epoch 1525 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6049.5 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 123525 | lr 0.00017995 | gnorm 0.443 | train_wall 40 | gb_free 10.2 | wall 120437
2022-08-18 07:39:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:39:01 | INFO | fairseq.trainer | begin training epoch 1526
2022-08-18 07:39:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:39:40 | INFO | train_inner | epoch 1526:     75 / 81 loss=3.368, nll_loss=0.337, ppl=1.26, wps=6614.6, ups=1.19, wpb=5547.4, bsz=360.6, num_updates=123600, lr=0.000179896, gnorm=0.312, train_wall=50, gb_free=10.1, wall=120477
2022-08-18 07:39:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:39:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:39:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:39:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:39:52 | INFO | valid | epoch 1526 | valid on 'valid' subset | loss 5.278 | nll_loss 2.698 | ppl 6.49 | bleu 55.57 | wps 1858.9 | wpb 933.5 | bsz 59.6 | num_updates 123606 | best_bleu 57.52
2022-08-18 07:39:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1526 @ 123606 updates
2022-08-18 07:39:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1526.pt
2022-08-18 07:39:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1526.pt
2022-08-18 07:40:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1526.pt (epoch 1526 @ 123606 updates, score 55.57) (writing took 32.6931502148509 seconds)
2022-08-18 07:40:25 | INFO | fairseq_cli.train | end of epoch 1526 (average epoch stats below)
2022-08-18 07:40:25 | INFO | train | epoch 1526 | loss 3.369 | nll_loss 0.337 | ppl 1.26 | wps 5303.9 | ups 0.96 | wpb 5523.2 | bsz 358 | num_updates 123606 | lr 0.000179891 | gnorm 0.385 | train_wall 40 | gb_free 10.2 | wall 120521
2022-08-18 07:40:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:40:25 | INFO | fairseq.trainer | begin training epoch 1527
2022-08-18 07:40:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:41:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:41:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:41:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:41:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:41:18 | INFO | valid | epoch 1527 | valid on 'valid' subset | loss 5.274 | nll_loss 2.693 | ppl 6.47 | bleu 56.19 | wps 1740.7 | wpb 933.5 | bsz 59.6 | num_updates 123687 | best_bleu 57.52
2022-08-18 07:41:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1527 @ 123687 updates
2022-08-18 07:41:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1527.pt
2022-08-18 07:41:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1527.pt
2022-08-18 07:41:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1527.pt (epoch 1527 @ 123687 updates, score 56.19) (writing took 2.3373814187943935 seconds)
2022-08-18 07:41:20 | INFO | fairseq_cli.train | end of epoch 1527 (average epoch stats below)
2022-08-18 07:41:20 | INFO | train | epoch 1527 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 8119.4 | ups 1.47 | wpb 5523.2 | bsz 358 | num_updates 123687 | lr 0.000179832 | gnorm 0.399 | train_wall 40 | gb_free 10.2 | wall 120576
2022-08-18 07:41:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:41:20 | INFO | fairseq.trainer | begin training epoch 1528
2022-08-18 07:41:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:41:28 | INFO | train_inner | epoch 1528:     13 / 81 loss=3.37, nll_loss=0.338, ppl=1.26, wps=5103.7, ups=0.93, wpb=5494.4, bsz=355, num_updates=123700, lr=0.000179823, gnorm=0.427, train_wall=49, gb_free=10.1, wall=120584
2022-08-18 07:42:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:42:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:42:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:42:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:42:12 | INFO | valid | epoch 1528 | valid on 'valid' subset | loss 5.28 | nll_loss 2.7 | ppl 6.5 | bleu 56.36 | wps 1798.4 | wpb 933.5 | bsz 59.6 | num_updates 123768 | best_bleu 57.52
2022-08-18 07:42:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1528 @ 123768 updates
2022-08-18 07:42:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1528.pt
2022-08-18 07:42:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1528.pt
2022-08-18 07:42:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1528.pt (epoch 1528 @ 123768 updates, score 56.36) (writing took 25.667577780783176 seconds)
2022-08-18 07:42:38 | INFO | fairseq_cli.train | end of epoch 1528 (average epoch stats below)
2022-08-18 07:42:38 | INFO | train | epoch 1528 | loss 3.369 | nll_loss 0.337 | ppl 1.26 | wps 5773.6 | ups 1.05 | wpb 5523.2 | bsz 358 | num_updates 123768 | lr 0.000179774 | gnorm 0.308 | train_wall 41 | gb_free 10.1 | wall 120654
2022-08-18 07:42:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:42:38 | INFO | fairseq.trainer | begin training epoch 1529
2022-08-18 07:42:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:42:55 | INFO | train_inner | epoch 1529:     32 / 81 loss=3.368, nll_loss=0.337, ppl=1.26, wps=6363.4, ups=1.15, wpb=5528.8, bsz=358.6, num_updates=123800, lr=0.00017975, gnorm=0.32, train_wall=49, gb_free=10.1, wall=120671
2022-08-18 07:43:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:43:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:43:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:43:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:43:29 | INFO | valid | epoch 1529 | valid on 'valid' subset | loss 5.256 | nll_loss 2.67 | ppl 6.37 | bleu 56.75 | wps 1778.8 | wpb 933.5 | bsz 59.6 | num_updates 123849 | best_bleu 57.52
2022-08-18 07:43:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1529 @ 123849 updates
2022-08-18 07:43:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1529.pt
2022-08-18 07:43:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1529.pt
2022-08-18 07:43:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1529.pt (epoch 1529 @ 123849 updates, score 56.75) (writing took 2.4286401718854904 seconds)
2022-08-18 07:43:32 | INFO | fairseq_cli.train | end of epoch 1529 (average epoch stats below)
2022-08-18 07:43:32 | INFO | train | epoch 1529 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 8225.2 | ups 1.49 | wpb 5523.2 | bsz 358 | num_updates 123849 | lr 0.000179715 | gnorm 0.467 | train_wall 40 | gb_free 10.1 | wall 120708
2022-08-18 07:43:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:43:32 | INFO | fairseq.trainer | begin training epoch 1530
2022-08-18 07:43:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:43:59 | INFO | train_inner | epoch 1530:     51 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=8594.5, ups=1.55, wpb=5533.6, bsz=356.2, num_updates=123900, lr=0.000179678, gnorm=0.451, train_wall=49, gb_free=10.1, wall=120735
2022-08-18 07:44:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:44:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:44:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:44:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:44:23 | INFO | valid | epoch 1530 | valid on 'valid' subset | loss 5.272 | nll_loss 2.69 | ppl 6.45 | bleu 56.44 | wps 1829.1 | wpb 933.5 | bsz 59.6 | num_updates 123930 | best_bleu 57.52
2022-08-18 07:44:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1530 @ 123930 updates
2022-08-18 07:44:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1530.pt
2022-08-18 07:44:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1530.pt
2022-08-18 07:44:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1530.pt (epoch 1530 @ 123930 updates, score 56.44) (writing took 26.3635573387146 seconds)
2022-08-18 07:44:50 | INFO | fairseq_cli.train | end of epoch 1530 (average epoch stats below)
2022-08-18 07:44:50 | INFO | train | epoch 1530 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5740.6 | ups 1.04 | wpb 5523.2 | bsz 358 | num_updates 123930 | lr 0.000179656 | gnorm 0.405 | train_wall 40 | gb_free 10.1 | wall 120786
2022-08-18 07:44:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:44:50 | INFO | fairseq.trainer | begin training epoch 1531
2022-08-18 07:44:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:45:26 | INFO | train_inner | epoch 1531:     70 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6353, ups=1.15, wpb=5516.6, bsz=360.8, num_updates=124000, lr=0.000179605, gnorm=0.428, train_wall=49, gb_free=10, wall=120822
2022-08-18 07:45:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:45:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:45:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:45:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:45:41 | INFO | valid | epoch 1531 | valid on 'valid' subset | loss 5.266 | nll_loss 2.682 | ppl 6.42 | bleu 56.52 | wps 1802.8 | wpb 933.5 | bsz 59.6 | num_updates 124011 | best_bleu 57.52
2022-08-18 07:45:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1531 @ 124011 updates
2022-08-18 07:45:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1531.pt
2022-08-18 07:45:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1531.pt
2022-08-18 07:46:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1531.pt (epoch 1531 @ 124011 updates, score 56.52) (writing took 37.49190242961049 seconds)
2022-08-18 07:46:18 | INFO | fairseq_cli.train | end of epoch 1531 (average epoch stats below)
2022-08-18 07:46:18 | INFO | train | epoch 1531 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5054.1 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 124011 | lr 0.000179597 | gnorm 0.416 | train_wall 39 | gb_free 10.2 | wall 120875
2022-08-18 07:46:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:46:19 | INFO | fairseq.trainer | begin training epoch 1532
2022-08-18 07:46:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:47:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:47:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:47:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:47:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:47:10 | INFO | valid | epoch 1532 | valid on 'valid' subset | loss 5.264 | nll_loss 2.68 | ppl 6.41 | bleu 56.19 | wps 1847.3 | wpb 933.5 | bsz 59.6 | num_updates 124092 | best_bleu 57.52
2022-08-18 07:47:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1532 @ 124092 updates
2022-08-18 07:47:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1532.pt
2022-08-18 07:47:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1532.pt
2022-08-18 07:47:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1532.pt (epoch 1532 @ 124092 updates, score 56.19) (writing took 18.08250242471695 seconds)
2022-08-18 07:47:29 | INFO | fairseq_cli.train | end of epoch 1532 (average epoch stats below)
2022-08-18 07:47:29 | INFO | train | epoch 1532 | loss 3.369 | nll_loss 0.339 | ppl 1.26 | wps 6372.1 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 124092 | lr 0.000179539 | gnorm 0.393 | train_wall 39 | gb_free 10 | wall 120945
2022-08-18 07:47:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:47:29 | INFO | fairseq.trainer | begin training epoch 1533
2022-08-18 07:47:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:47:34 | INFO | train_inner | epoch 1533:      8 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=4281.7, ups=0.78, wpb=5466.1, bsz=354.2, num_updates=124100, lr=0.000179533, gnorm=0.42, train_wall=48, gb_free=10.1, wall=120950
2022-08-18 07:48:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:48:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:48:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:48:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:48:21 | INFO | valid | epoch 1533 | valid on 'valid' subset | loss 5.283 | nll_loss 2.701 | ppl 6.5 | bleu 56.25 | wps 1795.6 | wpb 933.5 | bsz 59.6 | num_updates 124173 | best_bleu 57.52
2022-08-18 07:48:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1533 @ 124173 updates
2022-08-18 07:48:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1533.pt
2022-08-18 07:48:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1533.pt
2022-08-18 07:48:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1533.pt (epoch 1533 @ 124173 updates, score 56.25) (writing took 15.43383102118969 seconds)
2022-08-18 07:48:36 | INFO | fairseq_cli.train | end of epoch 1533 (average epoch stats below)
2022-08-18 07:48:36 | INFO | train | epoch 1533 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6627.2 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 124173 | lr 0.00017948 | gnorm 0.407 | train_wall 40 | gb_free 10 | wall 121012
2022-08-18 07:48:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:48:36 | INFO | fairseq.trainer | begin training epoch 1534
2022-08-18 07:48:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:48:51 | INFO | train_inner | epoch 1534:     27 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=7190, ups=1.29, wpb=5563.4, bsz=356.8, num_updates=124200, lr=0.000179461, gnorm=0.376, train_wall=50, gb_free=10.1, wall=121027
2022-08-18 07:49:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:49:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:49:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:49:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:49:27 | INFO | valid | epoch 1534 | valid on 'valid' subset | loss 5.292 | nll_loss 2.712 | ppl 6.55 | bleu 56.55 | wps 1839.6 | wpb 933.5 | bsz 59.6 | num_updates 124254 | best_bleu 57.52
2022-08-18 07:49:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1534 @ 124254 updates
2022-08-18 07:49:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1534.pt
2022-08-18 07:49:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1534.pt
2022-08-18 07:50:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1534.pt (epoch 1534 @ 124254 updates, score 56.55) (writing took 39.12612594291568 seconds)
2022-08-18 07:50:06 | INFO | fairseq_cli.train | end of epoch 1534 (average epoch stats below)
2022-08-18 07:50:06 | INFO | train | epoch 1534 | loss 3.369 | nll_loss 0.337 | ppl 1.26 | wps 4968.1 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 124254 | lr 0.000179422 | gnorm 0.344 | train_wall 40 | gb_free 10.1 | wall 121102
2022-08-18 07:50:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:50:06 | INFO | fairseq.trainer | begin training epoch 1535
2022-08-18 07:50:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:50:31 | INFO | train_inner | epoch 1535:     46 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5526, ups=1, wpb=5517.5, bsz=365.9, num_updates=124300, lr=0.000179388, gnorm=0.372, train_wall=48, gb_free=10.1, wall=121127
2022-08-18 07:50:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:50:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:50:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:50:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:50:57 | INFO | valid | epoch 1535 | valid on 'valid' subset | loss 5.268 | nll_loss 2.684 | ppl 6.43 | bleu 56.75 | wps 1857.2 | wpb 933.5 | bsz 59.6 | num_updates 124335 | best_bleu 57.52
2022-08-18 07:50:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1535 @ 124335 updates
2022-08-18 07:50:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1535.pt
2022-08-18 07:50:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1535.pt
2022-08-18 07:51:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1535.pt (epoch 1535 @ 124335 updates, score 56.75) (writing took 19.231204487383366 seconds)
2022-08-18 07:51:17 | INFO | fairseq_cli.train | end of epoch 1535 (average epoch stats below)
2022-08-18 07:51:17 | INFO | train | epoch 1535 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6349.7 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 124335 | lr 0.000179363 | gnorm 0.39 | train_wall 39 | gb_free 10.2 | wall 121173
2022-08-18 07:51:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:51:17 | INFO | fairseq.trainer | begin training epoch 1536
2022-08-18 07:51:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:51:49 | INFO | train_inner | epoch 1536:     65 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=7086.5, ups=1.28, wpb=5557.7, bsz=354.2, num_updates=124400, lr=0.000179316, gnorm=0.388, train_wall=48, gb_free=10.1, wall=121206
2022-08-18 07:51:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:51:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:51:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:51:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:52:07 | INFO | valid | epoch 1536 | valid on 'valid' subset | loss 5.271 | nll_loss 2.687 | ppl 6.44 | bleu 55.58 | wps 1842.4 | wpb 933.5 | bsz 59.6 | num_updates 124416 | best_bleu 57.52
2022-08-18 07:52:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1536 @ 124416 updates
2022-08-18 07:52:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1536.pt
2022-08-18 07:52:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1536.pt
2022-08-18 07:52:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1536.pt (epoch 1536 @ 124416 updates, score 55.58) (writing took 22.32094257324934 seconds)
2022-08-18 07:52:29 | INFO | fairseq_cli.train | end of epoch 1536 (average epoch stats below)
2022-08-18 07:52:29 | INFO | train | epoch 1536 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6176 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 124416 | lr 0.000179305 | gnorm 0.433 | train_wall 38 | gb_free 10.1 | wall 121245
2022-08-18 07:52:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:52:29 | INFO | fairseq.trainer | begin training epoch 1537
2022-08-18 07:52:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:53:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:53:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:53:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:53:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:53:21 | INFO | valid | epoch 1537 | valid on 'valid' subset | loss 5.269 | nll_loss 2.685 | ppl 6.43 | bleu 56.28 | wps 1910.1 | wpb 933.5 | bsz 59.6 | num_updates 124497 | best_bleu 57.52
2022-08-18 07:53:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1537 @ 124497 updates
2022-08-18 07:53:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1537.pt
2022-08-18 07:53:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1537.pt
2022-08-18 07:53:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1537.pt (epoch 1537 @ 124497 updates, score 56.28) (writing took 36.604526579380035 seconds)
2022-08-18 07:53:58 | INFO | fairseq_cli.train | end of epoch 1537 (average epoch stats below)
2022-08-18 07:53:58 | INFO | train | epoch 1537 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5043.6 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 124497 | lr 0.000179246 | gnorm 0.333 | train_wall 40 | gb_free 10.2 | wall 121334
2022-08-18 07:53:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:53:58 | INFO | fairseq.trainer | begin training epoch 1538
2022-08-18 07:53:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:54:01 | INFO | train_inner | epoch 1538:      3 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=4188, ups=0.76, wpb=5495.1, bsz=357.3, num_updates=124500, lr=0.000179244, gnorm=0.379, train_wall=49, gb_free=10.1, wall=121337
2022-08-18 07:54:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:54:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:54:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:54:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:54:48 | INFO | valid | epoch 1538 | valid on 'valid' subset | loss 5.279 | nll_loss 2.699 | ppl 6.49 | bleu 56.59 | wps 1889.2 | wpb 933.5 | bsz 59.6 | num_updates 124578 | best_bleu 57.52
2022-08-18 07:54:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1538 @ 124578 updates
2022-08-18 07:54:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1538.pt
2022-08-18 07:54:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1538.pt
2022-08-18 07:55:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1538.pt (epoch 1538 @ 124578 updates, score 56.59) (writing took 25.70478382706642 seconds)
2022-08-18 07:55:14 | INFO | fairseq_cli.train | end of epoch 1538 (average epoch stats below)
2022-08-18 07:55:14 | INFO | train | epoch 1538 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5884.9 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 124578 | lr 0.000179188 | gnorm 0.457 | train_wall 39 | gb_free 10.2 | wall 121410
2022-08-18 07:55:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:55:14 | INFO | fairseq.trainer | begin training epoch 1539
2022-08-18 07:55:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:55:28 | INFO | train_inner | epoch 1539:     22 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6312.1, ups=1.14, wpb=5532.6, bsz=361.3, num_updates=124600, lr=0.000179172, gnorm=0.435, train_wall=48, gb_free=10.1, wall=121425
2022-08-18 07:56:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:56:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:56:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:56:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:56:11 | INFO | valid | epoch 1539 | valid on 'valid' subset | loss 5.293 | nll_loss 2.713 | ppl 6.56 | bleu 56.19 | wps 1869.5 | wpb 933.5 | bsz 59.6 | num_updates 124659 | best_bleu 57.52
2022-08-18 07:56:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1539 @ 124659 updates
2022-08-18 07:56:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1539.pt
2022-08-18 07:56:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1539.pt
2022-08-18 07:56:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1539.pt (epoch 1539 @ 124659 updates, score 56.19) (writing took 17.54732569679618 seconds)
2022-08-18 07:56:29 | INFO | fairseq_cli.train | end of epoch 1539 (average epoch stats below)
2022-08-18 07:56:29 | INFO | train | epoch 1539 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5980.6 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 124659 | lr 0.00017913 | gnorm 0.363 | train_wall 40 | gb_free 10 | wall 121485
2022-08-18 07:56:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:56:29 | INFO | fairseq.trainer | begin training epoch 1540
2022-08-18 07:56:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:56:51 | INFO | train_inner | epoch 1540:     41 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6727.1, ups=1.22, wpb=5533.2, bsz=355.9, num_updates=124700, lr=0.0001791, gnorm=0.362, train_wall=49, gb_free=10.1, wall=121507
2022-08-18 07:57:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:57:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:57:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:57:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:57:19 | INFO | valid | epoch 1540 | valid on 'valid' subset | loss 5.28 | nll_loss 2.701 | ppl 6.5 | bleu 56.48 | wps 1879.7 | wpb 933.5 | bsz 59.6 | num_updates 124740 | best_bleu 57.52
2022-08-18 07:57:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1540 @ 124740 updates
2022-08-18 07:57:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1540.pt
2022-08-18 07:57:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1540.pt
2022-08-18 07:57:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1540.pt (epoch 1540 @ 124740 updates, score 56.48) (writing took 31.038513589650393 seconds)
2022-08-18 07:57:51 | INFO | fairseq_cli.train | end of epoch 1540 (average epoch stats below)
2022-08-18 07:57:51 | INFO | train | epoch 1540 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5465.1 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 124740 | lr 0.000179072 | gnorm 0.359 | train_wall 39 | gb_free 10.1 | wall 121567
2022-08-18 07:57:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:57:51 | INFO | fairseq.trainer | begin training epoch 1541
2022-08-18 07:57:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:58:22 | INFO | train_inner | epoch 1541:     60 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=6035.6, ups=1.09, wpb=5528.1, bsz=355.4, num_updates=124800, lr=0.000179029, gnorm=0.369, train_wall=49, gb_free=10.1, wall=121598
2022-08-18 07:58:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:58:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:58:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:58:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 07:58:42 | INFO | valid | epoch 1541 | valid on 'valid' subset | loss 5.266 | nll_loss 2.682 | ppl 6.42 | bleu 56.49 | wps 1917.4 | wpb 933.5 | bsz 59.6 | num_updates 124821 | best_bleu 57.52
2022-08-18 07:58:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1541 @ 124821 updates
2022-08-18 07:58:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1541.pt
2022-08-18 07:58:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1541.pt
2022-08-18 07:59:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1541.pt (epoch 1541 @ 124821 updates, score 56.49) (writing took 32.01664727553725 seconds)
2022-08-18 07:59:14 | INFO | fairseq_cli.train | end of epoch 1541 (average epoch stats below)
2022-08-18 07:59:14 | INFO | train | epoch 1541 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 5377.5 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 124821 | lr 0.000179014 | gnorm 0.404 | train_wall 40 | gb_free 10.1 | wall 121650
2022-08-18 07:59:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 07:59:14 | INFO | fairseq.trainer | begin training epoch 1542
2022-08-18 07:59:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 07:59:55 | INFO | train_inner | epoch 1542:     79 / 81 loss=3.37, nll_loss=0.34, ppl=1.27, wps=5939.6, ups=1.08, wpb=5523, bsz=361.6, num_updates=124900, lr=0.000178957, gnorm=0.518, train_wall=50, gb_free=10.1, wall=121691
2022-08-18 07:59:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 07:59:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 07:59:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 07:59:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:00:05 | INFO | valid | epoch 1542 | valid on 'valid' subset | loss 5.286 | nll_loss 2.705 | ppl 6.52 | bleu 56.34 | wps 1837.4 | wpb 933.5 | bsz 59.6 | num_updates 124902 | best_bleu 57.52
2022-08-18 08:00:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1542 @ 124902 updates
2022-08-18 08:00:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1542.pt
2022-08-18 08:00:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1542.pt
2022-08-18 08:00:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1542.pt (epoch 1542 @ 124902 updates, score 56.34) (writing took 16.783589389175177 seconds)
2022-08-18 08:00:22 | INFO | fairseq_cli.train | end of epoch 1542 (average epoch stats below)
2022-08-18 08:00:22 | INFO | train | epoch 1542 | loss 3.37 | nll_loss 0.34 | ppl 1.27 | wps 6531.8 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 124902 | lr 0.000178956 | gnorm 0.52 | train_wall 40 | gb_free 10.2 | wall 121718
2022-08-18 08:00:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:00:23 | INFO | fairseq.trainer | begin training epoch 1543
2022-08-18 08:00:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:01:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:01:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:01:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:01:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:01:13 | INFO | valid | epoch 1543 | valid on 'valid' subset | loss 5.284 | nll_loss 2.706 | ppl 6.53 | bleu 56.3 | wps 1828.8 | wpb 933.5 | bsz 59.6 | num_updates 124983 | best_bleu 57.52
2022-08-18 08:01:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1543 @ 124983 updates
2022-08-18 08:01:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1543.pt
2022-08-18 08:01:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1543.pt
2022-08-18 08:01:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1543.pt (epoch 1543 @ 124983 updates, score 56.3) (writing took 32.86403677612543 seconds)
2022-08-18 08:01:46 | INFO | fairseq_cli.train | end of epoch 1543 (average epoch stats below)
2022-08-18 08:01:46 | INFO | train | epoch 1543 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 5348.5 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 124983 | lr 0.000178898 | gnorm 0.398 | train_wall 39 | gb_free 10.3 | wall 121802
2022-08-18 08:01:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:01:46 | INFO | fairseq.trainer | begin training epoch 1544
2022-08-18 08:01:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:01:56 | INFO | train_inner | epoch 1544:     17 / 81 loss=3.369, nll_loss=0.337, ppl=1.26, wps=4546.8, ups=0.83, wpb=5500.5, bsz=358.7, num_updates=125000, lr=0.000178885, gnorm=0.397, train_wall=48, gb_free=10, wall=121812
2022-08-18 08:02:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:02:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:02:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:02:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:02:37 | INFO | valid | epoch 1544 | valid on 'valid' subset | loss 5.302 | nll_loss 2.724 | ppl 6.61 | bleu 56.17 | wps 1842.8 | wpb 933.5 | bsz 59.6 | num_updates 125064 | best_bleu 57.52
2022-08-18 08:02:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1544 @ 125064 updates
2022-08-18 08:02:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1544.pt
2022-08-18 08:02:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1544.pt
2022-08-18 08:03:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1544.pt (epoch 1544 @ 125064 updates, score 56.17) (writing took 33.28549150004983 seconds)
2022-08-18 08:03:11 | INFO | fairseq_cli.train | end of epoch 1544 (average epoch stats below)
2022-08-18 08:03:11 | INFO | train | epoch 1544 | loss 3.369 | nll_loss 0.339 | ppl 1.26 | wps 5273.7 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 125064 | lr 0.00017884 | gnorm 0.403 | train_wall 40 | gb_free 10.1 | wall 121887
2022-08-18 08:03:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:03:11 | INFO | fairseq.trainer | begin training epoch 1545
2022-08-18 08:03:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:03:31 | INFO | train_inner | epoch 1545:     36 / 81 loss=3.37, nll_loss=0.339, ppl=1.26, wps=5852.9, ups=1.05, wpb=5551.6, bsz=357.8, num_updates=125100, lr=0.000178814, gnorm=0.458, train_wall=49, gb_free=10.1, wall=121907
2022-08-18 08:03:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:03:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:03:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:03:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:04:03 | INFO | valid | epoch 1545 | valid on 'valid' subset | loss 5.305 | nll_loss 2.729 | ppl 6.63 | bleu 56.27 | wps 1818.1 | wpb 933.5 | bsz 59.6 | num_updates 125145 | best_bleu 57.52
2022-08-18 08:04:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1545 @ 125145 updates
2022-08-18 08:04:03 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1545.pt
2022-08-18 08:04:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1545.pt
2022-08-18 08:04:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1545.pt (epoch 1545 @ 125145 updates, score 56.27) (writing took 20.974398344755173 seconds)
2022-08-18 08:04:24 | INFO | fairseq_cli.train | end of epoch 1545 (average epoch stats below)
2022-08-18 08:04:24 | INFO | train | epoch 1545 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6101.4 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 125145 | lr 0.000178782 | gnorm 0.451 | train_wall 40 | gb_free 10.1 | wall 121960
2022-08-18 08:04:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:04:24 | INFO | fairseq.trainer | begin training epoch 1546
2022-08-18 08:04:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:04:54 | INFO | train_inner | epoch 1546:     55 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6653.3, ups=1.21, wpb=5509.3, bsz=355.7, num_updates=125200, lr=0.000178743, gnorm=0.359, train_wall=49, gb_free=10.1, wall=121990
2022-08-18 08:05:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:05:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:05:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:05:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:05:16 | INFO | valid | epoch 1546 | valid on 'valid' subset | loss 5.275 | nll_loss 2.693 | ppl 6.47 | bleu 56.85 | wps 1799.7 | wpb 933.5 | bsz 59.6 | num_updates 125226 | best_bleu 57.52
2022-08-18 08:05:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1546 @ 125226 updates
2022-08-18 08:05:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1546.pt
2022-08-18 08:05:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1546.pt
2022-08-18 08:05:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1546.pt (epoch 1546 @ 125226 updates, score 56.85) (writing took 13.648201752454042 seconds)
2022-08-18 08:05:30 | INFO | fairseq_cli.train | end of epoch 1546 (average epoch stats below)
2022-08-18 08:05:30 | INFO | train | epoch 1546 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 6767.4 | ups 1.23 | wpb 5523.2 | bsz 358 | num_updates 125226 | lr 0.000178724 | gnorm 0.339 | train_wall 39 | gb_free 10.1 | wall 122026
2022-08-18 08:05:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:05:30 | INFO | fairseq.trainer | begin training epoch 1547
2022-08-18 08:05:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:06:10 | INFO | train_inner | epoch 1547:     74 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=7316.9, ups=1.32, wpb=5538.6, bsz=358.6, num_updates=125300, lr=0.000178671, gnorm=0.418, train_wall=49, gb_free=10.1, wall=122066
2022-08-18 08:06:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:06:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:06:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:06:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:06:22 | INFO | valid | epoch 1547 | valid on 'valid' subset | loss 5.287 | nll_loss 2.708 | ppl 6.53 | bleu 56.04 | wps 1855.7 | wpb 933.5 | bsz 59.6 | num_updates 125307 | best_bleu 57.52
2022-08-18 08:06:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1547 @ 125307 updates
2022-08-18 08:06:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1547.pt
2022-08-18 08:06:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1547.pt
2022-08-18 08:06:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1547.pt (epoch 1547 @ 125307 updates, score 56.04) (writing took 31.780918940901756 seconds)
2022-08-18 08:06:54 | INFO | fairseq_cli.train | end of epoch 1547 (average epoch stats below)
2022-08-18 08:06:54 | INFO | train | epoch 1547 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5343.1 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 125307 | lr 0.000178666 | gnorm 0.461 | train_wall 40 | gb_free 10.1 | wall 122110
2022-08-18 08:06:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:06:54 | INFO | fairseq.trainer | begin training epoch 1548
2022-08-18 08:06:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:07:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:07:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:07:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:07:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:07:46 | INFO | valid | epoch 1548 | valid on 'valid' subset | loss 5.29 | nll_loss 2.709 | ppl 6.54 | bleu 56.42 | wps 1778.9 | wpb 933.5 | bsz 59.6 | num_updates 125388 | best_bleu 57.52
2022-08-18 08:07:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1548 @ 125388 updates
2022-08-18 08:07:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1548.pt
2022-08-18 08:07:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1548.pt
2022-08-18 08:07:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1548.pt (epoch 1548 @ 125388 updates, score 56.42) (writing took 2.514359485358 seconds)
2022-08-18 08:07:49 | INFO | fairseq_cli.train | end of epoch 1548 (average epoch stats below)
2022-08-18 08:07:49 | INFO | train | epoch 1548 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 8093.8 | ups 1.47 | wpb 5523.2 | bsz 358 | num_updates 125388 | lr 0.000178608 | gnorm 0.438 | train_wall 39 | gb_free 10.2 | wall 122165
2022-08-18 08:07:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:07:49 | INFO | fairseq.trainer | begin training epoch 1549
2022-08-18 08:07:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:07:57 | INFO | train_inner | epoch 1549:     12 / 81 loss=3.369, nll_loss=0.337, ppl=1.26, wps=5134.9, ups=0.93, wpb=5498.5, bsz=352.5, num_updates=125400, lr=0.0001786, gnorm=0.429, train_wall=48, gb_free=10.1, wall=122173
2022-08-18 08:08:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:08:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:08:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:08:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:08:41 | INFO | valid | epoch 1549 | valid on 'valid' subset | loss 5.296 | nll_loss 2.714 | ppl 6.56 | bleu 56.24 | wps 1884.1 | wpb 933.5 | bsz 59.6 | num_updates 125469 | best_bleu 57.52
2022-08-18 08:08:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1549 @ 125469 updates
2022-08-18 08:08:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1549.pt
2022-08-18 08:08:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1549.pt
2022-08-18 08:09:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1549.pt (epoch 1549 @ 125469 updates, score 56.24) (writing took 31.77056248486042 seconds)
2022-08-18 08:09:13 | INFO | fairseq_cli.train | end of epoch 1549 (average epoch stats below)
2022-08-18 08:09:13 | INFO | train | epoch 1549 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5339.4 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 125469 | lr 0.000178551 | gnorm 0.4 | train_wall 39 | gb_free 10.1 | wall 122249
2022-08-18 08:09:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:09:13 | INFO | fairseq.trainer | begin training epoch 1550
2022-08-18 08:09:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:09:32 | INFO | train_inner | epoch 1550:     31 / 81 loss=3.369, nll_loss=0.337, ppl=1.26, wps=5814.8, ups=1.05, wpb=5525.7, bsz=365.4, num_updates=125500, lr=0.000178529, gnorm=0.398, train_wall=49, gb_free=10, wall=122268
2022-08-18 08:09:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:09:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:09:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:09:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:10:07 | INFO | valid | epoch 1550 | valid on 'valid' subset | loss 5.286 | nll_loss 2.703 | ppl 6.51 | bleu 56.45 | wps 1770.5 | wpb 933.5 | bsz 59.6 | num_updates 125550 | best_bleu 57.52
2022-08-18 08:10:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1550 @ 125550 updates
2022-08-18 08:10:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1550.pt
2022-08-18 08:10:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1550.pt
2022-08-18 08:10:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1550.pt (epoch 1550 @ 125550 updates, score 56.45) (writing took 31.321684792637825 seconds)
2022-08-18 08:10:38 | INFO | fairseq_cli.train | end of epoch 1550 (average epoch stats below)
2022-08-18 08:10:38 | INFO | train | epoch 1550 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5246.2 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 125550 | lr 0.000178493 | gnorm 0.395 | train_wall 40 | gb_free 10 | wall 122334
2022-08-18 08:10:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:10:38 | INFO | fairseq.trainer | begin training epoch 1551
2022-08-18 08:10:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:11:07 | INFO | train_inner | epoch 1551:     50 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5772.5, ups=1.05, wpb=5516.3, bsz=355.5, num_updates=125600, lr=0.000178458, gnorm=0.384, train_wall=49, gb_free=10.1, wall=122363
2022-08-18 08:11:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:11:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:11:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:11:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:11:32 | INFO | valid | epoch 1551 | valid on 'valid' subset | loss 5.284 | nll_loss 2.705 | ppl 6.52 | bleu 56.9 | wps 1831.4 | wpb 933.5 | bsz 59.6 | num_updates 125631 | best_bleu 57.52
2022-08-18 08:11:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1551 @ 125631 updates
2022-08-18 08:11:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1551.pt
2022-08-18 08:11:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1551.pt
2022-08-18 08:11:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1551.pt (epoch 1551 @ 125631 updates, score 56.9) (writing took 2.7665708512067795 seconds)
2022-08-18 08:11:35 | INFO | fairseq_cli.train | end of epoch 1551 (average epoch stats below)
2022-08-18 08:11:35 | INFO | train | epoch 1551 | loss 3.369 | nll_loss 0.337 | ppl 1.26 | wps 7847.4 | ups 1.42 | wpb 5523.2 | bsz 358 | num_updates 125631 | lr 0.000178436 | gnorm 0.371 | train_wall 40 | gb_free 10 | wall 122391
2022-08-18 08:11:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:11:35 | INFO | fairseq.trainer | begin training epoch 1552
2022-08-18 08:11:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:12:14 | INFO | train_inner | epoch 1552:     69 / 81 loss=3.37, nll_loss=0.339, ppl=1.27, wps=8333.5, ups=1.5, wpb=5553.3, bsz=360.3, num_updates=125700, lr=0.000178387, gnorm=0.625, train_wall=50, gb_free=10.1, wall=122430
2022-08-18 08:12:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:12:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:12:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:12:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:12:29 | INFO | valid | epoch 1552 | valid on 'valid' subset | loss 5.274 | nll_loss 2.69 | ppl 6.45 | bleu 56.07 | wps 1919.9 | wpb 933.5 | bsz 59.6 | num_updates 125712 | best_bleu 57.52
2022-08-18 08:12:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1552 @ 125712 updates
2022-08-18 08:12:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1552.pt
2022-08-18 08:12:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1552.pt
2022-08-18 08:12:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1552.pt (epoch 1552 @ 125712 updates, score 56.07) (writing took 15.169891264289618 seconds)
2022-08-18 08:12:45 | INFO | fairseq_cli.train | end of epoch 1552 (average epoch stats below)
2022-08-18 08:12:45 | INFO | train | epoch 1552 | loss 3.37 | nll_loss 0.34 | ppl 1.27 | wps 6426.6 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 125712 | lr 0.000178378 | gnorm 0.685 | train_wall 41 | gb_free 10 | wall 122461
2022-08-18 08:12:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:12:45 | INFO | fairseq.trainer | begin training epoch 1553
2022-08-18 08:12:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:13:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:13:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:13:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:13:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:13:44 | INFO | valid | epoch 1553 | valid on 'valid' subset | loss 5.297 | nll_loss 2.721 | ppl 6.59 | bleu 56.18 | wps 1899 | wpb 933.5 | bsz 59.6 | num_updates 125793 | best_bleu 57.52
2022-08-18 08:13:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1553 @ 125793 updates
2022-08-18 08:13:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1553.pt
2022-08-18 08:13:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1553.pt
2022-08-18 08:14:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1553.pt (epoch 1553 @ 125793 updates, score 56.18) (writing took 19.62764659151435 seconds)
2022-08-18 08:14:03 | INFO | fairseq_cli.train | end of epoch 1553 (average epoch stats below)
2022-08-18 08:14:03 | INFO | train | epoch 1553 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5694.9 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 125793 | lr 0.000178321 | gnorm 0.333 | train_wall 41 | gb_free 10.1 | wall 122540
2022-08-18 08:14:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:14:04 | INFO | fairseq.trainer | begin training epoch 1554
2022-08-18 08:14:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:14:09 | INFO | train_inner | epoch 1554:      7 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=4783.5, ups=0.87, wpb=5486.4, bsz=354.2, num_updates=125800, lr=0.000178316, gnorm=0.346, train_wall=50, gb_free=10.1, wall=122545
2022-08-18 08:14:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:14:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:14:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:14:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:14:57 | INFO | valid | epoch 1554 | valid on 'valid' subset | loss 5.308 | nll_loss 2.732 | ppl 6.64 | bleu 56.08 | wps 1775 | wpb 933.5 | bsz 59.6 | num_updates 125874 | best_bleu 57.52
2022-08-18 08:14:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1554 @ 125874 updates
2022-08-18 08:14:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1554.pt
2022-08-18 08:14:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1554.pt
2022-08-18 08:15:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1554.pt (epoch 1554 @ 125874 updates, score 56.08) (writing took 15.272129692137241 seconds)
2022-08-18 08:15:12 | INFO | fairseq_cli.train | end of epoch 1554 (average epoch stats below)
2022-08-18 08:15:12 | INFO | train | epoch 1554 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6522.8 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 125874 | lr 0.000178263 | gnorm 0.387 | train_wall 40 | gb_free 10.1 | wall 122608
2022-08-18 08:15:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:15:12 | INFO | fairseq.trainer | begin training epoch 1555
2022-08-18 08:15:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:15:27 | INFO | train_inner | epoch 1555:     26 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=7123.3, ups=1.28, wpb=5565.1, bsz=359.9, num_updates=125900, lr=0.000178245, gnorm=0.392, train_wall=49, gb_free=10.2, wall=122623
2022-08-18 08:15:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:15:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:15:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:15:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:16:04 | INFO | valid | epoch 1555 | valid on 'valid' subset | loss 5.278 | nll_loss 2.698 | ppl 6.49 | bleu 56.64 | wps 1912.3 | wpb 933.5 | bsz 59.6 | num_updates 125955 | best_bleu 57.52
2022-08-18 08:16:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1555 @ 125955 updates
2022-08-18 08:16:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1555.pt
2022-08-18 08:16:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1555.pt
2022-08-18 08:16:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1555.pt (epoch 1555 @ 125955 updates, score 56.64) (writing took 34.455125119537115 seconds)
2022-08-18 08:16:38 | INFO | fairseq_cli.train | end of epoch 1555 (average epoch stats below)
2022-08-18 08:16:38 | INFO | train | epoch 1555 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5191.9 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 125955 | lr 0.000178206 | gnorm 0.389 | train_wall 40 | gb_free 10.1 | wall 122694
2022-08-18 08:16:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:16:38 | INFO | fairseq.trainer | begin training epoch 1556
2022-08-18 08:16:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:17:04 | INFO | train_inner | epoch 1556:     45 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5672.1, ups=1.03, wpb=5500.6, bsz=362.5, num_updates=126000, lr=0.000178174, gnorm=0.403, train_wall=49, gb_free=10.1, wall=122720
2022-08-18 08:17:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:17:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:17:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:17:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:17:31 | INFO | valid | epoch 1556 | valid on 'valid' subset | loss 5.276 | nll_loss 2.695 | ppl 6.47 | bleu 56.76 | wps 1859.4 | wpb 933.5 | bsz 59.6 | num_updates 126036 | best_bleu 57.52
2022-08-18 08:17:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1556 @ 126036 updates
2022-08-18 08:17:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1556.pt
2022-08-18 08:17:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1556.pt
2022-08-18 08:17:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1556.pt (epoch 1556 @ 126036 updates, score 56.76) (writing took 22.874713741242886 seconds)
2022-08-18 08:17:54 | INFO | fairseq_cli.train | end of epoch 1556 (average epoch stats below)
2022-08-18 08:17:54 | INFO | train | epoch 1556 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5879.1 | ups 1.06 | wpb 5523.2 | bsz 358 | num_updates 126036 | lr 0.000178149 | gnorm 0.416 | train_wall 40 | gb_free 10.2 | wall 122771
2022-08-18 08:17:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:17:55 | INFO | fairseq.trainer | begin training epoch 1557
2022-08-18 08:17:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:18:28 | INFO | train_inner | epoch 1557:     64 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6526.9, ups=1.19, wpb=5500.1, bsz=351.9, num_updates=126100, lr=0.000178103, gnorm=0.425, train_wall=48, gb_free=10.1, wall=122804
2022-08-18 08:18:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:18:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:18:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:18:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:18:46 | INFO | valid | epoch 1557 | valid on 'valid' subset | loss 5.299 | nll_loss 2.723 | ppl 6.6 | bleu 56.34 | wps 1845.7 | wpb 933.5 | bsz 59.6 | num_updates 126117 | best_bleu 57.52
2022-08-18 08:18:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1557 @ 126117 updates
2022-08-18 08:18:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1557.pt
2022-08-18 08:18:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1557.pt
2022-08-18 08:19:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1557.pt (epoch 1557 @ 126117 updates, score 56.34) (writing took 14.271427761763334 seconds)
2022-08-18 08:19:00 | INFO | fairseq_cli.train | end of epoch 1557 (average epoch stats below)
2022-08-18 08:19:00 | INFO | train | epoch 1557 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 6769.2 | ups 1.23 | wpb 5523.2 | bsz 358 | num_updates 126117 | lr 0.000178091 | gnorm 0.415 | train_wall 39 | gb_free 10.1 | wall 122837
2022-08-18 08:19:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:19:01 | INFO | fairseq.trainer | begin training epoch 1558
2022-08-18 08:19:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:19:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:19:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:19:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:19:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:19:52 | INFO | valid | epoch 1558 | valid on 'valid' subset | loss 5.3 | nll_loss 2.726 | ppl 6.62 | bleu 56.18 | wps 1978.3 | wpb 933.5 | bsz 59.6 | num_updates 126198 | best_bleu 57.52
2022-08-18 08:19:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1558 @ 126198 updates
2022-08-18 08:19:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1558.pt
2022-08-18 08:19:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1558.pt
2022-08-18 08:20:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1558.pt (epoch 1558 @ 126198 updates, score 56.18) (writing took 34.76430617272854 seconds)
2022-08-18 08:20:27 | INFO | fairseq_cli.train | end of epoch 1558 (average epoch stats below)
2022-08-18 08:20:27 | INFO | train | epoch 1558 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 5187.8 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 126198 | lr 0.000178034 | gnorm 0.389 | train_wall 40 | gb_free 10.1 | wall 122923
2022-08-18 08:20:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:20:27 | INFO | fairseq.trainer | begin training epoch 1559
2022-08-18 08:20:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:20:29 | INFO | train_inner | epoch 1559:      2 / 81 loss=3.368, nll_loss=0.337, ppl=1.26, wps=4571.8, ups=0.83, wpb=5536.6, bsz=360.5, num_updates=126200, lr=0.000178033, gnorm=0.38, train_wall=49, gb_free=10, wall=122925
2022-08-18 08:21:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:21:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:21:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:21:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:21:23 | INFO | valid | epoch 1559 | valid on 'valid' subset | loss 5.298 | nll_loss 2.725 | ppl 6.61 | bleu 55.83 | wps 1841 | wpb 933.5 | bsz 59.6 | num_updates 126279 | best_bleu 57.52
2022-08-18 08:21:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1559 @ 126279 updates
2022-08-18 08:21:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1559.pt
2022-08-18 08:21:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1559.pt
2022-08-18 08:21:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1559.pt (epoch 1559 @ 126279 updates, score 55.83) (writing took 16.24944869056344 seconds)
2022-08-18 08:21:39 | INFO | fairseq_cli.train | end of epoch 1559 (average epoch stats below)
2022-08-18 08:21:39 | INFO | train | epoch 1559 | loss 3.369 | nll_loss 0.339 | ppl 1.26 | wps 6169.2 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 126279 | lr 0.000177977 | gnorm 0.368 | train_wall 38 | gb_free 10.2 | wall 122995
2022-08-18 08:21:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:21:39 | INFO | fairseq.trainer | begin training epoch 1560
2022-08-18 08:21:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:21:51 | INFO | train_inner | epoch 1560:     21 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6692.9, ups=1.21, wpb=5513.5, bsz=359.3, num_updates=126300, lr=0.000177962, gnorm=0.357, train_wall=48, gb_free=10.1, wall=123008
2022-08-18 08:22:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:22:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:22:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:22:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:22:32 | INFO | valid | epoch 1560 | valid on 'valid' subset | loss 5.3 | nll_loss 2.725 | ppl 6.61 | bleu 56.26 | wps 1781.9 | wpb 933.5 | bsz 59.6 | num_updates 126360 | best_bleu 57.52
2022-08-18 08:22:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1560 @ 126360 updates
2022-08-18 08:22:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1560.pt
2022-08-18 08:22:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1560.pt
2022-08-18 08:22:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1560.pt (epoch 1560 @ 126360 updates, score 56.26) (writing took 14.144418507814407 seconds)
2022-08-18 08:22:47 | INFO | fairseq_cli.train | end of epoch 1560 (average epoch stats below)
2022-08-18 08:22:47 | INFO | train | epoch 1560 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 6614.8 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 126360 | lr 0.00017792 | gnorm 0.328 | train_wall 40 | gb_free 10.3 | wall 123063
2022-08-18 08:22:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:22:47 | INFO | fairseq.trainer | begin training epoch 1561
2022-08-18 08:22:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:23:09 | INFO | train_inner | epoch 1561:     40 / 81 loss=3.368, nll_loss=0.337, ppl=1.26, wps=7131.4, ups=1.29, wpb=5538.4, bsz=360.7, num_updates=126400, lr=0.000177892, gnorm=0.381, train_wall=50, gb_free=10, wall=123085
2022-08-18 08:23:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:23:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:23:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:23:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:23:39 | INFO | valid | epoch 1561 | valid on 'valid' subset | loss 5.276 | nll_loss 2.697 | ppl 6.48 | bleu 56.67 | wps 1822.5 | wpb 933.5 | bsz 59.6 | num_updates 126441 | best_bleu 57.52
2022-08-18 08:23:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1561 @ 126441 updates
2022-08-18 08:23:39 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1561.pt
2022-08-18 08:23:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1561.pt
2022-08-18 08:24:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1561.pt (epoch 1561 @ 126441 updates, score 56.67) (writing took 30.801323920488358 seconds)
2022-08-18 08:24:10 | INFO | fairseq_cli.train | end of epoch 1561 (average epoch stats below)
2022-08-18 08:24:10 | INFO | train | epoch 1561 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5404 | ups 0.98 | wpb 5523.2 | bsz 358 | num_updates 126441 | lr 0.000177863 | gnorm 0.429 | train_wall 40 | gb_free 10.1 | wall 123146
2022-08-18 08:24:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:24:10 | INFO | fairseq.trainer | begin training epoch 1562
2022-08-18 08:24:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:24:43 | INFO | train_inner | epoch 1562:     59 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5888.8, ups=1.07, wpb=5519.9, bsz=354.7, num_updates=126500, lr=0.000177822, gnorm=0.385, train_wall=50, gb_free=10.1, wall=123179
2022-08-18 08:24:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:24:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:24:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:24:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:25:04 | INFO | valid | epoch 1562 | valid on 'valid' subset | loss 5.267 | nll_loss 2.685 | ppl 6.43 | bleu 56.28 | wps 1754.1 | wpb 933.5 | bsz 59.6 | num_updates 126522 | best_bleu 57.52
2022-08-18 08:25:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1562 @ 126522 updates
2022-08-18 08:25:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1562.pt
2022-08-18 08:25:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1562.pt
2022-08-18 08:25:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1562.pt (epoch 1562 @ 126522 updates, score 56.28) (writing took 2.5252981521189213 seconds)
2022-08-18 08:25:06 | INFO | fairseq_cli.train | end of epoch 1562 (average epoch stats below)
2022-08-18 08:25:06 | INFO | train | epoch 1562 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 7869 | ups 1.42 | wpb 5523.2 | bsz 358 | num_updates 126522 | lr 0.000177806 | gnorm 0.357 | train_wall 41 | gb_free 10.3 | wall 123203
2022-08-18 08:25:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:25:07 | INFO | fairseq.trainer | begin training epoch 1563
2022-08-18 08:25:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:25:48 | INFO | train_inner | epoch 1563:     78 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=8549.5, ups=1.54, wpb=5540.4, bsz=358.9, num_updates=126600, lr=0.000177751, gnorm=0.342, train_wall=50, gb_free=10.1, wall=123244
2022-08-18 08:25:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:25:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:25:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:25:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:25:59 | INFO | valid | epoch 1563 | valid on 'valid' subset | loss 5.279 | nll_loss 2.698 | ppl 6.49 | bleu 56.57 | wps 1811.7 | wpb 933.5 | bsz 59.6 | num_updates 126603 | best_bleu 57.52
2022-08-18 08:25:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1563 @ 126603 updates
2022-08-18 08:25:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1563.pt
2022-08-18 08:26:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1563.pt
2022-08-18 08:26:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1563.pt (epoch 1563 @ 126603 updates, score 56.57) (writing took 23.836891744285822 seconds)
2022-08-18 08:26:23 | INFO | fairseq_cli.train | end of epoch 1563 (average epoch stats below)
2022-08-18 08:26:23 | INFO | train | epoch 1563 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 5880.6 | ups 1.06 | wpb 5523.2 | bsz 358 | num_updates 126603 | lr 0.000177749 | gnorm 0.361 | train_wall 41 | gb_free 10.2 | wall 123279
2022-08-18 08:26:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:26:23 | INFO | fairseq.trainer | begin training epoch 1564
2022-08-18 08:26:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:27:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:27:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:27:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:27:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:27:16 | INFO | valid | epoch 1564 | valid on 'valid' subset | loss 5.279 | nll_loss 2.696 | ppl 6.48 | bleu 56.59 | wps 1661.5 | wpb 933.5 | bsz 59.6 | num_updates 126684 | best_bleu 57.52
2022-08-18 08:27:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1564 @ 126684 updates
2022-08-18 08:27:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1564.pt
2022-08-18 08:27:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1564.pt
2022-08-18 08:27:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1564.pt (epoch 1564 @ 126684 updates, score 56.59) (writing took 34.84700249880552 seconds)
2022-08-18 08:27:51 | INFO | fairseq_cli.train | end of epoch 1564 (average epoch stats below)
2022-08-18 08:27:51 | INFO | train | epoch 1564 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5055.7 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 126684 | lr 0.000177693 | gnorm 0.481 | train_wall 41 | gb_free 10.1 | wall 123367
2022-08-18 08:27:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:27:51 | INFO | fairseq.trainer | begin training epoch 1565
2022-08-18 08:27:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:28:02 | INFO | train_inner | epoch 1565:     16 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=4084.2, ups=0.74, wpb=5493.3, bsz=357.2, num_updates=126700, lr=0.000177681, gnorm=0.496, train_wall=50, gb_free=10, wall=123378
2022-08-18 08:28:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:28:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:28:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:28:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:28:53 | INFO | valid | epoch 1565 | valid on 'valid' subset | loss 5.275 | nll_loss 2.692 | ppl 6.46 | bleu 56.7 | wps 1812.5 | wpb 933.5 | bsz 59.6 | num_updates 126765 | best_bleu 57.52
2022-08-18 08:28:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1565 @ 126765 updates
2022-08-18 08:28:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1565.pt
2022-08-18 08:28:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1565.pt
2022-08-18 08:29:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1565.pt (epoch 1565 @ 126765 updates, score 56.7) (writing took 20.232025019824505 seconds)
2022-08-18 08:29:13 | INFO | fairseq_cli.train | end of epoch 1565 (average epoch stats below)
2022-08-18 08:29:13 | INFO | train | epoch 1565 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5462.7 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 126765 | lr 0.000177636 | gnorm 0.446 | train_wall 40 | gb_free 10.2 | wall 123449
2022-08-18 08:29:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:29:13 | INFO | fairseq.trainer | begin training epoch 1566
2022-08-18 08:29:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:29:32 | INFO | train_inner | epoch 1566:     35 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6118.6, ups=1.11, wpb=5521.2, bsz=353.8, num_updates=126800, lr=0.000177611, gnorm=0.422, train_wall=49, gb_free=10, wall=123469
2022-08-18 08:29:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:29:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:29:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:29:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:30:06 | INFO | valid | epoch 1566 | valid on 'valid' subset | loss 5.274 | nll_loss 2.689 | ppl 6.45 | bleu 55.85 | wps 1876.8 | wpb 933.5 | bsz 59.6 | num_updates 126846 | best_bleu 57.52
2022-08-18 08:30:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1566 @ 126846 updates
2022-08-18 08:30:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1566.pt
2022-08-18 08:30:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1566.pt
2022-08-18 08:30:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1566.pt (epoch 1566 @ 126846 updates, score 55.85) (writing took 16.305229015648365 seconds)
2022-08-18 08:30:23 | INFO | fairseq_cli.train | end of epoch 1566 (average epoch stats below)
2022-08-18 08:30:23 | INFO | train | epoch 1566 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6394.5 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 126846 | lr 0.000177579 | gnorm 0.419 | train_wall 40 | gb_free 10.1 | wall 123519
2022-08-18 08:30:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:30:23 | INFO | fairseq.trainer | begin training epoch 1567
2022-08-18 08:30:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:30:56 | INFO | train_inner | epoch 1567:     54 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6662.6, ups=1.2, wpb=5570.5, bsz=364, num_updates=126900, lr=0.000177541, gnorm=0.376, train_wall=50, gb_free=10, wall=123552
2022-08-18 08:31:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:31:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:31:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:31:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:31:19 | INFO | valid | epoch 1567 | valid on 'valid' subset | loss 5.272 | nll_loss 2.688 | ppl 6.45 | bleu 56.67 | wps 1888.2 | wpb 933.5 | bsz 59.6 | num_updates 126927 | best_bleu 57.52
2022-08-18 08:31:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1567 @ 126927 updates
2022-08-18 08:31:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1567.pt
2022-08-18 08:31:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1567.pt
2022-08-18 08:31:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1567.pt (epoch 1567 @ 126927 updates, score 56.67) (writing took 34.36385629698634 seconds)
2022-08-18 08:31:54 | INFO | fairseq_cli.train | end of epoch 1567 (average epoch stats below)
2022-08-18 08:31:54 | INFO | train | epoch 1567 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 4917.2 | ups 0.89 | wpb 5523.2 | bsz 358 | num_updates 126927 | lr 0.000177522 | gnorm 0.386 | train_wall 40 | gb_free 10.2 | wall 123610
2022-08-18 08:31:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:31:54 | INFO | fairseq.trainer | begin training epoch 1568
2022-08-18 08:31:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:32:34 | INFO | train_inner | epoch 1568:     73 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5627.3, ups=1.02, wpb=5513.1, bsz=356.2, num_updates=127000, lr=0.000177471, gnorm=0.413, train_wall=49, gb_free=10.1, wall=123650
2022-08-18 08:32:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:32:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:32:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:32:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:32:48 | INFO | valid | epoch 1568 | valid on 'valid' subset | loss 5.261 | nll_loss 2.675 | ppl 6.39 | bleu 56.75 | wps 1637 | wpb 933.5 | bsz 59.6 | num_updates 127008 | best_bleu 57.52
2022-08-18 08:32:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1568 @ 127008 updates
2022-08-18 08:32:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1568.pt
2022-08-18 08:32:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1568.pt
2022-08-18 08:32:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1568.pt (epoch 1568 @ 127008 updates, score 56.75) (writing took 2.4733707532286644 seconds)
2022-08-18 08:32:51 | INFO | fairseq_cli.train | end of epoch 1568 (average epoch stats below)
2022-08-18 08:32:51 | INFO | train | epoch 1568 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 7865.7 | ups 1.42 | wpb 5523.2 | bsz 358 | num_updates 127008 | lr 0.000177466 | gnorm 0.397 | train_wall 40 | gb_free 10.2 | wall 123667
2022-08-18 08:32:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:32:51 | INFO | fairseq.trainer | begin training epoch 1569
2022-08-18 08:32:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:33:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:33:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:33:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:33:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:33:43 | INFO | valid | epoch 1569 | valid on 'valid' subset | loss 5.283 | nll_loss 2.705 | ppl 6.52 | bleu 55.47 | wps 1885.7 | wpb 933.5 | bsz 59.6 | num_updates 127089 | best_bleu 57.52
2022-08-18 08:33:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1569 @ 127089 updates
2022-08-18 08:33:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1569.pt
2022-08-18 08:33:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1569.pt
2022-08-18 08:34:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1569.pt (epoch 1569 @ 127089 updates, score 55.47) (writing took 24.79984062537551 seconds)
2022-08-18 08:34:08 | INFO | fairseq_cli.train | end of epoch 1569 (average epoch stats below)
2022-08-18 08:34:08 | INFO | train | epoch 1569 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 5767.8 | ups 1.04 | wpb 5523.2 | bsz 358 | num_updates 127089 | lr 0.000177409 | gnorm 0.319 | train_wall 41 | gb_free 10.1 | wall 123745
2022-08-18 08:34:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:34:09 | INFO | fairseq.trainer | begin training epoch 1570
2022-08-18 08:34:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:34:15 | INFO | train_inner | epoch 1570:     11 / 81 loss=3.368, nll_loss=0.337, ppl=1.26, wps=5428.1, ups=0.99, wpb=5502.8, bsz=356.8, num_updates=127100, lr=0.000177401, gnorm=0.345, train_wall=50, gb_free=10.1, wall=123752
2022-08-18 08:34:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:34:59 | INFO | valid | epoch 1570 | valid on 'valid' subset | loss 5.293 | nll_loss 2.721 | ppl 6.59 | bleu 49.2 | wps 1804.8 | wpb 933.5 | bsz 59.6 | num_updates 127170 | best_bleu 57.52
2022-08-18 08:34:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1570 @ 127170 updates
2022-08-18 08:34:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1570.pt
2022-08-18 08:35:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1570.pt
2022-08-18 08:35:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1570.pt (epoch 1570 @ 127170 updates, score 49.2) (writing took 42.40007670596242 seconds)
2022-08-18 08:35:42 | INFO | fairseq_cli.train | end of epoch 1570 (average epoch stats below)
2022-08-18 08:35:42 | INFO | train | epoch 1570 | loss 3.378 | nll_loss 0.35 | ppl 1.27 | wps 4780.3 | ups 0.87 | wpb 5523.2 | bsz 358 | num_updates 127170 | lr 0.000177353 | gnorm 0.651 | train_wall 39 | gb_free 10.1 | wall 123838
2022-08-18 08:35:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:35:42 | INFO | fairseq.trainer | begin training epoch 1571
2022-08-18 08:35:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:35:59 | INFO | train_inner | epoch 1571:     30 / 81 loss=3.393, nll_loss=0.369, ppl=1.29, wps=5334.2, ups=0.97, wpb=5518.2, bsz=361.8, num_updates=127200, lr=0.000177332, gnorm=0.852, train_wall=48, gb_free=10.1, wall=123855
2022-08-18 08:36:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:36:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:36:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:36:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:36:34 | INFO | valid | epoch 1571 | valid on 'valid' subset | loss 5.266 | nll_loss 2.68 | ppl 6.41 | bleu 56.07 | wps 1803.1 | wpb 933.5 | bsz 59.6 | num_updates 127251 | best_bleu 57.52
2022-08-18 08:36:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1571 @ 127251 updates
2022-08-18 08:36:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1571.pt
2022-08-18 08:36:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1571.pt
2022-08-18 08:36:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1571.pt (epoch 1571 @ 127251 updates, score 56.07) (writing took 17.658646017313004 seconds)
2022-08-18 08:36:52 | INFO | fairseq_cli.train | end of epoch 1571 (average epoch stats below)
2022-08-18 08:36:52 | INFO | train | epoch 1571 | loss 3.394 | nll_loss 0.37 | ppl 1.29 | wps 6421.9 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 127251 | lr 0.000177296 | gnorm 0.841 | train_wall 39 | gb_free 10.1 | wall 123908
2022-08-18 08:36:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:36:52 | INFO | fairseq.trainer | begin training epoch 1572
2022-08-18 08:36:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:37:17 | INFO | train_inner | epoch 1572:     49 / 81 loss=3.374, nll_loss=0.344, ppl=1.27, wps=7037.4, ups=1.27, wpb=5524.6, bsz=353.6, num_updates=127300, lr=0.000177262, gnorm=0.576, train_wall=49, gb_free=10.1, wall=123934
2022-08-18 08:37:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:37:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:37:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:37:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:37:43 | INFO | valid | epoch 1572 | valid on 'valid' subset | loss 5.255 | nll_loss 2.667 | ppl 6.35 | bleu 56.01 | wps 1855.1 | wpb 933.5 | bsz 59.6 | num_updates 127332 | best_bleu 57.52
2022-08-18 08:37:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1572 @ 127332 updates
2022-08-18 08:37:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1572.pt
2022-08-18 08:37:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1572.pt
2022-08-18 08:38:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1572.pt (epoch 1572 @ 127332 updates, score 56.01) (writing took 20.25313415750861 seconds)
2022-08-18 08:38:03 | INFO | fairseq_cli.train | end of epoch 1572 (average epoch stats below)
2022-08-18 08:38:03 | INFO | train | epoch 1572 | loss 3.371 | nll_loss 0.341 | ppl 1.27 | wps 6239 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 127332 | lr 0.00017724 | gnorm 0.641 | train_wall 40 | gb_free 10.1 | wall 123980
2022-08-18 08:38:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:38:04 | INFO | fairseq.trainer | begin training epoch 1573
2022-08-18 08:38:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:38:39 | INFO | train_inner | epoch 1573:     68 / 81 loss=3.371, nll_loss=0.34, ppl=1.27, wps=6754, ups=1.22, wpb=5548.7, bsz=362.2, num_updates=127400, lr=0.000177192, gnorm=0.614, train_wall=50, gb_free=10, wall=124016
2022-08-18 08:38:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:38:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:38:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:38:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:38:57 | INFO | valid | epoch 1573 | valid on 'valid' subset | loss 5.279 | nll_loss 2.697 | ppl 6.48 | bleu 56.1 | wps 1541.4 | wpb 933.5 | bsz 59.6 | num_updates 127413 | best_bleu 57.52
2022-08-18 08:38:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1573 @ 127413 updates
2022-08-18 08:38:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1573.pt
2022-08-18 08:38:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1573.pt
2022-08-18 08:39:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1573.pt (epoch 1573 @ 127413 updates, score 56.1) (writing took 37.20707802847028 seconds)
2022-08-18 08:39:35 | INFO | fairseq_cli.train | end of epoch 1573 (average epoch stats below)
2022-08-18 08:39:35 | INFO | train | epoch 1573 | loss 3.37 | nll_loss 0.34 | ppl 1.27 | wps 4902.1 | ups 0.89 | wpb 5523.2 | bsz 358 | num_updates 127413 | lr 0.000177183 | gnorm 0.504 | train_wall 41 | gb_free 10.3 | wall 124071
2022-08-18 08:39:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:39:35 | INFO | fairseq.trainer | begin training epoch 1574
2022-08-18 08:39:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:40:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:40:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:40:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:40:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:40:27 | INFO | valid | epoch 1574 | valid on 'valid' subset | loss 5.272 | nll_loss 2.691 | ppl 6.46 | bleu 55.96 | wps 1887.5 | wpb 933.5 | bsz 59.6 | num_updates 127494 | best_bleu 57.52
2022-08-18 08:40:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1574 @ 127494 updates
2022-08-18 08:40:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1574.pt
2022-08-18 08:40:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1574.pt
2022-08-18 08:40:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1574.pt (epoch 1574 @ 127494 updates, score 55.96) (writing took 19.3675359711051 seconds)
2022-08-18 08:40:47 | INFO | fairseq_cli.train | end of epoch 1574 (average epoch stats below)
2022-08-18 08:40:47 | INFO | train | epoch 1574 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6201.5 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 127494 | lr 0.000177127 | gnorm 0.347 | train_wall 40 | gb_free 10.2 | wall 124143
2022-08-18 08:40:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:40:47 | INFO | fairseq.trainer | begin training epoch 1575
2022-08-18 08:40:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:40:51 | INFO | train_inner | epoch 1575:      6 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=4158.8, ups=0.76, wpb=5486.5, bsz=353, num_updates=127500, lr=0.000177123, gnorm=0.349, train_wall=50, gb_free=10, wall=124148
2022-08-18 08:41:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:41:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:41:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:41:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:41:41 | INFO | valid | epoch 1575 | valid on 'valid' subset | loss 5.279 | nll_loss 2.698 | ppl 6.49 | bleu 56.46 | wps 1923 | wpb 933.5 | bsz 59.6 | num_updates 127575 | best_bleu 57.52
2022-08-18 08:41:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1575 @ 127575 updates
2022-08-18 08:41:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1575.pt
2022-08-18 08:41:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1575.pt
2022-08-18 08:41:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1575.pt (epoch 1575 @ 127575 updates, score 56.46) (writing took 13.916232571005821 seconds)
2022-08-18 08:41:55 | INFO | fairseq_cli.train | end of epoch 1575 (average epoch stats below)
2022-08-18 08:41:55 | INFO | train | epoch 1575 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6589.4 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 127575 | lr 0.000177071 | gnorm 0.422 | train_wall 41 | gb_free 10.2 | wall 124211
2022-08-18 08:41:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:41:55 | INFO | fairseq.trainer | begin training epoch 1576
2022-08-18 08:41:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:42:09 | INFO | train_inner | epoch 1576:     25 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=7144.6, ups=1.29, wpb=5528.7, bsz=362.6, num_updates=127600, lr=0.000177054, gnorm=0.409, train_wall=51, gb_free=10.1, wall=124225
2022-08-18 08:42:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:42:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:42:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:42:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:42:46 | INFO | valid | epoch 1576 | valid on 'valid' subset | loss 5.268 | nll_loss 2.685 | ppl 6.43 | bleu 56.31 | wps 1826 | wpb 933.5 | bsz 59.6 | num_updates 127656 | best_bleu 57.52
2022-08-18 08:42:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1576 @ 127656 updates
2022-08-18 08:42:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1576.pt
2022-08-18 08:42:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1576.pt
2022-08-18 08:43:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1576.pt (epoch 1576 @ 127656 updates, score 56.31) (writing took 34.89430504664779 seconds)
2022-08-18 08:43:21 | INFO | fairseq_cli.train | end of epoch 1576 (average epoch stats below)
2022-08-18 08:43:21 | INFO | train | epoch 1576 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 5202.6 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 127656 | lr 0.000177015 | gnorm 0.334 | train_wall 39 | gb_free 10.1 | wall 124297
2022-08-18 08:43:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:43:21 | INFO | fairseq.trainer | begin training epoch 1577
2022-08-18 08:43:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:43:44 | INFO | train_inner | epoch 1577:     44 / 81 loss=3.369, nll_loss=0.337, ppl=1.26, wps=5810.9, ups=1.05, wpb=5542.6, bsz=353, num_updates=127700, lr=0.000176984, gnorm=0.412, train_wall=49, gb_free=10, wall=124320
2022-08-18 08:44:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:44:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:44:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:44:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:44:12 | INFO | valid | epoch 1577 | valid on 'valid' subset | loss 5.269 | nll_loss 2.687 | ppl 6.44 | bleu 56.23 | wps 1765.4 | wpb 933.5 | bsz 59.6 | num_updates 127737 | best_bleu 57.52
2022-08-18 08:44:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1577 @ 127737 updates
2022-08-18 08:44:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1577.pt
2022-08-18 08:44:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1577.pt
2022-08-18 08:44:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1577.pt (epoch 1577 @ 127737 updates, score 56.23) (writing took 34.507327523082495 seconds)
2022-08-18 08:44:47 | INFO | fairseq_cli.train | end of epoch 1577 (average epoch stats below)
2022-08-18 08:44:47 | INFO | train | epoch 1577 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5170.6 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 127737 | lr 0.000176959 | gnorm 0.532 | train_wall 40 | gb_free 10.1 | wall 124383
2022-08-18 08:44:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:44:47 | INFO | fairseq.trainer | begin training epoch 1578
2022-08-18 08:44:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:45:20 | INFO | train_inner | epoch 1578:     63 / 81 loss=3.37, nll_loss=0.339, ppl=1.27, wps=5778, ups=1.04, wpb=5535, bsz=360.2, num_updates=127800, lr=0.000176915, gnorm=0.509, train_wall=49, gb_free=10.1, wall=124416
2022-08-18 08:45:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:45:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:45:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:45:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:45:40 | INFO | valid | epoch 1578 | valid on 'valid' subset | loss 5.257 | nll_loss 2.67 | ppl 6.36 | bleu 57.15 | wps 1693.5 | wpb 933.5 | bsz 59.6 | num_updates 127818 | best_bleu 57.52
2022-08-18 08:45:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1578 @ 127818 updates
2022-08-18 08:45:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1578.pt
2022-08-18 08:45:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1578.pt
2022-08-18 08:45:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1578.pt (epoch 1578 @ 127818 updates, score 57.15) (writing took 17.94514087587595 seconds)
2022-08-18 08:45:58 | INFO | fairseq_cli.train | end of epoch 1578 (average epoch stats below)
2022-08-18 08:45:58 | INFO | train | epoch 1578 | loss 3.37 | nll_loss 0.339 | ppl 1.26 | wps 6293.9 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 127818 | lr 0.000176903 | gnorm 0.468 | train_wall 41 | gb_free 10.1 | wall 124454
2022-08-18 08:45:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:45:58 | INFO | fairseq.trainer | begin training epoch 1579
2022-08-18 08:45:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:46:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:46:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:46:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:46:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:46:49 | INFO | valid | epoch 1579 | valid on 'valid' subset | loss 5.26 | nll_loss 2.673 | ppl 6.38 | bleu 56.18 | wps 1900.2 | wpb 933.5 | bsz 59.6 | num_updates 127899 | best_bleu 57.52
2022-08-18 08:46:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1579 @ 127899 updates
2022-08-18 08:46:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1579.pt
2022-08-18 08:46:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1579.pt
2022-08-18 08:47:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1579.pt (epoch 1579 @ 127899 updates, score 56.18) (writing took 36.432615756988525 seconds)
2022-08-18 08:47:26 | INFO | fairseq_cli.train | end of epoch 1579 (average epoch stats below)
2022-08-18 08:47:26 | INFO | train | epoch 1579 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 5099.8 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 127899 | lr 0.000176846 | gnorm 0.333 | train_wall 40 | gb_free 10.1 | wall 124542
2022-08-18 08:47:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:47:26 | INFO | fairseq.trainer | begin training epoch 1580
2022-08-18 08:47:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:47:28 | INFO | train_inner | epoch 1580:      1 / 81 loss=3.368, nll_loss=0.337, ppl=1.26, wps=4298.1, ups=0.78, wpb=5495.7, bsz=357.4, num_updates=127900, lr=0.000176846, gnorm=0.351, train_wall=51, gb_free=10.1, wall=124544
2022-08-18 08:48:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:48:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:48:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:48:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:48:18 | INFO | valid | epoch 1580 | valid on 'valid' subset | loss 5.27 | nll_loss 2.688 | ppl 6.44 | bleu 56.83 | wps 1871.7 | wpb 933.5 | bsz 59.6 | num_updates 127980 | best_bleu 57.52
2022-08-18 08:48:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1580 @ 127980 updates
2022-08-18 08:48:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1580.pt
2022-08-18 08:48:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1580.pt
2022-08-18 08:48:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1580.pt (epoch 1580 @ 127980 updates, score 56.83) (writing took 18.605473551899195 seconds)
2022-08-18 08:48:37 | INFO | fairseq_cli.train | end of epoch 1580 (average epoch stats below)
2022-08-18 08:48:37 | INFO | train | epoch 1580 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6297.8 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 127980 | lr 0.000176791 | gnorm 0.416 | train_wall 40 | gb_free 10.1 | wall 124613
2022-08-18 08:48:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:48:37 | INFO | fairseq.trainer | begin training epoch 1581
2022-08-18 08:48:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:48:48 | INFO | train_inner | epoch 1581:     20 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6881.1, ups=1.25, wpb=5525.6, bsz=359.5, num_updates=128000, lr=0.000176777, gnorm=0.414, train_wall=49, gb_free=10.1, wall=124624
2022-08-18 08:49:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:49:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:49:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:49:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:49:28 | INFO | valid | epoch 1581 | valid on 'valid' subset | loss 5.268 | nll_loss 2.685 | ppl 6.43 | bleu 56.45 | wps 1910.1 | wpb 933.5 | bsz 59.6 | num_updates 128061 | best_bleu 57.52
2022-08-18 08:49:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1581 @ 128061 updates
2022-08-18 08:49:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1581.pt
2022-08-18 08:49:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1581.pt
2022-08-18 08:49:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1581.pt (epoch 1581 @ 128061 updates, score 56.45) (writing took 29.078456547111273 seconds)
2022-08-18 08:49:57 | INFO | fairseq_cli.train | end of epoch 1581 (average epoch stats below)
2022-08-18 08:49:57 | INFO | train | epoch 1581 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5563.9 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 128061 | lr 0.000176735 | gnorm 0.358 | train_wall 40 | gb_free 10.2 | wall 124694
2022-08-18 08:49:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:49:58 | INFO | fairseq.trainer | begin training epoch 1582
2022-08-18 08:49:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:50:18 | INFO | train_inner | epoch 1582:     39 / 81 loss=3.369, nll_loss=0.337, ppl=1.26, wps=6137.9, ups=1.11, wpb=5528.9, bsz=353.4, num_updates=128100, lr=0.000176708, gnorm=0.357, train_wall=50, gb_free=10.1, wall=124714
2022-08-18 08:50:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:50:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:50:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:50:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:50:49 | INFO | valid | epoch 1582 | valid on 'valid' subset | loss 5.287 | nll_loss 2.708 | ppl 6.53 | bleu 56.58 | wps 1801.4 | wpb 933.5 | bsz 59.6 | num_updates 128142 | best_bleu 57.52
2022-08-18 08:50:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1582 @ 128142 updates
2022-08-18 08:50:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1582.pt
2022-08-18 08:50:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1582.pt
2022-08-18 08:51:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1582.pt (epoch 1582 @ 128142 updates, score 56.58) (writing took 27.460575826466084 seconds)
2022-08-18 08:51:17 | INFO | fairseq_cli.train | end of epoch 1582 (average epoch stats below)
2022-08-18 08:51:17 | INFO | train | epoch 1582 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 5618.9 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 128142 | lr 0.000176679 | gnorm 0.364 | train_wall 40 | gb_free 10.1 | wall 124773
2022-08-18 08:51:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:51:17 | INFO | fairseq.trainer | begin training epoch 1583
2022-08-18 08:51:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:51:48 | INFO | train_inner | epoch 1583:     58 / 81 loss=3.368, nll_loss=0.337, ppl=1.26, wps=6145.6, ups=1.11, wpb=5524.4, bsz=360.7, num_updates=128200, lr=0.000176639, gnorm=0.303, train_wall=49, gb_free=10, wall=124804
2022-08-18 08:51:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:52:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:52:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:52:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:52:08 | INFO | valid | epoch 1583 | valid on 'valid' subset | loss 5.263 | nll_loss 2.675 | ppl 6.39 | bleu 56.96 | wps 1849.7 | wpb 933.5 | bsz 59.6 | num_updates 128223 | best_bleu 57.52
2022-08-18 08:52:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1583 @ 128223 updates
2022-08-18 08:52:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1583.pt
2022-08-18 08:52:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1583.pt
2022-08-18 08:52:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1583.pt (epoch 1583 @ 128223 updates, score 56.96) (writing took 17.6890906766057 seconds)
2022-08-18 08:52:26 | INFO | fairseq_cli.train | end of epoch 1583 (average epoch stats below)
2022-08-18 08:52:26 | INFO | train | epoch 1583 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 6456.8 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 128223 | lr 0.000176623 | gnorm 0.311 | train_wall 40 | gb_free 10.1 | wall 124843
2022-08-18 08:52:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:52:27 | INFO | fairseq.trainer | begin training epoch 1584
2022-08-18 08:52:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:53:07 | INFO | train_inner | epoch 1584:     77 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=7005.1, ups=1.27, wpb=5525.7, bsz=358.5, num_updates=128300, lr=0.00017657, gnorm=0.365, train_wall=49, gb_free=10.1, wall=124883
2022-08-18 08:53:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:53:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:53:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:53:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:53:19 | INFO | valid | epoch 1584 | valid on 'valid' subset | loss 5.288 | nll_loss 2.703 | ppl 6.51 | bleu 56.35 | wps 1670.4 | wpb 933.5 | bsz 59.6 | num_updates 128304 | best_bleu 57.52
2022-08-18 08:53:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1584 @ 128304 updates
2022-08-18 08:53:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1584.pt
2022-08-18 08:53:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1584.pt
2022-08-18 08:53:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1584.pt (epoch 1584 @ 128304 updates, score 56.35) (writing took 14.113315004855394 seconds)
2022-08-18 08:53:33 | INFO | fairseq_cli.train | end of epoch 1584 (average epoch stats below)
2022-08-18 08:53:33 | INFO | train | epoch 1584 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6675 | ups 1.21 | wpb 5523.2 | bsz 358 | num_updates 128304 | lr 0.000176567 | gnorm 0.354 | train_wall 40 | gb_free 10.1 | wall 124910
2022-08-18 08:53:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:53:34 | INFO | fairseq.trainer | begin training epoch 1585
2022-08-18 08:53:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:54:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:54:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:54:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:54:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:54:37 | INFO | valid | epoch 1585 | valid on 'valid' subset | loss 5.298 | nll_loss 2.718 | ppl 6.58 | bleu 56.37 | wps 1575.2 | wpb 933.5 | bsz 59.6 | num_updates 128385 | best_bleu 57.52
2022-08-18 08:54:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1585 @ 128385 updates
2022-08-18 08:54:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1585.pt
2022-08-18 08:54:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1585.pt
2022-08-18 08:55:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1585.pt (epoch 1585 @ 128385 updates, score 56.37) (writing took 50.492283914238214 seconds)
2022-08-18 08:55:28 | INFO | fairseq_cli.train | end of epoch 1585 (average epoch stats below)
2022-08-18 08:55:28 | INFO | train | epoch 1585 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 3914.8 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 128385 | lr 0.000176511 | gnorm 0.464 | train_wall 50 | gb_free 10.3 | wall 125024
2022-08-18 08:55:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:55:28 | INFO | fairseq.trainer | begin training epoch 1586
2022-08-18 08:55:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:55:56 | INFO | train_inner | epoch 1586:     15 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=3274, ups=0.59, wpb=5521.4, bsz=358.1, num_updates=128400, lr=0.000176501, gnorm=0.441, train_wall=79, gb_free=10, wall=125052
2022-08-18 08:56:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:56:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:56:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:56:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:56:39 | INFO | valid | epoch 1586 | valid on 'valid' subset | loss 5.295 | nll_loss 2.715 | ppl 6.57 | bleu 56.28 | wps 1987.5 | wpb 933.5 | bsz 59.6 | num_updates 128466 | best_bleu 57.52
2022-08-18 08:56:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1586 @ 128466 updates
2022-08-18 08:56:39 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1586.pt
2022-08-18 08:56:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1586.pt
2022-08-18 08:57:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1586.pt (epoch 1586 @ 128466 updates, score 56.28) (writing took 42.45483659952879 seconds)
2022-08-18 08:57:21 | INFO | fairseq_cli.train | end of epoch 1586 (average epoch stats below)
2022-08-18 08:57:21 | INFO | train | epoch 1586 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 3933 | ups 0.71 | wpb 5523.2 | bsz 358 | num_updates 128466 | lr 0.000176456 | gnorm 0.421 | train_wall 60 | gb_free 10.3 | wall 125138
2022-08-18 08:57:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:57:22 | INFO | fairseq.trainer | begin training epoch 1587
2022-08-18 08:57:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:57:41 | INFO | train_inner | epoch 1587:     34 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5253.6, ups=0.95, wpb=5508.2, bsz=359.6, num_updates=128500, lr=0.000176432, gnorm=0.417, train_wall=50, gb_free=10.1, wall=125157
2022-08-18 08:58:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:58:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:58:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:58:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:58:13 | INFO | valid | epoch 1587 | valid on 'valid' subset | loss 5.286 | nll_loss 2.706 | ppl 6.52 | bleu 56.25 | wps 1822.5 | wpb 933.5 | bsz 59.6 | num_updates 128547 | best_bleu 57.52
2022-08-18 08:58:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1587 @ 128547 updates
2022-08-18 08:58:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1587.pt
2022-08-18 08:58:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1587.pt
2022-08-18 08:58:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1587.pt (epoch 1587 @ 128547 updates, score 56.25) (writing took 14.370187416672707 seconds)
2022-08-18 08:58:28 | INFO | fairseq_cli.train | end of epoch 1587 (average epoch stats below)
2022-08-18 08:58:28 | INFO | train | epoch 1587 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6734.9 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 128547 | lr 0.0001764 | gnorm 0.548 | train_wall 40 | gb_free 10.1 | wall 125204
2022-08-18 08:58:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:58:28 | INFO | fairseq.trainer | begin training epoch 1588
2022-08-18 08:58:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 08:58:57 | INFO | train_inner | epoch 1588:     53 / 81 loss=3.369, nll_loss=0.339, ppl=1.26, wps=7279.2, ups=1.31, wpb=5544.2, bsz=360.5, num_updates=128600, lr=0.000176364, gnorm=0.528, train_wall=50, gb_free=10.1, wall=125233
2022-08-18 08:59:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 08:59:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 08:59:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 08:59:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 08:59:21 | INFO | valid | epoch 1588 | valid on 'valid' subset | loss 5.27 | nll_loss 2.685 | ppl 6.43 | bleu 56.68 | wps 1805.6 | wpb 933.5 | bsz 59.6 | num_updates 128628 | best_bleu 57.52
2022-08-18 08:59:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1588 @ 128628 updates
2022-08-18 08:59:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1588.pt
2022-08-18 08:59:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1588.pt
2022-08-18 08:59:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1588.pt (epoch 1588 @ 128628 updates, score 56.68) (writing took 28.424630604684353 seconds)
2022-08-18 08:59:49 | INFO | fairseq_cli.train | end of epoch 1588 (average epoch stats below)
2022-08-18 08:59:49 | INFO | train | epoch 1588 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5492.4 | ups 0.99 | wpb 5523.2 | bsz 358 | num_updates 128628 | lr 0.000176345 | gnorm 0.424 | train_wall 41 | gb_free 10.2 | wall 125286
2022-08-18 08:59:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 08:59:49 | INFO | fairseq.trainer | begin training epoch 1589
2022-08-18 08:59:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:00:28 | INFO | train_inner | epoch 1589:     72 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6075.5, ups=1.1, wpb=5534.5, bsz=354.4, num_updates=128700, lr=0.000176295, gnorm=0.379, train_wall=50, gb_free=10.1, wall=125324
2022-08-18 09:00:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:00:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:00:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:00:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:00:41 | INFO | valid | epoch 1589 | valid on 'valid' subset | loss 5.298 | nll_loss 2.72 | ppl 6.59 | bleu 55.97 | wps 1876.8 | wpb 933.5 | bsz 59.6 | num_updates 128709 | best_bleu 57.52
2022-08-18 09:00:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1589 @ 128709 updates
2022-08-18 09:00:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1589.pt
2022-08-18 09:00:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1589.pt
2022-08-18 09:01:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1589.pt (epoch 1589 @ 128709 updates, score 55.97) (writing took 37.465160835534334 seconds)
2022-08-18 09:01:19 | INFO | fairseq_cli.train | end of epoch 1589 (average epoch stats below)
2022-08-18 09:01:19 | INFO | train | epoch 1589 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 4984.9 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 128709 | lr 0.000176289 | gnorm 0.352 | train_wall 40 | gb_free 10.2 | wall 125375
2022-08-18 09:01:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:01:19 | INFO | fairseq.trainer | begin training epoch 1590
2022-08-18 09:01:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:02:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:02:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:02:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:02:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:02:12 | INFO | valid | epoch 1590 | valid on 'valid' subset | loss 5.28 | nll_loss 2.7 | ppl 6.5 | bleu 56.76 | wps 1737.3 | wpb 933.5 | bsz 59.6 | num_updates 128790 | best_bleu 57.52
2022-08-18 09:02:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1590 @ 128790 updates
2022-08-18 09:02:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1590.pt
2022-08-18 09:02:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1590.pt
2022-08-18 09:02:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1590.pt (epoch 1590 @ 128790 updates, score 56.76) (writing took 18.463524762541056 seconds)
2022-08-18 09:02:30 | INFO | fairseq_cli.train | end of epoch 1590 (average epoch stats below)
2022-08-18 09:02:30 | INFO | train | epoch 1590 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 6275.3 | ups 1.14 | wpb 5523.2 | bsz 358 | num_updates 128790 | lr 0.000176234 | gnorm 0.378 | train_wall 40 | gb_free 10.1 | wall 125447
2022-08-18 09:02:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:02:31 | INFO | fairseq.trainer | begin training epoch 1591
2022-08-18 09:02:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:02:37 | INFO | train_inner | epoch 1591:     10 / 81 loss=3.368, nll_loss=0.337, ppl=1.26, wps=4257.6, ups=0.77, wpb=5515.9, bsz=362.4, num_updates=128800, lr=0.000176227, gnorm=0.37, train_wall=50, gb_free=10.1, wall=125454
2022-08-18 09:03:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:03:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:03:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:03:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:03:22 | INFO | valid | epoch 1591 | valid on 'valid' subset | loss 5.294 | nll_loss 2.714 | ppl 6.56 | bleu 56.56 | wps 1820.6 | wpb 933.5 | bsz 59.6 | num_updates 128871 | best_bleu 57.52
2022-08-18 09:03:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1591 @ 128871 updates
2022-08-18 09:03:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1591.pt
2022-08-18 09:03:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1591.pt
2022-08-18 09:03:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1591.pt (epoch 1591 @ 128871 updates, score 56.56) (writing took 20.869975235313177 seconds)
2022-08-18 09:03:43 | INFO | fairseq_cli.train | end of epoch 1591 (average epoch stats below)
2022-08-18 09:03:43 | INFO | train | epoch 1591 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 6177.5 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 128871 | lr 0.000176178 | gnorm 0.358 | train_wall 40 | gb_free 10.1 | wall 125519
2022-08-18 09:03:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:03:43 | INFO | fairseq.trainer | begin training epoch 1592
2022-08-18 09:03:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:03:59 | INFO | train_inner | epoch 1592:     29 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6783.9, ups=1.23, wpb=5507.2, bsz=352.6, num_updates=128900, lr=0.000176158, gnorm=0.409, train_wall=49, gb_free=10.1, wall=125535
2022-08-18 09:04:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:04:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:04:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:04:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:04:35 | INFO | valid | epoch 1592 | valid on 'valid' subset | loss 5.289 | nll_loss 2.706 | ppl 6.53 | bleu 55.97 | wps 1621.8 | wpb 933.5 | bsz 59.6 | num_updates 128952 | best_bleu 57.52
2022-08-18 09:04:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1592 @ 128952 updates
2022-08-18 09:04:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1592.pt
2022-08-18 09:04:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1592.pt
2022-08-18 09:05:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1592.pt (epoch 1592 @ 128952 updates, score 55.97) (writing took 49.945046216249466 seconds)
2022-08-18 09:05:25 | INFO | fairseq_cli.train | end of epoch 1592 (average epoch stats below)
2022-08-18 09:05:25 | INFO | train | epoch 1592 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 4368.5 | ups 0.79 | wpb 5523.2 | bsz 358 | num_updates 128952 | lr 0.000176123 | gnorm 0.473 | train_wall 40 | gb_free 10.1 | wall 125621
2022-08-18 09:05:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:05:25 | INFO | fairseq.trainer | begin training epoch 1593
2022-08-18 09:05:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:05:51 | INFO | train_inner | epoch 1593:     48 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=4892.4, ups=0.89, wpb=5521.6, bsz=360.4, num_updates=129000, lr=0.00017609, gnorm=0.41, train_wall=49, gb_free=10, wall=125648
2022-08-18 09:06:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:06:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:06:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:06:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:06:19 | INFO | valid | epoch 1593 | valid on 'valid' subset | loss 5.308 | nll_loss 2.732 | ppl 6.65 | bleu 55.79 | wps 1483.5 | wpb 933.5 | bsz 59.6 | num_updates 129033 | best_bleu 57.52
2022-08-18 09:06:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1593 @ 129033 updates
2022-08-18 09:06:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1593.pt
2022-08-18 09:06:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1593.pt
2022-08-18 09:06:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1593.pt (epoch 1593 @ 129033 updates, score 55.79) (writing took 15.35968204960227 seconds)
2022-08-18 09:06:35 | INFO | fairseq_cli.train | end of epoch 1593 (average epoch stats below)
2022-08-18 09:06:35 | INFO | train | epoch 1593 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6430.2 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 129033 | lr 0.000176068 | gnorm 0.389 | train_wall 40 | gb_free 10.2 | wall 125691
2022-08-18 09:06:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:06:35 | INFO | fairseq.trainer | begin training epoch 1594
2022-08-18 09:06:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:07:08 | INFO | train_inner | epoch 1594:     67 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=7213, ups=1.3, wpb=5538.1, bsz=353.8, num_updates=129100, lr=0.000176022, gnorm=0.451, train_wall=48, gb_free=10.1, wall=125724
2022-08-18 09:07:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:07:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:07:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:07:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:07:24 | INFO | valid | epoch 1594 | valid on 'valid' subset | loss 5.29 | nll_loss 2.712 | ppl 6.55 | bleu 55.91 | wps 1796.6 | wpb 933.5 | bsz 59.6 | num_updates 129114 | best_bleu 57.52
2022-08-18 09:07:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1594 @ 129114 updates
2022-08-18 09:07:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1594.pt
2022-08-18 09:07:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1594.pt
2022-08-18 09:07:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1594.pt (epoch 1594 @ 129114 updates, score 55.91) (writing took 27.09259334951639 seconds)
2022-08-18 09:07:52 | INFO | fairseq_cli.train | end of epoch 1594 (average epoch stats below)
2022-08-18 09:07:52 | INFO | train | epoch 1594 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5820.2 | ups 1.05 | wpb 5523.2 | bsz 358 | num_updates 129114 | lr 0.000176012 | gnorm 0.439 | train_wall 38 | gb_free 10 | wall 125768
2022-08-18 09:07:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:07:52 | INFO | fairseq.trainer | begin training epoch 1595
2022-08-18 09:07:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:08:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:08:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:08:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:08:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:08:43 | INFO | valid | epoch 1595 | valid on 'valid' subset | loss 5.286 | nll_loss 2.706 | ppl 6.52 | bleu 55.99 | wps 1804.8 | wpb 933.5 | bsz 59.6 | num_updates 129195 | best_bleu 57.52
2022-08-18 09:08:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1595 @ 129195 updates
2022-08-18 09:08:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1595.pt
2022-08-18 09:08:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1595.pt
2022-08-18 09:09:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1595.pt (epoch 1595 @ 129195 updates, score 55.99) (writing took 44.23401294276118 seconds)
2022-08-18 09:09:27 | INFO | fairseq_cli.train | end of epoch 1595 (average epoch stats below)
2022-08-18 09:09:27 | INFO | train | epoch 1595 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 4675.6 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 129195 | lr 0.000175957 | gnorm 0.357 | train_wall 39 | gb_free 10.2 | wall 125864
2022-08-18 09:09:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:09:28 | INFO | fairseq.trainer | begin training epoch 1596
2022-08-18 09:09:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:09:31 | INFO | train_inner | epoch 1596:      5 / 81 loss=3.368, nll_loss=0.337, ppl=1.26, wps=3853.7, ups=0.7, wpb=5507.8, bsz=361, num_updates=129200, lr=0.000175954, gnorm=0.353, train_wall=48, gb_free=10, wall=125867
2022-08-18 09:10:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:10:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:10:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:10:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:10:20 | INFO | valid | epoch 1596 | valid on 'valid' subset | loss 5.299 | nll_loss 2.723 | ppl 6.6 | bleu 56.18 | wps 1859.4 | wpb 933.5 | bsz 59.6 | num_updates 129276 | best_bleu 57.52
2022-08-18 09:10:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1596 @ 129276 updates
2022-08-18 09:10:20 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1596.pt
2022-08-18 09:10:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1596.pt
2022-08-18 09:10:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1596.pt (epoch 1596 @ 129276 updates, score 56.18) (writing took 21.8247077614069 seconds)
2022-08-18 09:10:42 | INFO | fairseq_cli.train | end of epoch 1596 (average epoch stats below)
2022-08-18 09:10:42 | INFO | train | epoch 1596 | loss 3.369 | nll_loss 0.337 | ppl 1.26 | wps 6031.7 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 129276 | lr 0.000175902 | gnorm 0.357 | train_wall 40 | gb_free 10.2 | wall 125938
2022-08-18 09:10:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:10:42 | INFO | fairseq.trainer | begin training epoch 1597
2022-08-18 09:10:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:10:55 | INFO | train_inner | epoch 1597:     24 / 81 loss=3.369, nll_loss=0.337, ppl=1.26, wps=6629.6, ups=1.2, wpb=5540.7, bsz=362.1, num_updates=129300, lr=0.000175886, gnorm=0.402, train_wall=49, gb_free=10.1, wall=125951
2022-08-18 09:11:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:11:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:11:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:11:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:11:33 | INFO | valid | epoch 1597 | valid on 'valid' subset | loss 5.285 | nll_loss 2.705 | ppl 6.52 | bleu 56.1 | wps 1885.5 | wpb 933.5 | bsz 59.6 | num_updates 129357 | best_bleu 57.52
2022-08-18 09:11:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1597 @ 129357 updates
2022-08-18 09:11:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1597.pt
2022-08-18 09:11:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1597.pt
2022-08-18 09:12:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1597.pt (epoch 1597 @ 129357 updates, score 56.1) (writing took 28.83644498884678 seconds)
2022-08-18 09:12:02 | INFO | fairseq_cli.train | end of epoch 1597 (average epoch stats below)
2022-08-18 09:12:02 | INFO | train | epoch 1597 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5524.2 | ups 1 | wpb 5523.2 | bsz 358 | num_updates 129357 | lr 0.000175847 | gnorm 0.436 | train_wall 41 | gb_free 10.1 | wall 126019
2022-08-18 09:12:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:12:03 | INFO | fairseq.trainer | begin training epoch 1598
2022-08-18 09:12:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:12:26 | INFO | train_inner | epoch 1598:     43 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6084.9, ups=1.1, wpb=5537.2, bsz=357.1, num_updates=129400, lr=0.000175818, gnorm=0.373, train_wall=51, gb_free=10, wall=126042
2022-08-18 09:12:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:12:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:12:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:12:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:12:56 | INFO | valid | epoch 1598 | valid on 'valid' subset | loss 5.291 | nll_loss 2.712 | ppl 6.55 | bleu 56.42 | wps 1560.7 | wpb 933.5 | bsz 59.6 | num_updates 129438 | best_bleu 57.52
2022-08-18 09:12:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1598 @ 129438 updates
2022-08-18 09:12:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1598.pt
2022-08-18 09:12:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1598.pt
2022-08-18 09:13:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1598.pt (epoch 1598 @ 129438 updates, score 56.42) (writing took 35.38099966943264 seconds)
2022-08-18 09:13:32 | INFO | fairseq_cli.train | end of epoch 1598 (average epoch stats below)
2022-08-18 09:13:32 | INFO | train | epoch 1598 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 5013.1 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 129438 | lr 0.000175792 | gnorm 0.395 | train_wall 41 | gb_free 10.1 | wall 126108
2022-08-18 09:13:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:13:32 | INFO | fairseq.trainer | begin training epoch 1599
2022-08-18 09:13:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:14:05 | INFO | train_inner | epoch 1599:     62 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5554.5, ups=1.01, wpb=5522.1, bsz=356.2, num_updates=129500, lr=0.00017575, gnorm=0.379, train_wall=51, gb_free=10, wall=126141
2022-08-18 09:14:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:14:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:14:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:14:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:14:23 | INFO | valid | epoch 1599 | valid on 'valid' subset | loss 5.293 | nll_loss 2.712 | ppl 6.55 | bleu 56.38 | wps 1930.3 | wpb 933.5 | bsz 59.6 | num_updates 129519 | best_bleu 57.52
2022-08-18 09:14:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1599 @ 129519 updates
2022-08-18 09:14:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1599.pt
2022-08-18 09:14:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1599.pt
2022-08-18 09:14:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1599.pt (epoch 1599 @ 129519 updates, score 56.38) (writing took 19.110188711434603 seconds)
2022-08-18 09:14:42 | INFO | fairseq_cli.train | end of epoch 1599 (average epoch stats below)
2022-08-18 09:14:42 | INFO | train | epoch 1599 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6352.8 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 129519 | lr 0.000175737 | gnorm 0.334 | train_wall 40 | gb_free 10 | wall 126178
2022-08-18 09:14:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:14:42 | INFO | fairseq.trainer | begin training epoch 1600
2022-08-18 09:14:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:15:27 | INFO | train_inner | epoch 1600:     81 / 81 loss=3.368, nll_loss=0.337, ppl=1.26, wps=6693.9, ups=1.22, wpb=5488, bsz=355.8, num_updates=129600, lr=0.000175682, gnorm=0.365, train_wall=49, gb_free=10.1, wall=126223
2022-08-18 09:15:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:15:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:15:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:15:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:15:36 | INFO | valid | epoch 1600 | valid on 'valid' subset | loss 5.299 | nll_loss 2.721 | ppl 6.59 | bleu 56.34 | wps 1888.3 | wpb 933.5 | bsz 59.6 | num_updates 129600 | best_bleu 57.52
2022-08-18 09:15:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1600 @ 129600 updates
2022-08-18 09:15:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1600.pt
2022-08-18 09:15:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1600.pt
2022-08-18 09:15:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1600.pt (epoch 1600 @ 129600 updates, score 56.34) (writing took 14.557915933430195 seconds)
2022-08-18 09:15:51 | INFO | fairseq_cli.train | end of epoch 1600 (average epoch stats below)
2022-08-18 09:15:51 | INFO | train | epoch 1600 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 6495 | ups 1.18 | wpb 5523.2 | bsz 358 | num_updates 129600 | lr 0.000175682 | gnorm 0.378 | train_wall 40 | gb_free 10.1 | wall 126247
2022-08-18 09:15:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:15:51 | INFO | fairseq.trainer | begin training epoch 1601
2022-08-18 09:15:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:16:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:16:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:16:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:16:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:16:43 | INFO | valid | epoch 1601 | valid on 'valid' subset | loss 5.289 | nll_loss 2.711 | ppl 6.55 | bleu 56.39 | wps 1867.3 | wpb 933.5 | bsz 59.6 | num_updates 129681 | best_bleu 57.52
2022-08-18 09:16:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1601 @ 129681 updates
2022-08-18 09:16:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1601.pt
2022-08-18 09:16:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1601.pt
2022-08-18 09:17:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1601.pt (epoch 1601 @ 129681 updates, score 56.39) (writing took 29.048883844166994 seconds)
2022-08-18 09:17:12 | INFO | fairseq_cli.train | end of epoch 1601 (average epoch stats below)
2022-08-18 09:17:12 | INFO | train | epoch 1601 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 5511.2 | ups 1 | wpb 5523.2 | bsz 358 | num_updates 129681 | lr 0.000175627 | gnorm 1.031 | train_wall 41 | gb_free 10.1 | wall 126328
2022-08-18 09:17:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:17:12 | INFO | fairseq.trainer | begin training epoch 1602
2022-08-18 09:17:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:17:23 | INFO | train_inner | epoch 1602:     19 / 81 loss=3.368, nll_loss=0.337, ppl=1.26, wps=4745.6, ups=0.86, wpb=5518.9, bsz=358, num_updates=129700, lr=0.000175614, gnorm=0.912, train_wall=51, gb_free=10.1, wall=126340
2022-08-18 09:17:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:17:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:17:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:17:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:18:04 | INFO | valid | epoch 1602 | valid on 'valid' subset | loss 5.287 | nll_loss 2.708 | ppl 6.54 | bleu 55.57 | wps 1943.2 | wpb 933.5 | bsz 59.6 | num_updates 129762 | best_bleu 57.52
2022-08-18 09:18:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1602 @ 129762 updates
2022-08-18 09:18:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1602.pt
2022-08-18 09:18:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1602.pt
2022-08-18 09:18:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1602.pt (epoch 1602 @ 129762 updates, score 55.57) (writing took 15.895511902868748 seconds)
2022-08-18 09:18:20 | INFO | fairseq_cli.train | end of epoch 1602 (average epoch stats below)
2022-08-18 09:18:20 | INFO | train | epoch 1602 | loss 3.369 | nll_loss 0.339 | ppl 1.26 | wps 6617.3 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 129762 | lr 0.000175572 | gnorm 0.491 | train_wall 41 | gb_free 10.1 | wall 126396
2022-08-18 09:18:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:18:20 | INFO | fairseq.trainer | begin training epoch 1603
2022-08-18 09:18:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:18:40 | INFO | train_inner | epoch 1603:     38 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=7230.8, ups=1.3, wpb=5541, bsz=357.4, num_updates=129800, lr=0.000175547, gnorm=0.458, train_wall=50, gb_free=10.1, wall=126416
2022-08-18 09:19:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:19:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:19:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:19:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:19:11 | INFO | valid | epoch 1603 | valid on 'valid' subset | loss 5.309 | nll_loss 2.736 | ppl 6.66 | bleu 55.42 | wps 1896.7 | wpb 933.5 | bsz 59.6 | num_updates 129843 | best_bleu 57.52
2022-08-18 09:19:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1603 @ 129843 updates
2022-08-18 09:19:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1603.pt
2022-08-18 09:19:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1603.pt
2022-08-18 09:19:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1603.pt (epoch 1603 @ 129843 updates, score 55.42) (writing took 32.990052938461304 seconds)
2022-08-18 09:19:44 | INFO | fairseq_cli.train | end of epoch 1603 (average epoch stats below)
2022-08-18 09:19:44 | INFO | train | epoch 1603 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 5337.2 | ups 0.97 | wpb 5523.2 | bsz 358 | num_updates 129843 | lr 0.000175518 | gnorm 0.526 | train_wall 40 | gb_free 10.2 | wall 126480
2022-08-18 09:19:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:19:44 | INFO | fairseq.trainer | begin training epoch 1604
2022-08-18 09:19:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:20:16 | INFO | train_inner | epoch 1604:     57 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5750.8, ups=1.04, wpb=5526.3, bsz=358, num_updates=129900, lr=0.000175479, gnorm=0.53, train_wall=50, gb_free=10.1, wall=126512
2022-08-18 09:20:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:20:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:20:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:20:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:20:39 | INFO | valid | epoch 1604 | valid on 'valid' subset | loss 5.296 | nll_loss 2.72 | ppl 6.59 | bleu 55.58 | wps 1870.8 | wpb 933.5 | bsz 59.6 | num_updates 129924 | best_bleu 57.52
2022-08-18 09:20:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1604 @ 129924 updates
2022-08-18 09:20:39 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1604.pt
2022-08-18 09:20:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1604.pt
2022-08-18 09:21:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1604.pt (epoch 1604 @ 129924 updates, score 55.58) (writing took 24.327646102756262 seconds)
2022-08-18 09:21:03 | INFO | fairseq_cli.train | end of epoch 1604 (average epoch stats below)
2022-08-18 09:21:03 | INFO | train | epoch 1604 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5630.7 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 129924 | lr 0.000175463 | gnorm 0.424 | train_wall 41 | gb_free 10.2 | wall 126559
2022-08-18 09:21:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:21:03 | INFO | fairseq.trainer | begin training epoch 1605
2022-08-18 09:21:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:21:45 | INFO | train_inner | epoch 1605:     76 / 81 loss=3.368, nll_loss=0.337, ppl=1.26, wps=6236, ups=1.13, wpb=5522.3, bsz=359.3, num_updates=130000, lr=0.000175412, gnorm=0.377, train_wall=50, gb_free=10.1, wall=126601
2022-08-18 09:21:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:21:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:21:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:21:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:21:57 | INFO | valid | epoch 1605 | valid on 'valid' subset | loss 5.293 | nll_loss 2.714 | ppl 6.56 | bleu 55.83 | wps 1789.6 | wpb 933.5 | bsz 59.6 | num_updates 130005 | best_bleu 57.52
2022-08-18 09:21:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1605 @ 130005 updates
2022-08-18 09:21:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1605.pt
2022-08-18 09:21:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1605.pt
2022-08-18 09:22:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1605.pt (epoch 1605 @ 130005 updates, score 55.83) (writing took 16.87083264812827 seconds)
2022-08-18 09:22:14 | INFO | fairseq_cli.train | end of epoch 1605 (average epoch stats below)
2022-08-18 09:22:14 | INFO | train | epoch 1605 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 6343.2 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 130005 | lr 0.000175408 | gnorm 0.378 | train_wall 41 | gb_free 10.1 | wall 126630
2022-08-18 09:22:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:22:14 | INFO | fairseq.trainer | begin training epoch 1606
2022-08-18 09:22:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:22:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:23:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:23:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:23:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:23:08 | INFO | valid | epoch 1606 | valid on 'valid' subset | loss 5.283 | nll_loss 2.703 | ppl 6.51 | bleu 56.51 | wps 1757.9 | wpb 933.5 | bsz 59.6 | num_updates 130086 | best_bleu 57.52
2022-08-18 09:23:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1606 @ 130086 updates
2022-08-18 09:23:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1606.pt
2022-08-18 09:23:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1606.pt
2022-08-18 09:23:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1606.pt (epoch 1606 @ 130086 updates, score 56.51) (writing took 34.906214378774166 seconds)
2022-08-18 09:23:43 | INFO | fairseq_cli.train | end of epoch 1606 (average epoch stats below)
2022-08-18 09:23:43 | INFO | train | epoch 1606 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 4986.2 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 130086 | lr 0.000175354 | gnorm 0.366 | train_wall 42 | gb_free 10.1 | wall 126720
2022-08-18 09:23:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:23:44 | INFO | fairseq.trainer | begin training epoch 1607
2022-08-18 09:23:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:23:52 | INFO | train_inner | epoch 1607:     14 / 81 loss=3.368, nll_loss=0.337, ppl=1.26, wps=4343.2, ups=0.79, wpb=5516.6, bsz=357.2, num_updates=130100, lr=0.000175344, gnorm=0.369, train_wall=50, gb_free=10.1, wall=126728
2022-08-18 09:24:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:24:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:24:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:24:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:24:35 | INFO | valid | epoch 1607 | valid on 'valid' subset | loss 5.288 | nll_loss 2.708 | ppl 6.54 | bleu 56.05 | wps 1847.5 | wpb 933.5 | bsz 59.6 | num_updates 130167 | best_bleu 57.52
2022-08-18 09:24:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1607 @ 130167 updates
2022-08-18 09:24:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1607.pt
2022-08-18 09:24:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1607.pt
2022-08-18 09:24:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1607.pt (epoch 1607 @ 130167 updates, score 56.05) (writing took 20.936018835753202 seconds)
2022-08-18 09:24:56 | INFO | fairseq_cli.train | end of epoch 1607 (average epoch stats below)
2022-08-18 09:24:56 | INFO | train | epoch 1607 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 6155.8 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 130167 | lr 0.000175299 | gnorm 0.356 | train_wall 40 | gb_free 10.1 | wall 126792
2022-08-18 09:24:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:24:56 | INFO | fairseq.trainer | begin training epoch 1608
2022-08-18 09:24:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:25:14 | INFO | train_inner | epoch 1608:     33 / 81 loss=3.369, nll_loss=0.337, ppl=1.26, wps=6692.1, ups=1.21, wpb=5527.2, bsz=361.3, num_updates=130200, lr=0.000175277, gnorm=0.394, train_wall=49, gb_free=10.1, wall=126811
2022-08-18 09:25:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:25:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:25:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:25:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:25:49 | INFO | valid | epoch 1608 | valid on 'valid' subset | loss 5.289 | nll_loss 2.711 | ppl 6.55 | bleu 55.85 | wps 1782.5 | wpb 933.5 | bsz 59.6 | num_updates 130248 | best_bleu 57.52
2022-08-18 09:25:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1608 @ 130248 updates
2022-08-18 09:25:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1608.pt
2022-08-18 09:25:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1608.pt
2022-08-18 09:26:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1608.pt (epoch 1608 @ 130248 updates, score 55.85) (writing took 38.32970001921058 seconds)
2022-08-18 09:26:28 | INFO | fairseq_cli.train | end of epoch 1608 (average epoch stats below)
2022-08-18 09:26:28 | INFO | train | epoch 1608 | loss 3.369 | nll_loss 0.339 | ppl 1.26 | wps 4874.6 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 130248 | lr 0.000175245 | gnorm 0.492 | train_wall 40 | gb_free 10.2 | wall 126884
2022-08-18 09:26:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:26:28 | INFO | fairseq.trainer | begin training epoch 1609
2022-08-18 09:26:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:26:57 | INFO | train_inner | epoch 1609:     52 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5391.4, ups=0.97, wpb=5537.2, bsz=356.7, num_updates=130300, lr=0.00017521, gnorm=0.416, train_wall=50, gb_free=10.1, wall=126913
2022-08-18 09:27:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:27:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:27:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:27:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:27:21 | INFO | valid | epoch 1609 | valid on 'valid' subset | loss 5.289 | nll_loss 2.711 | ppl 6.55 | bleu 56.17 | wps 1785.4 | wpb 933.5 | bsz 59.6 | num_updates 130329 | best_bleu 57.52
2022-08-18 09:27:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1609 @ 130329 updates
2022-08-18 09:27:21 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1609.pt
2022-08-18 09:27:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1609.pt
2022-08-18 09:27:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1609.pt (epoch 1609 @ 130329 updates, score 56.17) (writing took 34.074516244232655 seconds)
2022-08-18 09:27:55 | INFO | fairseq_cli.train | end of epoch 1609 (average epoch stats below)
2022-08-18 09:27:55 | INFO | train | epoch 1609 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5142.5 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 130329 | lr 0.00017519 | gnorm 0.359 | train_wall 40 | gb_free 10.2 | wall 126971
2022-08-18 09:27:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:27:55 | INFO | fairseq.trainer | begin training epoch 1610
2022-08-18 09:27:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:28:33 | INFO | train_inner | epoch 1610:     71 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5735.6, ups=1.04, wpb=5509.9, bsz=360.7, num_updates=130400, lr=0.000175142, gnorm=0.422, train_wall=49, gb_free=10, wall=127009
2022-08-18 09:28:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:28:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:28:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:28:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:28:48 | INFO | valid | epoch 1610 | valid on 'valid' subset | loss 5.288 | nll_loss 2.708 | ppl 6.53 | bleu 55.9 | wps 1728.5 | wpb 933.5 | bsz 59.6 | num_updates 130410 | best_bleu 57.52
2022-08-18 09:28:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1610 @ 130410 updates
2022-08-18 09:28:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1610.pt
2022-08-18 09:28:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1610.pt
2022-08-18 09:29:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1610.pt (epoch 1610 @ 130410 updates, score 55.9) (writing took 34.61951947212219 seconds)
2022-08-18 09:29:23 | INFO | fairseq_cli.train | end of epoch 1610 (average epoch stats below)
2022-08-18 09:29:23 | INFO | train | epoch 1610 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5100.8 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 130410 | lr 0.000175136 | gnorm 0.432 | train_wall 40 | gb_free 10.1 | wall 127059
2022-08-18 09:29:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:29:23 | INFO | fairseq.trainer | begin training epoch 1611
2022-08-18 09:29:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:30:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:30:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:30:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:30:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:30:14 | INFO | valid | epoch 1611 | valid on 'valid' subset | loss 5.285 | nll_loss 2.708 | ppl 6.53 | bleu 56.19 | wps 1777.1 | wpb 933.5 | bsz 59.6 | num_updates 130491 | best_bleu 57.52
2022-08-18 09:30:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1611 @ 130491 updates
2022-08-18 09:30:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1611.pt
2022-08-18 09:30:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1611.pt
2022-08-18 09:30:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1611.pt (epoch 1611 @ 130491 updates, score 56.19) (writing took 20.655186116695404 seconds)
2022-08-18 09:30:35 | INFO | fairseq_cli.train | end of epoch 1611 (average epoch stats below)
2022-08-18 09:30:35 | INFO | train | epoch 1611 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6188.3 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 130491 | lr 0.000175081 | gnorm 0.442 | train_wall 40 | gb_free 10.2 | wall 127131
2022-08-18 09:30:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:30:35 | INFO | fairseq.trainer | begin training epoch 1612
2022-08-18 09:30:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:30:41 | INFO | train_inner | epoch 1612:      9 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=4326.1, ups=0.78, wpb=5512, bsz=351.6, num_updates=130500, lr=0.000175075, gnorm=0.444, train_wall=49, gb_free=10.1, wall=127137
2022-08-18 09:31:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:31:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:31:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:31:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:31:26 | INFO | valid | epoch 1612 | valid on 'valid' subset | loss 5.313 | nll_loss 2.74 | ppl 6.68 | bleu 55.59 | wps 1876.3 | wpb 933.5 | bsz 59.6 | num_updates 130572 | best_bleu 57.52
2022-08-18 09:31:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1612 @ 130572 updates
2022-08-18 09:31:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1612.pt
2022-08-18 09:31:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1612.pt
2022-08-18 09:31:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1612.pt (epoch 1612 @ 130572 updates, score 55.59) (writing took 23.065209984779358 seconds)
2022-08-18 09:31:49 | INFO | fairseq_cli.train | end of epoch 1612 (average epoch stats below)
2022-08-18 09:31:49 | INFO | train | epoch 1612 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6026.8 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 130572 | lr 0.000175027 | gnorm 0.384 | train_wall 39 | gb_free 10.2 | wall 127205
2022-08-18 09:31:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:31:49 | INFO | fairseq.trainer | begin training epoch 1613
2022-08-18 09:31:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:32:04 | INFO | train_inner | epoch 1613:     28 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6616.9, ups=1.2, wpb=5522, bsz=354.9, num_updates=130600, lr=0.000175008, gnorm=0.411, train_wall=49, gb_free=10.1, wall=127220
2022-08-18 09:32:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:32:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:32:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:32:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:32:41 | INFO | valid | epoch 1613 | valid on 'valid' subset | loss 5.288 | nll_loss 2.711 | ppl 6.55 | bleu 55.53 | wps 1672.1 | wpb 933.5 | bsz 59.6 | num_updates 130653 | best_bleu 57.52
2022-08-18 09:32:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1613 @ 130653 updates
2022-08-18 09:32:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1613.pt
2022-08-18 09:32:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1613.pt
2022-08-18 09:33:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1613.pt (epoch 1613 @ 130653 updates, score 55.53) (writing took 39.59191136807203 seconds)
2022-08-18 09:33:21 | INFO | fairseq_cli.train | end of epoch 1613 (average epoch stats below)
2022-08-18 09:33:21 | INFO | train | epoch 1613 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 4885.9 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 130653 | lr 0.000174973 | gnorm 0.419 | train_wall 40 | gb_free 10.2 | wall 127297
2022-08-18 09:33:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:33:21 | INFO | fairseq.trainer | begin training epoch 1614
2022-08-18 09:33:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:33:46 | INFO | train_inner | epoch 1614:     47 / 81 loss=3.368, nll_loss=0.337, ppl=1.26, wps=5405.9, ups=0.98, wpb=5522.2, bsz=365.9, num_updates=130700, lr=0.000174941, gnorm=0.395, train_wall=50, gb_free=10.1, wall=127322
2022-08-18 09:34:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:34:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:34:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:34:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:34:12 | INFO | valid | epoch 1614 | valid on 'valid' subset | loss 5.294 | nll_loss 2.714 | ppl 6.56 | bleu 55.91 | wps 1912.6 | wpb 933.5 | bsz 59.6 | num_updates 130734 | best_bleu 57.52
2022-08-18 09:34:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1614 @ 130734 updates
2022-08-18 09:34:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1614.pt
2022-08-18 09:34:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1614.pt
2022-08-18 09:34:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1614.pt (epoch 1614 @ 130734 updates, score 55.91) (writing took 2.5498858988285065 seconds)
2022-08-18 09:34:15 | INFO | fairseq_cli.train | end of epoch 1614 (average epoch stats below)
2022-08-18 09:34:15 | INFO | train | epoch 1614 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 8280.8 | ups 1.5 | wpb 5523.2 | bsz 358 | num_updates 130734 | lr 0.000174918 | gnorm 0.39 | train_wall 40 | gb_free 10.1 | wall 127351
2022-08-18 09:34:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:34:15 | INFO | fairseq.trainer | begin training epoch 1615
2022-08-18 09:34:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:34:55 | INFO | train_inner | epoch 1615:     66 / 81 loss=3.368, nll_loss=0.337, ppl=1.26, wps=8091.7, ups=1.46, wpb=5535.4, bsz=356.2, num_updates=130800, lr=0.000174874, gnorm=0.406, train_wall=50, gb_free=10, wall=127391
2022-08-18 09:35:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:35:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:35:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:35:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:35:13 | INFO | valid | epoch 1615 | valid on 'valid' subset | loss 5.31 | nll_loss 2.738 | ppl 6.67 | bleu 55.7 | wps 1798.8 | wpb 933.5 | bsz 59.6 | num_updates 130815 | best_bleu 57.52
2022-08-18 09:35:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1615 @ 130815 updates
2022-08-18 09:35:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1615.pt
2022-08-18 09:35:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1615.pt
2022-08-18 09:35:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1615.pt (epoch 1615 @ 130815 updates, score 55.7) (writing took 19.894520543515682 seconds)
2022-08-18 09:35:33 | INFO | fairseq_cli.train | end of epoch 1615 (average epoch stats below)
2022-08-18 09:35:33 | INFO | train | epoch 1615 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 5690.3 | ups 1.03 | wpb 5523.2 | bsz 358 | num_updates 130815 | lr 0.000174864 | gnorm 0.419 | train_wall 42 | gb_free 10.1 | wall 127430
2022-08-18 09:35:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:35:34 | INFO | fairseq.trainer | begin training epoch 1616
2022-08-18 09:35:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:36:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:36:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:36:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:36:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:36:27 | INFO | valid | epoch 1616 | valid on 'valid' subset | loss 5.284 | nll_loss 2.706 | ppl 6.53 | bleu 56.06 | wps 1794.6 | wpb 933.5 | bsz 59.6 | num_updates 130896 | best_bleu 57.52
2022-08-18 09:36:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1616 @ 130896 updates
2022-08-18 09:36:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1616.pt
2022-08-18 09:36:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1616.pt
2022-08-18 09:36:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1616.pt (epoch 1616 @ 130896 updates, score 56.06) (writing took 2.4879324324429035 seconds)
2022-08-18 09:36:30 | INFO | fairseq_cli.train | end of epoch 1616 (average epoch stats below)
2022-08-18 09:36:30 | INFO | train | epoch 1616 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 7925.1 | ups 1.43 | wpb 5523.2 | bsz 358 | num_updates 130896 | lr 0.00017481 | gnorm 0.376 | train_wall 40 | gb_free 10.2 | wall 127486
2022-08-18 09:36:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:36:30 | INFO | fairseq.trainer | begin training epoch 1617
2022-08-18 09:36:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:36:33 | INFO | train_inner | epoch 1617:      4 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5576.9, ups=1.01, wpb=5514.2, bsz=357.2, num_updates=130900, lr=0.000174808, gnorm=0.381, train_wall=50, gb_free=10.1, wall=127490
2022-08-18 09:37:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:37:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:37:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:37:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:37:23 | INFO | valid | epoch 1617 | valid on 'valid' subset | loss 5.302 | nll_loss 2.724 | ppl 6.61 | bleu 56.19 | wps 1898 | wpb 933.5 | bsz 59.6 | num_updates 130977 | best_bleu 57.52
2022-08-18 09:37:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1617 @ 130977 updates
2022-08-18 09:37:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1617.pt
2022-08-18 09:37:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1617.pt
2022-08-18 09:37:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1617.pt (epoch 1617 @ 130977 updates, score 56.19) (writing took 24.114452864974737 seconds)
2022-08-18 09:37:48 | INFO | fairseq_cli.train | end of epoch 1617 (average epoch stats below)
2022-08-18 09:37:48 | INFO | train | epoch 1617 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 5754.3 | ups 1.04 | wpb 5523.2 | bsz 358 | num_updates 130977 | lr 0.000174756 | gnorm 0.329 | train_wall 41 | gb_free 10.1 | wall 127564
2022-08-18 09:37:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:37:48 | INFO | fairseq.trainer | begin training epoch 1618
2022-08-18 09:37:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:38:01 | INFO | train_inner | epoch 1618:     23 / 81 loss=3.368, nll_loss=0.337, ppl=1.26, wps=6299.2, ups=1.14, wpb=5530.7, bsz=364, num_updates=131000, lr=0.000174741, gnorm=0.332, train_wall=50, gb_free=10.1, wall=127577
2022-08-18 09:38:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:38:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:38:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:38:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:38:41 | INFO | valid | epoch 1618 | valid on 'valid' subset | loss 5.279 | nll_loss 2.698 | ppl 6.49 | bleu 56.46 | wps 1832.1 | wpb 933.5 | bsz 59.6 | num_updates 131058 | best_bleu 57.52
2022-08-18 09:38:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1618 @ 131058 updates
2022-08-18 09:38:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1618.pt
2022-08-18 09:38:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1618.pt
2022-08-18 09:39:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1618.pt (epoch 1618 @ 131058 updates, score 56.46) (writing took 36.27806515991688 seconds)
2022-08-18 09:39:17 | INFO | fairseq_cli.train | end of epoch 1618 (average epoch stats below)
2022-08-18 09:39:17 | INFO | train | epoch 1618 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 4986.1 | ups 0.9 | wpb 5523.2 | bsz 358 | num_updates 131058 | lr 0.000174702 | gnorm 0.361 | train_wall 40 | gb_free 10.1 | wall 127653
2022-08-18 09:39:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:39:17 | INFO | fairseq.trainer | begin training epoch 1619
2022-08-18 09:39:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:39:40 | INFO | train_inner | epoch 1619:     42 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5621.2, ups=1.02, wpb=5537.6, bsz=351.1, num_updates=131100, lr=0.000174674, gnorm=0.415, train_wall=50, gb_free=10.1, wall=127676
2022-08-18 09:40:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:40:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:40:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:40:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:40:09 | INFO | valid | epoch 1619 | valid on 'valid' subset | loss 5.299 | nll_loss 2.72 | ppl 6.59 | bleu 55.63 | wps 1890.1 | wpb 933.5 | bsz 59.6 | num_updates 131139 | best_bleu 57.52
2022-08-18 09:40:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1619 @ 131139 updates
2022-08-18 09:40:09 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1619.pt
2022-08-18 09:40:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1619.pt
2022-08-18 09:40:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1619.pt (epoch 1619 @ 131139 updates, score 55.63) (writing took 19.912579085677862 seconds)
2022-08-18 09:40:29 | INFO | fairseq_cli.train | end of epoch 1619 (average epoch stats below)
2022-08-18 09:40:29 | INFO | train | epoch 1619 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6266.2 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 131139 | lr 0.000174648 | gnorm 0.469 | train_wall 40 | gb_free 10.1 | wall 127725
2022-08-18 09:40:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:40:29 | INFO | fairseq.trainer | begin training epoch 1620
2022-08-18 09:40:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:41:06 | INFO | train_inner | epoch 1620:     61 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6348.3, ups=1.15, wpb=5499.8, bsz=359, num_updates=131200, lr=0.000174608, gnorm=0.444, train_wall=51, gb_free=10.1, wall=127763
2022-08-18 09:41:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:41:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:41:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:41:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:41:27 | INFO | valid | epoch 1620 | valid on 'valid' subset | loss 5.298 | nll_loss 2.72 | ppl 6.59 | bleu 56.03 | wps 1961.1 | wpb 933.5 | bsz 59.6 | num_updates 131220 | best_bleu 57.52
2022-08-18 09:41:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1620 @ 131220 updates
2022-08-18 09:41:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1620.pt
2022-08-18 09:41:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1620.pt
2022-08-18 09:41:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1620.pt (epoch 1620 @ 131220 updates, score 56.03) (writing took 15.137985538691282 seconds)
2022-08-18 09:41:42 | INFO | fairseq_cli.train | end of epoch 1620 (average epoch stats below)
2022-08-18 09:41:42 | INFO | train | epoch 1620 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6111.5 | ups 1.11 | wpb 5523.2 | bsz 358 | num_updates 131220 | lr 0.000174594 | gnorm 0.416 | train_wall 41 | gb_free 10.1 | wall 127798
2022-08-18 09:41:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:41:42 | INFO | fairseq.trainer | begin training epoch 1621
2022-08-18 09:41:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:42:24 | INFO | train_inner | epoch 1621:     80 / 81 loss=3.368, nll_loss=0.338, ppl=1.26, wps=7160.7, ups=1.29, wpb=5550.3, bsz=359.8, num_updates=131300, lr=0.000174541, gnorm=0.408, train_wall=49, gb_free=10.1, wall=127840
2022-08-18 09:42:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:42:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:42:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:42:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:42:34 | INFO | valid | epoch 1621 | valid on 'valid' subset | loss 5.302 | nll_loss 2.727 | ppl 6.62 | bleu 56.47 | wps 1864.1 | wpb 933.5 | bsz 59.6 | num_updates 131301 | best_bleu 57.52
2022-08-18 09:42:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1621 @ 131301 updates
2022-08-18 09:42:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1621.pt
2022-08-18 09:42:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1621.pt
2022-08-18 09:42:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1621.pt (epoch 1621 @ 131301 updates, score 56.47) (writing took 14.182864740490913 seconds)
2022-08-18 09:42:48 | INFO | fairseq_cli.train | end of epoch 1621 (average epoch stats below)
2022-08-18 09:42:48 | INFO | train | epoch 1621 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 6774.4 | ups 1.23 | wpb 5523.2 | bsz 358 | num_updates 131301 | lr 0.00017454 | gnorm 0.426 | train_wall 39 | gb_free 10 | wall 127864
2022-08-18 09:42:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:42:48 | INFO | fairseq.trainer | begin training epoch 1622
2022-08-18 09:42:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:43:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:43:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:43:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:43:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:43:40 | INFO | valid | epoch 1622 | valid on 'valid' subset | loss 5.283 | nll_loss 2.702 | ppl 6.51 | bleu 56.95 | wps 1875.3 | wpb 933.5 | bsz 59.6 | num_updates 131382 | best_bleu 57.52
2022-08-18 09:43:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1622 @ 131382 updates
2022-08-18 09:43:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1622.pt
2022-08-18 09:43:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1622.pt
2022-08-18 09:43:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1622.pt (epoch 1622 @ 131382 updates, score 56.95) (writing took 19.79020319506526 seconds)
2022-08-18 09:44:00 | INFO | fairseq_cli.train | end of epoch 1622 (average epoch stats below)
2022-08-18 09:44:00 | INFO | train | epoch 1622 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 6248.2 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 131382 | lr 0.000174487 | gnorm 0.448 | train_wall 40 | gb_free 10.1 | wall 127936
2022-08-18 09:44:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:44:00 | INFO | fairseq.trainer | begin training epoch 1623
2022-08-18 09:44:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:44:10 | INFO | train_inner | epoch 1623:     18 / 81 loss=3.368, nll_loss=0.337, ppl=1.26, wps=5179.8, ups=0.94, wpb=5504.1, bsz=356.7, num_updates=131400, lr=0.000174475, gnorm=0.43, train_wall=49, gb_free=10, wall=127946
2022-08-18 09:44:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:44:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:44:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:44:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:44:52 | INFO | valid | epoch 1623 | valid on 'valid' subset | loss 5.299 | nll_loss 2.723 | ppl 6.6 | bleu 56.22 | wps 1803.7 | wpb 933.5 | bsz 59.6 | num_updates 131463 | best_bleu 57.52
2022-08-18 09:44:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1623 @ 131463 updates
2022-08-18 09:44:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1623.pt
2022-08-18 09:44:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1623.pt
2022-08-18 09:45:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1623.pt (epoch 1623 @ 131463 updates, score 56.22) (writing took 12.578905165195465 seconds)
2022-08-18 09:45:05 | INFO | fairseq_cli.train | end of epoch 1623 (average epoch stats below)
2022-08-18 09:45:05 | INFO | train | epoch 1623 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 6819.6 | ups 1.23 | wpb 5523.2 | bsz 358 | num_updates 131463 | lr 0.000174433 | gnorm 0.329 | train_wall 41 | gb_free 10.2 | wall 128001
2022-08-18 09:45:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:45:05 | INFO | fairseq.trainer | begin training epoch 1624
2022-08-18 09:45:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:45:25 | INFO | train_inner | epoch 1624:     37 / 81 loss=3.368, nll_loss=0.337, ppl=1.26, wps=7405.4, ups=1.34, wpb=5543.2, bsz=358.3, num_updates=131500, lr=0.000174408, gnorm=0.353, train_wall=50, gb_free=10.2, wall=128021
2022-08-18 09:45:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:45:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:45:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:45:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:45:56 | INFO | valid | epoch 1624 | valid on 'valid' subset | loss 5.291 | nll_loss 2.706 | ppl 6.53 | bleu 56.64 | wps 1826.6 | wpb 933.5 | bsz 59.6 | num_updates 131544 | best_bleu 57.52
2022-08-18 09:45:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1624 @ 131544 updates
2022-08-18 09:45:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1624.pt
2022-08-18 09:45:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1624.pt
2022-08-18 09:46:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1624.pt (epoch 1624 @ 131544 updates, score 56.64) (writing took 56.93011173978448 seconds)
2022-08-18 09:46:53 | INFO | fairseq_cli.train | end of epoch 1624 (average epoch stats below)
2022-08-18 09:46:53 | INFO | train | epoch 1624 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 4141 | ups 0.75 | wpb 5523.2 | bsz 358 | num_updates 131544 | lr 0.000174379 | gnorm 0.431 | train_wall 40 | gb_free 10.3 | wall 128109
2022-08-18 09:46:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:46:53 | INFO | fairseq.trainer | begin training epoch 1625
2022-08-18 09:46:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:47:23 | INFO | train_inner | epoch 1625:     56 / 81 loss=3.369, nll_loss=0.339, ppl=1.26, wps=4642.8, ups=0.85, wpb=5485, bsz=355.8, num_updates=131600, lr=0.000174342, gnorm=0.419, train_wall=49, gb_free=10.1, wall=128139
2022-08-18 09:47:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:47:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:47:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:47:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:47:46 | INFO | valid | epoch 1625 | valid on 'valid' subset | loss 5.3 | nll_loss 2.72 | ppl 6.59 | bleu 56.59 | wps 1836.9 | wpb 933.5 | bsz 59.6 | num_updates 131625 | best_bleu 57.52
2022-08-18 09:47:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1625 @ 131625 updates
2022-08-18 09:47:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1625.pt
2022-08-18 09:47:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1625.pt
2022-08-18 09:48:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1625.pt (epoch 1625 @ 131625 updates, score 56.59) (writing took 21.574284750968218 seconds)
2022-08-18 09:48:07 | INFO | fairseq_cli.train | end of epoch 1625 (average epoch stats below)
2022-08-18 09:48:07 | INFO | train | epoch 1625 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6031.4 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 131625 | lr 0.000174325 | gnorm 0.379 | train_wall 40 | gb_free 10.1 | wall 128184
2022-08-18 09:48:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:48:08 | INFO | fairseq.trainer | begin training epoch 1626
2022-08-18 09:48:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:48:47 | INFO | train_inner | epoch 1626:     75 / 81 loss=3.368, nll_loss=0.337, ppl=1.26, wps=6625.8, ups=1.19, wpb=5567.7, bsz=361.7, num_updates=131700, lr=0.000174276, gnorm=0.55, train_wall=51, gb_free=10, wall=128223
2022-08-18 09:48:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:48:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:48:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:48:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:48:59 | INFO | valid | epoch 1626 | valid on 'valid' subset | loss 5.288 | nll_loss 2.705 | ppl 6.52 | bleu 56.33 | wps 1817 | wpb 933.5 | bsz 59.6 | num_updates 131706 | best_bleu 57.52
2022-08-18 09:48:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1626 @ 131706 updates
2022-08-18 09:48:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1626.pt
2022-08-18 09:49:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1626.pt
2022-08-18 09:49:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1626.pt (epoch 1626 @ 131706 updates, score 56.33) (writing took 15.56912774592638 seconds)
2022-08-18 09:49:15 | INFO | fairseq_cli.train | end of epoch 1626 (average epoch stats below)
2022-08-18 09:49:15 | INFO | train | epoch 1626 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 6593.4 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 131706 | lr 0.000174272 | gnorm 0.616 | train_wall 41 | gb_free 10.2 | wall 128251
2022-08-18 09:49:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:49:15 | INFO | fairseq.trainer | begin training epoch 1627
2022-08-18 09:49:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:49:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:49:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:49:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:49:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:50:07 | INFO | valid | epoch 1627 | valid on 'valid' subset | loss 5.282 | nll_loss 2.702 | ppl 6.51 | bleu 56.52 | wps 1758.5 | wpb 933.5 | bsz 59.6 | num_updates 131787 | best_bleu 57.52
2022-08-18 09:50:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1627 @ 131787 updates
2022-08-18 09:50:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1627.pt
2022-08-18 09:50:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1627.pt
2022-08-18 09:50:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1627.pt (epoch 1627 @ 131787 updates, score 56.52) (writing took 39.67972516641021 seconds)
2022-08-18 09:50:47 | INFO | fairseq_cli.train | end of epoch 1627 (average epoch stats below)
2022-08-18 09:50:47 | INFO | train | epoch 1627 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 4864.8 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 131787 | lr 0.000174218 | gnorm 0.437 | train_wall 40 | gb_free 10.1 | wall 128343
2022-08-18 09:50:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:50:47 | INFO | fairseq.trainer | begin training epoch 1628
2022-08-18 09:50:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:50:55 | INFO | train_inner | epoch 1628:     13 / 81 loss=3.369, nll_loss=0.337, ppl=1.26, wps=4276.4, ups=0.78, wpb=5486.1, bsz=357.5, num_updates=131800, lr=0.00017421, gnorm=0.446, train_wall=49, gb_free=10.1, wall=128352
2022-08-18 09:51:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:51:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:51:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:51:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:51:41 | INFO | valid | epoch 1628 | valid on 'valid' subset | loss 5.296 | nll_loss 2.718 | ppl 6.58 | bleu 56.31 | wps 1919.1 | wpb 933.5 | bsz 59.6 | num_updates 131868 | best_bleu 57.52
2022-08-18 09:51:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1628 @ 131868 updates
2022-08-18 09:51:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1628.pt
2022-08-18 09:51:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1628.pt
2022-08-18 09:52:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1628.pt (epoch 1628 @ 131868 updates, score 56.31) (writing took 20.143334418535233 seconds)
2022-08-18 09:52:01 | INFO | fairseq_cli.train | end of epoch 1628 (average epoch stats below)
2022-08-18 09:52:01 | INFO | train | epoch 1628 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6032.8 | ups 1.09 | wpb 5523.2 | bsz 358 | num_updates 131868 | lr 0.000174165 | gnorm 0.447 | train_wall 40 | gb_free 10.2 | wall 128418
2022-08-18 09:52:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:52:02 | INFO | fairseq.trainer | begin training epoch 1629
2022-08-18 09:52:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:52:18 | INFO | train_inner | epoch 1629:     32 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6681.6, ups=1.21, wpb=5534.9, bsz=356.9, num_updates=131900, lr=0.000174144, gnorm=0.486, train_wall=49, gb_free=10, wall=128435
2022-08-18 09:52:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:52:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:52:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:52:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:52:52 | INFO | valid | epoch 1629 | valid on 'valid' subset | loss 5.295 | nll_loss 2.715 | ppl 6.56 | bleu 56.32 | wps 1967.2 | wpb 933.5 | bsz 59.6 | num_updates 131949 | best_bleu 57.52
2022-08-18 09:52:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1629 @ 131949 updates
2022-08-18 09:52:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1629.pt
2022-08-18 09:52:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1629.pt
2022-08-18 09:53:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1629.pt (epoch 1629 @ 131949 updates, score 56.32) (writing took 29.389306370168924 seconds)
2022-08-18 09:53:21 | INFO | fairseq_cli.train | end of epoch 1629 (average epoch stats below)
2022-08-18 09:53:21 | INFO | train | epoch 1629 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5601.1 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 131949 | lr 0.000174111 | gnorm 0.441 | train_wall 40 | gb_free 10.2 | wall 128497
2022-08-18 09:53:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:53:21 | INFO | fairseq.trainer | begin training epoch 1630
2022-08-18 09:53:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:53:49 | INFO | train_inner | epoch 1630:     51 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6107.2, ups=1.1, wpb=5547.8, bsz=358.8, num_updates=132000, lr=0.000174078, gnorm=0.431, train_wall=50, gb_free=10.1, wall=128525
2022-08-18 09:54:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:54:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:54:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:54:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:54:14 | INFO | valid | epoch 1630 | valid on 'valid' subset | loss 5.293 | nll_loss 2.715 | ppl 6.57 | bleu 56.28 | wps 1802 | wpb 933.5 | bsz 59.6 | num_updates 132030 | best_bleu 57.52
2022-08-18 09:54:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1630 @ 132030 updates
2022-08-18 09:54:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1630.pt
2022-08-18 09:54:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1630.pt
2022-08-18 09:54:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1630.pt (epoch 1630 @ 132030 updates, score 56.28) (writing took 36.27832463756204 seconds)
2022-08-18 09:54:50 | INFO | fairseq_cli.train | end of epoch 1630 (average epoch stats below)
2022-08-18 09:54:50 | INFO | train | epoch 1630 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 5038.8 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 132030 | lr 0.000174058 | gnorm 0.434 | train_wall 41 | gb_free 10.2 | wall 128586
2022-08-18 09:54:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:54:50 | INFO | fairseq.trainer | begin training epoch 1631
2022-08-18 09:54:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:55:34 | INFO | train_inner | epoch 1631:     70 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5260, ups=0.95, wpb=5509.1, bsz=354.1, num_updates=132100, lr=0.000174012, gnorm=0.366, train_wall=49, gb_free=10.1, wall=128630
2022-08-18 09:55:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:55:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:55:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:55:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:55:50 | INFO | valid | epoch 1631 | valid on 'valid' subset | loss 5.279 | nll_loss 2.695 | ppl 6.48 | bleu 56.73 | wps 1846.5 | wpb 933.5 | bsz 59.6 | num_updates 132111 | best_bleu 57.52
2022-08-18 09:55:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1631 @ 132111 updates
2022-08-18 09:55:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1631.pt
2022-08-18 09:55:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1631.pt
2022-08-18 09:56:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1631.pt (epoch 1631 @ 132111 updates, score 56.73) (writing took 32.71828820928931 seconds)
2022-08-18 09:56:23 | INFO | fairseq_cli.train | end of epoch 1631 (average epoch stats below)
2022-08-18 09:56:23 | INFO | train | epoch 1631 | loss 3.368 | nll_loss 0.338 | ppl 1.26 | wps 4801.5 | ups 0.87 | wpb 5523.2 | bsz 358 | num_updates 132111 | lr 0.000174005 | gnorm 0.385 | train_wall 40 | gb_free 10.2 | wall 128679
2022-08-18 09:56:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:56:23 | INFO | fairseq.trainer | begin training epoch 1632
2022-08-18 09:56:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:57:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:57:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:57:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:57:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:57:15 | INFO | valid | epoch 1632 | valid on 'valid' subset | loss 5.28 | nll_loss 2.696 | ppl 6.48 | bleu 57.04 | wps 1847.7 | wpb 933.5 | bsz 59.6 | num_updates 132192 | best_bleu 57.52
2022-08-18 09:57:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1632 @ 132192 updates
2022-08-18 09:57:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1632.pt
2022-08-18 09:57:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1632.pt
2022-08-18 09:57:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1632.pt (epoch 1632 @ 132192 updates, score 57.04) (writing took 34.395076513290405 seconds)
2022-08-18 09:57:50 | INFO | fairseq_cli.train | end of epoch 1632 (average epoch stats below)
2022-08-18 09:57:50 | INFO | train | epoch 1632 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5174.4 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 132192 | lr 0.000173951 | gnorm 0.432 | train_wall 40 | gb_free 10.1 | wall 128766
2022-08-18 09:57:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:57:50 | INFO | fairseq.trainer | begin training epoch 1633
2022-08-18 09:57:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:57:56 | INFO | train_inner | epoch 1633:      8 / 81 loss=3.368, nll_loss=0.338, ppl=1.26, wps=3886.8, ups=0.71, wpb=5504.3, bsz=359.8, num_updates=132200, lr=0.000173946, gnorm=0.439, train_wall=50, gb_free=10.1, wall=128772
2022-08-18 09:58:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:58:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:58:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:58:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:58:43 | INFO | valid | epoch 1633 | valid on 'valid' subset | loss 5.283 | nll_loss 2.701 | ppl 6.5 | bleu 56.74 | wps 1789.2 | wpb 933.5 | bsz 59.6 | num_updates 132273 | best_bleu 57.52
2022-08-18 09:58:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1633 @ 132273 updates
2022-08-18 09:58:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1633.pt
2022-08-18 09:58:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1633.pt
2022-08-18 09:59:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1633.pt (epoch 1633 @ 132273 updates, score 56.74) (writing took 20.50505656003952 seconds)
2022-08-18 09:59:04 | INFO | fairseq_cli.train | end of epoch 1633 (average epoch stats below)
2022-08-18 09:59:04 | INFO | train | epoch 1633 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 6054.1 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 132273 | lr 0.000173898 | gnorm 0.363 | train_wall 41 | gb_free 10 | wall 128840
2022-08-18 09:59:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 09:59:04 | INFO | fairseq.trainer | begin training epoch 1634
2022-08-18 09:59:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 09:59:19 | INFO | train_inner | epoch 1634:     27 / 81 loss=3.368, nll_loss=0.337, ppl=1.26, wps=6623.5, ups=1.2, wpb=5527.9, bsz=360.7, num_updates=132300, lr=0.00017388, gnorm=0.373, train_wall=50, gb_free=10.1, wall=128855
2022-08-18 09:59:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 09:59:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 09:59:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 09:59:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 09:59:57 | INFO | valid | epoch 1634 | valid on 'valid' subset | loss 5.294 | nll_loss 2.715 | ppl 6.56 | bleu 56.23 | wps 1732.8 | wpb 933.5 | bsz 59.6 | num_updates 132354 | best_bleu 57.52
2022-08-18 09:59:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1634 @ 132354 updates
2022-08-18 09:59:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1634.pt
2022-08-18 09:59:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1634.pt
2022-08-18 10:00:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1634.pt (epoch 1634 @ 132354 updates, score 56.23) (writing took 17.30315473675728 seconds)
2022-08-18 10:00:14 | INFO | fairseq_cli.train | end of epoch 1634 (average epoch stats below)
2022-08-18 10:00:14 | INFO | train | epoch 1634 | loss 3.369 | nll_loss 0.339 | ppl 1.26 | wps 6345.9 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 132354 | lr 0.000173845 | gnorm 0.429 | train_wall 41 | gb_free 10.2 | wall 128910
2022-08-18 10:00:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:00:14 | INFO | fairseq.trainer | begin training epoch 1635
2022-08-18 10:00:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:00:38 | INFO | train_inner | epoch 1635:     46 / 81 loss=3.369, nll_loss=0.339, ppl=1.26, wps=6983.5, ups=1.26, wpb=5541.8, bsz=357.8, num_updates=132400, lr=0.000173814, gnorm=0.434, train_wall=50, gb_free=10.1, wall=128935
2022-08-18 10:00:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:00:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:00:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:00:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:01:05 | INFO | valid | epoch 1635 | valid on 'valid' subset | loss 5.296 | nll_loss 2.719 | ppl 6.58 | bleu 55.89 | wps 1823.7 | wpb 933.5 | bsz 59.6 | num_updates 132435 | best_bleu 57.52
2022-08-18 10:01:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1635 @ 132435 updates
2022-08-18 10:01:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1635.pt
2022-08-18 10:01:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1635.pt
2022-08-18 10:01:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1635.pt (epoch 1635 @ 132435 updates, score 55.89) (writing took 29.002432215958834 seconds)
2022-08-18 10:01:35 | INFO | fairseq_cli.train | end of epoch 1635 (average epoch stats below)
2022-08-18 10:01:35 | INFO | train | epoch 1635 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5553.6 | ups 1.01 | wpb 5523.2 | bsz 358 | num_updates 132435 | lr 0.000173792 | gnorm 0.456 | train_wall 39 | gb_free 10.1 | wall 128991
2022-08-18 10:01:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:01:35 | INFO | fairseq.trainer | begin training epoch 1636
2022-08-18 10:01:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:02:09 | INFO | train_inner | epoch 1636:     65 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6109.4, ups=1.11, wpb=5521.3, bsz=356.7, num_updates=132500, lr=0.000173749, gnorm=0.429, train_wall=50, gb_free=10.1, wall=129025
2022-08-18 10:02:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:02:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:02:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:02:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:02:26 | INFO | valid | epoch 1636 | valid on 'valid' subset | loss 5.288 | nll_loss 2.711 | ppl 6.55 | bleu 56.28 | wps 1837.7 | wpb 933.5 | bsz 59.6 | num_updates 132516 | best_bleu 57.52
2022-08-18 10:02:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1636 @ 132516 updates
2022-08-18 10:02:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1636.pt
2022-08-18 10:02:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1636.pt
2022-08-18 10:03:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1636.pt (epoch 1636 @ 132516 updates, score 56.28) (writing took 50.07581301406026 seconds)
2022-08-18 10:03:16 | INFO | fairseq_cli.train | end of epoch 1636 (average epoch stats below)
2022-08-18 10:03:16 | INFO | train | epoch 1636 | loss 3.369 | nll_loss 0.337 | ppl 1.26 | wps 4400.5 | ups 0.8 | wpb 5523.2 | bsz 358 | num_updates 132516 | lr 0.000173738 | gnorm 0.41 | train_wall 40 | gb_free 10 | wall 129093
2022-08-18 10:03:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:03:16 | INFO | fairseq.trainer | begin training epoch 1637
2022-08-18 10:03:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:04:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:04:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:04:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:04:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:04:16 | INFO | valid | epoch 1637 | valid on 'valid' subset | loss 5.285 | nll_loss 2.704 | ppl 6.52 | bleu 55.9 | wps 1922.4 | wpb 933.5 | bsz 59.6 | num_updates 132597 | best_bleu 57.52
2022-08-18 10:04:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1637 @ 132597 updates
2022-08-18 10:04:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1637.pt
2022-08-18 10:04:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1637.pt
2022-08-18 10:04:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1637.pt (epoch 1637 @ 132597 updates, score 55.9) (writing took 14.208947151899338 seconds)
2022-08-18 10:04:30 | INFO | fairseq_cli.train | end of epoch 1637 (average epoch stats below)
2022-08-18 10:04:30 | INFO | train | epoch 1637 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6072.6 | ups 1.1 | wpb 5523.2 | bsz 358 | num_updates 132597 | lr 0.000173685 | gnorm 0.442 | train_wall 40 | gb_free 10.1 | wall 129166
2022-08-18 10:04:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:04:30 | INFO | fairseq.trainer | begin training epoch 1638
2022-08-18 10:04:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:04:33 | INFO | train_inner | epoch 1638:      3 / 81 loss=3.369, nll_loss=0.339, ppl=1.26, wps=3819.9, ups=0.69, wpb=5503.8, bsz=357.8, num_updates=132600, lr=0.000173683, gnorm=0.432, train_wall=50, gb_free=10, wall=129169
2022-08-18 10:05:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:05:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:05:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:05:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:05:22 | INFO | valid | epoch 1638 | valid on 'valid' subset | loss 5.294 | nll_loss 2.714 | ppl 6.56 | bleu 56.31 | wps 1901.5 | wpb 933.5 | bsz 59.6 | num_updates 132678 | best_bleu 57.52
2022-08-18 10:05:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1638 @ 132678 updates
2022-08-18 10:05:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1638.pt
2022-08-18 10:05:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1638.pt
2022-08-18 10:06:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1638.pt (epoch 1638 @ 132678 updates, score 56.31) (writing took 41.207258086651564 seconds)
2022-08-18 10:06:03 | INFO | fairseq_cli.train | end of epoch 1638 (average epoch stats below)
2022-08-18 10:06:03 | INFO | train | epoch 1638 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 4795.3 | ups 0.87 | wpb 5523.2 | bsz 358 | num_updates 132678 | lr 0.000173632 | gnorm 0.368 | train_wall 40 | gb_free 10.2 | wall 129259
2022-08-18 10:06:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:06:04 | INFO | fairseq.trainer | begin training epoch 1639
2022-08-18 10:06:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:06:15 | INFO | train_inner | epoch 1639:     22 / 81 loss=3.369, nll_loss=0.337, ppl=1.26, wps=5374.8, ups=0.97, wpb=5517.8, bsz=355.8, num_updates=132700, lr=0.000173618, gnorm=0.385, train_wall=49, gb_free=10, wall=129272
2022-08-18 10:06:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:06:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:06:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:06:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:06:55 | INFO | valid | epoch 1639 | valid on 'valid' subset | loss 5.286 | nll_loss 2.705 | ppl 6.52 | bleu 56.5 | wps 1899.3 | wpb 933.5 | bsz 59.6 | num_updates 132759 | best_bleu 57.52
2022-08-18 10:06:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1639 @ 132759 updates
2022-08-18 10:06:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1639.pt
2022-08-18 10:06:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1639.pt
2022-08-18 10:07:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1639.pt (epoch 1639 @ 132759 updates, score 56.5) (writing took 19.610773265361786 seconds)
2022-08-18 10:07:15 | INFO | fairseq_cli.train | end of epoch 1639 (average epoch stats below)
2022-08-18 10:07:15 | INFO | train | epoch 1639 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6254.2 | ups 1.13 | wpb 5523.2 | bsz 358 | num_updates 132759 | lr 0.000173579 | gnorm 0.445 | train_wall 41 | gb_free 10.2 | wall 129331
2022-08-18 10:07:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:07:15 | INFO | fairseq.trainer | begin training epoch 1640
2022-08-18 10:07:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:07:37 | INFO | train_inner | epoch 1640:     41 / 81 loss=3.368, nll_loss=0.338, ppl=1.26, wps=6796.4, ups=1.22, wpb=5549.8, bsz=362.3, num_updates=132800, lr=0.000173553, gnorm=0.416, train_wall=50, gb_free=10.1, wall=129353
2022-08-18 10:07:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:07:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:07:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:07:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:08:07 | INFO | valid | epoch 1640 | valid on 'valid' subset | loss 5.289 | nll_loss 2.71 | ppl 6.55 | bleu 55.89 | wps 1874.2 | wpb 933.5 | bsz 59.6 | num_updates 132840 | best_bleu 57.52
2022-08-18 10:08:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1640 @ 132840 updates
2022-08-18 10:08:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1640.pt
2022-08-18 10:08:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1640.pt
2022-08-18 10:08:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1640.pt (epoch 1640 @ 132840 updates, score 55.89) (writing took 17.895908869802952 seconds)
2022-08-18 10:08:25 | INFO | fairseq_cli.train | end of epoch 1640 (average epoch stats below)
2022-08-18 10:08:25 | INFO | train | epoch 1640 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 6335 | ups 1.15 | wpb 5523.2 | bsz 358 | num_updates 132840 | lr 0.000173526 | gnorm 0.365 | train_wall 41 | gb_free 10.1 | wall 129402
2022-08-18 10:08:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:08:26 | INFO | fairseq.trainer | begin training epoch 1641
2022-08-18 10:08:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:08:59 | INFO | train_inner | epoch 1641:     60 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6699.4, ups=1.22, wpb=5513.5, bsz=353.8, num_updates=132900, lr=0.000173487, gnorm=0.419, train_wall=52, gb_free=10.1, wall=129436
2022-08-18 10:09:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:09:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:09:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:09:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:09:20 | INFO | valid | epoch 1641 | valid on 'valid' subset | loss 5.287 | nll_loss 2.707 | ppl 6.53 | bleu 56.06 | wps 1742.4 | wpb 933.5 | bsz 59.6 | num_updates 132921 | best_bleu 57.52
2022-08-18 10:09:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1641 @ 132921 updates
2022-08-18 10:09:20 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1641.pt
2022-08-18 10:09:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1641.pt
2022-08-18 10:09:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1641.pt (epoch 1641 @ 132921 updates, score 56.06) (writing took 37.378001261502504 seconds)
2022-08-18 10:09:57 | INFO | fairseq_cli.train | end of epoch 1641 (average epoch stats below)
2022-08-18 10:09:57 | INFO | train | epoch 1641 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 4864.4 | ups 0.88 | wpb 5523.2 | bsz 358 | num_updates 132921 | lr 0.000173474 | gnorm 0.461 | train_wall 41 | gb_free 10.2 | wall 129494
2022-08-18 10:09:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:09:58 | INFO | fairseq.trainer | begin training epoch 1642
2022-08-18 10:09:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:10:47 | INFO | train_inner | epoch 1642:     79 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5135.9, ups=0.93, wpb=5545.6, bsz=362.2, num_updates=133000, lr=0.000173422, gnorm=0.438, train_wall=50, gb_free=10, wall=129544
2022-08-18 10:10:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:10:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:10:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:10:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:10:57 | INFO | valid | epoch 1642 | valid on 'valid' subset | loss 5.303 | nll_loss 2.725 | ppl 6.61 | bleu 56.2 | wps 1909.9 | wpb 933.5 | bsz 59.6 | num_updates 133002 | best_bleu 57.52
2022-08-18 10:10:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1642 @ 133002 updates
2022-08-18 10:10:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1642.pt
2022-08-18 10:10:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1642.pt
2022-08-18 10:11:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1642.pt (epoch 1642 @ 133002 updates, score 56.2) (writing took 24.897588938474655 seconds)
2022-08-18 10:11:22 | INFO | fairseq_cli.train | end of epoch 1642 (average epoch stats below)
2022-08-18 10:11:22 | INFO | train | epoch 1642 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 5260.3 | ups 0.95 | wpb 5523.2 | bsz 358 | num_updates 133002 | lr 0.000173421 | gnorm 0.434 | train_wall 40 | gb_free 10.1 | wall 129579
2022-08-18 10:11:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:11:23 | INFO | fairseq.trainer | begin training epoch 1643
2022-08-18 10:11:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:12:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:12:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:12:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:12:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:12:15 | INFO | valid | epoch 1643 | valid on 'valid' subset | loss 5.298 | nll_loss 2.719 | ppl 6.58 | bleu 56.54 | wps 1860.7 | wpb 933.5 | bsz 59.6 | num_updates 133083 | best_bleu 57.52
2022-08-18 10:12:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1643 @ 133083 updates
2022-08-18 10:12:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1643.pt
2022-08-18 10:12:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1643.pt
2022-08-18 10:12:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1643.pt (epoch 1643 @ 133083 updates, score 56.54) (writing took 17.120459884405136 seconds)
2022-08-18 10:12:32 | INFO | fairseq_cli.train | end of epoch 1643 (average epoch stats below)
2022-08-18 10:12:32 | INFO | train | epoch 1643 | loss 3.368 | nll_loss 0.338 | ppl 1.26 | wps 6434.8 | ups 1.17 | wpb 5523.2 | bsz 358 | num_updates 133083 | lr 0.000173368 | gnorm 0.358 | train_wall 40 | gb_free 10.2 | wall 129648
2022-08-18 10:12:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:12:32 | INFO | fairseq.trainer | begin training epoch 1644
2022-08-18 10:12:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:12:42 | INFO | train_inner | epoch 1644:     17 / 81 loss=3.368, nll_loss=0.337, ppl=1.26, wps=4811, ups=0.88, wpb=5492.9, bsz=354.9, num_updates=133100, lr=0.000173357, gnorm=0.382, train_wall=49, gb_free=10.2, wall=129658
2022-08-18 10:13:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:13:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:13:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:13:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:13:22 | INFO | valid | epoch 1644 | valid on 'valid' subset | loss 5.307 | nll_loss 2.731 | ppl 6.64 | bleu 55.8 | wps 1841.5 | wpb 933.5 | bsz 59.6 | num_updates 133164 | best_bleu 57.52
2022-08-18 10:13:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1644 @ 133164 updates
2022-08-18 10:13:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1644.pt
2022-08-18 10:13:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1644.pt
2022-08-18 10:13:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1644.pt (epoch 1644 @ 133164 updates, score 55.8) (writing took 16.903848815709352 seconds)
2022-08-18 10:13:39 | INFO | fairseq_cli.train | end of epoch 1644 (average epoch stats below)
2022-08-18 10:13:39 | INFO | train | epoch 1644 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6628.7 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 133164 | lr 0.000173315 | gnorm 0.472 | train_wall 40 | gb_free 10.2 | wall 129716
2022-08-18 10:13:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:13:40 | INFO | fairseq.trainer | begin training epoch 1645
2022-08-18 10:13:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:13:59 | INFO | train_inner | epoch 1645:     36 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=7114.7, ups=1.29, wpb=5532.6, bsz=356.1, num_updates=133200, lr=0.000173292, gnorm=0.44, train_wall=49, gb_free=10.1, wall=129736
2022-08-18 10:14:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:14:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:14:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:14:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:14:31 | INFO | valid | epoch 1645 | valid on 'valid' subset | loss 5.304 | nll_loss 2.727 | ppl 6.62 | bleu 56.4 | wps 1855.4 | wpb 933.5 | bsz 59.6 | num_updates 133245 | best_bleu 57.52
2022-08-18 10:14:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1645 @ 133245 updates
2022-08-18 10:14:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1645.pt
2022-08-18 10:14:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1645.pt
2022-08-18 10:14:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1645.pt (epoch 1645 @ 133245 updates, score 56.4) (writing took 20.388035509735346 seconds)
2022-08-18 10:14:52 | INFO | fairseq_cli.train | end of epoch 1645 (average epoch stats below)
2022-08-18 10:14:52 | INFO | train | epoch 1645 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6198.8 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 133245 | lr 0.000173262 | gnorm 0.393 | train_wall 40 | gb_free 10.2 | wall 129788
2022-08-18 10:14:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:14:52 | INFO | fairseq.trainer | begin training epoch 1646
2022-08-18 10:14:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:15:21 | INFO | train_inner | epoch 1646:     55 / 81 loss=3.369, nll_loss=0.339, ppl=1.26, wps=6788.7, ups=1.23, wpb=5521.2, bsz=359.7, num_updates=133300, lr=0.000173227, gnorm=0.449, train_wall=50, gb_free=10, wall=129817
2022-08-18 10:15:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:15:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:15:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:15:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:15:43 | INFO | valid | epoch 1646 | valid on 'valid' subset | loss 5.291 | nll_loss 2.712 | ppl 6.55 | bleu 56.46 | wps 1846.6 | wpb 933.5 | bsz 59.6 | num_updates 133326 | best_bleu 57.52
2022-08-18 10:15:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1646 @ 133326 updates
2022-08-18 10:15:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1646.pt
2022-08-18 10:15:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1646.pt
2022-08-18 10:15:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1646.pt (epoch 1646 @ 133326 updates, score 56.46) (writing took 14.95120833069086 seconds)
2022-08-18 10:15:58 | INFO | fairseq_cli.train | end of epoch 1646 (average epoch stats below)
2022-08-18 10:15:58 | INFO | train | epoch 1646 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6747.2 | ups 1.22 | wpb 5523.2 | bsz 358 | num_updates 133326 | lr 0.00017321 | gnorm 0.448 | train_wall 40 | gb_free 10 | wall 129854
2022-08-18 10:15:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:15:58 | INFO | fairseq.trainer | begin training epoch 1647
2022-08-18 10:15:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:16:37 | INFO | train_inner | epoch 1647:     74 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=7252.3, ups=1.31, wpb=5524.2, bsz=359.6, num_updates=133400, lr=0.000173162, gnorm=0.368, train_wall=50, gb_free=10.1, wall=129893
2022-08-18 10:16:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:16:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:16:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:16:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:16:49 | INFO | valid | epoch 1647 | valid on 'valid' subset | loss 5.293 | nll_loss 2.712 | ppl 6.55 | bleu 56.4 | wps 1869.9 | wpb 933.5 | bsz 59.6 | num_updates 133407 | best_bleu 57.52
2022-08-18 10:16:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1647 @ 133407 updates
2022-08-18 10:16:49 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1647.pt
2022-08-18 10:16:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1647.pt
2022-08-18 10:17:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1647.pt (epoch 1647 @ 133407 updates, score 56.4) (writing took 36.18568426370621 seconds)
2022-08-18 10:17:26 | INFO | fairseq_cli.train | end of epoch 1647 (average epoch stats below)
2022-08-18 10:17:26 | INFO | train | epoch 1647 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5097.4 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 133407 | lr 0.000173157 | gnorm 0.363 | train_wall 40 | gb_free 10.1 | wall 129942
2022-08-18 10:17:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:17:26 | INFO | fairseq.trainer | begin training epoch 1648
2022-08-18 10:17:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:18:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:18:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:18:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:18:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:18:26 | INFO | valid | epoch 1648 | valid on 'valid' subset | loss 5.291 | nll_loss 2.71 | ppl 6.54 | bleu 56.36 | wps 1858.9 | wpb 933.5 | bsz 59.6 | num_updates 133488 | best_bleu 57.52
2022-08-18 10:18:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1648 @ 133488 updates
2022-08-18 10:18:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1648.pt
2022-08-18 10:18:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1648.pt
2022-08-18 10:18:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1648.pt (epoch 1648 @ 133488 updates, score 56.36) (writing took 32.249307859689 seconds)
2022-08-18 10:18:58 | INFO | fairseq_cli.train | end of epoch 1648 (average epoch stats below)
2022-08-18 10:18:58 | INFO | train | epoch 1648 | loss 3.369 | nll_loss 0.339 | ppl 1.26 | wps 4826.2 | ups 0.87 | wpb 5523.2 | bsz 358 | num_updates 133488 | lr 0.000173105 | gnorm 0.421 | train_wall 39 | gb_free 10.1 | wall 130035
2022-08-18 10:18:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:18:59 | INFO | fairseq.trainer | begin training epoch 1649
2022-08-18 10:18:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:19:07 | INFO | train_inner | epoch 1649:     12 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=3699.5, ups=0.67, wpb=5541.1, bsz=360.1, num_updates=133500, lr=0.000173097, gnorm=0.39, train_wall=48, gb_free=10.1, wall=130043
2022-08-18 10:19:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:19:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:19:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:19:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:19:51 | INFO | valid | epoch 1649 | valid on 'valid' subset | loss 5.294 | nll_loss 2.714 | ppl 6.56 | bleu 56.5 | wps 1889.4 | wpb 933.5 | bsz 59.6 | num_updates 133569 | best_bleu 57.52
2022-08-18 10:19:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1649 @ 133569 updates
2022-08-18 10:19:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1649.pt
2022-08-18 10:19:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1649.pt
2022-08-18 10:20:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1649.pt (epoch 1649 @ 133569 updates, score 56.5) (writing took 22.960719604045153 seconds)
2022-08-18 10:20:14 | INFO | fairseq_cli.train | end of epoch 1649 (average epoch stats below)
2022-08-18 10:20:14 | INFO | train | epoch 1649 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 5897.1 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 133569 | lr 0.000173052 | gnorm 0.438 | train_wall 40 | gb_free 10.3 | wall 130111
2022-08-18 10:20:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:20:15 | INFO | fairseq.trainer | begin training epoch 1650
2022-08-18 10:20:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:20:32 | INFO | train_inner | epoch 1650:     31 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6487.2, ups=1.18, wpb=5509.4, bsz=354.2, num_updates=133600, lr=0.000173032, gnorm=0.437, train_wall=49, gb_free=10, wall=130128
2022-08-18 10:20:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:20:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:20:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:20:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:21:07 | INFO | valid | epoch 1650 | valid on 'valid' subset | loss 5.293 | nll_loss 2.713 | ppl 6.55 | bleu 56.34 | wps 1704.7 | wpb 933.5 | bsz 59.6 | num_updates 133650 | best_bleu 57.52
2022-08-18 10:21:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1650 @ 133650 updates
2022-08-18 10:21:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1650.pt
2022-08-18 10:21:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1650.pt
2022-08-18 10:21:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1650.pt (epoch 1650 @ 133650 updates, score 56.34) (writing took 16.777721904218197 seconds)
2022-08-18 10:21:24 | INFO | fairseq_cli.train | end of epoch 1650 (average epoch stats below)
2022-08-18 10:21:24 | INFO | train | epoch 1650 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6398 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 133650 | lr 0.000173 | gnorm 0.362 | train_wall 40 | gb_free 10 | wall 130180
2022-08-18 10:21:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:21:24 | INFO | fairseq.trainer | begin training epoch 1651
2022-08-18 10:21:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:21:52 | INFO | train_inner | epoch 1651:     50 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6849.2, ups=1.24, wpb=5534.5, bsz=363.1, num_updates=133700, lr=0.000172967, gnorm=0.396, train_wall=50, gb_free=10.1, wall=130209
2022-08-18 10:22:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:22:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:22:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:22:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:22:18 | INFO | valid | epoch 1651 | valid on 'valid' subset | loss 5.299 | nll_loss 2.722 | ppl 6.6 | bleu 56.14 | wps 1812.6 | wpb 933.5 | bsz 59.6 | num_updates 133731 | best_bleu 57.52
2022-08-18 10:22:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1651 @ 133731 updates
2022-08-18 10:22:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1651.pt
2022-08-18 10:22:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1651.pt
2022-08-18 10:22:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1651.pt (epoch 1651 @ 133731 updates, score 56.14) (writing took 40.96824074536562 seconds)
2022-08-18 10:22:59 | INFO | fairseq_cli.train | end of epoch 1651 (average epoch stats below)
2022-08-18 10:22:59 | INFO | train | epoch 1651 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 4721.7 | ups 0.85 | wpb 5523.2 | bsz 358 | num_updates 133731 | lr 0.000172947 | gnorm 0.499 | train_wall 41 | gb_free 10.2 | wall 130275
2022-08-18 10:22:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:22:59 | INFO | fairseq.trainer | begin training epoch 1652
2022-08-18 10:22:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:23:40 | INFO | train_inner | epoch 1652:     69 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5133.5, ups=0.93, wpb=5524.7, bsz=356.6, num_updates=133800, lr=0.000172903, gnorm=0.443, train_wall=49, gb_free=10.1, wall=130316
2022-08-18 10:23:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:23:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:23:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:23:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:23:55 | INFO | valid | epoch 1652 | valid on 'valid' subset | loss 5.294 | nll_loss 2.718 | ppl 6.58 | bleu 56.61 | wps 1817.4 | wpb 933.5 | bsz 59.6 | num_updates 133812 | best_bleu 57.52
2022-08-18 10:23:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1652 @ 133812 updates
2022-08-18 10:23:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1652.pt
2022-08-18 10:23:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1652.pt
2022-08-18 10:24:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1652.pt (epoch 1652 @ 133812 updates, score 56.61) (writing took 19.265490617603064 seconds)
2022-08-18 10:24:15 | INFO | fairseq_cli.train | end of epoch 1652 (average epoch stats below)
2022-08-18 10:24:15 | INFO | train | epoch 1652 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5906.2 | ups 1.07 | wpb 5523.2 | bsz 358 | num_updates 133812 | lr 0.000172895 | gnorm 0.367 | train_wall 40 | gb_free 10.1 | wall 130351
2022-08-18 10:24:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:24:15 | INFO | fairseq.trainer | begin training epoch 1653
2022-08-18 10:24:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:24:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:24:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:24:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:24:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:25:06 | INFO | valid | epoch 1653 | valid on 'valid' subset | loss 5.295 | nll_loss 2.716 | ppl 6.57 | bleu 56.73 | wps 1908 | wpb 933.5 | bsz 59.6 | num_updates 133893 | best_bleu 57.52
2022-08-18 10:25:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1653 @ 133893 updates
2022-08-18 10:25:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1653.pt
2022-08-18 10:25:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1653.pt
2022-08-18 10:25:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1653.pt (epoch 1653 @ 133893 updates, score 56.73) (writing took 23.621288366615772 seconds)
2022-08-18 10:25:29 | INFO | fairseq_cli.train | end of epoch 1653 (average epoch stats below)
2022-08-18 10:25:29 | INFO | train | epoch 1653 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5989 | ups 1.08 | wpb 5523.2 | bsz 358 | num_updates 133893 | lr 0.000172843 | gnorm 0.401 | train_wall 40 | gb_free 10.1 | wall 130426
2022-08-18 10:25:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:25:30 | INFO | fairseq.trainer | begin training epoch 1654
2022-08-18 10:25:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:25:35 | INFO | train_inner | epoch 1654:      7 / 81 loss=3.369, nll_loss=0.339, ppl=1.26, wps=4787.5, ups=0.87, wpb=5490.2, bsz=355.9, num_updates=133900, lr=0.000172838, gnorm=0.4, train_wall=49, gb_free=10.1, wall=130431
2022-08-18 10:26:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:26:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:26:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:26:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:26:22 | INFO | valid | epoch 1654 | valid on 'valid' subset | loss 5.292 | nll_loss 2.715 | ppl 6.56 | bleu 56.22 | wps 1784.1 | wpb 933.5 | bsz 59.6 | num_updates 133974 | best_bleu 57.52
2022-08-18 10:26:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1654 @ 133974 updates
2022-08-18 10:26:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1654.pt
2022-08-18 10:26:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1654.pt
2022-08-18 10:26:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1654.pt (epoch 1654 @ 133974 updates, score 56.22) (writing took 32.88097187876701 seconds)
2022-08-18 10:26:55 | INFO | fairseq_cli.train | end of epoch 1654 (average epoch stats below)
2022-08-18 10:26:56 | INFO | train | epoch 1654 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 5199.3 | ups 0.94 | wpb 5523.2 | bsz 358 | num_updates 133974 | lr 0.00017279 | gnorm 0.362 | train_wall 41 | gb_free 10 | wall 130512
2022-08-18 10:26:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:26:56 | INFO | fairseq.trainer | begin training epoch 1655
2022-08-18 10:26:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:27:11 | INFO | train_inner | epoch 1655:     26 / 81 loss=3.368, nll_loss=0.337, ppl=1.26, wps=5791.6, ups=1.04, wpb=5552.8, bsz=358.1, num_updates=134000, lr=0.000172774, gnorm=0.385, train_wall=50, gb_free=10.1, wall=130527
2022-08-18 10:27:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:27:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:27:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:27:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:27:48 | INFO | valid | epoch 1655 | valid on 'valid' subset | loss 5.291 | nll_loss 2.714 | ppl 6.56 | bleu 56.58 | wps 1900.8 | wpb 933.5 | bsz 59.6 | num_updates 134055 | best_bleu 57.52
2022-08-18 10:27:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1655 @ 134055 updates
2022-08-18 10:27:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1655.pt
2022-08-18 10:27:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1655.pt
2022-08-18 10:28:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1655.pt (epoch 1655 @ 134055 updates, score 56.58) (writing took 36.69471626356244 seconds)
2022-08-18 10:28:25 | INFO | fairseq_cli.train | end of epoch 1655 (average epoch stats below)
2022-08-18 10:28:25 | INFO | train | epoch 1655 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5025.6 | ups 0.91 | wpb 5523.2 | bsz 358 | num_updates 134055 | lr 0.000172738 | gnorm 0.41 | train_wall 40 | gb_free 10.1 | wall 130601
2022-08-18 10:28:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:28:25 | INFO | fairseq.trainer | begin training epoch 1656
2022-08-18 10:28:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:28:48 | INFO | train_inner | epoch 1656:     45 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5691.4, ups=1.03, wpb=5517.7, bsz=356.4, num_updates=134100, lr=0.000172709, gnorm=0.371, train_wall=49, gb_free=10.1, wall=130624
2022-08-18 10:29:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:29:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:29:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:29:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:29:15 | INFO | valid | epoch 1656 | valid on 'valid' subset | loss 5.288 | nll_loss 2.709 | ppl 6.54 | bleu 56.35 | wps 1896.3 | wpb 933.5 | bsz 59.6 | num_updates 134136 | best_bleu 57.52
2022-08-18 10:29:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1656 @ 134136 updates
2022-08-18 10:29:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1656.pt
2022-08-18 10:29:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1656.pt
2022-08-18 10:29:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1656.pt (epoch 1656 @ 134136 updates, score 56.35) (writing took 16.819437880069017 seconds)
2022-08-18 10:29:32 | INFO | fairseq_cli.train | end of epoch 1656 (average epoch stats below)
2022-08-18 10:29:32 | INFO | train | epoch 1656 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6623.4 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 134136 | lr 0.000172686 | gnorm 0.351 | train_wall 40 | gb_free 10.1 | wall 130668
2022-08-18 10:29:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:29:32 | INFO | fairseq.trainer | begin training epoch 1657
2022-08-18 10:29:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:30:07 | INFO | train_inner | epoch 1657:     64 / 81 loss=3.369, nll_loss=0.339, ppl=1.26, wps=7023.3, ups=1.27, wpb=5547.9, bsz=362.4, num_updates=134200, lr=0.000172645, gnorm=0.385, train_wall=50, gb_free=10, wall=130703
2022-08-18 10:30:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:30:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:30:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:30:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:30:24 | INFO | valid | epoch 1657 | valid on 'valid' subset | loss 5.301 | nll_loss 2.725 | ppl 6.61 | bleu 56.49 | wps 1908.2 | wpb 933.5 | bsz 59.6 | num_updates 134217 | best_bleu 57.52
2022-08-18 10:30:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1657 @ 134217 updates
2022-08-18 10:30:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1657.pt
2022-08-18 10:30:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1657.pt
2022-08-18 10:30:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1657.pt (epoch 1657 @ 134217 updates, score 56.49) (writing took 27.670433219522238 seconds)
2022-08-18 10:30:52 | INFO | fairseq_cli.train | end of epoch 1657 (average epoch stats below)
2022-08-18 10:30:52 | INFO | train | epoch 1657 | loss 3.369 | nll_loss 0.339 | ppl 1.26 | wps 5621.6 | ups 1.02 | wpb 5523.2 | bsz 358 | num_updates 134217 | lr 0.000172634 | gnorm 0.415 | train_wall 40 | gb_free 10.1 | wall 130748
2022-08-18 10:30:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:30:52 | INFO | fairseq.trainer | begin training epoch 1658
2022-08-18 10:30:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:31:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:31:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:31:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:31:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:31:44 | INFO | valid | epoch 1658 | valid on 'valid' subset | loss 5.286 | nll_loss 2.703 | ppl 6.51 | bleu 56.39 | wps 1882.4 | wpb 933.5 | bsz 59.6 | num_updates 134298 | best_bleu 57.52
2022-08-18 10:31:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1658 @ 134298 updates
2022-08-18 10:31:44 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1658.pt
2022-08-18 10:31:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1658.pt
2022-08-18 10:32:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1658.pt (epoch 1658 @ 134298 updates, score 56.39) (writing took 34.089381478726864 seconds)
2022-08-18 10:32:18 | INFO | fairseq_cli.train | end of epoch 1658 (average epoch stats below)
2022-08-18 10:32:18 | INFO | train | epoch 1658 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 5161.3 | ups 0.93 | wpb 5523.2 | bsz 358 | num_updates 134298 | lr 0.000172582 | gnorm 0.401 | train_wall 40 | gb_free 10.1 | wall 130835
2022-08-18 10:32:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:32:18 | INFO | fairseq.trainer | begin training epoch 1659
2022-08-18 10:32:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:32:21 | INFO | train_inner | epoch 1659:      2 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=4092.3, ups=0.75, wpb=5481.8, bsz=355.4, num_updates=134300, lr=0.000172581, gnorm=0.407, train_wall=49, gb_free=10.1, wall=130837
2022-08-18 10:33:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:33:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:33:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:33:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:33:11 | INFO | valid | epoch 1659 | valid on 'valid' subset | loss 5.294 | nll_loss 2.715 | ppl 6.57 | bleu 56.5 | wps 1800.8 | wpb 933.5 | bsz 59.6 | num_updates 134379 | best_bleu 57.52
2022-08-18 10:33:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1659 @ 134379 updates
2022-08-18 10:33:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1659.pt
2022-08-18 10:33:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1659.pt
2022-08-18 10:33:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1659.pt (epoch 1659 @ 134379 updates, score 56.5) (writing took 35.5367255397141 seconds)
2022-08-18 10:33:46 | INFO | fairseq_cli.train | end of epoch 1659 (average epoch stats below)
2022-08-18 10:33:46 | INFO | train | epoch 1659 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 5081.4 | ups 0.92 | wpb 5523.2 | bsz 358 | num_updates 134379 | lr 0.00017253 | gnorm 0.439 | train_wall 40 | gb_free 10.2 | wall 130923
2022-08-18 10:33:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:33:47 | INFO | fairseq.trainer | begin training epoch 1660
2022-08-18 10:33:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:33:59 | INFO | train_inner | epoch 1660:     21 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=5636.6, ups=1.02, wpb=5529.1, bsz=362.6, num_updates=134400, lr=0.000172516, gnorm=0.439, train_wall=49, gb_free=10.1, wall=130935
2022-08-18 10:34:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:34:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:34:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:34:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:34:38 | INFO | valid | epoch 1660 | valid on 'valid' subset | loss 5.299 | nll_loss 2.721 | ppl 6.59 | bleu 56.09 | wps 1869.5 | wpb 933.5 | bsz 59.6 | num_updates 134460 | best_bleu 57.52
2022-08-18 10:34:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1660 @ 134460 updates
2022-08-18 10:34:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1660.pt
2022-08-18 10:34:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1660.pt
2022-08-18 10:34:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1660.pt (epoch 1660 @ 134460 updates, score 56.09) (writing took 2.306736830621958 seconds)
2022-08-18 10:34:41 | INFO | fairseq_cli.train | end of epoch 1660 (average epoch stats below)
2022-08-18 10:34:41 | INFO | train | epoch 1660 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 8213.2 | ups 1.49 | wpb 5523.2 | bsz 358 | num_updates 134460 | lr 0.000172478 | gnorm 0.458 | train_wall 40 | gb_free 10.3 | wall 130977
2022-08-18 10:34:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:34:41 | INFO | fairseq.trainer | begin training epoch 1661
2022-08-18 10:34:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:35:03 | INFO | train_inner | epoch 1661:     40 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=8585.8, ups=1.55, wpb=5541.5, bsz=355, num_updates=134500, lr=0.000172452, gnorm=0.417, train_wall=50, gb_free=10.1, wall=130999
2022-08-18 10:35:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:35:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:35:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:35:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:35:32 | INFO | valid | epoch 1661 | valid on 'valid' subset | loss 5.294 | nll_loss 2.716 | ppl 6.57 | bleu 56.29 | wps 1862.5 | wpb 933.5 | bsz 59.6 | num_updates 134541 | best_bleu 57.52
2022-08-18 10:35:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1661 @ 134541 updates
2022-08-18 10:35:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1661.pt
2022-08-18 10:35:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1661.pt
2022-08-18 10:35:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1661.pt (epoch 1661 @ 134541 updates, score 56.29) (writing took 15.982862159609795 seconds)
2022-08-18 10:35:49 | INFO | fairseq_cli.train | end of epoch 1661 (average epoch stats below)
2022-08-18 10:35:49 | INFO | train | epoch 1661 | loss 3.368 | nll_loss 0.336 | ppl 1.26 | wps 6607.1 | ups 1.2 | wpb 5523.2 | bsz 358 | num_updates 134541 | lr 0.000172426 | gnorm 0.334 | train_wall 40 | gb_free 10.2 | wall 131045
2022-08-18 10:35:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:35:49 | INFO | fairseq.trainer | begin training epoch 1662
2022-08-18 10:35:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:36:19 | INFO | train_inner | epoch 1662:     59 / 81 loss=3.368, nll_loss=0.337, ppl=1.26, wps=7236.7, ups=1.31, wpb=5513.8, bsz=357.6, num_updates=134600, lr=0.000172388, gnorm=0.388, train_wall=49, gb_free=10.1, wall=131076
2022-08-18 10:36:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:36:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:36:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:36:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:36:39 | INFO | valid | epoch 1662 | valid on 'valid' subset | loss 5.298 | nll_loss 2.72 | ppl 6.59 | bleu 54.89 | wps 1925.6 | wpb 933.5 | bsz 59.6 | num_updates 134622 | best_bleu 57.52
2022-08-18 10:36:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1662 @ 134622 updates
2022-08-18 10:36:39 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1662.pt
2022-08-18 10:36:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1662.pt
2022-08-18 10:36:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1662.pt (epoch 1662 @ 134622 updates, score 54.89) (writing took 19.130788780748844 seconds)
2022-08-18 10:36:58 | INFO | fairseq_cli.train | end of epoch 1662 (average epoch stats below)
2022-08-18 10:36:58 | INFO | train | epoch 1662 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6412 | ups 1.16 | wpb 5523.2 | bsz 358 | num_updates 134622 | lr 0.000172374 | gnorm 0.438 | train_wall 40 | gb_free 10.1 | wall 131115
2022-08-18 10:36:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:36:59 | INFO | fairseq.trainer | begin training epoch 1663
2022-08-18 10:36:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:37:39 | INFO | train_inner | epoch 1663:     78 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=6895.7, ups=1.25, wpb=5524.6, bsz=358.3, num_updates=134700, lr=0.000172324, gnorm=0.373, train_wall=50, gb_free=10.1, wall=131156
2022-08-18 10:37:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:37:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:37:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:37:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:37:51 | INFO | valid | epoch 1663 | valid on 'valid' subset | loss 5.303 | nll_loss 2.725 | ppl 6.61 | bleu 56.49 | wps 1798.2 | wpb 933.5 | bsz 59.6 | num_updates 134703 | best_bleu 57.52
2022-08-18 10:37:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1663 @ 134703 updates
2022-08-18 10:37:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1663.pt
2022-08-18 10:37:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1663.pt
2022-08-18 10:38:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1663.pt (epoch 1663 @ 134703 updates, score 56.49) (writing took 15.594793379306793 seconds)
2022-08-18 10:38:06 | INFO | fairseq_cli.train | end of epoch 1663 (average epoch stats below)
2022-08-18 10:38:06 | INFO | train | epoch 1663 | loss 3.368 | nll_loss 0.337 | ppl 1.26 | wps 6586.2 | ups 1.19 | wpb 5523.2 | bsz 358 | num_updates 134703 | lr 0.000172322 | gnorm 0.35 | train_wall 41 | gb_free 10.2 | wall 131183
2022-08-18 10:38:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:38:06 | INFO | fairseq.trainer | begin training epoch 1664
2022-08-18 10:38:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:38:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:38:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:38:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:38:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:38:58 | INFO | valid | epoch 1664 | valid on 'valid' subset | loss 5.297 | nll_loss 2.72 | ppl 6.59 | bleu 56.17 | wps 1879.9 | wpb 933.5 | bsz 59.6 | num_updates 134784 | best_bleu 57.52
2022-08-18 10:38:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1664 @ 134784 updates
2022-08-18 10:38:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1664.pt
2022-08-18 10:38:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1664.pt
2022-08-18 10:39:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1664.pt (epoch 1664 @ 134784 updates, score 56.17) (writing took 50.19463123381138 seconds)
2022-08-18 10:39:48 | INFO | fairseq_cli.train | end of epoch 1664 (average epoch stats below)
2022-08-18 10:39:48 | INFO | train | epoch 1664 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 4389.4 | ups 0.79 | wpb 5523.2 | bsz 358 | num_updates 134784 | lr 0.00017227 | gnorm 0.583 | train_wall 40 | gb_free 10.1 | wall 131284
2022-08-18 10:39:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:39:48 | INFO | fairseq.trainer | begin training epoch 1665
2022-08-18 10:39:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-08-18 10:39:57 | INFO | train_inner | epoch 1665:     16 / 81 loss=3.369, nll_loss=0.338, ppl=1.26, wps=3990, ups=0.73, wpb=5495, bsz=356.2, num_updates=134800, lr=0.00017226, gnorm=0.549, train_wall=49, gb_free=10, wall=131293
2022-08-18 10:40:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 10:40:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-08-18 10:40:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-08-18 10:40:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-08-18 10:40:40 | INFO | valid | epoch 1665 | valid on 'valid' subset | loss 5.302 | nll_loss 2.727 | ppl 6.62 | bleu 55.78 | wps 1857.7 | wpb 933.5 | bsz 59.6 | num_updates 134865 | best_bleu 57.52
2022-08-18 10:40:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1665 @ 134865 updates
2022-08-18 10:40:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1665.pt
2022-08-18 10:40:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-fr/mmt/checkpoint1665.pt
2022-08-18 10:41:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-fr/mmt/checkpoint1665.pt (epoch 1665 @ 134865 updates, score 55.78) (writing took 20.86729634925723 seconds)
2022-08-18 10:41:01 | INFO | fairseq_cli.train | end of epoch 1665 (average epoch stats below)
2022-08-18 10:41:01 | INFO | train | epoch 1665 | loss 3.369 | nll_loss 0.338 | ppl 1.26 | wps 6173.6 | ups 1.12 | wpb 5523.2 | bsz 358 | num_updates 134865 | lr 0.000172219 | gnorm 0.411 | train_wall 40 | gb_free 10.1 | wall 131357
2022-08-18 10:41:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 81
2022-08-18 10:41:01 | INFO | fairseq.trainer | begin training epoch 1666
2022-08-18 10:41:01 | INFO | fairseq_cli.train | Start iterating over samples
train.sh: line 24: 31234 Killed                  fairseq-train ${ACMMT_ROOT}/data_bin/$SRC-$TGT --user-dir ${RGMMT_ROOT} --criterion label_smoothed_cross_entropy --task rgmmt_translation_task --arch rgmmt_model --optimizer adam --adam-betas 0.9,0.98 --clip-norm 0 --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 4000 --reset-optimizer --lr 0.001 --weight-decay 0.0001 --label-smoothing 0.2 --dropout 0.3 --max-tokens 1536 --no-progress-bar --log-interval 100 --stop-min-lr 1e-09 --keep-last-epochs 12 --update-freq 4 --eval-bleu --maximize-best-checkpoint-metric --save-dir ${SAVE_DIR} --share-decoder-input-output-embed --source-lang ${SRC} --target-lang ${TGT} --tensorboard-logdir ${SAVE_DIR}/bl_log1 --log-format simple --img-grid-prefix ${IMG_DATA_PREFIX}/resnet101-dlmmt --img-region-prefix ${IMG_DATA_PREFIX}/faster-dlmmt
2022-09-25 07:20:59 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': 'simple', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'results/en-de/mmt/bl_log1', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/data1/home/turghun/project/acmmt/examples/rgmmt', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1536, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1536, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [4], 'lr': [0.001], 'stop_min_lr': 1e-09, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'results/en-de/mmt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 12, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'rgmmt_model', 'activation_fn': relu, 'dropout': 0.3, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'adaptive_input': False, 'encoder': {'_name': None, 'embed_path': None, 'embed_dim': 512, 'ffn_embed_dim': 2048, 'layers': 6, 'attention_heads': 8, 'normalize_before': False, 'learned_pos': False, 'layerdrop': 0.0, 'layers_to_keep': None}, 'max_source_positions': 1024, 'decoder': {'_name': None, 'embed_path': None, 'embed_dim': 512, 'ffn_embed_dim': 2048, 'layers': 6, 'attention_heads': 8, 'normalize_before': False, 'learned_pos': False, 'layerdrop': 0.0, 'layers_to_keep': None, 'input_dim': 512, 'output_dim': 512}, 'max_target_positions': 1024, 'share_decoder_input_output_embed': True, 'share_all_embeddings': False, 'no_token_positional_embeddings': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'layernorm_embedding': False, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'no_cross_attention': False, 'cross_self_attention': False, 'quant_noise': {'_name': None, 'pq': 0.0, 'pq_block_size': 8, 'scalar': 0.0}, 'min_params_to_wrap': 100000000, 'char_inputs': False, 'relu_dropout': 0.0, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'export': False, 'no_decoder_final_norm': False, 'img_feature_dim': 512}, 'task': {'_name': 'rgmmt_translation_task', 'data': '/data1/home/turghun/project/acmmt/data_bin/en-de', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False, 'img_grid_prefix': '/data1/home/turghun/project/images/features/resnet101-dlmmt', 'img_region_prefix': '/data1/home/turghun/project/images/features/faster-dlmmt'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.2, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '0.9,0.98', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-09-25 07:20:59 | INFO | rgmmt.tasks.tasks | [en] dictionary: 8512 types
2022-09-25 07:20:59 | INFO | rgmmt.tasks.tasks | [de] dictionary: 9392 types
2022-09-25 07:21:00 | INFO | fairseq_cli.train | RGMMTTModel(
  (encoder): RGMMTEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(8512, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (region_img_project): Imgprojector(
      (linear): Linear(in_features=2048, out_features=512, bias=True)
    )
    (grid_img_project): Imgprojector(
      (linear): Linear(in_features=2048, out_features=512, bias=True)
    )
    (merge_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=512, out_features=512, bias=True)
      (v_proj): Linear(in_features=512, out_features=512, bias=True)
      (q_proj): Linear(in_features=512, out_features=512, bias=True)
      (out_proj): Linear(in_features=512, out_features=512, bias=True)
    )
    (gate): Linear(in_features=1024, out_features=512, bias=True)
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(9392, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=9392, bias=False)
  )
)
2022-09-25 07:21:00 | INFO | fairseq_cli.train | task: RGMMTTask
2022-09-25 07:21:00 | INFO | fairseq_cli.train | model: RGMMTTModel
2022-09-25 07:21:00 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-09-25 07:21:00 | INFO | fairseq_cli.train | num. shared model params: 56,978,944 (num. trained: 56,978,944)
2022-09-25 07:21:00 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-09-25 07:21:00 | INFO | fairseq.data.data_utils | loaded 1,014 examples from: /data1/home/turghun/project/acmmt/data_bin/en-de/valid.en-de.en
2022-09-25 07:21:00 | INFO | fairseq.data.data_utils | loaded 1,014 examples from: /data1/home/turghun/project/acmmt/data_bin/en-de/valid.en-de.de
2022-09-25 07:21:00 | INFO | rgmmt.tasks.tasks | /data1/home/turghun/project/acmmt/data_bin/en-de valid en-de 1014 examples
2022-09-25 07:21:00 | INFO | rgmmt.tasks.tasks | load 1014 grid image examples for valid
2022-09-25 07:21:00 | INFO | rgmmt.tasks.tasks | load 1014 region image examples for valid
2022-09-25 07:21:04 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-09-25 07:21:04 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-09-25 07:21:04 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 11.910 GB ; name = TITAN Xp                                
2022-09-25 07:21:04 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-09-25 07:21:04 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-09-25 07:21:04 | INFO | fairseq_cli.train | max tokens per device = 1536 and max sentences per device = None
2022-09-25 07:21:04 | INFO | fairseq.trainer | Preparing to load checkpoint results/en-de/mmt/checkpoint_last.pt
2022-09-25 07:21:04 | INFO | fairseq.trainer | Loaded checkpoint results/en-de/mmt/checkpoint_last.pt (epoch 2 @ 0 updates)
2022-09-25 07:21:04 | INFO | fairseq.trainer | loading train data for epoch 2
2022-09-25 07:21:04 | INFO | fairseq.data.data_utils | loaded 29,000 examples from: /data1/home/turghun/project/acmmt/data_bin/en-de/train.en-de.en
2022-09-25 07:21:04 | INFO | fairseq.data.data_utils | loaded 29,000 examples from: /data1/home/turghun/project/acmmt/data_bin/en-de/train.en-de.de
2022-09-25 07:21:13 | INFO | rgmmt.tasks.tasks | /data1/home/turghun/project/acmmt/data_bin/en-de train en-de 29000 examples
2022-09-25 07:21:13 | INFO | rgmmt.tasks.tasks | load 29000 grid image examples for train
2022-09-25 07:21:13 | INFO | rgmmt.tasks.tasks | load 29000 region image examples for train
2022-09-25 07:21:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 07:21:13 | INFO | fairseq.trainer | begin training epoch 2
2022-09-25 07:21:13 | INFO | fairseq_cli.train | Start iterating over samples
/data1/home/turghun/project/acmmt/examples/rgmmt/tasks/data_loader.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  region_img_features = torch.tensor(torch.stack(region_img_features_tmp, dim=0))
2022-09-25 07:21:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
/data1/home/turghun/project/acmmt/fairseq/utils.py:361: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2022-09-25 07:21:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 07:21:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 07:21:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 07:22:04 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.14 | nll_loss 8.996 | ppl 510.41 | bleu 0.21 | wps 2396.4 | wpb 1003.7 | bsz 67.6 | num_updates 78
2022-09-25 07:22:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 78 updates
2022-09-25 07:22:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint2.pt
2022-09-25 07:22:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint2.pt
2022-09-25 07:22:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint2.pt (epoch 2 @ 78 updates, score 0.21) (writing took 38.24443897977471 seconds)
2022-09-25 07:22:42 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-09-25 07:22:42 | INFO | train | epoch 002 | loss 10.56 | nll_loss 9.658 | ppl 807.64 | wps 4706 | ups 0.89 | wpb 5319.8 | bsz 371.8 | num_updates 78 | lr 1.9598e-05 | gnorm 1.379 | train_wall 37 | gb_free 10.2 | wall 99
2022-09-25 07:22:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 07:22:43 | INFO | fairseq.trainer | begin training epoch 3
2022-09-25 07:22:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 07:22:56 | INFO | train_inner | epoch 003:     22 / 78 loss=10.42, nll_loss=9.475, ppl=711.73, wps=5212.9, ups=0.98, wpb=5310.7, bsz=372.2, num_updates=100, lr=2.50975e-05, gnorm=1.318, train_wall=47, gb_free=10.2, wall=112
2022-09-25 07:23:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 07:23:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 07:23:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 07:23:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 07:23:59 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.371 | nll_loss 8.01 | ppl 257.73 | bleu 0.96 | wps 447.3 | wpb 1003.7 | bsz 67.6 | num_updates 156 | best_bleu 0.96
2022-09-25 07:23:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 156 updates
2022-09-25 07:23:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint3.pt
2022-09-25 07:24:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint3.pt
2022-09-25 07:24:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint3.pt (epoch 3 @ 156 updates, score 0.96) (writing took 35.30118830874562 seconds)
2022-09-25 07:24:34 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-09-25 07:24:34 | INFO | train | epoch 003 | loss 9.66 | nll_loss 8.478 | ppl 356.47 | wps 3708.2 | ups 0.7 | wpb 5319.8 | bsz 371.8 | num_updates 156 | lr 3.90961e-05 | gnorm 1.317 | train_wall 35 | gb_free 10.2 | wall 211
2022-09-25 07:24:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 07:24:34 | INFO | fairseq.trainer | begin training epoch 4
2022-09-25 07:24:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 07:24:59 | INFO | train_inner | epoch 004:     44 / 78 loss=9.366, nll_loss=8.084, ppl=271.37, wps=4338.1, ups=0.82, wpb=5309.6, bsz=370, num_updates=200, lr=5.0095e-05, gnorm=1.488, train_wall=45, gb_free=10.2, wall=235
2022-09-25 07:25:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 07:25:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 07:25:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 07:25:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 07:25:30 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.859 | nll_loss 7.221 | ppl 149.23 | bleu 4.37 | wps 1176.1 | wpb 1003.7 | bsz 67.6 | num_updates 234 | best_bleu 4.37
2022-09-25 07:25:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 234 updates
2022-09-25 07:25:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint4.pt
2022-09-25 07:25:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint4.pt
2022-09-25 07:26:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint4.pt (epoch 4 @ 234 updates, score 4.37) (writing took 38.925923839211464 seconds)
2022-09-25 07:26:09 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-09-25 07:26:09 | INFO | train | epoch 004 | loss 9.012 | nll_loss 7.602 | ppl 194.29 | wps 4383.8 | ups 0.82 | wpb 5319.8 | bsz 371.8 | num_updates 234 | lr 5.85941e-05 | gnorm 1.463 | train_wall 35 | gb_free 10.1 | wall 305
2022-09-25 07:26:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 07:26:09 | INFO | fairseq.trainer | begin training epoch 5
2022-09-25 07:26:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 07:26:47 | INFO | train_inner | epoch 005:     66 / 78 loss=8.709, nll_loss=7.183, ppl=145.29, wps=4918, ups=0.92, wpb=5353.6, bsz=372.2, num_updates=300, lr=7.50925e-05, gnorm=1.483, train_wall=43, gb_free=10.1, wall=344
2022-09-25 07:26:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 07:26:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 07:26:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 07:26:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 07:27:11 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.492 | nll_loss 6.716 | ppl 105.1 | bleu 4.97 | wps 887.4 | wpb 1003.7 | bsz 67.6 | num_updates 312 | best_bleu 4.97
2022-09-25 07:27:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 312 updates
2022-09-25 07:27:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint5.pt
2022-09-25 07:27:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint5.pt
2022-09-25 07:27:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint5.pt (epoch 5 @ 312 updates, score 4.97) (writing took 37.610497668385506 seconds)
2022-09-25 07:27:49 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-09-25 07:27:49 | INFO | train | epoch 005 | loss 8.59 | nll_loss 7.02 | ppl 129.78 | wps 4149.2 | ups 0.78 | wpb 5319.8 | bsz 371.8 | num_updates 312 | lr 7.80922e-05 | gnorm 1.514 | train_wall 33 | gb_free 10.2 | wall 405
2022-09-25 07:27:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 07:27:49 | INFO | fairseq.trainer | begin training epoch 6
2022-09-25 07:27:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 07:28:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 07:28:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 07:28:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 07:28:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 07:28:53 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.178 | nll_loss 6.303 | ppl 78.95 | bleu 5.05 | wps 685.3 | wpb 1003.7 | bsz 67.6 | num_updates 390 | best_bleu 5.05
2022-09-25 07:28:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 390 updates
2022-09-25 07:28:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint6.pt
2022-09-25 07:28:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint6.pt
2022-09-25 07:29:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint6.pt (epoch 6 @ 390 updates, score 5.05) (writing took 38.836407244205475 seconds)
2022-09-25 07:29:32 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-09-25 07:29:32 | INFO | train | epoch 006 | loss 8.236 | nll_loss 6.546 | ppl 93.44 | wps 4023 | ups 0.76 | wpb 5319.8 | bsz 371.8 | num_updates 390 | lr 9.75902e-05 | gnorm 1.343 | train_wall 34 | gb_free 10.2 | wall 508
2022-09-25 07:29:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 07:29:32 | INFO | fairseq.trainer | begin training epoch 7
2022-09-25 07:29:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 07:29:38 | INFO | train_inner | epoch 007:     10 / 78 loss=8.247, nll_loss=6.56, ppl=94.36, wps=3094.4, ups=0.58, wpb=5289.9, bsz=369.9, num_updates=400, lr=0.00010009, gnorm=1.297, train_wall=44, gb_free=10.1, wall=515
2022-09-25 07:30:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 07:30:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 07:30:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 07:30:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 07:30:36 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.86 | nll_loss 5.9 | ppl 59.71 | bleu 8.33 | wps 701.2 | wpb 1003.7 | bsz 67.6 | num_updates 468 | best_bleu 8.33
2022-09-25 07:30:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 468 updates
2022-09-25 07:30:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint7.pt
2022-09-25 07:30:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint7.pt
2022-09-25 07:31:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint7.pt (epoch 7 @ 468 updates, score 8.33) (writing took 38.08829636126757 seconds)
2022-09-25 07:31:14 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-09-25 07:31:14 | INFO | train | epoch 007 | loss 7.934 | nll_loss 6.146 | ppl 70.84 | wps 4053.9 | ups 0.76 | wpb 5319.8 | bsz 371.8 | num_updates 468 | lr 0.000117088 | gnorm 1.348 | train_wall 35 | gb_free 10.1 | wall 611
2022-09-25 07:31:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 07:31:15 | INFO | fairseq.trainer | begin training epoch 8
2022-09-25 07:31:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 07:31:33 | INFO | train_inner | epoch 008:     32 / 78 loss=7.807, nll_loss=5.981, ppl=63.15, wps=4647.3, ups=0.87, wpb=5347, bsz=382.4, num_updates=500, lr=0.000125087, gnorm=1.367, train_wall=45, gb_free=10.1, wall=630
2022-09-25 07:31:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 07:31:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 07:31:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 07:31:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 07:32:11 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.617 | nll_loss 5.548 | ppl 46.79 | bleu 9.85 | wps 1056.8 | wpb 1003.7 | bsz 67.6 | num_updates 546 | best_bleu 9.85
2022-09-25 07:32:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 546 updates
2022-09-25 07:32:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint8.pt
2022-09-25 07:32:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint8.pt
2022-09-25 07:32:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint8.pt (epoch 8 @ 546 updates, score 9.85) (writing took 37.68145191296935 seconds)
2022-09-25 07:32:49 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-09-25 07:32:49 | INFO | train | epoch 008 | loss 7.657 | nll_loss 5.78 | ppl 54.96 | wps 4397.8 | ups 0.83 | wpb 5319.8 | bsz 371.8 | num_updates 546 | lr 0.000136586 | gnorm 1.304 | train_wall 36 | gb_free 10.2 | wall 705
2022-09-25 07:32:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 07:32:49 | INFO | fairseq.trainer | begin training epoch 9
2022-09-25 07:32:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 07:33:17 | INFO | train_inner | epoch 009:     54 / 78 loss=7.557, nll_loss=5.644, ppl=49.99, wps=5123.8, ups=0.97, wpb=5302.5, bsz=361.3, num_updates=600, lr=0.000150085, gnorm=1.262, train_wall=46, gb_free=10.2, wall=733
2022-09-25 07:33:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 07:33:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 07:33:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 07:33:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 07:33:42 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.379 | nll_loss 5.215 | ppl 37.15 | bleu 11.62 | wps 1185.2 | wpb 1003.7 | bsz 67.6 | num_updates 624 | best_bleu 11.62
2022-09-25 07:33:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 624 updates
2022-09-25 07:33:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint9.pt
2022-09-25 07:33:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint9.pt
2022-09-25 07:34:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint9.pt (epoch 9 @ 624 updates, score 11.62) (writing took 41.74191980808973 seconds)
2022-09-25 07:34:24 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-09-25 07:34:24 | INFO | train | epoch 009 | loss 7.399 | nll_loss 5.433 | ppl 43.21 | wps 4350.5 | ups 0.82 | wpb 5319.8 | bsz 371.8 | num_updates 624 | lr 0.000156084 | gnorm 1.271 | train_wall 36 | gb_free 10.2 | wall 800
2022-09-25 07:34:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 07:34:24 | INFO | fairseq.trainer | begin training epoch 10
2022-09-25 07:34:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 07:35:07 | INFO | train_inner | epoch 010:     76 / 78 loss=7.191, nll_loss=5.155, ppl=35.62, wps=4864, ups=0.91, wpb=5332.6, bsz=375.4, num_updates=700, lr=0.000175082, gnorm=1.233, train_wall=46, gb_free=10.2, wall=843
2022-09-25 07:35:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 07:35:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 07:35:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 07:35:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 07:35:34 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.105 | nll_loss 4.852 | ppl 28.89 | bleu 13.39 | wps 615.6 | wpb 1003.7 | bsz 67.6 | num_updates 702 | best_bleu 13.39
2022-09-25 07:35:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 702 updates
2022-09-25 07:35:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint10.pt
2022-09-25 07:35:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint10.pt
2022-09-25 07:36:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint10.pt (epoch 10 @ 702 updates, score 13.39) (writing took 42.49665280804038 seconds)
2022-09-25 07:36:17 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-09-25 07:36:17 | INFO | train | epoch 010 | loss 7.148 | nll_loss 5.098 | ppl 34.25 | wps 3692.8 | ups 0.69 | wpb 5319.8 | bsz 371.8 | num_updates 702 | lr 0.000175582 | gnorm 1.2 | train_wall 36 | gb_free 10.1 | wall 913
2022-09-25 07:36:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 07:36:17 | INFO | fairseq.trainer | begin training epoch 11
2022-09-25 07:36:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 07:37:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 07:37:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 07:37:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 07:37:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 07:37:18 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.89 | nll_loss 4.56 | ppl 23.58 | bleu 14.95 | wps 2085.4 | wpb 1003.7 | bsz 67.6 | num_updates 780 | best_bleu 14.95
2022-09-25 07:37:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 780 updates
2022-09-25 07:37:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint11.pt
2022-09-25 07:37:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint11.pt
2022-09-25 07:38:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint11.pt (epoch 11 @ 780 updates, score 14.95) (writing took 64.48272571340203 seconds)
2022-09-25 07:38:23 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-09-25 07:38:23 | INFO | train | epoch 011 | loss 6.909 | nll_loss 4.779 | ppl 27.45 | wps 3281.8 | ups 0.62 | wpb 5319.8 | bsz 371.8 | num_updates 780 | lr 0.00019508 | gnorm 1.194 | train_wall 46 | gb_free 10.2 | wall 1039
2022-09-25 07:38:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 07:38:23 | INFO | fairseq.trainer | begin training epoch 12
2022-09-25 07:38:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 07:38:35 | INFO | train_inner | epoch 012:     20 / 78 loss=6.855, nll_loss=4.707, ppl=26.12, wps=2565.7, ups=0.48, wpb=5345.6, bsz=374.2, num_updates=800, lr=0.00020008, gnorm=1.186, train_wall=56, gb_free=10.1, wall=1051
2022-09-25 07:39:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 07:39:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 07:39:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 07:39:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 07:39:16 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.752 | nll_loss 4.327 | ppl 20.07 | bleu 16.63 | wps 1387.4 | wpb 1003.7 | bsz 67.6 | num_updates 858 | best_bleu 16.63
2022-09-25 07:39:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 858 updates
2022-09-25 07:39:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint12.pt
2022-09-25 07:39:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint12.pt
2022-09-25 07:40:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint12.pt (epoch 12 @ 858 updates, score 16.63) (writing took 66.67945702373981 seconds)
2022-09-25 07:40:23 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-09-25 07:40:23 | INFO | train | epoch 012 | loss 6.695 | nll_loss 4.493 | ppl 22.52 | wps 3466.7 | ups 0.65 | wpb 5319.8 | bsz 371.8 | num_updates 858 | lr 0.000214579 | gnorm 1.161 | train_wall 35 | gb_free 10.1 | wall 1159
2022-09-25 07:40:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 07:40:23 | INFO | fairseq.trainer | begin training epoch 13
2022-09-25 07:40:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 07:40:46 | INFO | train_inner | epoch 013:     42 / 78 loss=6.64, nll_loss=4.419, ppl=21.38, wps=4056.9, ups=0.76, wpb=5306.3, bsz=369.2, num_updates=900, lr=0.000225077, gnorm=1.166, train_wall=45, gb_free=10.2, wall=1182
2022-09-25 07:41:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 07:41:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 07:41:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 07:41:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 07:41:15 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.577 | nll_loss 4.109 | ppl 17.26 | bleu 18.34 | wps 1452.2 | wpb 1003.7 | bsz 67.6 | num_updates 936 | best_bleu 18.34
2022-09-25 07:41:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 936 updates
2022-09-25 07:41:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint13.pt
2022-09-25 07:41:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint13.pt
2022-09-25 07:41:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint13.pt (epoch 13 @ 936 updates, score 18.34) (writing took 39.82480303943157 seconds)
2022-09-25 07:41:55 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-09-25 07:41:55 | INFO | train | epoch 013 | loss 6.501 | nll_loss 4.233 | ppl 18.81 | wps 4502 | ups 0.85 | wpb 5319.8 | bsz 371.8 | num_updates 936 | lr 0.000234077 | gnorm 1.144 | train_wall 34 | gb_free 10.2 | wall 1251
2022-09-25 07:41:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 07:41:55 | INFO | fairseq.trainer | begin training epoch 14
2022-09-25 07:41:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 07:42:32 | INFO | train_inner | epoch 014:     64 / 78 loss=6.37, nll_loss=4.058, ppl=16.65, wps=5032, ups=0.94, wpb=5339.2, bsz=371.1, num_updates=1000, lr=0.000250075, gnorm=1.103, train_wall=45, gb_free=10.1, wall=1288
2022-09-25 07:42:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 07:42:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 07:42:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 07:42:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 07:42:47 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.457 | nll_loss 3.954 | ppl 15.49 | bleu 18.09 | wps 1965.6 | wpb 1003.7 | bsz 67.6 | num_updates 1014 | best_bleu 18.34
2022-09-25 07:42:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 1014 updates
2022-09-25 07:42:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint14.pt
2022-09-25 07:42:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint14.pt
2022-09-25 07:43:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint14.pt (epoch 14 @ 1014 updates, score 18.09) (writing took 16.731949996203184 seconds)
2022-09-25 07:43:04 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-09-25 07:43:04 | INFO | train | epoch 014 | loss 6.314 | nll_loss 3.984 | ppl 15.82 | wps 6012.9 | ups 1.13 | wpb 5319.8 | bsz 371.8 | num_updates 1014 | lr 0.000253575 | gnorm 1.105 | train_wall 35 | gb_free 10.1 | wall 1320
2022-09-25 07:43:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 07:43:04 | INFO | fairseq.trainer | begin training epoch 15
2022-09-25 07:43:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 07:43:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 07:43:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 07:43:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 07:43:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 07:43:55 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.315 | nll_loss 3.77 | ppl 13.64 | bleu 21.44 | wps 1977.2 | wpb 1003.7 | bsz 67.6 | num_updates 1092 | best_bleu 21.44
2022-09-25 07:43:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 1092 updates
2022-09-25 07:43:55 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint15.pt
2022-09-25 07:43:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint15.pt
2022-09-25 07:44:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint15.pt (epoch 15 @ 1092 updates, score 21.44) (writing took 41.475904036313295 seconds)
2022-09-25 07:44:37 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-09-25 07:44:37 | INFO | train | epoch 015 | loss 6.143 | nll_loss 3.757 | ppl 13.52 | wps 4461.6 | ups 0.84 | wpb 5319.8 | bsz 371.8 | num_updates 1092 | lr 0.000273073 | gnorm 1.001 | train_wall 35 | gb_free 10.2 | wall 1413
2022-09-25 07:44:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 07:44:37 | INFO | fairseq.trainer | begin training epoch 16
2022-09-25 07:44:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 07:44:42 | INFO | train_inner | epoch 016:      8 / 78 loss=6.156, nll_loss=3.775, ppl=13.69, wps=4060.8, ups=0.77, wpb=5298.1, bsz=369.8, num_updates=1100, lr=0.000275072, gnorm=1.019, train_wall=45, gb_free=10.1, wall=1419
2022-09-25 07:45:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 07:45:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 07:45:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 07:45:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 07:45:45 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.197 | nll_loss 3.586 | ppl 12.01 | bleu 23.22 | wps 805.4 | wpb 1003.7 | bsz 67.6 | num_updates 1170 | best_bleu 23.22
2022-09-25 07:45:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 1170 updates
2022-09-25 07:45:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint16.pt
2022-09-25 07:45:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint16.pt
2022-09-25 07:46:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint16.pt (epoch 16 @ 1170 updates, score 23.22) (writing took 18.967816334217787 seconds)
2022-09-25 07:46:04 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-09-25 07:46:04 | INFO | train | epoch 016 | loss 5.981 | nll_loss 3.54 | ppl 11.63 | wps 4740.7 | ups 0.89 | wpb 5319.8 | bsz 371.8 | num_updates 1170 | lr 0.000292571 | gnorm 0.956 | train_wall 40 | gb_free 10.2 | wall 1501
2022-09-25 07:46:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 07:46:05 | INFO | fairseq.trainer | begin training epoch 17
2022-09-25 07:46:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 07:46:31 | INFO | train_inner | epoch 017:     30 / 78 loss=5.915, nll_loss=3.453, ppl=10.95, wps=4880.9, ups=0.92, wpb=5293.1, bsz=376.5, num_updates=1200, lr=0.00030007, gnorm=0.963, train_wall=56, gb_free=10.1, wall=1527
2022-09-25 07:46:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 07:46:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 07:46:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 07:46:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 07:47:04 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.152 | nll_loss 3.482 | ppl 11.17 | bleu 22.87 | wps 2084.5 | wpb 1003.7 | bsz 67.6 | num_updates 1248 | best_bleu 23.22
2022-09-25 07:47:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1248 updates
2022-09-25 07:47:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint17.pt
2022-09-25 07:47:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint17.pt
2022-09-25 07:47:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint17.pt (epoch 17 @ 1248 updates, score 22.87) (writing took 33.11325382813811 seconds)
2022-09-25 07:47:37 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-09-25 07:47:37 | INFO | train | epoch 017 | loss 5.839 | nll_loss 3.351 | ppl 10.2 | wps 4473 | ups 0.84 | wpb 5319.8 | bsz 371.8 | num_updates 1248 | lr 0.000312069 | gnorm 0.987 | train_wall 41 | gb_free 10.2 | wall 1593
2022-09-25 07:47:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 07:47:37 | INFO | fairseq.trainer | begin training epoch 18
2022-09-25 07:47:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 07:48:06 | INFO | train_inner | epoch 018:     52 / 78 loss=5.784, nll_loss=3.275, ppl=9.68, wps=5597.5, ups=1.05, wpb=5313.3, bsz=370.4, num_updates=1300, lr=0.000325067, gnorm=0.96, train_wall=44, gb_free=10.1, wall=1622
2022-09-25 07:48:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 07:48:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 07:48:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 07:48:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 07:48:28 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.069 | nll_loss 3.388 | ppl 10.47 | bleu 23.31 | wps 1996.1 | wpb 1003.7 | bsz 67.6 | num_updates 1326 | best_bleu 23.31
2022-09-25 07:48:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1326 updates
2022-09-25 07:48:28 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint18.pt
2022-09-25 07:48:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint18.pt
2022-09-25 07:48:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint18.pt (epoch 18 @ 1326 updates, score 23.31) (writing took 27.031053591519594 seconds)
2022-09-25 07:48:55 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-09-25 07:48:55 | INFO | train | epoch 018 | loss 5.71 | nll_loss 3.175 | ppl 9.03 | wps 5351.4 | ups 1.01 | wpb 5319.8 | bsz 371.8 | num_updates 1326 | lr 0.000331567 | gnorm 0.934 | train_wall 35 | gb_free 10.2 | wall 1671
2022-09-25 07:48:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 07:48:55 | INFO | fairseq.trainer | begin training epoch 19
2022-09-25 07:48:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 07:49:35 | INFO | train_inner | epoch 019:     74 / 78 loss=5.614, nll_loss=3.045, ppl=8.25, wps=6019, ups=1.13, wpb=5346.1, bsz=373.8, num_updates=1400, lr=0.000350065, gnorm=0.912, train_wall=44, gb_free=10.1, wall=1711
2022-09-25 07:49:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 07:49:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 07:49:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 07:49:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 07:49:45 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 5.963 | nll_loss 3.267 | ppl 9.63 | bleu 25.17 | wps 1942.2 | wpb 1003.7 | bsz 67.6 | num_updates 1404 | best_bleu 25.17
2022-09-25 07:49:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1404 updates
2022-09-25 07:49:45 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint19.pt
2022-09-25 07:49:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint19.pt
2022-09-25 07:50:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint19.pt (epoch 19 @ 1404 updates, score 25.17) (writing took 36.65283128246665 seconds)
2022-09-25 07:50:21 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-09-25 07:50:21 | INFO | train | epoch 019 | loss 5.588 | nll_loss 3.011 | ppl 8.06 | wps 4785 | ups 0.9 | wpb 5319.8 | bsz 371.8 | num_updates 1404 | lr 0.000351065 | gnorm 0.906 | train_wall 34 | gb_free 10.1 | wall 1758
2022-09-25 07:50:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 07:50:22 | INFO | fairseq.trainer | begin training epoch 20
2022-09-25 07:50:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 07:51:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 07:51:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 07:51:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 07:51:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 07:51:16 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 5.92 | nll_loss 3.191 | ppl 9.13 | bleu 25.88 | wps 1293.1 | wpb 1003.7 | bsz 67.6 | num_updates 1482 | best_bleu 25.88
2022-09-25 07:51:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 1482 updates
2022-09-25 07:51:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint20.pt
2022-09-25 07:51:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint20.pt
2022-09-25 07:51:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint20.pt (epoch 20 @ 1482 updates, score 25.88) (writing took 41.951018039137125 seconds)
2022-09-25 07:51:58 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-09-25 07:51:58 | INFO | train | epoch 020 | loss 5.467 | nll_loss 2.848 | ppl 7.2 | wps 4303 | ups 0.81 | wpb 5319.8 | bsz 371.8 | num_updates 1482 | lr 0.000370563 | gnorm 0.876 | train_wall 35 | gb_free 10.2 | wall 1854
2022-09-25 07:51:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 07:51:58 | INFO | fairseq.trainer | begin training epoch 21
2022-09-25 07:51:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 07:52:08 | INFO | train_inner | epoch 021:     18 / 78 loss=5.454, nll_loss=2.829, ppl=7.11, wps=3459.5, ups=0.65, wpb=5311.4, bsz=367.1, num_updates=1500, lr=0.000375062, gnorm=0.866, train_wall=45, gb_free=10.1, wall=1864
2022-09-25 07:52:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 07:52:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 07:52:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 07:52:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 07:52:50 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 5.853 | nll_loss 3.06 | ppl 8.34 | bleu 26.95 | wps 1861 | wpb 1003.7 | bsz 67.6 | num_updates 1560 | best_bleu 26.95
2022-09-25 07:52:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 1560 updates
2022-09-25 07:52:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint21.pt
2022-09-25 07:52:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint21.pt
2022-09-25 07:53:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint21.pt (epoch 21 @ 1560 updates, score 26.95) (writing took 39.09054120257497 seconds)
2022-09-25 07:53:29 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-09-25 07:53:29 | INFO | train | epoch 021 | loss 5.345 | nll_loss 2.683 | ppl 6.42 | wps 4565.9 | ups 0.86 | wpb 5319.8 | bsz 371.8 | num_updates 1560 | lr 0.000390061 | gnorm 0.848 | train_wall 35 | gb_free 10.1 | wall 1945
2022-09-25 07:53:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 07:53:29 | INFO | fairseq.trainer | begin training epoch 22
2022-09-25 07:53:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 07:53:55 | INFO | train_inner | epoch 022:     40 / 78 loss=5.297, nll_loss=2.618, ppl=6.14, wps=4995.4, ups=0.94, wpb=5318.7, bsz=372, num_updates=1600, lr=0.00040006, gnorm=0.847, train_wall=45, gb_free=10.2, wall=1971
2022-09-25 07:54:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 07:54:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 07:54:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 07:54:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 07:54:31 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 5.8 | nll_loss 3.023 | ppl 8.13 | bleu 28.17 | wps 1276.4 | wpb 1003.7 | bsz 67.6 | num_updates 1638 | best_bleu 28.17
2022-09-25 07:54:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 1638 updates
2022-09-25 07:54:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint22.pt
2022-09-25 07:54:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint22.pt
2022-09-25 07:55:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint22.pt (epoch 22 @ 1638 updates, score 28.17) (writing took 36.31279365345836 seconds)
2022-09-25 07:55:07 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-09-25 07:55:07 | INFO | train | epoch 022 | loss 5.242 | nll_loss 2.542 | ppl 5.83 | wps 4204.8 | ups 0.79 | wpb 5319.8 | bsz 371.8 | num_updates 1638 | lr 0.000409559 | gnorm 0.846 | train_wall 35 | gb_free 10.2 | wall 2044
2022-09-25 07:55:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 07:55:08 | INFO | fairseq.trainer | begin training epoch 23
2022-09-25 07:55:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 07:55:43 | INFO | train_inner | epoch 023:     62 / 78 loss=5.191, nll_loss=2.473, ppl=5.55, wps=4924.6, ups=0.93, wpb=5316, bsz=373.7, num_updates=1700, lr=0.000425057, gnorm=0.838, train_wall=45, gb_free=10.2, wall=2079
2022-09-25 07:55:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 07:55:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 07:55:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 07:55:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 07:56:06 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 5.793 | nll_loss 2.991 | ppl 7.95 | bleu 28.13 | wps 1045.7 | wpb 1003.7 | bsz 67.6 | num_updates 1716 | best_bleu 28.17
2022-09-25 07:56:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 1716 updates
2022-09-25 07:56:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint23.pt
2022-09-25 07:56:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint23.pt
2022-09-25 07:56:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint23.pt (epoch 23 @ 1716 updates, score 28.13) (writing took 17.19334391504526 seconds)
2022-09-25 07:56:24 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-09-25 07:56:24 | INFO | train | epoch 023 | loss 5.142 | nll_loss 2.407 | ppl 5.31 | wps 5456.3 | ups 1.03 | wpb 5319.8 | bsz 371.8 | num_updates 1716 | lr 0.000429057 | gnorm 0.823 | train_wall 34 | gb_free 10.1 | wall 2120
2022-09-25 07:56:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 07:56:24 | INFO | fairseq.trainer | begin training epoch 24
2022-09-25 07:56:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 07:57:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 07:57:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 07:57:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 07:57:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 07:57:16 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 5.792 | nll_loss 2.989 | ppl 7.94 | bleu 27.46 | wps 1834.3 | wpb 1003.7 | bsz 67.6 | num_updates 1794 | best_bleu 28.17
2022-09-25 07:57:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 1794 updates
2022-09-25 07:57:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint24.pt
2022-09-25 07:57:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint24.pt
2022-09-25 07:57:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint24.pt (epoch 24 @ 1794 updates, score 27.46) (writing took 14.499708388000727 seconds)
2022-09-25 07:57:31 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-09-25 07:57:31 | INFO | train | epoch 024 | loss 5.039 | nll_loss 2.268 | ppl 4.82 | wps 6147.1 | ups 1.16 | wpb 5319.8 | bsz 371.8 | num_updates 1794 | lr 0.000448555 | gnorm 0.808 | train_wall 34 | gb_free 10.1 | wall 2187
2022-09-25 07:57:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 07:57:31 | INFO | fairseq.trainer | begin training epoch 25
2022-09-25 07:57:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 07:57:35 | INFO | train_inner | epoch 025:      6 / 78 loss=5.053, nll_loss=2.286, ppl=4.88, wps=4709, ups=0.89, wpb=5319.1, bsz=370.2, num_updates=1800, lr=0.000450055, gnorm=0.809, train_wall=44, gb_free=10.2, wall=2192
2022-09-25 07:58:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 07:58:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 07:58:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 07:58:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 07:58:26 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 5.74 | nll_loss 2.925 | ppl 7.59 | bleu 28.43 | wps 2086.5 | wpb 1003.7 | bsz 67.6 | num_updates 1872 | best_bleu 28.43
2022-09-25 07:58:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 1872 updates
2022-09-25 07:58:26 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint25.pt
2022-09-25 07:58:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint25.pt
2022-09-25 07:59:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint25.pt (epoch 25 @ 1872 updates, score 28.43) (writing took 40.03225589171052 seconds)
2022-09-25 07:59:06 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-09-25 07:59:06 | INFO | train | epoch 025 | loss 4.95 | nll_loss 2.146 | ppl 4.43 | wps 4357.3 | ups 0.82 | wpb 5319.8 | bsz 371.8 | num_updates 1872 | lr 0.000468053 | gnorm 0.814 | train_wall 33 | gb_free 10.2 | wall 2283
2022-09-25 07:59:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 07:59:06 | INFO | fairseq.trainer | begin training epoch 26
2022-09-25 07:59:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 07:59:23 | INFO | train_inner | epoch 026:     28 / 78 loss=4.914, nll_loss=2.097, ppl=4.28, wps=4952.2, ups=0.93, wpb=5332.6, bsz=372.4, num_updates=1900, lr=0.000475052, gnorm=0.812, train_wall=43, gb_free=10.2, wall=2299
2022-09-25 07:59:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 07:59:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 07:59:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 07:59:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:00:04 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 5.712 | nll_loss 2.873 | ppl 7.33 | bleu 29.81 | wps 1411.4 | wpb 1003.7 | bsz 67.6 | num_updates 1950 | best_bleu 29.81
2022-09-25 08:00:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 1950 updates
2022-09-25 08:00:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint26.pt
2022-09-25 08:00:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint26.pt
2022-09-25 08:00:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint26.pt (epoch 26 @ 1950 updates, score 29.81) (writing took 37.68470136448741 seconds)
2022-09-25 08:00:42 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-09-25 08:00:42 | INFO | train | epoch 026 | loss 4.856 | nll_loss 2.019 | ppl 4.05 | wps 4341.4 | ups 0.82 | wpb 5319.8 | bsz 371.8 | num_updates 1950 | lr 0.000487551 | gnorm 0.795 | train_wall 35 | gb_free 10.1 | wall 2378
2022-09-25 08:00:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:00:42 | INFO | fairseq.trainer | begin training epoch 27
2022-09-25 08:00:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:01:11 | INFO | train_inner | epoch 027:     50 / 78 loss=4.797, nll_loss=1.942, ppl=3.84, wps=4927.7, ups=0.93, wpb=5321, bsz=377.4, num_updates=2000, lr=0.00050005, gnorm=0.782, train_wall=45, gb_free=10.1, wall=2407
2022-09-25 08:01:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:01:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:01:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:01:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:01:39 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 5.709 | nll_loss 2.88 | ppl 7.36 | bleu 29.21 | wps 1156.4 | wpb 1003.7 | bsz 67.6 | num_updates 2028 | best_bleu 29.81
2022-09-25 08:01:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 2028 updates
2022-09-25 08:01:39 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint27.pt
2022-09-25 08:01:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint27.pt
2022-09-25 08:01:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint27.pt (epoch 27 @ 2028 updates, score 29.21) (writing took 16.3086021207273 seconds)
2022-09-25 08:01:55 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-09-25 08:01:55 | INFO | train | epoch 027 | loss 4.767 | nll_loss 1.9 | ppl 3.73 | wps 5655.7 | ups 1.06 | wpb 5319.8 | bsz 371.8 | num_updates 2028 | lr 0.000507049 | gnorm 0.774 | train_wall 35 | gb_free 10.2 | wall 2452
2022-09-25 08:01:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:01:55 | INFO | fairseq.trainer | begin training epoch 28
2022-09-25 08:01:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:02:34 | INFO | train_inner | epoch 028:     72 / 78 loss=4.741, nll_loss=1.861, ppl=3.63, wps=6417.9, ups=1.21, wpb=5314.6, bsz=364.7, num_updates=2100, lr=0.000525047, gnorm=0.783, train_wall=45, gb_free=10.2, wall=2490
2022-09-25 08:02:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:02:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:02:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:02:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:02:50 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 5.718 | nll_loss 2.888 | ppl 7.4 | bleu 30.53 | wps 1258.4 | wpb 1003.7 | bsz 67.6 | num_updates 2106 | best_bleu 30.53
2022-09-25 08:02:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 2106 updates
2022-09-25 08:02:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint28.pt
2022-09-25 08:02:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint28.pt
2022-09-25 08:03:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint28.pt (epoch 28 @ 2106 updates, score 30.53) (writing took 37.466099083423615 seconds)
2022-09-25 08:03:27 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-09-25 08:03:27 | INFO | train | epoch 028 | loss 4.696 | nll_loss 1.802 | ppl 3.49 | wps 4503.6 | ups 0.85 | wpb 5319.8 | bsz 371.8 | num_updates 2106 | lr 0.000526547 | gnorm 0.787 | train_wall 35 | gb_free 10.2 | wall 2544
2022-09-25 08:03:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:03:28 | INFO | fairseq.trainer | begin training epoch 29
2022-09-25 08:03:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:04:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:04:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:04:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:04:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:04:18 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 5.673 | nll_loss 2.826 | ppl 7.09 | bleu 31.62 | wps 1718.4 | wpb 1003.7 | bsz 67.6 | num_updates 2184 | best_bleu 31.62
2022-09-25 08:04:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 2184 updates
2022-09-25 08:04:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint29.pt
2022-09-25 08:04:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint29.pt
2022-09-25 08:04:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint29.pt (epoch 29 @ 2184 updates, score 31.62) (writing took 40.12196161597967 seconds)
2022-09-25 08:04:58 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-09-25 08:04:58 | INFO | train | epoch 029 | loss 4.619 | nll_loss 1.697 | ppl 3.24 | wps 4569 | ups 0.86 | wpb 5319.8 | bsz 371.8 | num_updates 2184 | lr 0.000546045 | gnorm 0.782 | train_wall 34 | gb_free 10.3 | wall 2634
2022-09-25 08:04:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:04:58 | INFO | fairseq.trainer | begin training epoch 30
2022-09-25 08:04:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:05:08 | INFO | train_inner | epoch 030:     16 / 78 loss=4.605, nll_loss=1.679, ppl=3.2, wps=3435.8, ups=0.65, wpb=5305.2, bsz=370.8, num_updates=2200, lr=0.000550045, gnorm=0.78, train_wall=43, gb_free=10.1, wall=2645
2022-09-25 08:05:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:05:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:05:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:05:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:05:54 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 5.694 | nll_loss 2.831 | ppl 7.12 | bleu 30.38 | wps 1228.7 | wpb 1003.7 | bsz 67.6 | num_updates 2262 | best_bleu 31.62
2022-09-25 08:05:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 2262 updates
2022-09-25 08:05:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint30.pt
2022-09-25 08:05:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint30.pt
2022-09-25 08:06:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint30.pt (epoch 30 @ 2262 updates, score 30.38) (writing took 14.327013798058033 seconds)
2022-09-25 08:06:08 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-09-25 08:06:08 | INFO | train | epoch 030 | loss 4.549 | nll_loss 1.604 | ppl 3.04 | wps 5912.8 | ups 1.11 | wpb 5319.8 | bsz 371.8 | num_updates 2262 | lr 0.000565543 | gnorm 0.771 | train_wall 35 | gb_free 10.1 | wall 2705
2022-09-25 08:06:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:06:09 | INFO | fairseq.trainer | begin training epoch 31
2022-09-25 08:06:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:06:29 | INFO | train_inner | epoch 031:     38 / 78 loss=4.515, nll_loss=1.558, ppl=2.94, wps=6608.4, ups=1.24, wpb=5345.4, bsz=377.8, num_updates=2300, lr=0.000575042, gnorm=0.765, train_wall=46, gb_free=10.1, wall=2726
2022-09-25 08:06:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:06:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:06:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:06:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:06:58 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 5.719 | nll_loss 2.886 | ppl 7.39 | bleu 30.64 | wps 2156.3 | wpb 1003.7 | bsz 67.6 | num_updates 2340 | best_bleu 31.62
2022-09-25 08:06:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 2340 updates
2022-09-25 08:06:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint31.pt
2022-09-25 08:06:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint31.pt
2022-09-25 08:07:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint31.pt (epoch 31 @ 2340 updates, score 30.64) (writing took 18.08127401024103 seconds)
2022-09-25 08:07:16 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-09-25 08:07:16 | INFO | train | epoch 031 | loss 4.484 | nll_loss 1.515 | ppl 2.86 | wps 6142.7 | ups 1.15 | wpb 5319.8 | bsz 371.8 | num_updates 2340 | lr 0.000585041 | gnorm 0.763 | train_wall 35 | gb_free 10.1 | wall 2772
2022-09-25 08:07:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:07:16 | INFO | fairseq.trainer | begin training epoch 32
2022-09-25 08:07:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:07:47 | INFO | train_inner | epoch 032:     60 / 78 loss=4.453, nll_loss=1.474, ppl=2.78, wps=6858.5, ups=1.29, wpb=5312.5, bsz=368.5, num_updates=2400, lr=0.00060004, gnorm=0.753, train_wall=45, gb_free=10.2, wall=2803
2022-09-25 08:07:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:07:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:07:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:07:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:08:04 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 5.689 | nll_loss 2.856 | ppl 7.24 | bleu 30.8 | wps 2077.3 | wpb 1003.7 | bsz 67.6 | num_updates 2418 | best_bleu 31.62
2022-09-25 08:08:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 2418 updates
2022-09-25 08:08:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint32.pt
2022-09-25 08:08:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint32.pt
2022-09-25 08:08:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint32.pt (epoch 32 @ 2418 updates, score 30.8) (writing took 17.515004463493824 seconds)
2022-09-25 08:08:21 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-09-25 08:08:21 | INFO | train | epoch 032 | loss 4.429 | nll_loss 1.441 | ppl 2.72 | wps 6352.4 | ups 1.19 | wpb 5319.8 | bsz 371.8 | num_updates 2418 | lr 0.00060454 | gnorm 0.758 | train_wall 35 | gb_free 10.1 | wall 2838
2022-09-25 08:08:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:08:21 | INFO | fairseq.trainer | begin training epoch 33
2022-09-25 08:08:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:09:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:09:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:09:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:09:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:09:10 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 5.718 | nll_loss 2.87 | ppl 7.31 | bleu 30.85 | wps 1979 | wpb 1003.7 | bsz 67.6 | num_updates 2496 | best_bleu 31.62
2022-09-25 08:09:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 2496 updates
2022-09-25 08:09:10 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint33.pt
2022-09-25 08:09:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint33.pt
2022-09-25 08:09:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint33.pt (epoch 33 @ 2496 updates, score 30.85) (writing took 16.3143654987216 seconds)
2022-09-25 08:09:26 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-09-25 08:09:26 | INFO | train | epoch 033 | loss 4.375 | nll_loss 1.371 | ppl 2.59 | wps 6388.6 | ups 1.2 | wpb 5319.8 | bsz 371.8 | num_updates 2496 | lr 0.000624038 | gnorm 0.753 | train_wall 35 | gb_free 10.1 | wall 2902
2022-09-25 08:09:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:09:26 | INFO | fairseq.trainer | begin training epoch 34
2022-09-25 08:09:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:09:30 | INFO | train_inner | epoch 034:      4 / 78 loss=4.39, nll_loss=1.389, ppl=2.62, wps=5162.2, ups=0.97, wpb=5322, bsz=374.5, num_updates=2500, lr=0.000625037, gnorm=0.761, train_wall=44, gb_free=10.1, wall=2906
2022-09-25 08:10:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:10:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:10:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:10:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:10:17 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 5.707 | nll_loss 2.858 | ppl 7.25 | bleu 31.53 | wps 2013.4 | wpb 1003.7 | bsz 67.6 | num_updates 2574 | best_bleu 31.62
2022-09-25 08:10:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 2574 updates
2022-09-25 08:10:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint34.pt
2022-09-25 08:10:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint34.pt
2022-09-25 08:10:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint34.pt (epoch 34 @ 2574 updates, score 31.53) (writing took 16.789005286991596 seconds)
2022-09-25 08:10:34 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-09-25 08:10:34 | INFO | train | epoch 034 | loss 4.33 | nll_loss 1.311 | ppl 2.48 | wps 6119.2 | ups 1.15 | wpb 5319.8 | bsz 371.8 | num_updates 2574 | lr 0.000643536 | gnorm 0.746 | train_wall 35 | gb_free 10.1 | wall 2970
2022-09-25 08:10:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:10:34 | INFO | fairseq.trainer | begin training epoch 35
2022-09-25 08:10:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:10:50 | INFO | train_inner | epoch 035:     26 / 78 loss=4.304, nll_loss=1.277, ppl=2.42, wps=6653.3, ups=1.25, wpb=5333.4, bsz=375.1, num_updates=2600, lr=0.000650035, gnorm=0.731, train_wall=46, gb_free=10.2, wall=2986
2022-09-25 08:11:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:11:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:11:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:11:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:11:24 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 5.71 | nll_loss 2.859 | ppl 7.26 | bleu 31.37 | wps 2063.3 | wpb 1003.7 | bsz 67.6 | num_updates 2652 | best_bleu 31.62
2022-09-25 08:11:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 2652 updates
2022-09-25 08:11:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint35.pt
2022-09-25 08:11:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint35.pt
2022-09-25 08:11:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint35.pt (epoch 35 @ 2652 updates, score 31.37) (writing took 15.832635454833508 seconds)
2022-09-25 08:11:40 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-09-25 08:11:40 | INFO | train | epoch 035 | loss 4.28 | nll_loss 1.246 | ppl 2.37 | wps 6248 | ups 1.17 | wpb 5319.8 | bsz 371.8 | num_updates 2652 | lr 0.000663034 | gnorm 0.734 | train_wall 36 | gb_free 10.1 | wall 3037
2022-09-25 08:11:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:11:41 | INFO | fairseq.trainer | begin training epoch 36
2022-09-25 08:11:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:12:07 | INFO | train_inner | epoch 036:     48 / 78 loss=4.261, nll_loss=1.221, ppl=2.33, wps=6869.7, ups=1.3, wpb=5295.5, bsz=366.2, num_updates=2700, lr=0.000675032, gnorm=0.73, train_wall=46, gb_free=10.1, wall=3063
2022-09-25 08:12:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:12:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:12:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:12:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:12:31 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 5.729 | nll_loss 2.903 | ppl 7.48 | bleu 30.75 | wps 2185.8 | wpb 1003.7 | bsz 67.6 | num_updates 2730 | best_bleu 31.62
2022-09-25 08:12:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 2730 updates
2022-09-25 08:12:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint36.pt
2022-09-25 08:12:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint36.pt
2022-09-25 08:12:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint36.pt (epoch 36 @ 2730 updates, score 30.75) (writing took 13.186587296426296 seconds)
2022-09-25 08:12:44 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-09-25 08:12:44 | INFO | train | epoch 036 | loss 4.24 | nll_loss 1.194 | ppl 2.29 | wps 6538.8 | ups 1.23 | wpb 5319.8 | bsz 371.8 | num_updates 2730 | lr 0.000682532 | gnorm 0.72 | train_wall 35 | gb_free 10.1 | wall 3100
2022-09-25 08:12:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:12:44 | INFO | fairseq.trainer | begin training epoch 37
2022-09-25 08:12:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:13:22 | INFO | train_inner | epoch 037:     70 / 78 loss=4.227, nll_loss=1.178, ppl=2.26, wps=7151.4, ups=1.34, wpb=5345.4, bsz=373.3, num_updates=2800, lr=0.00070003, gnorm=0.72, train_wall=44, gb_free=10.1, wall=3138
2022-09-25 08:13:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:13:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:13:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:13:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:13:33 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 5.762 | nll_loss 2.951 | ppl 7.73 | bleu 29.74 | wps 2274.5 | wpb 1003.7 | bsz 67.6 | num_updates 2808 | best_bleu 31.62
2022-09-25 08:13:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 2808 updates
2022-09-25 08:13:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint37.pt
2022-09-25 08:13:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint37.pt
2022-09-25 08:13:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint37.pt (epoch 37 @ 2808 updates, score 29.74) (writing took 15.322022013366222 seconds)
2022-09-25 08:13:48 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-09-25 08:13:48 | INFO | train | epoch 037 | loss 4.198 | nll_loss 1.141 | ppl 2.21 | wps 6429.6 | ups 1.21 | wpb 5319.8 | bsz 371.8 | num_updates 2808 | lr 0.00070203 | gnorm 0.711 | train_wall 34 | gb_free 10.2 | wall 3165
2022-09-25 08:13:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:13:49 | INFO | fairseq.trainer | begin training epoch 38
2022-09-25 08:13:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:14:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:14:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:14:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:14:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:14:40 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 5.752 | nll_loss 2.962 | ppl 7.79 | bleu 30.44 | wps 2204.3 | wpb 1003.7 | bsz 67.6 | num_updates 2886 | best_bleu 31.62
2022-09-25 08:14:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 2886 updates
2022-09-25 08:14:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint38.pt
2022-09-25 08:14:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint38.pt
2022-09-25 08:14:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint38.pt (epoch 38 @ 2886 updates, score 30.44) (writing took 16.76851811632514 seconds)
2022-09-25 08:14:56 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-09-25 08:14:56 | INFO | train | epoch 038 | loss 4.177 | nll_loss 1.116 | ppl 2.17 | wps 6101.5 | ups 1.15 | wpb 5319.8 | bsz 371.8 | num_updates 2886 | lr 0.000721528 | gnorm 0.712 | train_wall 35 | gb_free 10.1 | wall 3233
2022-09-25 08:14:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:14:57 | INFO | fairseq.trainer | begin training epoch 39
2022-09-25 08:14:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:15:05 | INFO | train_inner | epoch 039:     14 / 78 loss=4.167, nll_loss=1.103, ppl=2.15, wps=5131.2, ups=0.97, wpb=5301.5, bsz=370.9, num_updates=2900, lr=0.000725027, gnorm=0.709, train_wall=45, gb_free=10.1, wall=3241
2022-09-25 08:15:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:15:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:15:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:15:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:15:46 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 5.756 | nll_loss 2.944 | ppl 7.7 | bleu 31.39 | wps 2065.1 | wpb 1003.7 | bsz 67.6 | num_updates 2964 | best_bleu 31.62
2022-09-25 08:15:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 2964 updates
2022-09-25 08:15:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint39.pt
2022-09-25 08:15:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint39.pt
2022-09-25 08:16:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint39.pt (epoch 39 @ 2964 updates, score 31.39) (writing took 15.810364443808794 seconds)
2022-09-25 08:16:02 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-09-25 08:16:02 | INFO | train | epoch 039 | loss 4.15 | nll_loss 1.082 | ppl 2.12 | wps 6316.7 | ups 1.19 | wpb 5319.8 | bsz 371.8 | num_updates 2964 | lr 0.000741026 | gnorm 0.712 | train_wall 36 | gb_free 10.1 | wall 3298
2022-09-25 08:16:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:16:02 | INFO | fairseq.trainer | begin training epoch 40
2022-09-25 08:16:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:16:22 | INFO | train_inner | epoch 040:     36 / 78 loss=4.144, nll_loss=1.073, ppl=2.1, wps=6928.4, ups=1.3, wpb=5331.2, bsz=370.1, num_updates=3000, lr=0.000750025, gnorm=0.71, train_wall=46, gb_free=10, wall=3318
2022-09-25 08:16:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:16:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:16:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:16:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:16:52 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 5.733 | nll_loss 2.918 | ppl 7.56 | bleu 32.2 | wps 2082.5 | wpb 1003.7 | bsz 67.6 | num_updates 3042 | best_bleu 32.2
2022-09-25 08:16:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 3042 updates
2022-09-25 08:16:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint40.pt
2022-09-25 08:16:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint40.pt
2022-09-25 08:17:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint40.pt (epoch 40 @ 3042 updates, score 32.2) (writing took 41.20153337717056 seconds)
2022-09-25 08:17:34 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-09-25 08:17:34 | INFO | train | epoch 040 | loss 4.125 | nll_loss 1.052 | ppl 2.07 | wps 4540.8 | ups 0.85 | wpb 5319.8 | bsz 371.8 | num_updates 3042 | lr 0.000760524 | gnorm 0.697 | train_wall 34 | gb_free 10.2 | wall 3390
2022-09-25 08:17:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:17:34 | INFO | fairseq.trainer | begin training epoch 41
2022-09-25 08:17:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:18:05 | INFO | train_inner | epoch 041:     58 / 78 loss=4.107, nll_loss=1.032, ppl=2.04, wps=5158.5, ups=0.97, wpb=5314.4, bsz=375.8, num_updates=3100, lr=0.000775022, gnorm=0.692, train_wall=44, gb_free=10.1, wall=3421
2022-09-25 08:18:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:18:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:18:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:18:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:18:23 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 5.729 | nll_loss 2.935 | ppl 7.65 | bleu 30.34 | wps 2129.6 | wpb 1003.7 | bsz 67.6 | num_updates 3120 | best_bleu 32.2
2022-09-25 08:18:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 3120 updates
2022-09-25 08:18:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint41.pt
2022-09-25 08:18:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint41.pt
2022-09-25 08:18:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint41.pt (epoch 41 @ 3120 updates, score 30.34) (writing took 15.047090239822865 seconds)
2022-09-25 08:18:38 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-09-25 08:18:38 | INFO | train | epoch 041 | loss 4.1 | nll_loss 1.022 | ppl 2.03 | wps 6393.8 | ups 1.2 | wpb 5319.8 | bsz 371.8 | num_updates 3120 | lr 0.000780022 | gnorm 0.685 | train_wall 35 | gb_free 10.1 | wall 3455
2022-09-25 08:18:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:18:39 | INFO | fairseq.trainer | begin training epoch 42
2022-09-25 08:18:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:19:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:19:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:19:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:19:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:19:29 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 5.762 | nll_loss 2.989 | ppl 7.94 | bleu 30.02 | wps 2263.2 | wpb 1003.7 | bsz 67.6 | num_updates 3198 | best_bleu 32.2
2022-09-25 08:19:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 3198 updates
2022-09-25 08:19:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint42.pt
2022-09-25 08:19:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint42.pt
2022-09-25 08:19:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint42.pt (epoch 42 @ 3198 updates, score 30.02) (writing took 16.58183180168271 seconds)
2022-09-25 08:19:45 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-09-25 08:19:46 | INFO | train | epoch 042 | loss 4.085 | nll_loss 1.006 | ppl 2.01 | wps 6186.7 | ups 1.16 | wpb 5319.8 | bsz 371.8 | num_updates 3198 | lr 0.00079952 | gnorm 0.687 | train_wall 34 | gb_free 10.3 | wall 3522
2022-09-25 08:19:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:19:46 | INFO | fairseq.trainer | begin training epoch 43
2022-09-25 08:19:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:19:48 | INFO | train_inner | epoch 043:      2 / 78 loss=4.097, nll_loss=1.02, ppl=2.03, wps=5172.6, ups=0.97, wpb=5307.5, bsz=368.1, num_updates=3200, lr=0.00080002, gnorm=0.689, train_wall=44, gb_free=10.1, wall=3524
2022-09-25 08:20:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:20:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:20:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:20:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:20:36 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 5.745 | nll_loss 2.976 | ppl 7.87 | bleu 31.31 | wps 2144 | wpb 1003.7 | bsz 67.6 | num_updates 3276 | best_bleu 32.2
2022-09-25 08:20:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 3276 updates
2022-09-25 08:20:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint43.pt
2022-09-25 08:20:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint43.pt
2022-09-25 08:20:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint43.pt (epoch 43 @ 3276 updates, score 31.31) (writing took 15.747881807386875 seconds)
2022-09-25 08:20:52 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-09-25 08:20:52 | INFO | train | epoch 043 | loss 4.074 | nll_loss 0.992 | ppl 1.99 | wps 6270.3 | ups 1.18 | wpb 5319.8 | bsz 371.8 | num_updates 3276 | lr 0.000819018 | gnorm 0.684 | train_wall 35 | gb_free 10.2 | wall 3588
2022-09-25 08:20:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:20:52 | INFO | fairseq.trainer | begin training epoch 44
2022-09-25 08:20:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:21:06 | INFO | train_inner | epoch 044:     24 / 78 loss=4.063, nll_loss=0.977, ppl=1.97, wps=6824.2, ups=1.28, wpb=5317.9, bsz=370.9, num_updates=3300, lr=0.000825017, gnorm=0.676, train_wall=45, gb_free=10.2, wall=3602
2022-09-25 08:21:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:21:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:21:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:21:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:21:41 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 5.727 | nll_loss 2.965 | ppl 7.81 | bleu 32.03 | wps 2253.6 | wpb 1003.7 | bsz 67.6 | num_updates 3354 | best_bleu 32.2
2022-09-25 08:21:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 3354 updates
2022-09-25 08:21:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint44.pt
2022-09-25 08:21:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint44.pt
2022-09-25 08:21:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint44.pt (epoch 44 @ 3354 updates, score 32.03) (writing took 17.496069762855768 seconds)
2022-09-25 08:21:59 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-09-25 08:21:59 | INFO | train | epoch 044 | loss 4.05 | nll_loss 0.965 | ppl 1.95 | wps 6187 | ups 1.16 | wpb 5319.8 | bsz 371.8 | num_updates 3354 | lr 0.000838516 | gnorm 0.672 | train_wall 35 | gb_free 10.1 | wall 3655
2022-09-25 08:21:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:21:59 | INFO | fairseq.trainer | begin training epoch 45
2022-09-25 08:21:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:22:26 | INFO | train_inner | epoch 045:     46 / 78 loss=4.03, nll_loss=0.942, ppl=1.92, wps=6645.1, ups=1.24, wpb=5339.9, bsz=379.6, num_updates=3400, lr=0.000850015, gnorm=0.655, train_wall=45, gb_free=10.1, wall=3682
2022-09-25 08:22:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:22:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:22:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:22:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:22:51 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 5.741 | nll_loss 2.971 | ppl 7.84 | bleu 32.13 | wps 2018.2 | wpb 1003.7 | bsz 67.6 | num_updates 3432 | best_bleu 32.2
2022-09-25 08:22:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 3432 updates
2022-09-25 08:22:51 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint45.pt
2022-09-25 08:22:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint45.pt
2022-09-25 08:23:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint45.pt (epoch 45 @ 3432 updates, score 32.13) (writing took 16.169436652213335 seconds)
2022-09-25 08:23:07 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-09-25 08:23:07 | INFO | train | epoch 045 | loss 4.027 | nll_loss 0.938 | ppl 1.92 | wps 6072.3 | ups 1.14 | wpb 5319.8 | bsz 371.8 | num_updates 3432 | lr 0.000858014 | gnorm 0.653 | train_wall 36 | gb_free 10.1 | wall 3723
2022-09-25 08:23:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:23:07 | INFO | fairseq.trainer | begin training epoch 46
2022-09-25 08:23:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:23:51 | INFO | train_inner | epoch 046:     68 / 78 loss=4.04, nll_loss=0.955, ppl=1.94, wps=6195.2, ups=1.17, wpb=5286.5, bsz=364.6, num_updates=3500, lr=0.000875012, gnorm=0.674, train_wall=45, gb_free=10.2, wall=3768
2022-09-25 08:23:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:23:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:23:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:23:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:24:05 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 5.75 | nll_loss 3.006 | ppl 8.03 | bleu 31.41 | wps 2148.1 | wpb 1003.7 | bsz 67.6 | num_updates 3510 | best_bleu 32.2
2022-09-25 08:24:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 3510 updates
2022-09-25 08:24:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint46.pt
2022-09-25 08:24:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint46.pt
2022-09-25 08:24:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint46.pt (epoch 46 @ 3510 updates, score 31.41) (writing took 18.692562110722065 seconds)
2022-09-25 08:24:24 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-09-25 08:24:24 | INFO | train | epoch 046 | loss 4.028 | nll_loss 0.941 | ppl 1.92 | wps 5386.2 | ups 1.01 | wpb 5319.8 | bsz 371.8 | num_updates 3510 | lr 0.000877512 | gnorm 0.664 | train_wall 35 | gb_free 10.1 | wall 3800
2022-09-25 08:24:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:24:24 | INFO | fairseq.trainer | begin training epoch 47
2022-09-25 08:24:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:25:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:25:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:25:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:25:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:25:14 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 5.769 | nll_loss 3.029 | ppl 8.16 | bleu 32.11 | wps 2316.4 | wpb 1003.7 | bsz 67.6 | num_updates 3588 | best_bleu 32.2
2022-09-25 08:25:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 3588 updates
2022-09-25 08:25:14 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint47.pt
2022-09-25 08:25:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint47.pt
2022-09-25 08:25:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint47.pt (epoch 47 @ 3588 updates, score 32.11) (writing took 16.269204828888178 seconds)
2022-09-25 08:25:30 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-09-25 08:25:30 | INFO | train | epoch 047 | loss 4.006 | nll_loss 0.915 | ppl 1.88 | wps 6296.2 | ups 1.18 | wpb 5319.8 | bsz 371.8 | num_updates 3588 | lr 0.00089701 | gnorm 0.661 | train_wall 35 | gb_free 10.1 | wall 3866
2022-09-25 08:25:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:25:30 | INFO | fairseq.trainer | begin training epoch 48
2022-09-25 08:25:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:25:38 | INFO | train_inner | epoch 048:     12 / 78 loss=4.01, nll_loss=0.919, ppl=1.89, wps=5013.6, ups=0.94, wpb=5322.2, bsz=370.8, num_updates=3600, lr=0.00090001, gnorm=0.66, train_wall=45, gb_free=10.2, wall=3874
2022-09-25 08:26:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:26:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:26:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:26:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:26:20 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 5.736 | nll_loss 2.982 | ppl 7.9 | bleu 32.02 | wps 2214.7 | wpb 1003.7 | bsz 67.6 | num_updates 3666 | best_bleu 32.2
2022-09-25 08:26:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 3666 updates
2022-09-25 08:26:20 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint48.pt
2022-09-25 08:26:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint48.pt
2022-09-25 08:26:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint48.pt (epoch 48 @ 3666 updates, score 32.02) (writing took 17.23736083507538 seconds)
2022-09-25 08:26:37 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-09-25 08:26:37 | INFO | train | epoch 048 | loss 4.007 | nll_loss 0.918 | ppl 1.89 | wps 6163.9 | ups 1.16 | wpb 5319.8 | bsz 371.8 | num_updates 3666 | lr 0.000916508 | gnorm 0.663 | train_wall 35 | gb_free 10.2 | wall 3934
2022-09-25 08:26:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:26:38 | INFO | fairseq.trainer | begin training epoch 49
2022-09-25 08:26:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:27:00 | INFO | train_inner | epoch 049:     34 / 78 loss=3.991, nll_loss=0.899, ppl=1.86, wps=6479.3, ups=1.21, wpb=5333.9, bsz=374.4, num_updates=3700, lr=0.000925007, gnorm=0.656, train_wall=45, gb_free=10.2, wall=3956
2022-09-25 08:27:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:27:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:27:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:27:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:27:35 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 5.733 | nll_loss 2.995 | ppl 7.97 | bleu 31.97 | wps 2123.5 | wpb 1003.7 | bsz 67.6 | num_updates 3744 | best_bleu 32.2
2022-09-25 08:27:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 3744 updates
2022-09-25 08:27:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint49.pt
2022-09-25 08:27:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint49.pt
2022-09-25 08:27:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint49.pt (epoch 49 @ 3744 updates, score 31.97) (writing took 16.46998655050993 seconds)
2022-09-25 08:27:51 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-09-25 08:27:51 | INFO | train | epoch 049 | loss 3.994 | nll_loss 0.903 | ppl 1.87 | wps 5618.2 | ups 1.06 | wpb 5319.8 | bsz 371.8 | num_updates 3744 | lr 0.000936006 | gnorm 0.668 | train_wall 35 | gb_free 10.2 | wall 4008
2022-09-25 08:27:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:27:51 | INFO | fairseq.trainer | begin training epoch 50
2022-09-25 08:27:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:28:21 | INFO | train_inner | epoch 050:     56 / 78 loss=3.994, nll_loss=0.904, ppl=1.87, wps=6531.3, ups=1.23, wpb=5318.9, bsz=372.7, num_updates=3800, lr=0.000950005, gnorm=0.662, train_wall=46, gb_free=10.1, wall=4038
2022-09-25 08:28:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:28:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:28:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:28:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:28:40 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 5.779 | nll_loss 3.05 | ppl 8.28 | bleu 31.7 | wps 2219.1 | wpb 1003.7 | bsz 67.6 | num_updates 3822 | best_bleu 32.2
2022-09-25 08:28:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 3822 updates
2022-09-25 08:28:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint50.pt
2022-09-25 08:28:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint50.pt
2022-09-25 08:28:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint50.pt (epoch 50 @ 3822 updates, score 31.7) (writing took 17.907398715615273 seconds)
2022-09-25 08:28:59 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-09-25 08:28:59 | INFO | train | epoch 050 | loss 3.99 | nll_loss 0.901 | ppl 1.87 | wps 6166.9 | ups 1.16 | wpb 5319.8 | bsz 371.8 | num_updates 3822 | lr 0.000955504 | gnorm 0.659 | train_wall 36 | gb_free 10.2 | wall 4075
2022-09-25 08:28:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:28:59 | INFO | fairseq.trainer | begin training epoch 51
2022-09-25 08:28:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:29:42 | INFO | train_inner | epoch 051:     78 / 78 loss=3.996, nll_loss=0.909, ppl=1.88, wps=6571.6, ups=1.24, wpb=5318.1, bsz=370.1, num_updates=3900, lr=0.000975002, gnorm=0.667, train_wall=45, gb_free=10.2, wall=4119
2022-09-25 08:29:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:29:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:29:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:29:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:29:50 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 5.8 | nll_loss 3.071 | ppl 8.4 | bleu 31.23 | wps 2180.1 | wpb 1003.7 | bsz 67.6 | num_updates 3900 | best_bleu 32.2
2022-09-25 08:29:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 3900 updates
2022-09-25 08:29:50 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint51.pt
2022-09-25 08:29:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint51.pt
2022-09-25 08:30:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint51.pt (epoch 51 @ 3900 updates, score 31.23) (writing took 16.131976027041674 seconds)
2022-09-25 08:30:06 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-09-25 08:30:06 | INFO | train | epoch 051 | loss 3.983 | nll_loss 0.891 | ppl 1.85 | wps 6134.2 | ups 1.15 | wpb 5319.8 | bsz 371.8 | num_updates 3900 | lr 0.000975002 | gnorm 0.654 | train_wall 35 | gb_free 10.2 | wall 4142
2022-09-25 08:30:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:30:06 | INFO | fairseq.trainer | begin training epoch 52
2022-09-25 08:30:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:30:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:30:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:30:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:30:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:30:57 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 5.782 | nll_loss 3.051 | ppl 8.29 | bleu 32.13 | wps 2110.9 | wpb 1003.7 | bsz 67.6 | num_updates 3978 | best_bleu 32.2
2022-09-25 08:30:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 3978 updates
2022-09-25 08:30:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint52.pt
2022-09-25 08:30:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint52.pt
2022-09-25 08:31:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint52.pt (epoch 52 @ 3978 updates, score 32.13) (writing took 14.039288368076086 seconds)
2022-09-25 08:31:11 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-09-25 08:31:11 | INFO | train | epoch 052 | loss 3.978 | nll_loss 0.887 | ppl 1.85 | wps 6376.1 | ups 1.2 | wpb 5319.8 | bsz 371.8 | num_updates 3978 | lr 0.000994501 | gnorm 0.667 | train_wall 35 | gb_free 10.1 | wall 4208
2022-09-25 08:31:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:31:11 | INFO | fairseq.trainer | begin training epoch 53
2022-09-25 08:31:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:31:25 | INFO | train_inner | epoch 053:     22 / 78 loss=3.968, nll_loss=0.873, ppl=1.83, wps=5189.5, ups=0.98, wpb=5315.6, bsz=369, num_updates=4000, lr=0.001, gnorm=0.661, train_wall=45, gb_free=10.1, wall=4221
2022-09-25 08:31:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:31:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:31:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:31:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:32:03 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 5.8 | nll_loss 3.078 | ppl 8.44 | bleu 31.31 | wps 2130.5 | wpb 1003.7 | bsz 67.6 | num_updates 4056 | best_bleu 32.2
2022-09-25 08:32:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 4056 updates
2022-09-25 08:32:03 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint53.pt
2022-09-25 08:32:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint53.pt
2022-09-25 08:32:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint53.pt (epoch 53 @ 4056 updates, score 31.31) (writing took 17.62926645576954 seconds)
2022-09-25 08:32:20 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-09-25 08:32:20 | INFO | train | epoch 053 | loss 3.972 | nll_loss 0.88 | ppl 1.84 | wps 5998.4 | ups 1.13 | wpb 5319.8 | bsz 371.8 | num_updates 4056 | lr 0.000993073 | gnorm 0.665 | train_wall 36 | gb_free 10.1 | wall 4277
2022-09-25 08:32:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:32:21 | INFO | fairseq.trainer | begin training epoch 54
2022-09-25 08:32:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:32:45 | INFO | train_inner | epoch 054:     44 / 78 loss=3.963, nll_loss=0.87, ppl=1.83, wps=6595, ups=1.24, wpb=5327.3, bsz=373, num_updates=4100, lr=0.00098773, gnorm=0.654, train_wall=46, gb_free=10.2, wall=4302
2022-09-25 08:33:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:33:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:33:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:33:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:33:11 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 5.797 | nll_loss 3.078 | ppl 8.44 | bleu 31.68 | wps 2020.5 | wpb 1003.7 | bsz 67.6 | num_updates 4134 | best_bleu 32.2
2022-09-25 08:33:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 4134 updates
2022-09-25 08:33:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint54.pt
2022-09-25 08:33:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint54.pt
2022-09-25 08:33:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint54.pt (epoch 54 @ 4134 updates, score 31.68) (writing took 15.976189974695444 seconds)
2022-09-25 08:33:28 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-09-25 08:33:28 | INFO | train | epoch 054 | loss 3.95 | nll_loss 0.855 | ppl 1.81 | wps 6175.2 | ups 1.16 | wpb 5319.8 | bsz 371.8 | num_updates 4134 | lr 0.000983659 | gnorm 0.649 | train_wall 35 | gb_free 10.2 | wall 4344
2022-09-25 08:33:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:33:28 | INFO | fairseq.trainer | begin training epoch 55
2022-09-25 08:33:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:34:04 | INFO | train_inner | epoch 055:     66 / 78 loss=3.937, nll_loss=0.84, ppl=1.79, wps=6819.5, ups=1.28, wpb=5329.1, bsz=375.1, num_updates=4200, lr=0.0009759, gnorm=0.642, train_wall=45, gb_free=10.1, wall=4380
2022-09-25 08:34:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:34:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:34:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:34:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:34:17 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 5.744 | nll_loss 3.046 | ppl 8.26 | bleu 31.23 | wps 2237.2 | wpb 1003.7 | bsz 67.6 | num_updates 4212 | best_bleu 32.2
2022-09-25 08:34:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 4212 updates
2022-09-25 08:34:17 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint55.pt
2022-09-25 08:34:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint55.pt
2022-09-25 08:34:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint55.pt (epoch 55 @ 4212 updates, score 31.23) (writing took 15.433373861014843 seconds)
2022-09-25 08:34:33 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-09-25 08:34:33 | INFO | train | epoch 055 | loss 3.926 | nll_loss 0.826 | ppl 1.77 | wps 6352.6 | ups 1.19 | wpb 5319.8 | bsz 371.8 | num_updates 4212 | lr 0.000974509 | gnorm 0.629 | train_wall 35 | gb_free 10.1 | wall 4409
2022-09-25 08:34:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:34:33 | INFO | fairseq.trainer | begin training epoch 56
2022-09-25 08:34:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:35:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:35:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:35:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:35:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:35:22 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 5.76 | nll_loss 3.056 | ppl 8.32 | bleu 31.68 | wps 2289.6 | wpb 1003.7 | bsz 67.6 | num_updates 4290 | best_bleu 32.2
2022-09-25 08:35:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 4290 updates
2022-09-25 08:35:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint56.pt
2022-09-25 08:35:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint56.pt
2022-09-25 08:35:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint56.pt (epoch 56 @ 4290 updates, score 31.68) (writing took 17.09041591733694 seconds)
2022-09-25 08:35:39 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-09-25 08:35:39 | INFO | train | epoch 056 | loss 3.911 | nll_loss 0.81 | ppl 1.75 | wps 6262.5 | ups 1.18 | wpb 5319.8 | bsz 371.8 | num_updates 4290 | lr 0.000965609 | gnorm 0.623 | train_wall 35 | gb_free 10.2 | wall 4475
2022-09-25 08:35:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:35:39 | INFO | fairseq.trainer | begin training epoch 57
2022-09-25 08:35:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:35:46 | INFO | train_inner | epoch 057:     10 / 78 loss=3.913, nll_loss=0.813, ppl=1.76, wps=5197.2, ups=0.98, wpb=5302.4, bsz=368.4, num_updates=4300, lr=0.000964486, gnorm=0.624, train_wall=45, gb_free=10.1, wall=4482
2022-09-25 08:36:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:36:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:36:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:36:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:36:31 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 5.775 | nll_loss 3.072 | ppl 8.41 | bleu 31.96 | wps 2260.2 | wpb 1003.7 | bsz 67.6 | num_updates 4368 | best_bleu 32.2
2022-09-25 08:36:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 4368 updates
2022-09-25 08:36:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint57.pt
2022-09-25 08:36:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint57.pt
2022-09-25 08:36:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint57.pt (epoch 57 @ 4368 updates, score 31.96) (writing took 14.548781275749207 seconds)
2022-09-25 08:36:45 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-09-25 08:36:45 | INFO | train | epoch 057 | loss 3.902 | nll_loss 0.801 | ppl 1.74 | wps 6268.8 | ups 1.18 | wpb 5319.8 | bsz 371.8 | num_updates 4368 | lr 0.000956949 | gnorm 0.632 | train_wall 35 | gb_free 10.1 | wall 4542
2022-09-25 08:36:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:36:46 | INFO | fairseq.trainer | begin training epoch 58
2022-09-25 08:36:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:37:03 | INFO | train_inner | epoch 058:     32 / 78 loss=3.896, nll_loss=0.791, ppl=1.73, wps=6843.1, ups=1.29, wpb=5311.8, bsz=369.8, num_updates=4400, lr=0.000953463, gnorm=0.631, train_wall=45, gb_free=10.2, wall=4560
2022-09-25 08:37:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:37:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:37:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:37:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:37:36 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 5.734 | nll_loss 3.038 | ppl 8.21 | bleu 31.85 | wps 2147.5 | wpb 1003.7 | bsz 67.6 | num_updates 4446 | best_bleu 32.2
2022-09-25 08:37:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 4446 updates
2022-09-25 08:37:36 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint58.pt
2022-09-25 08:37:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint58.pt
2022-09-25 08:37:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint58.pt (epoch 58 @ 4446 updates, score 31.85) (writing took 16.036065228283405 seconds)
2022-09-25 08:37:52 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-09-25 08:37:52 | INFO | train | epoch 058 | loss 3.879 | nll_loss 0.773 | ppl 1.71 | wps 6236.9 | ups 1.17 | wpb 5319.8 | bsz 371.8 | num_updates 4446 | lr 0.000948517 | gnorm 0.619 | train_wall 35 | gb_free 10.2 | wall 4608
2022-09-25 08:37:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:37:52 | INFO | fairseq.trainer | begin training epoch 59
2022-09-25 08:37:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:38:22 | INFO | train_inner | epoch 059:     54 / 78 loss=3.864, nll_loss=0.759, ppl=1.69, wps=6788.5, ups=1.28, wpb=5317.2, bsz=374.5, num_updates=4500, lr=0.000942809, gnorm=0.657, train_wall=45, gb_free=10.1, wall=4638
2022-09-25 08:38:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:38:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:38:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:38:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:38:43 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 5.768 | nll_loss 3.079 | ppl 8.45 | bleu 32.33 | wps 1973.2 | wpb 1003.7 | bsz 67.6 | num_updates 4524 | best_bleu 32.33
2022-09-25 08:38:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 4524 updates
2022-09-25 08:38:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint59.pt
2022-09-25 08:38:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint59.pt
2022-09-25 08:39:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint59.pt (epoch 59 @ 4524 updates, score 32.33) (writing took 38.10178508609533 seconds)
2022-09-25 08:39:21 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-09-25 08:39:21 | INFO | train | epoch 059 | loss 3.863 | nll_loss 0.758 | ppl 1.69 | wps 4658.7 | ups 0.88 | wpb 5319.8 | bsz 371.8 | num_updates 4524 | lr 0.000940305 | gnorm 0.67 | train_wall 35 | gb_free 10.1 | wall 4697
2022-09-25 08:39:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:39:21 | INFO | fairseq.trainer | begin training epoch 60
2022-09-25 08:39:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:40:02 | INFO | train_inner | epoch 060:     76 / 78 loss=3.858, nll_loss=0.754, ppl=1.69, wps=5328.9, ups=0.99, wpb=5358.1, bsz=373.9, num_updates=4600, lr=0.000932505, gnorm=0.595, train_wall=44, gb_free=10.1, wall=4738
2022-09-25 08:40:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:40:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:40:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:40:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:40:11 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 5.821 | nll_loss 3.145 | ppl 8.85 | bleu 31.76 | wps 2138.3 | wpb 1003.7 | bsz 67.6 | num_updates 4602 | best_bleu 32.33
2022-09-25 08:40:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 4602 updates
2022-09-25 08:40:11 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint60.pt
2022-09-25 08:40:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint60.pt
2022-09-25 08:40:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint60.pt (epoch 60 @ 4602 updates, score 31.76) (writing took 14.405960571020842 seconds)
2022-09-25 08:40:25 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-09-25 08:40:25 | INFO | train | epoch 060 | loss 3.846 | nll_loss 0.739 | ppl 1.67 | wps 6445.4 | ups 1.21 | wpb 5319.8 | bsz 371.8 | num_updates 4602 | lr 0.000932302 | gnorm 0.588 | train_wall 35 | gb_free 10.2 | wall 4762
2022-09-25 08:40:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:40:26 | INFO | fairseq.trainer | begin training epoch 61
2022-09-25 08:40:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:41:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:41:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:41:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:41:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:41:16 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 5.774 | nll_loss 3.107 | ppl 8.61 | bleu 31.48 | wps 2275.5 | wpb 1003.7 | bsz 67.6 | num_updates 4680 | best_bleu 32.33
2022-09-25 08:41:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 4680 updates
2022-09-25 08:41:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint61.pt
2022-09-25 08:41:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint61.pt
2022-09-25 08:41:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint61.pt (epoch 61 @ 4680 updates, score 31.48) (writing took 17.31322953850031 seconds)
2022-09-25 08:41:33 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-09-25 08:41:33 | INFO | train | epoch 061 | loss 3.83 | nll_loss 0.72 | ppl 1.65 | wps 6096.2 | ups 1.15 | wpb 5319.8 | bsz 371.8 | num_updates 4680 | lr 0.0009245 | gnorm 0.602 | train_wall 34 | gb_free 10.1 | wall 4830
2022-09-25 08:41:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:41:34 | INFO | fairseq.trainer | begin training epoch 62
2022-09-25 08:41:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:41:45 | INFO | train_inner | epoch 062:     20 / 78 loss=3.826, nll_loss=0.715, ppl=1.64, wps=5136.1, ups=0.97, wpb=5286.3, bsz=367.3, num_updates=4700, lr=0.000922531, gnorm=0.598, train_wall=44, gb_free=10.2, wall=4841
2022-09-25 08:42:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:42:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:42:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:42:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:42:23 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 5.758 | nll_loss 3.081 | ppl 8.46 | bleu 33.25 | wps 2137.2 | wpb 1003.7 | bsz 67.6 | num_updates 4758 | best_bleu 33.25
2022-09-25 08:42:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 4758 updates
2022-09-25 08:42:23 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint62.pt
2022-09-25 08:42:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint62.pt
2022-09-25 08:43:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint62.pt (epoch 62 @ 4758 updates, score 33.25) (writing took 40.28859822079539 seconds)
2022-09-25 08:43:04 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-09-25 08:43:04 | INFO | train | epoch 062 | loss 3.82 | nll_loss 0.711 | ppl 1.64 | wps 4585.6 | ups 0.86 | wpb 5319.8 | bsz 371.8 | num_updates 4758 | lr 0.000916891 | gnorm 0.58 | train_wall 35 | gb_free 10.1 | wall 4920
2022-09-25 08:43:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:43:04 | INFO | fairseq.trainer | begin training epoch 63
2022-09-25 08:43:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:43:26 | INFO | train_inner | epoch 063:     42 / 78 loss=3.809, nll_loss=0.699, ppl=1.62, wps=5277.1, ups=0.99, wpb=5339.3, bsz=373.4, num_updates=4800, lr=0.000912871, gnorm=0.568, train_wall=46, gb_free=10.2, wall=4943
2022-09-25 08:43:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:43:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:43:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:43:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:43:52 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 5.77 | nll_loss 3.105 | ppl 8.6 | bleu 32.23 | wps 2186.1 | wpb 1003.7 | bsz 67.6 | num_updates 4836 | best_bleu 33.25
2022-09-25 08:43:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 4836 updates
2022-09-25 08:43:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint63.pt
2022-09-25 08:43:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint63.pt
2022-09-25 08:44:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint63.pt (epoch 63 @ 4836 updates, score 32.23) (writing took 15.875631481409073 seconds)
2022-09-25 08:44:08 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-09-25 08:44:08 | INFO | train | epoch 063 | loss 3.805 | nll_loss 0.695 | ppl 1.62 | wps 6434.1 | ups 1.21 | wpb 5319.8 | bsz 371.8 | num_updates 4836 | lr 0.000909467 | gnorm 0.566 | train_wall 36 | gb_free 10.1 | wall 4985
2022-09-25 08:44:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:44:09 | INFO | fairseq.trainer | begin training epoch 64
2022-09-25 08:44:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:44:44 | INFO | train_inner | epoch 064:     64 / 78 loss=3.804, nll_loss=0.695, ppl=1.62, wps=6794.2, ups=1.28, wpb=5310.8, bsz=375.3, num_updates=4900, lr=0.000903508, gnorm=0.572, train_wall=46, gb_free=10.1, wall=5021
2022-09-25 08:44:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:44:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:44:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:44:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:44:59 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 5.762 | nll_loss 3.102 | ppl 8.58 | bleu 32.06 | wps 2362.4 | wpb 1003.7 | bsz 67.6 | num_updates 4914 | best_bleu 33.25
2022-09-25 08:44:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 4914 updates
2022-09-25 08:44:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint64.pt
2022-09-25 08:45:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint64.pt
2022-09-25 08:45:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint64.pt (epoch 64 @ 4914 updates, score 32.06) (writing took 17.02975133061409 seconds)
2022-09-25 08:45:16 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-09-25 08:45:16 | INFO | train | epoch 064 | loss 3.797 | nll_loss 0.688 | ppl 1.61 | wps 6130.7 | ups 1.15 | wpb 5319.8 | bsz 371.8 | num_updates 4914 | lr 0.00090222 | gnorm 0.57 | train_wall 35 | gb_free 10.2 | wall 5052
2022-09-25 08:45:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:45:16 | INFO | fairseq.trainer | begin training epoch 65
2022-09-25 08:45:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:45:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:45:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:45:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:45:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:46:05 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 5.791 | nll_loss 3.143 | ppl 8.84 | bleu 31.71 | wps 2159.8 | wpb 1003.7 | bsz 67.6 | num_updates 4992 | best_bleu 33.25
2022-09-25 08:46:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 4992 updates
2022-09-25 08:46:05 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint65.pt
2022-09-25 08:46:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint65.pt
2022-09-25 08:46:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint65.pt (epoch 65 @ 4992 updates, score 31.71) (writing took 17.116016305983067 seconds)
2022-09-25 08:46:22 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-09-25 08:46:22 | INFO | train | epoch 065 | loss 3.779 | nll_loss 0.667 | ppl 1.59 | wps 6257.3 | ups 1.18 | wpb 5319.8 | bsz 371.8 | num_updates 4992 | lr 0.000895144 | gnorm 0.537 | train_wall 35 | gb_free 10.1 | wall 5119
2022-09-25 08:46:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:46:23 | INFO | fairseq.trainer | begin training epoch 66
2022-09-25 08:46:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:46:28 | INFO | train_inner | epoch 066:      8 / 78 loss=3.783, nll_loss=0.672, ppl=1.59, wps=5138.7, ups=0.97, wpb=5318.9, bsz=370.7, num_updates=5000, lr=0.000894427, gnorm=0.549, train_wall=45, gb_free=10.1, wall=5124
2022-09-25 08:47:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:47:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:47:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:47:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:47:12 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 5.823 | nll_loss 3.181 | ppl 9.07 | bleu 32.19 | wps 2175.4 | wpb 1003.7 | bsz 67.6 | num_updates 5070 | best_bleu 33.25
2022-09-25 08:47:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 5070 updates
2022-09-25 08:47:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint66.pt
2022-09-25 08:47:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint66.pt
2022-09-25 08:47:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint66.pt (epoch 66 @ 5070 updates, score 32.19) (writing took 14.113187670707703 seconds)
2022-09-25 08:47:26 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-09-25 08:47:26 | INFO | train | epoch 066 | loss 3.769 | nll_loss 0.656 | ppl 1.58 | wps 6488.2 | ups 1.22 | wpb 5319.8 | bsz 371.8 | num_updates 5070 | lr 0.000888231 | gnorm 0.537 | train_wall 35 | gb_free 10.2 | wall 5183
2022-09-25 08:47:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:47:27 | INFO | fairseq.trainer | begin training epoch 67
2022-09-25 08:47:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:47:43 | INFO | train_inner | epoch 067:     30 / 78 loss=3.767, nll_loss=0.654, ppl=1.57, wps=7108.5, ups=1.34, wpb=5320.5, bsz=368.4, num_updates=5100, lr=0.000885615, gnorm=0.548, train_wall=45, gb_free=10.2, wall=5199
2022-09-25 08:48:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:48:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:48:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:48:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:48:15 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 5.802 | nll_loss 3.152 | ppl 8.89 | bleu 32.91 | wps 2083.7 | wpb 1003.7 | bsz 67.6 | num_updates 5148 | best_bleu 33.25
2022-09-25 08:48:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 5148 updates
2022-09-25 08:48:15 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint67.pt
2022-09-25 08:48:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint67.pt
2022-09-25 08:48:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint67.pt (epoch 67 @ 5148 updates, score 32.91) (writing took 16.818488772958517 seconds)
2022-09-25 08:48:32 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-09-25 08:48:32 | INFO | train | epoch 067 | loss 3.763 | nll_loss 0.651 | ppl 1.57 | wps 6335.4 | ups 1.19 | wpb 5319.8 | bsz 371.8 | num_updates 5148 | lr 0.000881476 | gnorm 0.557 | train_wall 35 | gb_free 10.2 | wall 5248
2022-09-25 08:48:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:48:32 | INFO | fairseq.trainer | begin training epoch 68
2022-09-25 08:48:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:49:04 | INFO | train_inner | epoch 068:     52 / 78 loss=3.752, nll_loss=0.64, ppl=1.56, wps=6552.8, ups=1.22, wpb=5351, bsz=379.7, num_updates=5200, lr=0.000877058, gnorm=0.521, train_wall=46, gb_free=10.1, wall=5281
2022-09-25 08:49:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:49:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:49:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:49:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:49:27 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 5.832 | nll_loss 3.186 | ppl 9.1 | bleu 32.06 | wps 2279.5 | wpb 1003.7 | bsz 67.6 | num_updates 5226 | best_bleu 33.25
2022-09-25 08:49:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 5226 updates
2022-09-25 08:49:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint68.pt
2022-09-25 08:49:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint68.pt
2022-09-25 08:49:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint68.pt (epoch 68 @ 5226 updates, score 32.06) (writing took 15.894352607429028 seconds)
2022-09-25 08:49:43 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-09-25 08:49:43 | INFO | train | epoch 068 | loss 3.749 | nll_loss 0.637 | ppl 1.56 | wps 5866.8 | ups 1.1 | wpb 5319.8 | bsz 371.8 | num_updates 5226 | lr 0.000874874 | gnorm 0.524 | train_wall 35 | gb_free 10.2 | wall 5319
2022-09-25 08:49:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:49:43 | INFO | fairseq.trainer | begin training epoch 69
2022-09-25 08:49:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:50:23 | INFO | train_inner | epoch 069:     74 / 78 loss=3.749, nll_loss=0.64, ppl=1.56, wps=6759.1, ups=1.27, wpb=5303.7, bsz=370.6, num_updates=5300, lr=0.000868744, gnorm=0.538, train_wall=45, gb_free=10.1, wall=5359
2022-09-25 08:50:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:50:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:50:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:50:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:50:33 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 5.817 | nll_loss 3.183 | ppl 9.08 | bleu 33.25 | wps 2119.6 | wpb 1003.7 | bsz 67.6 | num_updates 5304 | best_bleu 33.25
2022-09-25 08:50:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 5304 updates
2022-09-25 08:50:33 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint69.pt
2022-09-25 08:50:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint69.pt
2022-09-25 08:51:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint69.pt (epoch 69 @ 5304 updates, score 33.25) (writing took 40.967494919896126 seconds)
2022-09-25 08:51:14 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-09-25 08:51:14 | INFO | train | epoch 069 | loss 3.746 | nll_loss 0.635 | ppl 1.55 | wps 4546.1 | ups 0.85 | wpb 5319.8 | bsz 371.8 | num_updates 5304 | lr 0.000868417 | gnorm 0.532 | train_wall 35 | gb_free 10.2 | wall 5410
2022-09-25 08:51:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:51:14 | INFO | fairseq.trainer | begin training epoch 70
2022-09-25 08:51:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:51:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:51:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:51:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:51:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:52:04 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 5.829 | nll_loss 3.209 | ppl 9.25 | bleu 31.99 | wps 2164.2 | wpb 1003.7 | bsz 67.6 | num_updates 5382 | best_bleu 33.25
2022-09-25 08:52:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 5382 updates
2022-09-25 08:52:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint70.pt
2022-09-25 08:52:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint70.pt
2022-09-25 08:52:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint70.pt (epoch 70 @ 5382 updates, score 31.99) (writing took 32.835411217063665 seconds)
2022-09-25 08:52:37 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-09-25 08:52:37 | INFO | train | epoch 070 | loss 3.733 | nll_loss 0.62 | ppl 1.54 | wps 4986.6 | ups 0.94 | wpb 5319.8 | bsz 371.8 | num_updates 5382 | lr 0.000862101 | gnorm 0.568 | train_wall 35 | gb_free 10.1 | wall 5493
2022-09-25 08:52:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:52:37 | INFO | fairseq.trainer | begin training epoch 71
2022-09-25 08:52:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:52:49 | INFO | train_inner | epoch 071:     18 / 78 loss=3.733, nll_loss=0.619, ppl=1.54, wps=3644.8, ups=0.69, wpb=5305.3, bsz=368.2, num_updates=5400, lr=0.000860663, gnorm=0.556, train_wall=45, gb_free=10.1, wall=5505
2022-09-25 08:53:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:53:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:53:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:53:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:53:32 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 5.819 | nll_loss 3.196 | ppl 9.16 | bleu 31.87 | wps 2180.3 | wpb 1003.7 | bsz 67.6 | num_updates 5460 | best_bleu 33.25
2022-09-25 08:53:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 5460 updates
2022-09-25 08:53:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint71.pt
2022-09-25 08:53:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint71.pt
2022-09-25 08:53:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint71.pt (epoch 71 @ 5460 updates, score 31.87) (writing took 19.479183718562126 seconds)
2022-09-25 08:53:51 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-09-25 08:53:51 | INFO | train | epoch 071 | loss 3.724 | nll_loss 0.612 | ppl 1.53 | wps 5599.2 | ups 1.05 | wpb 5319.8 | bsz 371.8 | num_updates 5460 | lr 0.000855921 | gnorm 0.507 | train_wall 35 | gb_free 10.1 | wall 5568
2022-09-25 08:53:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:53:51 | INFO | fairseq.trainer | begin training epoch 72
2022-09-25 08:53:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:54:13 | INFO | train_inner | epoch 072:     40 / 78 loss=3.719, nll_loss=0.606, ppl=1.52, wps=6288.3, ups=1.18, wpb=5333.1, bsz=372.2, num_updates=5500, lr=0.000852803, gnorm=0.497, train_wall=45, gb_free=10.1, wall=5590
2022-09-25 08:54:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:54:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:54:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:54:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:54:40 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 5.82 | nll_loss 3.206 | ppl 9.23 | bleu 31.7 | wps 2199.2 | wpb 1003.7 | bsz 67.6 | num_updates 5538 | best_bleu 33.25
2022-09-25 08:54:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 5538 updates
2022-09-25 08:54:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint72.pt
2022-09-25 08:54:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint72.pt
2022-09-25 08:54:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint72.pt (epoch 72 @ 5538 updates, score 31.7) (writing took 15.950921341776848 seconds)
2022-09-25 08:54:57 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-09-25 08:54:57 | INFO | train | epoch 072 | loss 3.713 | nll_loss 0.6 | ppl 1.52 | wps 6353.4 | ups 1.19 | wpb 5319.8 | bsz 371.8 | num_updates 5538 | lr 0.000849872 | gnorm 0.489 | train_wall 35 | gb_free 10.2 | wall 5633
2022-09-25 08:54:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:54:57 | INFO | fairseq.trainer | begin training epoch 73
2022-09-25 08:54:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:55:31 | INFO | train_inner | epoch 073:     62 / 78 loss=3.712, nll_loss=0.602, ppl=1.52, wps=6842.1, ups=1.28, wpb=5332.3, bsz=371.4, num_updates=5600, lr=0.000845154, gnorm=0.5, train_wall=45, gb_free=10.1, wall=5668
2022-09-25 08:55:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:55:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:55:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:55:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:55:48 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 5.8 | nll_loss 3.19 | ppl 9.13 | bleu 32.79 | wps 1985.6 | wpb 1003.7 | bsz 67.6 | num_updates 5616 | best_bleu 33.25
2022-09-25 08:55:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 5616 updates
2022-09-25 08:55:48 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint73.pt
2022-09-25 08:55:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint73.pt
2022-09-25 08:56:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint73.pt (epoch 73 @ 5616 updates, score 32.79) (writing took 17.691627573221922 seconds)
2022-09-25 08:56:06 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-09-25 08:56:06 | INFO | train | epoch 073 | loss 3.71 | nll_loss 0.599 | ppl 1.51 | wps 5996.9 | ups 1.13 | wpb 5319.8 | bsz 371.8 | num_updates 5616 | lr 0.000843949 | gnorm 0.509 | train_wall 36 | gb_free 10.2 | wall 5702
2022-09-25 08:56:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:56:06 | INFO | fairseq.trainer | begin training epoch 74
2022-09-25 08:56:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:56:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:56:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:56:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:56:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:56:56 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 5.82 | nll_loss 3.198 | ppl 9.18 | bleu 32.75 | wps 2043.9 | wpb 1003.7 | bsz 67.6 | num_updates 5694 | best_bleu 33.25
2022-09-25 08:56:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 5694 updates
2022-09-25 08:56:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint74.pt
2022-09-25 08:56:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint74.pt
2022-09-25 08:57:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint74.pt (epoch 74 @ 5694 updates, score 32.75) (writing took 15.962486419826746 seconds)
2022-09-25 08:57:12 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-09-25 08:57:12 | INFO | train | epoch 074 | loss 3.701 | nll_loss 0.589 | ppl 1.5 | wps 6224.2 | ups 1.17 | wpb 5319.8 | bsz 371.8 | num_updates 5694 | lr 0.000838149 | gnorm 0.497 | train_wall 33 | gb_free 10.2 | wall 5769
2022-09-25 08:57:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:57:13 | INFO | fairseq.trainer | begin training epoch 75
2022-09-25 08:57:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:57:17 | INFO | train_inner | epoch 075:      6 / 78 loss=3.703, nll_loss=0.592, ppl=1.51, wps=5016.1, ups=0.95, wpb=5297.8, bsz=370.7, num_updates=5700, lr=0.000837708, gnorm=0.504, train_wall=43, gb_free=10, wall=5773
2022-09-25 08:57:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:57:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:57:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:57:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:58:04 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 5.801 | nll_loss 3.196 | ppl 9.16 | bleu 33.25 | wps 1959.7 | wpb 1003.7 | bsz 67.6 | num_updates 5772 | best_bleu 33.25
2022-09-25 08:58:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 5772 updates
2022-09-25 08:58:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint75.pt
2022-09-25 08:58:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint75.pt
2022-09-25 08:58:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint75.pt (epoch 75 @ 5772 updates, score 33.25) (writing took 39.326385986059904 seconds)
2022-09-25 08:58:43 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-09-25 08:58:43 | INFO | train | epoch 075 | loss 3.698 | nll_loss 0.587 | ppl 1.5 | wps 4559.5 | ups 0.86 | wpb 5319.8 | bsz 371.8 | num_updates 5772 | lr 0.000832467 | gnorm 0.487 | train_wall 35 | gb_free 10.2 | wall 5860
2022-09-25 08:58:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:58:44 | INFO | fairseq.trainer | begin training epoch 76
2022-09-25 08:58:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 08:59:00 | INFO | train_inner | epoch 076:     28 / 78 loss=3.692, nll_loss=0.579, ppl=1.49, wps=5152.5, ups=0.97, wpb=5316.6, bsz=374.6, num_updates=5800, lr=0.000830455, gnorm=0.478, train_wall=45, gb_free=10.2, wall=5876
2022-09-25 08:59:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 08:59:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 08:59:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 08:59:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 08:59:34 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 5.809 | nll_loss 3.197 | ppl 9.17 | bleu 32.59 | wps 2146.5 | wpb 1003.7 | bsz 67.6 | num_updates 5850 | best_bleu 33.25
2022-09-25 08:59:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 5850 updates
2022-09-25 08:59:34 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint76.pt
2022-09-25 08:59:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint76.pt
2022-09-25 08:59:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint76.pt (epoch 76 @ 5850 updates, score 32.59) (writing took 17.531817756593227 seconds)
2022-09-25 08:59:52 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-09-25 08:59:52 | INFO | train | epoch 076 | loss 3.687 | nll_loss 0.575 | ppl 1.49 | wps 6071.5 | ups 1.14 | wpb 5319.8 | bsz 371.8 | num_updates 5850 | lr 0.000826898 | gnorm 0.475 | train_wall 35 | gb_free 10.2 | wall 5928
2022-09-25 08:59:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 08:59:52 | INFO | fairseq.trainer | begin training epoch 77
2022-09-25 08:59:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:00:19 | INFO | train_inner | epoch 077:     50 / 78 loss=3.687, nll_loss=0.577, ppl=1.49, wps=6769.2, ups=1.27, wpb=5327.1, bsz=367.4, num_updates=5900, lr=0.000823387, gnorm=0.478, train_wall=45, gb_free=10.2, wall=5955
2022-09-25 09:00:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:00:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:00:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:00:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:00:41 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 5.792 | nll_loss 3.196 | ppl 9.17 | bleu 32.76 | wps 2145.1 | wpb 1003.7 | bsz 67.6 | num_updates 5928 | best_bleu 33.25
2022-09-25 09:00:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 5928 updates
2022-09-25 09:00:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint77.pt
2022-09-25 09:00:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint77.pt
2022-09-25 09:00:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint77.pt (epoch 77 @ 5928 updates, score 32.76) (writing took 16.482674777507782 seconds)
2022-09-25 09:00:58 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-09-25 09:00:58 | INFO | train | epoch 077 | loss 3.683 | nll_loss 0.572 | ppl 1.49 | wps 6283.1 | ups 1.18 | wpb 5319.8 | bsz 371.8 | num_updates 5928 | lr 0.00082144 | gnorm 0.48 | train_wall 35 | gb_free 10.1 | wall 5994
2022-09-25 09:00:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:00:58 | INFO | fairseq.trainer | begin training epoch 78
2022-09-25 09:00:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:01:36 | INFO | train_inner | epoch 078:     72 / 78 loss=3.68, nll_loss=0.571, ppl=1.49, wps=6846.4, ups=1.29, wpb=5313.6, bsz=375.7, num_updates=6000, lr=0.000816497, gnorm=0.501, train_wall=44, gb_free=10.2, wall=6033
2022-09-25 09:01:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:01:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:01:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:01:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:01:47 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 5.811 | nll_loss 3.223 | ppl 9.34 | bleu 32.66 | wps 2145.7 | wpb 1003.7 | bsz 67.6 | num_updates 6006 | best_bleu 33.25
2022-09-25 09:01:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 6006 updates
2022-09-25 09:01:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint78.pt
2022-09-25 09:01:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint78.pt
2022-09-25 09:02:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint78.pt (epoch 78 @ 6006 updates, score 32.66) (writing took 17.122065287083387 seconds)
2022-09-25 09:02:04 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-09-25 09:02:04 | INFO | train | epoch 078 | loss 3.68 | nll_loss 0.57 | ppl 1.48 | wps 6277 | ups 1.18 | wpb 5319.8 | bsz 371.8 | num_updates 6006 | lr 0.000816089 | gnorm 0.506 | train_wall 34 | gb_free 10.1 | wall 6060
2022-09-25 09:02:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:02:04 | INFO | fairseq.trainer | begin training epoch 79
2022-09-25 09:02:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:02:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:02:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:02:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:02:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:02:53 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 5.813 | nll_loss 3.216 | ppl 9.29 | bleu 32.5 | wps 2184.8 | wpb 1003.7 | bsz 67.6 | num_updates 6084 | best_bleu 33.25
2022-09-25 09:02:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 6084 updates
2022-09-25 09:02:53 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint79.pt
2022-09-25 09:02:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint79.pt
2022-09-25 09:03:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint79.pt (epoch 79 @ 6084 updates, score 32.5) (writing took 16.942834068089724 seconds)
2022-09-25 09:03:10 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-09-25 09:03:10 | INFO | train | epoch 079 | loss 3.67 | nll_loss 0.56 | ppl 1.47 | wps 6309 | ups 1.19 | wpb 5319.8 | bsz 371.8 | num_updates 6084 | lr 0.00081084 | gnorm 0.462 | train_wall 35 | gb_free 10.1 | wall 6126
2022-09-25 09:03:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:03:10 | INFO | fairseq.trainer | begin training epoch 80
2022-09-25 09:03:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:03:19 | INFO | train_inner | epoch 080:     16 / 78 loss=3.671, nll_loss=0.56, ppl=1.47, wps=5163, ups=0.98, wpb=5291.2, bsz=366.9, num_updates=6100, lr=0.000809776, gnorm=0.467, train_wall=44, gb_free=10.1, wall=6135
2022-09-25 09:03:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:03:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:03:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:03:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:04:00 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 5.802 | nll_loss 3.202 | ppl 9.2 | bleu 33.7 | wps 2121.3 | wpb 1003.7 | bsz 67.6 | num_updates 6162 | best_bleu 33.7
2022-09-25 09:04:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 6162 updates
2022-09-25 09:04:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint80.pt
2022-09-25 09:04:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint80.pt
2022-09-25 09:04:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint80.pt (epoch 80 @ 6162 updates, score 33.7) (writing took 29.217787463217974 seconds)
2022-09-25 09:04:29 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-09-25 09:04:29 | INFO | train | epoch 080 | loss 3.664 | nll_loss 0.553 | ppl 1.47 | wps 5237.3 | ups 0.98 | wpb 5319.8 | bsz 371.8 | num_updates 6162 | lr 0.000805692 | gnorm 0.461 | train_wall 35 | gb_free 10.1 | wall 6205
2022-09-25 09:04:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:04:29 | INFO | fairseq.trainer | begin training epoch 81
2022-09-25 09:04:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:04:50 | INFO | train_inner | epoch 081:     38 / 78 loss=3.659, nll_loss=0.548, ppl=1.46, wps=5824.8, ups=1.09, wpb=5331.4, bsz=377.6, num_updates=6200, lr=0.000803219, gnorm=0.456, train_wall=45, gb_free=10.2, wall=6227
2022-09-25 09:05:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:05:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:05:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:05:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:05:18 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 5.829 | nll_loss 3.245 | ppl 9.48 | bleu 33.15 | wps 2074.5 | wpb 1003.7 | bsz 67.6 | num_updates 6240 | best_bleu 33.7
2022-09-25 09:05:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 6240 updates
2022-09-25 09:05:18 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint81.pt
2022-09-25 09:05:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint81.pt
2022-09-25 09:05:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint81.pt (epoch 81 @ 6240 updates, score 33.15) (writing took 16.613620344549417 seconds)
2022-09-25 09:05:34 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-09-25 09:05:34 | INFO | train | epoch 081 | loss 3.657 | nll_loss 0.547 | ppl 1.46 | wps 6342.6 | ups 1.19 | wpb 5319.8 | bsz 371.8 | num_updates 6240 | lr 0.000800641 | gnorm 0.453 | train_wall 35 | gb_free 10.1 | wall 6271
2022-09-25 09:05:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:05:35 | INFO | fairseq.trainer | begin training epoch 82
2022-09-25 09:05:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:06:08 | INFO | train_inner | epoch 082:     60 / 78 loss=3.66, nll_loss=0.551, ppl=1.47, wps=6941.1, ups=1.3, wpb=5349.2, bsz=367.8, num_updates=6300, lr=0.000796819, gnorm=0.46, train_wall=45, gb_free=10.2, wall=6304
2022-09-25 09:06:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:06:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:06:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:06:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:06:25 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 5.811 | nll_loss 3.232 | ppl 9.4 | bleu 32.83 | wps 2117.9 | wpb 1003.7 | bsz 67.6 | num_updates 6318 | best_bleu 33.7
2022-09-25 09:06:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 6318 updates
2022-09-25 09:06:25 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint82.pt
2022-09-25 09:06:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint82.pt
2022-09-25 09:06:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint82.pt (epoch 82 @ 6318 updates, score 32.83) (writing took 16.53030302375555 seconds)
2022-09-25 09:06:42 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-09-25 09:06:42 | INFO | train | epoch 082 | loss 3.659 | nll_loss 0.55 | ppl 1.46 | wps 6153.5 | ups 1.16 | wpb 5319.8 | bsz 371.8 | num_updates 6318 | lr 0.000795683 | gnorm 0.465 | train_wall 35 | gb_free 10.1 | wall 6338
2022-09-25 09:06:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:06:42 | INFO | fairseq.trainer | begin training epoch 83
2022-09-25 09:06:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:07:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:07:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:07:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:07:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:07:30 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 5.828 | nll_loss 3.242 | ppl 9.46 | bleu 32.66 | wps 2165.3 | wpb 1003.7 | bsz 67.6 | num_updates 6396 | best_bleu 33.7
2022-09-25 09:07:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 6396 updates
2022-09-25 09:07:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint83.pt
2022-09-25 09:07:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint83.pt
2022-09-25 09:07:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint83.pt (epoch 83 @ 6396 updates, score 32.66) (writing took 13.698786027729511 seconds)
2022-09-25 09:07:44 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-09-25 09:07:44 | INFO | train | epoch 083 | loss 3.652 | nll_loss 0.543 | ppl 1.46 | wps 6664.2 | ups 1.25 | wpb 5319.8 | bsz 371.8 | num_updates 6396 | lr 0.000790817 | gnorm 0.473 | train_wall 35 | gb_free 10.2 | wall 6400
2022-09-25 09:07:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:07:44 | INFO | fairseq.trainer | begin training epoch 84
2022-09-25 09:07:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:07:47 | INFO | train_inner | epoch 084:      4 / 78 loss=3.654, nll_loss=0.546, ppl=1.46, wps=5306.9, ups=1, wpb=5292.2, bsz=372.5, num_updates=6400, lr=0.000790569, gnorm=0.473, train_wall=45, gb_free=10.2, wall=6404
2022-09-25 09:08:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:08:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:08:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:08:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:08:31 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 5.853 | nll_loss 3.277 | ppl 9.69 | bleu 32.98 | wps 2154.4 | wpb 1003.7 | bsz 67.6 | num_updates 6474 | best_bleu 33.7
2022-09-25 09:08:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 6474 updates
2022-09-25 09:08:31 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint84.pt
2022-09-25 09:08:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint84.pt
2022-09-25 09:08:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint84.pt (epoch 84 @ 6474 updates, score 32.98) (writing took 15.210960250347853 seconds)
2022-09-25 09:08:47 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-09-25 09:08:47 | INFO | train | epoch 084 | loss 3.646 | nll_loss 0.538 | ppl 1.45 | wps 6630.6 | ups 1.25 | wpb 5319.8 | bsz 371.8 | num_updates 6474 | lr 0.000786038 | gnorm 0.454 | train_wall 35 | gb_free 10.2 | wall 6463
2022-09-25 09:08:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:08:47 | INFO | fairseq.trainer | begin training epoch 85
2022-09-25 09:08:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:09:01 | INFO | train_inner | epoch 085:     26 / 78 loss=3.641, nll_loss=0.531, ppl=1.45, wps=7223.9, ups=1.35, wpb=5337.9, bsz=371.8, num_updates=6500, lr=0.000784465, gnorm=0.442, train_wall=44, gb_free=10.1, wall=6477
2022-09-25 09:09:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:09:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:09:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:09:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:09:35 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 5.8 | nll_loss 3.219 | ppl 9.31 | bleu 33.15 | wps 2181.3 | wpb 1003.7 | bsz 67.6 | num_updates 6552 | best_bleu 33.7
2022-09-25 09:09:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 6552 updates
2022-09-25 09:09:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint85.pt
2022-09-25 09:09:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint85.pt
2022-09-25 09:09:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint85.pt (epoch 85 @ 6552 updates, score 33.15) (writing took 16.39828021079302 seconds)
2022-09-25 09:09:52 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-09-25 09:09:52 | INFO | train | epoch 085 | loss 3.636 | nll_loss 0.527 | ppl 1.44 | wps 6358 | ups 1.2 | wpb 5319.8 | bsz 371.8 | num_updates 6552 | lr 0.000781345 | gnorm 0.432 | train_wall 37 | gb_free 10.2 | wall 6528
2022-09-25 09:09:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:09:52 | INFO | fairseq.trainer | begin training epoch 86
2022-09-25 09:09:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:10:18 | INFO | train_inner | epoch 086:     48 / 78 loss=3.636, nll_loss=0.528, ppl=1.44, wps=6948.9, ups=1.31, wpb=5322.2, bsz=375.6, num_updates=6600, lr=0.000778499, gnorm=0.451, train_wall=47, gb_free=10.2, wall=6554
2022-09-25 09:10:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:10:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:10:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:10:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:10:41 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 5.8 | nll_loss 3.216 | ppl 9.29 | bleu 32.42 | wps 2043.3 | wpb 1003.7 | bsz 67.6 | num_updates 6630 | best_bleu 33.7
2022-09-25 09:10:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 6630 updates
2022-09-25 09:10:41 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint86.pt
2022-09-25 09:10:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint86.pt
2022-09-25 09:11:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint86.pt (epoch 86 @ 6630 updates, score 32.42) (writing took 27.512088548392057 seconds)
2022-09-25 09:11:08 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-09-25 09:11:08 | INFO | train | epoch 086 | loss 3.638 | nll_loss 0.531 | ppl 1.44 | wps 5425.1 | ups 1.02 | wpb 5319.8 | bsz 371.8 | num_updates 6630 | lr 0.000776736 | gnorm 0.477 | train_wall 35 | gb_free 10.1 | wall 6605
2022-09-25 09:11:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:11:09 | INFO | fairseq.trainer | begin training epoch 87
2022-09-25 09:11:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:11:45 | INFO | train_inner | epoch 087:     70 / 78 loss=3.638, nll_loss=0.531, ppl=1.45, wps=6102.3, ups=1.15, wpb=5327.2, bsz=368.2, num_updates=6700, lr=0.000772667, gnorm=0.454, train_wall=44, gb_free=10.2, wall=6641
2022-09-25 09:11:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:11:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:11:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:11:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:11:57 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 5.833 | nll_loss 3.26 | ppl 9.58 | bleu 32.17 | wps 2125.4 | wpb 1003.7 | bsz 67.6 | num_updates 6708 | best_bleu 33.7
2022-09-25 09:11:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 6708 updates
2022-09-25 09:11:57 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint87.pt
2022-09-25 09:11:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint87.pt
2022-09-25 09:12:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint87.pt (epoch 87 @ 6708 updates, score 32.17) (writing took 17.513568967580795 seconds)
2022-09-25 09:12:15 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-09-25 09:12:15 | INFO | train | epoch 087 | loss 3.633 | nll_loss 0.525 | ppl 1.44 | wps 6259.8 | ups 1.18 | wpb 5319.8 | bsz 371.8 | num_updates 6708 | lr 0.000772207 | gnorm 0.436 | train_wall 35 | gb_free 10.1 | wall 6671
2022-09-25 09:12:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:12:15 | INFO | fairseq.trainer | begin training epoch 88
2022-09-25 09:12:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:12:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:12:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:12:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:12:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:13:04 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 5.816 | nll_loss 3.245 | ppl 9.48 | bleu 32.74 | wps 2245.2 | wpb 1003.7 | bsz 67.6 | num_updates 6786 | best_bleu 33.7
2022-09-25 09:13:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 6786 updates
2022-09-25 09:13:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint88.pt
2022-09-25 09:13:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint88.pt
2022-09-25 09:13:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint88.pt (epoch 88 @ 6786 updates, score 32.74) (writing took 14.802953790873289 seconds)
2022-09-25 09:13:19 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-09-25 09:13:19 | INFO | train | epoch 088 | loss 3.633 | nll_loss 0.526 | ppl 1.44 | wps 6479.1 | ups 1.22 | wpb 5319.8 | bsz 371.8 | num_updates 6786 | lr 0.000767756 | gnorm 0.471 | train_wall 35 | gb_free 10.1 | wall 6735
2022-09-25 09:13:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:13:19 | INFO | fairseq.trainer | begin training epoch 89
2022-09-25 09:13:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:13:27 | INFO | train_inner | epoch 089:     14 / 78 loss=3.63, nll_loss=0.523, ppl=1.44, wps=5190.6, ups=0.98, wpb=5313.9, bsz=371, num_updates=6800, lr=0.000766965, gnorm=0.462, train_wall=44, gb_free=10.1, wall=6744
2022-09-25 09:14:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:14:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:14:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:14:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:14:08 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 5.836 | nll_loss 3.265 | ppl 9.61 | bleu 33.1 | wps 2159.5 | wpb 1003.7 | bsz 67.6 | num_updates 6864 | best_bleu 33.7
2022-09-25 09:14:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 6864 updates
2022-09-25 09:14:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint89.pt
2022-09-25 09:14:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint89.pt
2022-09-25 09:14:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint89.pt (epoch 89 @ 6864 updates, score 33.1) (writing took 13.046823587268591 seconds)
2022-09-25 09:14:21 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-09-25 09:14:21 | INFO | train | epoch 089 | loss 3.625 | nll_loss 0.519 | ppl 1.43 | wps 6630.8 | ups 1.25 | wpb 5319.8 | bsz 371.8 | num_updates 6864 | lr 0.000763381 | gnorm 0.436 | train_wall 35 | gb_free 10.2 | wall 6798
2022-09-25 09:14:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:14:22 | INFO | fairseq.trainer | begin training epoch 90
2022-09-25 09:14:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:14:42 | INFO | train_inner | epoch 090:     36 / 78 loss=3.623, nll_loss=0.516, ppl=1.43, wps=7096.2, ups=1.33, wpb=5324.9, bsz=374.2, num_updates=6900, lr=0.000761387, gnorm=0.439, train_wall=44, gb_free=10.2, wall=6819
2022-09-25 09:15:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:15:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:15:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:15:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:15:12 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 5.809 | nll_loss 3.238 | ppl 9.44 | bleu 32.93 | wps 2149.4 | wpb 1003.7 | bsz 67.6 | num_updates 6942 | best_bleu 33.7
2022-09-25 09:15:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 6942 updates
2022-09-25 09:15:12 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint90.pt
2022-09-25 09:15:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint90.pt
2022-09-25 09:15:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint90.pt (epoch 90 @ 6942 updates, score 32.93) (writing took 20.582444325089455 seconds)
2022-09-25 09:15:33 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-09-25 09:15:33 | INFO | train | epoch 090 | loss 3.621 | nll_loss 0.515 | ppl 1.43 | wps 5780.5 | ups 1.09 | wpb 5319.8 | bsz 371.8 | num_updates 6942 | lr 0.00075908 | gnorm 0.432 | train_wall 35 | gb_free 10.2 | wall 6869
2022-09-25 09:15:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:15:33 | INFO | fairseq.trainer | begin training epoch 91
2022-09-25 09:15:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:16:06 | INFO | train_inner | epoch 091:     58 / 78 loss=3.622, nll_loss=0.517, ppl=1.43, wps=6364.6, ups=1.2, wpb=5311.3, bsz=372.1, num_updates=7000, lr=0.000755929, gnorm=0.637, train_wall=48, gb_free=10.2, wall=6902
2022-09-25 09:16:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:16:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:16:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:16:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:16:22 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 5.832 | nll_loss 3.282 | ppl 9.72 | bleu 32.25 | wps 2147.4 | wpb 1003.7 | bsz 67.6 | num_updates 7020 | best_bleu 33.7
2022-09-25 09:16:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 7020 updates
2022-09-25 09:16:22 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint91.pt
2022-09-25 09:16:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint91.pt
2022-09-25 09:16:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint91.pt (epoch 91 @ 7020 updates, score 32.25) (writing took 13.49099987372756 seconds)
2022-09-25 09:16:36 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-09-25 09:16:36 | INFO | train | epoch 091 | loss 3.62 | nll_loss 0.514 | ppl 1.43 | wps 6590.1 | ups 1.24 | wpb 5319.8 | bsz 371.8 | num_updates 7020 | lr 0.000754851 | gnorm 0.709 | train_wall 37 | gb_free 10.1 | wall 6932
2022-09-25 09:16:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:16:36 | INFO | fairseq.trainer | begin training epoch 92
2022-09-25 09:16:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:17:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:17:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:17:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:17:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:17:24 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 5.856 | nll_loss 3.301 | ppl 9.85 | bleu 31.98 | wps 2261.7 | wpb 1003.7 | bsz 67.6 | num_updates 7098 | best_bleu 33.7
2022-09-25 09:17:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 7098 updates
2022-09-25 09:17:24 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint92.pt
2022-09-25 09:17:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint92.pt
2022-09-25 09:17:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint92.pt (epoch 92 @ 7098 updates, score 31.98) (writing took 14.723425973206758 seconds)
2022-09-25 09:17:39 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-09-25 09:17:39 | INFO | train | epoch 092 | loss 3.615 | nll_loss 0.51 | ppl 1.42 | wps 6580.5 | ups 1.24 | wpb 5319.8 | bsz 371.8 | num_updates 7098 | lr 0.000750692 | gnorm 0.422 | train_wall 34 | gb_free 10.2 | wall 6995
2022-09-25 09:17:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:17:39 | INFO | fairseq.trainer | begin training epoch 93
2022-09-25 09:17:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:17:42 | INFO | train_inner | epoch 093:      2 / 78 loss=3.618, nll_loss=0.512, ppl=1.43, wps=5561.2, ups=1.05, wpb=5315.3, bsz=370.2, num_updates=7100, lr=0.000750587, gnorm=0.435, train_wall=44, gb_free=10.1, wall=6998
2022-09-25 09:18:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:18:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:18:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:18:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:18:30 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 5.855 | nll_loss 3.302 | ppl 9.86 | bleu 33.06 | wps 2139 | wpb 1003.7 | bsz 67.6 | num_updates 7176 | best_bleu 33.7
2022-09-25 09:18:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 7176 updates
2022-09-25 09:18:30 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint93.pt
2022-09-25 09:18:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint93.pt
2022-09-25 09:18:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint93.pt (epoch 93 @ 7176 updates, score 33.06) (writing took 17.901098165661097 seconds)
2022-09-25 09:18:48 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-09-25 09:18:48 | INFO | train | epoch 093 | loss 3.613 | nll_loss 0.508 | ppl 1.42 | wps 6061 | ups 1.14 | wpb 5319.8 | bsz 371.8 | num_updates 7176 | lr 0.000746601 | gnorm 0.42 | train_wall 35 | gb_free 10.1 | wall 7064
2022-09-25 09:18:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:18:48 | INFO | fairseq.trainer | begin training epoch 94
2022-09-25 09:18:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:19:01 | INFO | train_inner | epoch 094:     24 / 78 loss=3.608, nll_loss=0.503, ppl=1.42, wps=6687, ups=1.25, wpb=5328.3, bsz=375.5, num_updates=7200, lr=0.000745356, gnorm=0.415, train_wall=45, gb_free=10.2, wall=7077
2022-09-25 09:19:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:19:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:19:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:19:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:19:35 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 5.854 | nll_loss 3.305 | ppl 9.88 | bleu 32.93 | wps 2149 | wpb 1003.7 | bsz 67.6 | num_updates 7254 | best_bleu 33.7
2022-09-25 09:19:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 7254 updates
2022-09-25 09:19:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint94.pt
2022-09-25 09:19:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint94.pt
2022-09-25 09:19:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint94.pt (epoch 94 @ 7254 updates, score 32.93) (writing took 14.707633253186941 seconds)
2022-09-25 09:19:50 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-09-25 09:19:50 | INFO | train | epoch 094 | loss 3.606 | nll_loss 0.5 | ppl 1.41 | wps 6642.8 | ups 1.25 | wpb 5319.8 | bsz 371.8 | num_updates 7254 | lr 0.000742577 | gnorm 0.514 | train_wall 34 | gb_free 10.2 | wall 7126
2022-09-25 09:19:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:19:50 | INFO | fairseq.trainer | begin training epoch 95
2022-09-25 09:19:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:20:15 | INFO | train_inner | epoch 095:     46 / 78 loss=3.607, nll_loss=0.502, ppl=1.42, wps=7206.6, ups=1.36, wpb=5318, bsz=365.7, num_updates=7300, lr=0.000740233, gnorm=0.496, train_wall=44, gb_free=10, wall=7151
2022-09-25 09:20:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:20:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:20:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:20:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:20:40 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 5.85 | nll_loss 3.292 | ppl 9.8 | bleu 33.68 | wps 1739.4 | wpb 1003.7 | bsz 67.6 | num_updates 7332 | best_bleu 33.7
2022-09-25 09:20:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 7332 updates
2022-09-25 09:20:40 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint95.pt
2022-09-25 09:20:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint95.pt
2022-09-25 09:20:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint95.pt (epoch 95 @ 7332 updates, score 33.68) (writing took 15.989950187504292 seconds)
2022-09-25 09:20:56 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-09-25 09:20:56 | INFO | train | epoch 095 | loss 3.606 | nll_loss 0.502 | ppl 1.42 | wps 6267.8 | ups 1.18 | wpb 5319.8 | bsz 371.8 | num_updates 7332 | lr 0.000738616 | gnorm 0.416 | train_wall 34 | gb_free 10.2 | wall 7193
2022-09-25 09:20:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:20:57 | INFO | fairseq.trainer | begin training epoch 96
2022-09-25 09:20:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:21:34 | INFO | train_inner | epoch 096:     68 / 78 loss=3.603, nll_loss=0.5, ppl=1.41, wps=6749.7, ups=1.27, wpb=5330.2, bsz=372.1, num_updates=7400, lr=0.000735215, gnorm=0.405, train_wall=44, gb_free=10.1, wall=7230
2022-09-25 09:21:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:21:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:21:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:21:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:21:47 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 5.876 | nll_loss 3.325 | ppl 10.02 | bleu 32.47 | wps 2148.1 | wpb 1003.7 | bsz 67.6 | num_updates 7410 | best_bleu 33.7
2022-09-25 09:21:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 7410 updates
2022-09-25 09:21:47 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint96.pt
2022-09-25 09:21:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint96.pt
2022-09-25 09:22:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint96.pt (epoch 96 @ 7410 updates, score 32.47) (writing took 29.87543487548828 seconds)
2022-09-25 09:22:17 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-09-25 09:22:17 | INFO | train | epoch 096 | loss 3.6 | nll_loss 0.495 | ppl 1.41 | wps 5142.6 | ups 0.97 | wpb 5319.8 | bsz 371.8 | num_updates 7410 | lr 0.000734718 | gnorm 0.401 | train_wall 35 | gb_free 10.1 | wall 7273
2022-09-25 09:22:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:22:17 | INFO | fairseq.trainer | begin training epoch 97
2022-09-25 09:22:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:22:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:22:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:22:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:22:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:23:04 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 5.878 | nll_loss 3.333 | ppl 10.08 | bleu 32.83 | wps 2157.2 | wpb 1003.7 | bsz 67.6 | num_updates 7488 | best_bleu 33.7
2022-09-25 09:23:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 7488 updates
2022-09-25 09:23:04 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint97.pt
2022-09-25 09:23:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint97.pt
2022-09-25 09:23:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint97.pt (epoch 97 @ 7488 updates, score 32.83) (writing took 2.0664961971342564 seconds)
2022-09-25 09:23:06 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-09-25 09:23:06 | INFO | train | epoch 097 | loss 3.599 | nll_loss 0.496 | ppl 1.41 | wps 8399.8 | ups 1.58 | wpb 5319.8 | bsz 371.8 | num_updates 7488 | lr 0.000730882 | gnorm 0.428 | train_wall 34 | gb_free 10.1 | wall 7323
2022-09-25 09:23:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:23:07 | INFO | fairseq.trainer | begin training epoch 98
2022-09-25 09:23:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:23:14 | INFO | train_inner | epoch 098:     12 / 78 loss=3.598, nll_loss=0.494, ppl=1.41, wps=5308.2, ups=1, wpb=5330.2, bsz=378.2, num_updates=7500, lr=0.000730297, gnorm=0.426, train_wall=44, gb_free=10.1, wall=7331
2022-09-25 09:23:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:23:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:23:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:23:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:23:56 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 5.842 | nll_loss 3.301 | ppl 9.85 | bleu 33.15 | wps 2105.8 | wpb 1003.7 | bsz 67.6 | num_updates 7566 | best_bleu 33.7
2022-09-25 09:23:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 7566 updates
2022-09-25 09:23:56 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint98.pt
2022-09-25 09:23:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint98.pt
2022-09-25 09:24:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint98.pt (epoch 98 @ 7566 updates, score 33.15) (writing took 16.19207375869155 seconds)
2022-09-25 09:24:12 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-09-25 09:24:12 | INFO | train | epoch 098 | loss 3.597 | nll_loss 0.493 | ppl 1.41 | wps 6291.9 | ups 1.18 | wpb 5319.8 | bsz 371.8 | num_updates 7566 | lr 0.000727104 | gnorm 0.447 | train_wall 35 | gb_free 10.2 | wall 7389
2022-09-25 09:24:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:24:12 | INFO | fairseq.trainer | begin training epoch 99
2022-09-25 09:24:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:24:32 | INFO | train_inner | epoch 099:     34 / 78 loss=3.595, nll_loss=0.492, ppl=1.41, wps=6802.7, ups=1.28, wpb=5303.3, bsz=366.2, num_updates=7600, lr=0.000725476, gnorm=0.431, train_wall=44, gb_free=10.1, wall=7409
2022-09-25 09:24:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:24:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:24:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:24:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:25:06 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 5.821 | nll_loss 3.268 | ppl 9.63 | bleu 34.1 | wps 1915 | wpb 1003.7 | bsz 67.6 | num_updates 7644 | best_bleu 34.1
2022-09-25 09:25:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 7644 updates
2022-09-25 09:25:06 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint99.pt
2022-09-25 09:25:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint99.pt
2022-09-25 09:25:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint99.pt (epoch 99 @ 7644 updates, score 34.1) (writing took 32.98398136347532 seconds)
2022-09-25 09:25:39 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-09-25 09:25:39 | INFO | train | epoch 099 | loss 3.594 | nll_loss 0.492 | ppl 1.41 | wps 4798.7 | ups 0.9 | wpb 5319.8 | bsz 371.8 | num_updates 7644 | lr 0.000723385 | gnorm 0.406 | train_wall 35 | gb_free 10.1 | wall 7475
2022-09-25 09:25:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:25:39 | INFO | fairseq.trainer | begin training epoch 100
2022-09-25 09:25:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:26:10 | INFO | train_inner | epoch 100:     56 / 78 loss=3.591, nll_loss=0.489, ppl=1.4, wps=5405.5, ups=1.02, wpb=5298, bsz=373.7, num_updates=7700, lr=0.00072075, gnorm=0.402, train_wall=45, gb_free=10.1, wall=7507
2022-09-25 09:26:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:26:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:26:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:26:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:26:29 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 5.845 | nll_loss 3.309 | ppl 9.91 | bleu 33.35 | wps 2080.2 | wpb 1003.7 | bsz 67.6 | num_updates 7722 | best_bleu 34.1
2022-09-25 09:26:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 7722 updates
2022-09-25 09:26:29 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint100.pt
2022-09-25 09:26:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint100.pt
2022-09-25 09:26:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint100.pt (epoch 100 @ 7722 updates, score 33.35) (writing took 15.494283210486174 seconds)
2022-09-25 09:26:45 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-09-25 09:26:45 | INFO | train | epoch 100 | loss 3.589 | nll_loss 0.486 | ppl 1.4 | wps 6291.9 | ups 1.18 | wpb 5319.8 | bsz 371.8 | num_updates 7722 | lr 0.000719723 | gnorm 0.393 | train_wall 35 | gb_free 10.1 | wall 7541
2022-09-25 09:26:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:26:45 | INFO | fairseq.trainer | begin training epoch 101
2022-09-25 09:26:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:27:30 | INFO | train_inner | epoch 101:     78 / 78 loss=3.59, nll_loss=0.488, ppl=1.4, wps=6660.6, ups=1.25, wpb=5330.4, bsz=371.3, num_updates=7800, lr=0.000716115, gnorm=0.401, train_wall=45, gb_free=10.2, wall=7587
2022-09-25 09:27:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:27:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:27:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:27:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:27:38 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 5.847 | nll_loss 3.302 | ppl 9.86 | bleu 33.42 | wps 2097.4 | wpb 1003.7 | bsz 67.6 | num_updates 7800 | best_bleu 34.1
2022-09-25 09:27:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 7800 updates
2022-09-25 09:27:38 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint101.pt
2022-09-25 09:27:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint101.pt
2022-09-25 09:27:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint101.pt (epoch 101 @ 7800 updates, score 33.42) (writing took 14.34094052016735 seconds)
2022-09-25 09:27:53 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-09-25 09:27:53 | INFO | train | epoch 101 | loss 3.588 | nll_loss 0.486 | ppl 1.4 | wps 6078.9 | ups 1.14 | wpb 5319.8 | bsz 371.8 | num_updates 7800 | lr 0.000716115 | gnorm 0.399 | train_wall 35 | gb_free 10.2 | wall 7609
2022-09-25 09:27:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:27:53 | INFO | fairseq.trainer | begin training epoch 102
2022-09-25 09:27:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:28:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:28:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:28:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:28:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:28:43 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 5.856 | nll_loss 3.322 | ppl 10 | bleu 32.69 | wps 1822.5 | wpb 1003.7 | bsz 67.6 | num_updates 7878 | best_bleu 34.1
2022-09-25 09:28:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 7878 updates
2022-09-25 09:28:43 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint102.pt
2022-09-25 09:28:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint102.pt
2022-09-25 09:28:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint102.pt (epoch 102 @ 7878 updates, score 32.69) (writing took 14.411392290145159 seconds)
2022-09-25 09:28:58 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-09-25 09:28:58 | INFO | train | epoch 102 | loss 3.588 | nll_loss 0.487 | ppl 1.4 | wps 6433.1 | ups 1.21 | wpb 5319.8 | bsz 371.8 | num_updates 7878 | lr 0.000712561 | gnorm 0.409 | train_wall 35 | gb_free 10.1 | wall 7674
2022-09-25 09:28:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:28:58 | INFO | fairseq.trainer | begin training epoch 103
2022-09-25 09:28:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:29:11 | INFO | train_inner | epoch 103:     22 / 78 loss=3.585, nll_loss=0.483, ppl=1.4, wps=5308.8, ups=0.99, wpb=5350.9, bsz=377.4, num_updates=7900, lr=0.000711568, gnorm=0.407, train_wall=45, gb_free=10.2, wall=7687
2022-09-25 09:29:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:29:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:29:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:29:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:29:46 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 5.836 | nll_loss 3.293 | ppl 9.8 | bleu 32.76 | wps 2153.9 | wpb 1003.7 | bsz 67.6 | num_updates 7956 | best_bleu 34.1
2022-09-25 09:29:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 7956 updates
2022-09-25 09:29:46 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint103.pt
2022-09-25 09:29:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint103.pt
2022-09-25 09:30:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint103.pt (epoch 103 @ 7956 updates, score 32.76) (writing took 22.03424073383212 seconds)
2022-09-25 09:30:09 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-09-25 09:30:09 | INFO | train | epoch 103 | loss 3.587 | nll_loss 0.486 | ppl 1.4 | wps 5834.8 | ups 1.1 | wpb 5319.8 | bsz 371.8 | num_updates 7956 | lr 0.000709059 | gnorm 0.426 | train_wall 34 | gb_free 10.1 | wall 7745
2022-09-25 09:30:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:30:09 | INFO | fairseq.trainer | begin training epoch 104
2022-09-25 09:30:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:30:33 | INFO | train_inner | epoch 104:     44 / 78 loss=3.584, nll_loss=0.481, ppl=1.4, wps=6525.4, ups=1.23, wpb=5311.1, bsz=365.9, num_updates=8000, lr=0.000707107, gnorm=0.408, train_wall=44, gb_free=10.1, wall=7769
2022-09-25 09:30:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:30:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:30:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:30:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:30:58 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 5.86 | nll_loss 3.327 | ppl 10.03 | bleu 32.67 | wps 1959.3 | wpb 1003.7 | bsz 67.6 | num_updates 8034 | best_bleu 34.1
2022-09-25 09:30:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 8034 updates
2022-09-25 09:30:58 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint104.pt
2022-09-25 09:30:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint104.pt
2022-09-25 09:31:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint104.pt (epoch 104 @ 8034 updates, score 32.67) (writing took 15.671592622995377 seconds)
2022-09-25 09:31:13 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-09-25 09:31:13 | INFO | train | epoch 104 | loss 3.577 | nll_loss 0.474 | ppl 1.39 | wps 6403.8 | ups 1.2 | wpb 5319.8 | bsz 371.8 | num_updates 8034 | lr 0.000705609 | gnorm 0.378 | train_wall 34 | gb_free 10.1 | wall 7810
2022-09-25 09:31:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:31:14 | INFO | fairseq.trainer | begin training epoch 105
2022-09-25 09:31:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:31:49 | INFO | train_inner | epoch 105:     66 / 78 loss=3.576, nll_loss=0.476, ppl=1.39, wps=6924.7, ups=1.3, wpb=5315.9, bsz=377.4, num_updates=8100, lr=0.000702728, gnorm=0.48, train_wall=44, gb_free=10.2, wall=7846
2022-09-25 09:31:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:31:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:31:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:31:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:32:03 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 5.863 | nll_loss 3.332 | ppl 10.07 | bleu 32.09 | wps 2133 | wpb 1003.7 | bsz 67.6 | num_updates 8112 | best_bleu 34.1
2022-09-25 09:32:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 8112 updates
2022-09-25 09:32:03 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint105.pt
2022-09-25 09:32:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint105.pt
2022-09-25 09:32:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint105.pt (epoch 105 @ 8112 updates, score 32.09) (writing took 17.960676960647106 seconds)
2022-09-25 09:32:21 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-09-25 09:32:21 | INFO | train | epoch 105 | loss 3.577 | nll_loss 0.476 | ppl 1.39 | wps 6143.1 | ups 1.15 | wpb 5319.8 | bsz 371.8 | num_updates 8112 | lr 0.000702208 | gnorm 0.51 | train_wall 35 | gb_free 10.1 | wall 7877
2022-09-25 09:32:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:32:21 | INFO | fairseq.trainer | begin training epoch 106
2022-09-25 09:32:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:33:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:33:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:33:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:33:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:33:08 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 5.838 | nll_loss 3.308 | ppl 9.9 | bleu 33 | wps 2108.8 | wpb 1003.7 | bsz 67.6 | num_updates 8190 | best_bleu 34.1
2022-09-25 09:33:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 8190 updates
2022-09-25 09:33:08 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint106.pt
2022-09-25 09:33:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint106.pt
2022-09-25 09:33:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint106.pt (epoch 106 @ 8190 updates, score 33.0) (writing took 33.17389329150319 seconds)
2022-09-25 09:33:42 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-09-25 09:33:42 | INFO | train | epoch 106 | loss 3.575 | nll_loss 0.474 | ppl 1.39 | wps 5129.9 | ups 0.96 | wpb 5319.8 | bsz 371.8 | num_updates 8190 | lr 0.000698857 | gnorm 0.383 | train_wall 35 | gb_free 10.2 | wall 7958
2022-09-25 09:33:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:33:42 | INFO | fairseq.trainer | begin training epoch 107
2022-09-25 09:33:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:33:49 | INFO | train_inner | epoch 107:     10 / 78 loss=3.576, nll_loss=0.475, ppl=1.39, wps=4444.8, ups=0.84, wpb=5306.6, bsz=368.5, num_updates=8200, lr=0.00069843, gnorm=0.384, train_wall=44, gb_free=10.2, wall=7965
2022-09-25 09:34:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:34:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:34:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:34:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:34:32 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 5.852 | nll_loss 3.317 | ppl 9.97 | bleu 33.83 | wps 2041.8 | wpb 1003.7 | bsz 67.6 | num_updates 8268 | best_bleu 34.1
2022-09-25 09:34:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 8268 updates
2022-09-25 09:34:32 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint107.pt
2022-09-25 09:34:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint107.pt
2022-09-25 09:34:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint107.pt (epoch 107 @ 8268 updates, score 33.83) (writing took 2.1098856292665005 seconds)
2022-09-25 09:34:34 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-09-25 09:34:34 | INFO | train | epoch 107 | loss 3.574 | nll_loss 0.474 | ppl 1.39 | wps 7900.6 | ups 1.49 | wpb 5319.8 | bsz 371.8 | num_updates 8268 | lr 0.000695552 | gnorm 0.396 | train_wall 35 | gb_free 10.2 | wall 8011
2022-09-25 09:34:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:34:35 | INFO | fairseq.trainer | begin training epoch 108
2022-09-25 09:34:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:34:53 | INFO | train_inner | epoch 108:     32 / 78 loss=3.574, nll_loss=0.473, ppl=1.39, wps=8223.7, ups=1.55, wpb=5310.4, bsz=369, num_updates=8300, lr=0.00069421, gnorm=0.39, train_wall=45, gb_free=10.1, wall=8030
2022-09-25 09:35:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:35:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:35:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:35:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:35:27 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 5.878 | nll_loss 3.36 | ppl 10.26 | bleu 33.09 | wps 1787.9 | wpb 1003.7 | bsz 67.6 | num_updates 8346 | best_bleu 34.1
2022-09-25 09:35:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 8346 updates
2022-09-25 09:35:27 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint108.pt
2022-09-25 09:35:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint108.pt
2022-09-25 09:35:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint108.pt (epoch 108 @ 8346 updates, score 33.09) (writing took 18.365545384585857 seconds)
2022-09-25 09:35:46 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-09-25 09:35:46 | INFO | train | epoch 108 | loss 3.57 | nll_loss 0.47 | ppl 1.39 | wps 5811.9 | ups 1.09 | wpb 5319.8 | bsz 371.8 | num_updates 8346 | lr 0.000692294 | gnorm 0.372 | train_wall 35 | gb_free 10.2 | wall 8082
2022-09-25 09:35:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:35:46 | INFO | fairseq.trainer | begin training epoch 109
2022-09-25 09:35:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:36:15 | INFO | train_inner | epoch 109:     54 / 78 loss=3.569, nll_loss=0.469, ppl=1.38, wps=6568, ups=1.23, wpb=5332.6, bsz=374.6, num_updates=8400, lr=0.000690066, gnorm=0.37, train_wall=44, gb_free=10.1, wall=8111
2022-09-25 09:36:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:36:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:36:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:36:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:36:35 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 5.846 | nll_loss 3.316 | ppl 9.96 | bleu 32.69 | wps 2069.1 | wpb 1003.7 | bsz 67.6 | num_updates 8424 | best_bleu 34.1
2022-09-25 09:36:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 8424 updates
2022-09-25 09:36:35 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint109.pt
2022-09-25 09:36:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint109.pt
2022-09-25 09:36:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint109.pt (epoch 109 @ 8424 updates, score 32.69) (writing took 17.623675659298897 seconds)
2022-09-25 09:36:53 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-09-25 09:36:53 | INFO | train | epoch 109 | loss 3.568 | nll_loss 0.469 | ppl 1.38 | wps 6221.9 | ups 1.17 | wpb 5319.8 | bsz 371.8 | num_updates 8424 | lr 0.000689082 | gnorm 0.373 | train_wall 34 | gb_free 10.2 | wall 8149
2022-09-25 09:36:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:36:53 | INFO | fairseq.trainer | begin training epoch 110
2022-09-25 09:36:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:37:34 | INFO | train_inner | epoch 110:     76 / 78 loss=3.569, nll_loss=0.471, ppl=1.39, wps=6707.1, ups=1.26, wpb=5325.5, bsz=371.4, num_updates=8500, lr=0.000685994, gnorm=0.389, train_wall=45, gb_free=10.2, wall=8190
2022-09-25 09:37:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:37:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:37:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:37:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:37:42 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 5.856 | nll_loss 3.324 | ppl 10.02 | bleu 32.15 | wps 2217.4 | wpb 1003.7 | bsz 67.6 | num_updates 8502 | best_bleu 34.1
2022-09-25 09:37:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 8502 updates
2022-09-25 09:37:42 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint110.pt
2022-09-25 09:37:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint110.pt
2022-09-25 09:38:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint110.pt (epoch 110 @ 8502 updates, score 32.15) (writing took 22.961871549487114 seconds)
2022-09-25 09:38:06 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-09-25 09:38:06 | INFO | train | epoch 110 | loss 3.568 | nll_loss 0.469 | ppl 1.38 | wps 5683.5 | ups 1.07 | wpb 5319.8 | bsz 371.8 | num_updates 8502 | lr 0.000685914 | gnorm 0.389 | train_wall 35 | gb_free 10.2 | wall 8222
2022-09-25 09:38:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:38:06 | INFO | fairseq.trainer | begin training epoch 111
2022-09-25 09:38:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:38:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:38:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:38:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:38:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:38:54 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 5.844 | nll_loss 3.318 | ppl 9.97 | bleu 32.21 | wps 1989 | wpb 1003.7 | bsz 67.6 | num_updates 8580 | best_bleu 34.1
2022-09-25 09:38:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 8580 updates
2022-09-25 09:38:54 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint111.pt
2022-09-25 09:38:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint111.pt
2022-09-25 09:39:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint111.pt (epoch 111 @ 8580 updates, score 32.21) (writing took 15.480072990059853 seconds)
2022-09-25 09:39:10 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-09-25 09:39:10 | INFO | train | epoch 111 | loss 3.564 | nll_loss 0.464 | ppl 1.38 | wps 6482.6 | ups 1.22 | wpb 5319.8 | bsz 371.8 | num_updates 8580 | lr 0.000682789 | gnorm 0.381 | train_wall 35 | gb_free 10.1 | wall 8286
2022-09-25 09:39:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:39:10 | INFO | fairseq.trainer | begin training epoch 112
2022-09-25 09:39:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:39:22 | INFO | train_inner | epoch 112:     20 / 78 loss=3.561, nll_loss=0.461, ppl=1.38, wps=4917.9, ups=0.93, wpb=5314.5, bsz=375.7, num_updates=8600, lr=0.000681994, gnorm=0.376, train_wall=45, gb_free=10.1, wall=8298
2022-09-25 09:39:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:39:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:39:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:39:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:40:00 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 5.86 | nll_loss 3.329 | ppl 10.05 | bleu 32.76 | wps 2202.7 | wpb 1003.7 | bsz 67.6 | num_updates 8658 | best_bleu 34.1
2022-09-25 09:40:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 8658 updates
2022-09-25 09:40:00 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint112.pt
2022-09-25 09:40:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint112.pt
2022-09-25 09:40:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint112.pt (epoch 112 @ 8658 updates, score 32.76) (writing took 24.033135324716568 seconds)
2022-09-25 09:40:24 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-09-25 09:40:24 | INFO | train | epoch 112 | loss 3.562 | nll_loss 0.463 | ppl 1.38 | wps 5549.6 | ups 1.04 | wpb 5319.8 | bsz 371.8 | num_updates 8658 | lr 0.000679706 | gnorm 0.381 | train_wall 35 | gb_free 10.1 | wall 8361
2022-09-25 09:40:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:40:25 | INFO | fairseq.trainer | begin training epoch 113
2022-09-25 09:40:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:40:47 | INFO | train_inner | epoch 113:     42 / 78 loss=3.563, nll_loss=0.465, ppl=1.38, wps=6293.1, ups=1.18, wpb=5352.5, bsz=369.5, num_updates=8700, lr=0.000678064, gnorm=0.537, train_wall=45, gb_free=10.2, wall=8383
2022-09-25 09:41:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:41:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:41:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:41:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:41:13 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 5.849 | nll_loss 3.321 | ppl 9.99 | bleu 32.81 | wps 2078.7 | wpb 1003.7 | bsz 67.6 | num_updates 8736 | best_bleu 34.1
2022-09-25 09:41:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 8736 updates
2022-09-25 09:41:13 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint113.pt
2022-09-25 09:41:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint113.pt
2022-09-25 09:41:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint113.pt (epoch 113 @ 8736 updates, score 32.81) (writing took 26.69003427773714 seconds)
2022-09-25 09:41:40 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-09-25 09:41:40 | INFO | train | epoch 113 | loss 3.563 | nll_loss 0.465 | ppl 1.38 | wps 5450.8 | ups 1.02 | wpb 5319.8 | bsz 371.8 | num_updates 8736 | lr 0.000676665 | gnorm 0.6 | train_wall 35 | gb_free 10.1 | wall 8437
2022-09-25 09:41:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:41:41 | INFO | fairseq.trainer | begin training epoch 114
2022-09-25 09:41:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:42:21 | INFO | train_inner | epoch 114:     64 / 78 loss=3.563, nll_loss=0.466, ppl=1.38, wps=5599.6, ups=1.06, wpb=5286.9, bsz=366.7, num_updates=8800, lr=0.0006742, gnorm=0.401, train_wall=45, gb_free=10, wall=8478
2022-09-25 09:42:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:42:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:42:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:42:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:42:37 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 5.857 | nll_loss 3.329 | ppl 10.05 | bleu 32.53 | wps 2179.3 | wpb 1003.7 | bsz 67.6 | num_updates 8814 | best_bleu 34.1
2022-09-25 09:42:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 8814 updates
2022-09-25 09:42:37 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint114.pt
2022-09-25 09:42:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint114.pt
2022-09-25 09:43:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint114.pt (epoch 114 @ 8814 updates, score 32.53) (writing took 25.165535047650337 seconds)
2022-09-25 09:43:03 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-09-25 09:43:03 | INFO | train | epoch 114 | loss 3.56 | nll_loss 0.462 | ppl 1.38 | wps 5033.3 | ups 0.95 | wpb 5319.8 | bsz 371.8 | num_updates 8814 | lr 0.000673664 | gnorm 0.386 | train_wall 35 | gb_free 10.2 | wall 8519
2022-09-25 09:43:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:43:03 | INFO | fairseq.trainer | begin training epoch 115
2022-09-25 09:43:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:43:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:43:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:43:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:43:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:43:52 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 5.844 | nll_loss 3.323 | ppl 10.01 | bleu 33.85 | wps 2121.5 | wpb 1003.7 | bsz 67.6 | num_updates 8892 | best_bleu 34.1
2022-09-25 09:43:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 8892 updates
2022-09-25 09:43:52 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint115.pt
2022-09-25 09:43:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint115.pt
2022-09-25 09:44:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint115.pt (epoch 115 @ 8892 updates, score 33.85) (writing took 15.767025347799063 seconds)
2022-09-25 09:44:08 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-09-25 09:44:08 | INFO | train | epoch 115 | loss 3.556 | nll_loss 0.458 | ppl 1.37 | wps 6335.4 | ups 1.19 | wpb 5319.8 | bsz 371.8 | num_updates 8892 | lr 0.000670703 | gnorm 0.36 | train_wall 35 | gb_free 10.1 | wall 8585
2022-09-25 09:44:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:44:09 | INFO | fairseq.trainer | begin training epoch 116
2022-09-25 09:44:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:44:14 | INFO | train_inner | epoch 116:      8 / 78 loss=3.556, nll_loss=0.458, ppl=1.37, wps=4731.7, ups=0.89, wpb=5314, bsz=373.4, num_updates=8900, lr=0.000670402, gnorm=0.366, train_wall=44, gb_free=10.1, wall=8590
2022-09-25 09:44:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:44:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:44:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:44:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:44:59 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 5.833 | nll_loss 3.303 | ppl 9.87 | bleu 33.15 | wps 2130 | wpb 1003.7 | bsz 67.6 | num_updates 8970 | best_bleu 34.1
2022-09-25 09:44:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 8970 updates
2022-09-25 09:44:59 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint116.pt
2022-09-25 09:45:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint116.pt
2022-09-25 09:45:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint116.pt (epoch 116 @ 8970 updates, score 33.15) (writing took 17.50636613368988 seconds)
2022-09-25 09:45:17 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-09-25 09:45:17 | INFO | train | epoch 116 | loss 3.555 | nll_loss 0.458 | ppl 1.37 | wps 6085 | ups 1.14 | wpb 5319.8 | bsz 371.8 | num_updates 8970 | lr 0.000667781 | gnorm 0.382 | train_wall 35 | gb_free 10.2 | wall 8653
2022-09-25 09:45:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:45:17 | INFO | fairseq.trainer | begin training epoch 117
2022-09-25 09:45:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:45:34 | INFO | train_inner | epoch 117:     30 / 78 loss=3.553, nll_loss=0.456, ppl=1.37, wps=6617.3, ups=1.24, wpb=5321.7, bsz=374.5, num_updates=9000, lr=0.000666667, gnorm=0.377, train_wall=45, gb_free=10.1, wall=8671
2022-09-25 09:45:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:46:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:46:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:46:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:46:07 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 5.861 | nll_loss 3.345 | ppl 10.16 | bleu 33.5 | wps 2163.3 | wpb 1003.7 | bsz 67.6 | num_updates 9048 | best_bleu 34.1
2022-09-25 09:46:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 9048 updates
2022-09-25 09:46:07 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint117.pt
2022-09-25 09:46:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint117.pt
2022-09-25 09:46:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint117.pt (epoch 117 @ 9048 updates, score 33.5) (writing took 17.778874777257442 seconds)
2022-09-25 09:46:25 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-09-25 09:46:25 | INFO | train | epoch 117 | loss 3.553 | nll_loss 0.456 | ppl 1.37 | wps 6080 | ups 1.14 | wpb 5319.8 | bsz 371.8 | num_updates 9048 | lr 0.000664896 | gnorm 0.364 | train_wall 35 | gb_free 10.1 | wall 8721
2022-09-25 09:46:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:46:25 | INFO | fairseq.trainer | begin training epoch 118
2022-09-25 09:46:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:46:53 | INFO | train_inner | epoch 118:     52 / 78 loss=3.555, nll_loss=0.458, ppl=1.37, wps=6738.1, ups=1.26, wpb=5330.6, bsz=367.8, num_updates=9100, lr=0.000662994, gnorm=0.416, train_wall=45, gb_free=10.1, wall=8750
2022-09-25 09:47:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:47:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:47:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:47:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:47:16 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 5.882 | nll_loss 3.37 | ppl 10.34 | bleu 31.85 | wps 1875.8 | wpb 1003.7 | bsz 67.6 | num_updates 9126 | best_bleu 34.1
2022-09-25 09:47:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 9126 updates
2022-09-25 09:47:16 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint118.pt
2022-09-25 09:47:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint118.pt
2022-09-25 09:47:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint118.pt (epoch 118 @ 9126 updates, score 31.85) (writing took 13.062349546700716 seconds)
2022-09-25 09:47:29 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-09-25 09:47:29 | INFO | train | epoch 118 | loss 3.554 | nll_loss 0.458 | ppl 1.37 | wps 6482.3 | ups 1.22 | wpb 5319.8 | bsz 371.8 | num_updates 9126 | lr 0.000662048 | gnorm 0.435 | train_wall 35 | gb_free 10.1 | wall 8785
2022-09-25 09:47:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:47:29 | INFO | fairseq.trainer | begin training epoch 119
2022-09-25 09:47:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:48:09 | INFO | train_inner | epoch 119:     74 / 78 loss=3.551, nll_loss=0.455, ppl=1.37, wps=7035.3, ups=1.32, wpb=5318.3, bsz=373.9, num_updates=9200, lr=0.00065938, gnorm=0.377, train_wall=45, gb_free=10.2, wall=8825
2022-09-25 09:48:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:48:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:48:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:48:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:48:19 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 5.864 | nll_loss 3.353 | ppl 10.22 | bleu 33.17 | wps 2147.7 | wpb 1003.7 | bsz 67.6 | num_updates 9204 | best_bleu 34.1
2022-09-25 09:48:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 9204 updates
2022-09-25 09:48:19 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint119.pt
2022-09-25 09:48:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint119.pt
2022-09-25 09:48:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint119.pt (epoch 119 @ 9204 updates, score 33.17) (writing took 13.877331629395485 seconds)
2022-09-25 09:48:32 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-09-25 09:48:32 | INFO | train | epoch 119 | loss 3.549 | nll_loss 0.452 | ppl 1.37 | wps 6520.4 | ups 1.23 | wpb 5319.8 | bsz 371.8 | num_updates 9204 | lr 0.000659237 | gnorm 0.373 | train_wall 34 | gb_free 10.1 | wall 8849
2022-09-25 09:48:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:48:33 | INFO | fairseq.trainer | begin training epoch 120
2022-09-25 09:48:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:49:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-09-25 09:49:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-09-25 09:49:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-09-25 09:49:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-09-25 09:49:20 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 5.883 | nll_loss 3.37 | ppl 10.34 | bleu 32.97 | wps 2226.5 | wpb 1003.7 | bsz 67.6 | num_updates 9282 | best_bleu 34.1
2022-09-25 09:49:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 9282 updates
2022-09-25 09:49:20 | INFO | fairseq.trainer | Saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint120.pt
2022-09-25 09:49:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data1/home/turghun/project/acmmt/examples/rgmmt/results/en-de/mmt/checkpoint120.pt
2022-09-25 09:49:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint results/en-de/mmt/checkpoint120.pt (epoch 120 @ 9282 updates, score 32.97) (writing took 15.289658479392529 seconds)
2022-09-25 09:49:35 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-09-25 09:49:35 | INFO | train | epoch 120 | loss 3.548 | nll_loss 0.451 | ppl 1.37 | wps 6619.1 | ups 1.24 | wpb 5319.8 | bsz 371.8 | num_updates 9282 | lr 0.000656461 | gnorm 0.369 | train_wall 37 | gb_free 10.2 | wall 8911
2022-09-25 09:49:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 78
2022-09-25 09:49:35 | INFO | fairseq.trainer | begin training epoch 121
2022-09-25 09:49:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-09-25 09:49:46 | INFO | train_inner | epoch 121:     18 / 78 loss=3.548, nll_loss=0.451, ppl=1.37, wps=5443.1, ups=1.03, wpb=5285.3, bsz=370.7, num_updates=9300, lr=0.000655826, gnorm=0.37, train_wall=47, gb_free=10.1, wall=8922
train.sh: line 24:  2138 Killed                  fairseq-train ${ACMMT_ROOT}/data_bin/$SRC-$TGT --user-dir ${RGMMT_ROOT} --criterion label_smoothed_cross_entropy --task rgmmt_translation_task --arch rgmmt_model --optimizer adam --adam-betas 0.9,0.98 --clip-norm 0 --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 4000 --reset-optimizer --lr 0.001 --weight-decay 0.0001 --label-smoothing 0.2 --dropout 0.3 --max-tokens 1536 --no-progress-bar --log-interval 100 --stop-min-lr 1e-09 --keep-last-epochs 12 --update-freq 4 --eval-bleu --maximize-best-checkpoint-metric --save-dir ${SAVE_DIR} --share-decoder-input-output-embed --source-lang ${SRC} --target-lang ${TGT} --tensorboard-logdir ${SAVE_DIR}/bl_log1 --log-format simple --img-grid-prefix ${IMG_DATA_PREFIX}/resnet101-dlmmt --img-region-prefix ${IMG_DATA_PREFIX}/faster-dlmmt
